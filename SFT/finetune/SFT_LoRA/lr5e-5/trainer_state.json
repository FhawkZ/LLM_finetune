{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0751818916734033,
  "eval_steps": 500,
  "global_step": 5320,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0010105092966855296,
      "grad_norm": 22.254825592041016,
      "learning_rate": 4.9999991041654185e-05,
      "loss": 6.3574,
      "num_input_tokens_seen": 62192,
      "step": 5,
      "train_runtime": 64.6976,
      "train_tokens_per_second": 961.272
    },
    {
      "epoch": 0.002021018593371059,
      "grad_norm": 7.435590744018555,
      "learning_rate": 4.999995464838527e-05,
      "loss": 2.425,
      "num_input_tokens_seen": 124880,
      "step": 10,
      "train_runtime": 128.9071,
      "train_tokens_per_second": 968.76
    },
    {
      "epoch": 0.0030315278900565883,
      "grad_norm": 4.335450649261475,
      "learning_rate": 4.999989026033739e-05,
      "loss": 1.2831,
      "num_input_tokens_seen": 188112,
      "step": 15,
      "train_runtime": 193.5178,
      "train_tokens_per_second": 972.066
    },
    {
      "epoch": 0.004042037186742118,
      "grad_norm": 4.620641708374023,
      "learning_rate": 4.999979787758263e-05,
      "loss": 0.8713,
      "num_input_tokens_seen": 250560,
      "step": 20,
      "train_runtime": 257.875,
      "train_tokens_per_second": 971.634
    },
    {
      "epoch": 0.0050525464834276475,
      "grad_norm": 4.04233455657959,
      "learning_rate": 4.999967750022444e-05,
      "loss": 0.8078,
      "num_input_tokens_seen": 313680,
      "step": 25,
      "train_runtime": 322.5788,
      "train_tokens_per_second": 972.414
    },
    {
      "epoch": 0.006063055780113177,
      "grad_norm": 2.8613340854644775,
      "learning_rate": 4.9999529128397625e-05,
      "loss": 0.6283,
      "num_input_tokens_seen": 376048,
      "step": 30,
      "train_runtime": 386.4847,
      "train_tokens_per_second": 972.996
    },
    {
      "epoch": 0.007073565076798707,
      "grad_norm": 3.1119778156280518,
      "learning_rate": 4.999935276226834e-05,
      "loss": 0.7267,
      "num_input_tokens_seen": 438016,
      "step": 35,
      "train_runtime": 450.1068,
      "train_tokens_per_second": 973.138
    },
    {
      "epoch": 0.008084074373484237,
      "grad_norm": 1.5086331367492676,
      "learning_rate": 4.999914840203405e-05,
      "loss": 0.5722,
      "num_input_tokens_seen": 500992,
      "step": 40,
      "train_runtime": 514.5909,
      "train_tokens_per_second": 973.573
    },
    {
      "epoch": 0.009094583670169765,
      "grad_norm": 2.7850146293640137,
      "learning_rate": 4.9998916047923624e-05,
      "loss": 0.6271,
      "num_input_tokens_seen": 563120,
      "step": 45,
      "train_runtime": 578.3394,
      "train_tokens_per_second": 973.684
    },
    {
      "epoch": 0.010105092966855295,
      "grad_norm": 5.10507345199585,
      "learning_rate": 4.9998655700197234e-05,
      "loss": 0.7295,
      "num_input_tokens_seen": 626336,
      "step": 50,
      "train_runtime": 643.3002,
      "train_tokens_per_second": 973.629
    },
    {
      "epoch": 0.011115602263540825,
      "grad_norm": 2.5218722820281982,
      "learning_rate": 4.999836735914642e-05,
      "loss": 0.7394,
      "num_input_tokens_seen": 689616,
      "step": 55,
      "train_runtime": 708.0237,
      "train_tokens_per_second": 974.001
    },
    {
      "epoch": 0.012126111560226353,
      "grad_norm": 2.957477331161499,
      "learning_rate": 4.9998051025094074e-05,
      "loss": 0.7971,
      "num_input_tokens_seen": 752400,
      "step": 60,
      "train_runtime": 772.4865,
      "train_tokens_per_second": 973.998
    },
    {
      "epoch": 0.013136620856911883,
      "grad_norm": 2.0186736583709717,
      "learning_rate": 4.999770669839441e-05,
      "loss": 0.713,
      "num_input_tokens_seen": 815760,
      "step": 65,
      "train_runtime": 837.1953,
      "train_tokens_per_second": 974.396
    },
    {
      "epoch": 0.014147130153597413,
      "grad_norm": 2.84124755859375,
      "learning_rate": 4.9997334379433006e-05,
      "loss": 0.6217,
      "num_input_tokens_seen": 877968,
      "step": 70,
      "train_runtime": 901.0686,
      "train_tokens_per_second": 974.363
    },
    {
      "epoch": 0.015157639450282943,
      "grad_norm": 2.0009140968322754,
      "learning_rate": 4.999693406862679e-05,
      "loss": 0.69,
      "num_input_tokens_seen": 941408,
      "step": 75,
      "train_runtime": 966.064,
      "train_tokens_per_second": 974.478
    },
    {
      "epoch": 0.016168148746968473,
      "grad_norm": 1.8896360397338867,
      "learning_rate": 4.9996505766424015e-05,
      "loss": 0.8035,
      "num_input_tokens_seen": 1003664,
      "step": 80,
      "train_runtime": 1029.84,
      "train_tokens_per_second": 974.582
    },
    {
      "epoch": 0.017178658043654,
      "grad_norm": 1.631388545036316,
      "learning_rate": 4.9996049473304306e-05,
      "loss": 0.5863,
      "num_input_tokens_seen": 1066112,
      "step": 85,
      "train_runtime": 1093.863,
      "train_tokens_per_second": 974.63
    },
    {
      "epoch": 0.01818916734033953,
      "grad_norm": 2.151624917984009,
      "learning_rate": 4.99955651897786e-05,
      "loss": 0.6009,
      "num_input_tokens_seen": 1129520,
      "step": 90,
      "train_runtime": 1158.7109,
      "train_tokens_per_second": 974.807
    },
    {
      "epoch": 0.01919967663702506,
      "grad_norm": 1.6947474479675293,
      "learning_rate": 4.999505291638922e-05,
      "loss": 0.5479,
      "num_input_tokens_seen": 1192496,
      "step": 95,
      "train_runtime": 1223.1651,
      "train_tokens_per_second": 974.926
    },
    {
      "epoch": 0.02021018593371059,
      "grad_norm": 2.609206199645996,
      "learning_rate": 4.9994512653709776e-05,
      "loss": 0.5294,
      "num_input_tokens_seen": 1255568,
      "step": 100,
      "train_runtime": 1287.9944,
      "train_tokens_per_second": 974.824
    },
    {
      "epoch": 0.02122069523039612,
      "grad_norm": 1.7728183269500732,
      "learning_rate": 4.999394440234527e-05,
      "loss": 0.6036,
      "num_input_tokens_seen": 1318256,
      "step": 105,
      "train_runtime": 1352.9811,
      "train_tokens_per_second": 974.334
    },
    {
      "epoch": 0.02223120452708165,
      "grad_norm": 1.701831340789795,
      "learning_rate": 4.999334816293202e-05,
      "loss": 0.5934,
      "num_input_tokens_seen": 1380864,
      "step": 110,
      "train_runtime": 1417.0894,
      "train_tokens_per_second": 974.437
    },
    {
      "epoch": 0.02324171382376718,
      "grad_norm": 1.9831647872924805,
      "learning_rate": 4.9992723936137686e-05,
      "loss": 0.5414,
      "num_input_tokens_seen": 1443840,
      "step": 115,
      "train_runtime": 1481.4141,
      "train_tokens_per_second": 974.636
    },
    {
      "epoch": 0.024252223120452707,
      "grad_norm": 1.8783503770828247,
      "learning_rate": 4.999207172266128e-05,
      "loss": 0.6259,
      "num_input_tokens_seen": 1506384,
      "step": 120,
      "train_runtime": 1545.5877,
      "train_tokens_per_second": 974.635
    },
    {
      "epoch": 0.025262732417138237,
      "grad_norm": 2.140345335006714,
      "learning_rate": 4.999139152323314e-05,
      "loss": 0.5068,
      "num_input_tokens_seen": 1568640,
      "step": 125,
      "train_runtime": 1609.7498,
      "train_tokens_per_second": 974.462
    },
    {
      "epoch": 0.026273241713823767,
      "grad_norm": 4.399719715118408,
      "learning_rate": 4.999068333861496e-05,
      "loss": 0.7494,
      "num_input_tokens_seen": 1629984,
      "step": 130,
      "train_runtime": 1672.7027,
      "train_tokens_per_second": 974.461
    },
    {
      "epoch": 0.027283751010509297,
      "grad_norm": 1.7061625719070435,
      "learning_rate": 4.998994716959976e-05,
      "loss": 0.642,
      "num_input_tokens_seen": 1692880,
      "step": 135,
      "train_runtime": 1737.0253,
      "train_tokens_per_second": 974.586
    },
    {
      "epoch": 0.028294260307194827,
      "grad_norm": 2.1925299167633057,
      "learning_rate": 4.998918301701188e-05,
      "loss": 0.5296,
      "num_input_tokens_seen": 1754960,
      "step": 140,
      "train_runtime": 1800.8066,
      "train_tokens_per_second": 974.541
    },
    {
      "epoch": 0.029304769603880357,
      "grad_norm": 1.8548287153244019,
      "learning_rate": 4.9988390881707024e-05,
      "loss": 0.516,
      "num_input_tokens_seen": 1818576,
      "step": 145,
      "train_runtime": 1865.8819,
      "train_tokens_per_second": 974.647
    },
    {
      "epoch": 0.030315278900565887,
      "grad_norm": 2.6151273250579834,
      "learning_rate": 4.998757076457222e-05,
      "loss": 0.6169,
      "num_input_tokens_seen": 1880640,
      "step": 150,
      "train_runtime": 1929.5397,
      "train_tokens_per_second": 974.657
    },
    {
      "epoch": 0.03132578819725142,
      "grad_norm": 2.3244004249572754,
      "learning_rate": 4.998672266652583e-05,
      "loss": 0.7399,
      "num_input_tokens_seen": 1942960,
      "step": 155,
      "train_runtime": 1993.6603,
      "train_tokens_per_second": 974.569
    },
    {
      "epoch": 0.03233629749393695,
      "grad_norm": 1.6788922548294067,
      "learning_rate": 4.998584658851755e-05,
      "loss": 0.4702,
      "num_input_tokens_seen": 2005200,
      "step": 160,
      "train_runtime": 2057.4852,
      "train_tokens_per_second": 974.588
    },
    {
      "epoch": 0.03334680679062248,
      "grad_norm": 2.08079195022583,
      "learning_rate": 4.998494253152839e-05,
      "loss": 0.5427,
      "num_input_tokens_seen": 2068416,
      "step": 165,
      "train_runtime": 2122.2448,
      "train_tokens_per_second": 974.636
    },
    {
      "epoch": 0.034357316087308,
      "grad_norm": 1.5848033428192139,
      "learning_rate": 4.998401049657073e-05,
      "loss": 0.8503,
      "num_input_tokens_seen": 2131328,
      "step": 170,
      "train_runtime": 2186.6207,
      "train_tokens_per_second": 974.713
    },
    {
      "epoch": 0.03536782538399353,
      "grad_norm": 1.9603215456008911,
      "learning_rate": 4.998305048468825e-05,
      "loss": 0.5302,
      "num_input_tokens_seen": 2193616,
      "step": 175,
      "train_runtime": 2250.5988,
      "train_tokens_per_second": 974.681
    },
    {
      "epoch": 0.03637833468067906,
      "grad_norm": 2.4955155849456787,
      "learning_rate": 4.998206249695596e-05,
      "loss": 0.5587,
      "num_input_tokens_seen": 2257216,
      "step": 180,
      "train_runtime": 2315.7875,
      "train_tokens_per_second": 974.708
    },
    {
      "epoch": 0.03738884397736459,
      "grad_norm": 1.9667134284973145,
      "learning_rate": 4.998104653448019e-05,
      "loss": 0.516,
      "num_input_tokens_seen": 2320544,
      "step": 185,
      "train_runtime": 2380.6385,
      "train_tokens_per_second": 974.757
    },
    {
      "epoch": 0.03839935327405012,
      "grad_norm": 2.8803863525390625,
      "learning_rate": 4.9980002598398634e-05,
      "loss": 0.5756,
      "num_input_tokens_seen": 2382608,
      "step": 190,
      "train_runtime": 2444.3256,
      "train_tokens_per_second": 974.751
    },
    {
      "epoch": 0.03940986257073565,
      "grad_norm": 2.70910382270813,
      "learning_rate": 4.997893068988027e-05,
      "loss": 0.6142,
      "num_input_tokens_seen": 2445056,
      "step": 195,
      "train_runtime": 2508.3246,
      "train_tokens_per_second": 974.777
    },
    {
      "epoch": 0.04042037186742118,
      "grad_norm": 1.6667606830596924,
      "learning_rate": 4.9977830810125414e-05,
      "loss": 0.4879,
      "num_input_tokens_seen": 2507808,
      "step": 200,
      "train_runtime": 2572.4966,
      "train_tokens_per_second": 974.854
    },
    {
      "epoch": 0.04143088116410671,
      "grad_norm": 1.8384541273117065,
      "learning_rate": 4.99767029603657e-05,
      "loss": 0.5648,
      "num_input_tokens_seen": 2571520,
      "step": 205,
      "train_runtime": 2638.4708,
      "train_tokens_per_second": 974.625
    },
    {
      "epoch": 0.04244139046079224,
      "grad_norm": 2.8152060508728027,
      "learning_rate": 4.99755471418641e-05,
      "loss": 0.6527,
      "num_input_tokens_seen": 2633552,
      "step": 210,
      "train_runtime": 2702.2384,
      "train_tokens_per_second": 974.582
    },
    {
      "epoch": 0.04345189975747777,
      "grad_norm": 2.386749267578125,
      "learning_rate": 4.9974363355914875e-05,
      "loss": 0.5608,
      "num_input_tokens_seen": 2696704,
      "step": 215,
      "train_runtime": 2767.091,
      "train_tokens_per_second": 974.563
    },
    {
      "epoch": 0.0444624090541633,
      "grad_norm": 1.7657840251922607,
      "learning_rate": 4.997315160384364e-05,
      "loss": 0.5701,
      "num_input_tokens_seen": 2759712,
      "step": 220,
      "train_runtime": 2831.5709,
      "train_tokens_per_second": 974.622
    },
    {
      "epoch": 0.04547291835084883,
      "grad_norm": 2.374821901321411,
      "learning_rate": 4.997191188700728e-05,
      "loss": 0.6124,
      "num_input_tokens_seen": 2822080,
      "step": 225,
      "train_runtime": 2895.4581,
      "train_tokens_per_second": 974.658
    },
    {
      "epoch": 0.04648342764753436,
      "grad_norm": 2.258249282836914,
      "learning_rate": 4.997064420679405e-05,
      "loss": 0.7043,
      "num_input_tokens_seen": 2884624,
      "step": 230,
      "train_runtime": 2959.5706,
      "train_tokens_per_second": 974.677
    },
    {
      "epoch": 0.04749393694421989,
      "grad_norm": 2.3448290824890137,
      "learning_rate": 4.996934856462347e-05,
      "loss": 0.495,
      "num_input_tokens_seen": 2948224,
      "step": 235,
      "train_runtime": 3024.5047,
      "train_tokens_per_second": 974.779
    },
    {
      "epoch": 0.04850444624090541,
      "grad_norm": 4.658020496368408,
      "learning_rate": 4.9968024961946396e-05,
      "loss": 0.53,
      "num_input_tokens_seen": 3010896,
      "step": 240,
      "train_runtime": 3088.7799,
      "train_tokens_per_second": 974.785
    },
    {
      "epoch": 0.04951495553759094,
      "grad_norm": 1.5823521614074707,
      "learning_rate": 4.996667340024499e-05,
      "loss": 0.4652,
      "num_input_tokens_seen": 3073568,
      "step": 245,
      "train_runtime": 3152.8439,
      "train_tokens_per_second": 974.856
    },
    {
      "epoch": 0.05052546483427647,
      "grad_norm": 1.9272745847702026,
      "learning_rate": 4.9965293881032736e-05,
      "loss": 0.5296,
      "num_input_tokens_seen": 3135856,
      "step": 250,
      "train_runtime": 3216.6048,
      "train_tokens_per_second": 974.896
    },
    {
      "epoch": 0.051535974130962,
      "grad_norm": 1.443247675895691,
      "learning_rate": 4.9963886405854386e-05,
      "loss": 0.7164,
      "num_input_tokens_seen": 3198352,
      "step": 255,
      "train_runtime": 3280.5895,
      "train_tokens_per_second": 974.932
    },
    {
      "epoch": 0.05254648342764753,
      "grad_norm": 1.6979275941848755,
      "learning_rate": 4.996245097628603e-05,
      "loss": 0.5224,
      "num_input_tokens_seen": 3260944,
      "step": 260,
      "train_runtime": 3345.004,
      "train_tokens_per_second": 974.87
    },
    {
      "epoch": 0.05355699272433306,
      "grad_norm": 1.8198368549346924,
      "learning_rate": 4.996098759393506e-05,
      "loss": 0.4562,
      "num_input_tokens_seen": 3323472,
      "step": 265,
      "train_runtime": 3409.0482,
      "train_tokens_per_second": 974.897
    },
    {
      "epoch": 0.054567502021018593,
      "grad_norm": 2.2077507972717285,
      "learning_rate": 4.995949626044016e-05,
      "loss": 0.6413,
      "num_input_tokens_seen": 3385536,
      "step": 270,
      "train_runtime": 3472.663,
      "train_tokens_per_second": 974.911
    },
    {
      "epoch": 0.055578011317704124,
      "grad_norm": 3.2368054389953613,
      "learning_rate": 4.9957976977471304e-05,
      "loss": 0.5532,
      "num_input_tokens_seen": 3448544,
      "step": 275,
      "train_runtime": 3537.0722,
      "train_tokens_per_second": 974.971
    },
    {
      "epoch": 0.056588520614389654,
      "grad_norm": 2.41013503074646,
      "learning_rate": 4.995642974672978e-05,
      "loss": 0.5885,
      "num_input_tokens_seen": 3510736,
      "step": 280,
      "train_runtime": 3600.7276,
      "train_tokens_per_second": 975.007
    },
    {
      "epoch": 0.057599029911075184,
      "grad_norm": 1.6061521768569946,
      "learning_rate": 4.9954854569948175e-05,
      "loss": 0.6056,
      "num_input_tokens_seen": 3573408,
      "step": 285,
      "train_runtime": 3665.0766,
      "train_tokens_per_second": 974.989
    },
    {
      "epoch": 0.058609539207760714,
      "grad_norm": 2.4567618370056152,
      "learning_rate": 4.995325144889035e-05,
      "loss": 0.6405,
      "num_input_tokens_seen": 3636704,
      "step": 290,
      "train_runtime": 3729.6969,
      "train_tokens_per_second": 975.067
    },
    {
      "epoch": 0.059620048504446244,
      "grad_norm": 2.9584529399871826,
      "learning_rate": 4.995162038535148e-05,
      "loss": 0.5401,
      "num_input_tokens_seen": 3698928,
      "step": 295,
      "train_runtime": 3793.4547,
      "train_tokens_per_second": 975.082
    },
    {
      "epoch": 0.060630557801131774,
      "grad_norm": 2.2311184406280518,
      "learning_rate": 4.9949961381158004e-05,
      "loss": 0.6403,
      "num_input_tokens_seen": 3761792,
      "step": 300,
      "train_runtime": 3857.8938,
      "train_tokens_per_second": 975.09
    },
    {
      "epoch": 0.0616410670978173,
      "grad_norm": 1.7463477849960327,
      "learning_rate": 4.994827443816768e-05,
      "loss": 0.5131,
      "num_input_tokens_seen": 3823280,
      "step": 305,
      "train_runtime": 3921.8849,
      "train_tokens_per_second": 974.858
    },
    {
      "epoch": 0.06265157639450283,
      "grad_norm": 2.5305986404418945,
      "learning_rate": 4.994655955826951e-05,
      "loss": 0.5537,
      "num_input_tokens_seen": 3885344,
      "step": 310,
      "train_runtime": 3985.7195,
      "train_tokens_per_second": 974.816
    },
    {
      "epoch": 0.06366208569118836,
      "grad_norm": 1.851933479309082,
      "learning_rate": 4.9944816743383837e-05,
      "loss": 0.6277,
      "num_input_tokens_seen": 3948368,
      "step": 315,
      "train_runtime": 4050.2016,
      "train_tokens_per_second": 974.857
    },
    {
      "epoch": 0.0646725949878739,
      "grad_norm": 1.993126630783081,
      "learning_rate": 4.994304599546224e-05,
      "loss": 0.5596,
      "num_input_tokens_seen": 4010480,
      "step": 320,
      "train_runtime": 4113.8505,
      "train_tokens_per_second": 974.873
    },
    {
      "epoch": 0.06568310428455942,
      "grad_norm": 3.047729253768921,
      "learning_rate": 4.994124731648758e-05,
      "loss": 0.5751,
      "num_input_tokens_seen": 4072464,
      "step": 325,
      "train_runtime": 4177.5551,
      "train_tokens_per_second": 974.844
    },
    {
      "epoch": 0.06669361358124495,
      "grad_norm": 3.20536732673645,
      "learning_rate": 4.9939420708474023e-05,
      "loss": 0.723,
      "num_input_tokens_seen": 4135472,
      "step": 330,
      "train_runtime": 4242.3495,
      "train_tokens_per_second": 974.807
    },
    {
      "epoch": 0.06770412287793048,
      "grad_norm": 1.7594974040985107,
      "learning_rate": 4.9937566173466975e-05,
      "loss": 0.5975,
      "num_input_tokens_seen": 4198016,
      "step": 335,
      "train_runtime": 4306.5762,
      "train_tokens_per_second": 974.792
    },
    {
      "epoch": 0.068714632174616,
      "grad_norm": 1.750923991203308,
      "learning_rate": 4.993568371354315e-05,
      "loss": 0.6019,
      "num_input_tokens_seen": 4261136,
      "step": 340,
      "train_runtime": 4371.175,
      "train_tokens_per_second": 974.826
    },
    {
      "epoch": 0.06972514147130153,
      "grad_norm": 1.8298773765563965,
      "learning_rate": 4.99337733308105e-05,
      "loss": 0.5583,
      "num_input_tokens_seen": 4323616,
      "step": 345,
      "train_runtime": 4435.2684,
      "train_tokens_per_second": 974.826
    },
    {
      "epoch": 0.07073565076798706,
      "grad_norm": 2.373859167098999,
      "learning_rate": 4.9931835027408256e-05,
      "loss": 0.5001,
      "num_input_tokens_seen": 4386112,
      "step": 350,
      "train_runtime": 4499.3416,
      "train_tokens_per_second": 974.834
    },
    {
      "epoch": 0.07174616006467259,
      "grad_norm": 2.1575112342834473,
      "learning_rate": 4.9929868805506924e-05,
      "loss": 0.5462,
      "num_input_tokens_seen": 4449232,
      "step": 355,
      "train_runtime": 4563.8898,
      "train_tokens_per_second": 974.877
    },
    {
      "epoch": 0.07275666936135812,
      "grad_norm": 2.1864876747131348,
      "learning_rate": 4.992787466730827e-05,
      "loss": 0.5845,
      "num_input_tokens_seen": 4511968,
      "step": 360,
      "train_runtime": 4628.4596,
      "train_tokens_per_second": 974.831
    },
    {
      "epoch": 0.07376717865804365,
      "grad_norm": 1.9257698059082031,
      "learning_rate": 4.992585261504531e-05,
      "loss": 0.3801,
      "num_input_tokens_seen": 4574736,
      "step": 365,
      "train_runtime": 4692.8676,
      "train_tokens_per_second": 974.827
    },
    {
      "epoch": 0.07477768795472918,
      "grad_norm": 1.8602269887924194,
      "learning_rate": 4.992380265098232e-05,
      "loss": 0.677,
      "num_input_tokens_seen": 4637088,
      "step": 370,
      "train_runtime": 4756.8789,
      "train_tokens_per_second": 974.817
    },
    {
      "epoch": 0.07578819725141471,
      "grad_norm": 2.3480966091156006,
      "learning_rate": 4.9921724777414854e-05,
      "loss": 0.4761,
      "num_input_tokens_seen": 4700208,
      "step": 375,
      "train_runtime": 4821.474,
      "train_tokens_per_second": 974.849
    },
    {
      "epoch": 0.07679870654810024,
      "grad_norm": 4.755831718444824,
      "learning_rate": 4.9919618996669684e-05,
      "loss": 0.5752,
      "num_input_tokens_seen": 4763072,
      "step": 380,
      "train_runtime": 4885.708,
      "train_tokens_per_second": 974.899
    },
    {
      "epoch": 0.07780921584478577,
      "grad_norm": 2.355079174041748,
      "learning_rate": 4.991748531110485e-05,
      "loss": 0.5808,
      "num_input_tokens_seen": 4824752,
      "step": 385,
      "train_runtime": 4949.0128,
      "train_tokens_per_second": 974.892
    },
    {
      "epoch": 0.0788197251414713,
      "grad_norm": 1.9241557121276855,
      "learning_rate": 4.9915323723109644e-05,
      "loss": 0.5848,
      "num_input_tokens_seen": 4887616,
      "step": 390,
      "train_runtime": 5013.5626,
      "train_tokens_per_second": 974.879
    },
    {
      "epoch": 0.07983023443815683,
      "grad_norm": 2.18023419380188,
      "learning_rate": 4.9913134235104594e-05,
      "loss": 0.556,
      "num_input_tokens_seen": 4949904,
      "step": 395,
      "train_runtime": 5077.3931,
      "train_tokens_per_second": 974.891
    },
    {
      "epoch": 0.08084074373484236,
      "grad_norm": 2.0102245807647705,
      "learning_rate": 4.991091684954148e-05,
      "loss": 0.562,
      "num_input_tokens_seen": 5013344,
      "step": 400,
      "train_runtime": 5142.2336,
      "train_tokens_per_second": 974.935
    },
    {
      "epoch": 0.08185125303152789,
      "grad_norm": 1.278671145439148,
      "learning_rate": 4.990867156890331e-05,
      "loss": 0.4689,
      "num_input_tokens_seen": 5076016,
      "step": 405,
      "train_runtime": 5207.2264,
      "train_tokens_per_second": 974.802
    },
    {
      "epoch": 0.08286176232821342,
      "grad_norm": 2.64900803565979,
      "learning_rate": 4.990639839570433e-05,
      "loss": 0.4076,
      "num_input_tokens_seen": 5138864,
      "step": 410,
      "train_runtime": 5271.8409,
      "train_tokens_per_second": 974.776
    },
    {
      "epoch": 0.08387227162489895,
      "grad_norm": 2.691551685333252,
      "learning_rate": 4.990409733249003e-05,
      "loss": 0.7865,
      "num_input_tokens_seen": 5201792,
      "step": 415,
      "train_runtime": 5336.2757,
      "train_tokens_per_second": 974.798
    },
    {
      "epoch": 0.08488278092158448,
      "grad_norm": 3.748655319213867,
      "learning_rate": 4.990176838183712e-05,
      "loss": 0.7636,
      "num_input_tokens_seen": 5264048,
      "step": 420,
      "train_runtime": 5400.126,
      "train_tokens_per_second": 974.801
    },
    {
      "epoch": 0.08589329021827001,
      "grad_norm": 2.0539917945861816,
      "learning_rate": 4.989941154635355e-05,
      "loss": 0.5496,
      "num_input_tokens_seen": 5326160,
      "step": 425,
      "train_runtime": 5463.8507,
      "train_tokens_per_second": 974.8
    },
    {
      "epoch": 0.08690379951495554,
      "grad_norm": 2.6303889751434326,
      "learning_rate": 4.9897026828678487e-05,
      "loss": 0.5746,
      "num_input_tokens_seen": 5388544,
      "step": 430,
      "train_runtime": 5527.8041,
      "train_tokens_per_second": 974.807
    },
    {
      "epoch": 0.08791430881164107,
      "grad_norm": 1.9403839111328125,
      "learning_rate": 4.989461423148232e-05,
      "loss": 0.5676,
      "num_input_tokens_seen": 5451072,
      "step": 435,
      "train_runtime": 5592.0594,
      "train_tokens_per_second": 974.788
    },
    {
      "epoch": 0.0889248181083266,
      "grad_norm": 2.1686761379241943,
      "learning_rate": 4.9892173757466656e-05,
      "loss": 0.5473,
      "num_input_tokens_seen": 5513872,
      "step": 440,
      "train_runtime": 5656.2593,
      "train_tokens_per_second": 974.827
    },
    {
      "epoch": 0.08993532740501213,
      "grad_norm": 2.3746297359466553,
      "learning_rate": 4.988970540936432e-05,
      "loss": 0.6022,
      "num_input_tokens_seen": 5578208,
      "step": 445,
      "train_runtime": 5721.6934,
      "train_tokens_per_second": 974.923
    },
    {
      "epoch": 0.09094583670169766,
      "grad_norm": 1.723975419998169,
      "learning_rate": 4.988720918993936e-05,
      "loss": 0.5299,
      "num_input_tokens_seen": 5640096,
      "step": 450,
      "train_runtime": 5785.2176,
      "train_tokens_per_second": 974.915
    },
    {
      "epoch": 0.09195634599838319,
      "grad_norm": 1.563830018043518,
      "learning_rate": 4.9884685101987015e-05,
      "loss": 0.4328,
      "num_input_tokens_seen": 5702000,
      "step": 455,
      "train_runtime": 5848.7511,
      "train_tokens_per_second": 974.909
    },
    {
      "epoch": 0.09296685529506872,
      "grad_norm": 2.0060925483703613,
      "learning_rate": 4.9882133148333756e-05,
      "loss": 0.4719,
      "num_input_tokens_seen": 5765456,
      "step": 460,
      "train_runtime": 5913.578,
      "train_tokens_per_second": 974.952
    },
    {
      "epoch": 0.09397736459175425,
      "grad_norm": 2.445335865020752,
      "learning_rate": 4.9879553331837226e-05,
      "loss": 0.6382,
      "num_input_tokens_seen": 5827632,
      "step": 465,
      "train_runtime": 5977.6617,
      "train_tokens_per_second": 974.902
    },
    {
      "epoch": 0.09498787388843978,
      "grad_norm": 2.2754082679748535,
      "learning_rate": 4.9876945655386295e-05,
      "loss": 0.681,
      "num_input_tokens_seen": 5890768,
      "step": 470,
      "train_runtime": 6042.2675,
      "train_tokens_per_second": 974.927
    },
    {
      "epoch": 0.0959983831851253,
      "grad_norm": 2.019113302230835,
      "learning_rate": 4.9874310121901016e-05,
      "loss": 0.5577,
      "num_input_tokens_seen": 5953152,
      "step": 475,
      "train_runtime": 6106.2808,
      "train_tokens_per_second": 974.923
    },
    {
      "epoch": 0.09700889248181083,
      "grad_norm": 1.5393818616867065,
      "learning_rate": 4.987164673433266e-05,
      "loss": 0.5848,
      "num_input_tokens_seen": 6015952,
      "step": 480,
      "train_runtime": 6170.4087,
      "train_tokens_per_second": 974.968
    },
    {
      "epoch": 0.09801940177849636,
      "grad_norm": 1.346982479095459,
      "learning_rate": 4.986895549566365e-05,
      "loss": 0.4477,
      "num_input_tokens_seen": 6079312,
      "step": 485,
      "train_runtime": 6235.1039,
      "train_tokens_per_second": 975.014
    },
    {
      "epoch": 0.09902991107518189,
      "grad_norm": 1.7265504598617554,
      "learning_rate": 4.9866236408907614e-05,
      "loss": 0.5869,
      "num_input_tokens_seen": 6142048,
      "step": 490,
      "train_runtime": 6299.4062,
      "train_tokens_per_second": 975.02
    },
    {
      "epoch": 0.10004042037186742,
      "grad_norm": 1.7744814157485962,
      "learning_rate": 4.9863489477109384e-05,
      "loss": 0.6464,
      "num_input_tokens_seen": 6205056,
      "step": 495,
      "train_runtime": 6363.811,
      "train_tokens_per_second": 975.053
    },
    {
      "epoch": 0.10105092966855295,
      "grad_norm": 2.204758644104004,
      "learning_rate": 4.986071470334495e-05,
      "loss": 0.5652,
      "num_input_tokens_seen": 6267344,
      "step": 500,
      "train_runtime": 6427.6124,
      "train_tokens_per_second": 975.066
    },
    {
      "epoch": 0.10206143896523848,
      "grad_norm": 1.549620270729065,
      "learning_rate": 4.985791209072147e-05,
      "loss": 0.5224,
      "num_input_tokens_seen": 6330496,
      "step": 505,
      "train_runtime": 6492.9381,
      "train_tokens_per_second": 974.982
    },
    {
      "epoch": 0.103071948261924,
      "grad_norm": 1.4591660499572754,
      "learning_rate": 4.985508164237731e-05,
      "loss": 0.4803,
      "num_input_tokens_seen": 6392816,
      "step": 510,
      "train_runtime": 6556.8749,
      "train_tokens_per_second": 974.979
    },
    {
      "epoch": 0.10408245755860954,
      "grad_norm": 1.4925103187561035,
      "learning_rate": 4.9852223361481976e-05,
      "loss": 0.7243,
      "num_input_tokens_seen": 6454528,
      "step": 515,
      "train_runtime": 6620.4547,
      "train_tokens_per_second": 974.937
    },
    {
      "epoch": 0.10509296685529507,
      "grad_norm": 1.4430254697799683,
      "learning_rate": 4.9849337251236164e-05,
      "loss": 0.513,
      "num_input_tokens_seen": 6517904,
      "step": 520,
      "train_runtime": 6685.2075,
      "train_tokens_per_second": 974.974
    },
    {
      "epoch": 0.1061034761519806,
      "grad_norm": 2.534200668334961,
      "learning_rate": 4.9846423314871705e-05,
      "loss": 0.5966,
      "num_input_tokens_seen": 6579648,
      "step": 525,
      "train_runtime": 6748.7375,
      "train_tokens_per_second": 974.945
    },
    {
      "epoch": 0.10711398544866613,
      "grad_norm": 1.5646306276321411,
      "learning_rate": 4.984348155565161e-05,
      "loss": 0.4807,
      "num_input_tokens_seen": 6643296,
      "step": 530,
      "train_runtime": 6813.555,
      "train_tokens_per_second": 975.012
    },
    {
      "epoch": 0.10812449474535166,
      "grad_norm": 2.3576912879943848,
      "learning_rate": 4.984051197687004e-05,
      "loss": 0.6098,
      "num_input_tokens_seen": 6705600,
      "step": 535,
      "train_runtime": 6877.3341,
      "train_tokens_per_second": 975.029
    },
    {
      "epoch": 0.10913500404203719,
      "grad_norm": 1.842487096786499,
      "learning_rate": 4.983751458185232e-05,
      "loss": 0.5284,
      "num_input_tokens_seen": 6768736,
      "step": 540,
      "train_runtime": 6942.2332,
      "train_tokens_per_second": 975.008
    },
    {
      "epoch": 0.11014551333872272,
      "grad_norm": 1.9636157751083374,
      "learning_rate": 4.98344893739549e-05,
      "loss": 0.4674,
      "num_input_tokens_seen": 6832432,
      "step": 545,
      "train_runtime": 7007.2124,
      "train_tokens_per_second": 975.057
    },
    {
      "epoch": 0.11115602263540825,
      "grad_norm": 1.4325505495071411,
      "learning_rate": 4.983143635656539e-05,
      "loss": 0.5032,
      "num_input_tokens_seen": 6896928,
      "step": 550,
      "train_runtime": 7072.8519,
      "train_tokens_per_second": 975.127
    },
    {
      "epoch": 0.11216653193209378,
      "grad_norm": 1.2831155061721802,
      "learning_rate": 4.9828355533102536e-05,
      "loss": 0.5402,
      "num_input_tokens_seen": 6958976,
      "step": 555,
      "train_runtime": 7136.5332,
      "train_tokens_per_second": 975.12
    },
    {
      "epoch": 0.11317704122877931,
      "grad_norm": 3.659543514251709,
      "learning_rate": 4.9825246907016234e-05,
      "loss": 0.5421,
      "num_input_tokens_seen": 7022448,
      "step": 560,
      "train_runtime": 7201.33,
      "train_tokens_per_second": 975.16
    },
    {
      "epoch": 0.11418755052546484,
      "grad_norm": 2.353712558746338,
      "learning_rate": 4.9822110481787495e-05,
      "loss": 0.5577,
      "num_input_tokens_seen": 7084960,
      "step": 565,
      "train_runtime": 7265.6933,
      "train_tokens_per_second": 975.125
    },
    {
      "epoch": 0.11519805982215037,
      "grad_norm": 1.9854860305786133,
      "learning_rate": 4.9818946260928456e-05,
      "loss": 0.609,
      "num_input_tokens_seen": 7148032,
      "step": 570,
      "train_runtime": 7330.2672,
      "train_tokens_per_second": 975.139
    },
    {
      "epoch": 0.1162085691188359,
      "grad_norm": 1.609859824180603,
      "learning_rate": 4.981575424798241e-05,
      "loss": 0.5708,
      "num_input_tokens_seen": 7211216,
      "step": 575,
      "train_runtime": 7394.8668,
      "train_tokens_per_second": 975.165
    },
    {
      "epoch": 0.11721907841552143,
      "grad_norm": 0.9888865947723389,
      "learning_rate": 4.981253444652374e-05,
      "loss": 0.5243,
      "num_input_tokens_seen": 7274432,
      "step": 580,
      "train_runtime": 7459.4702,
      "train_tokens_per_second": 975.194
    },
    {
      "epoch": 0.11822958771220696,
      "grad_norm": 3.218935966491699,
      "learning_rate": 4.980928686015795e-05,
      "loss": 0.4952,
      "num_input_tokens_seen": 7337472,
      "step": 585,
      "train_runtime": 7523.9792,
      "train_tokens_per_second": 975.212
    },
    {
      "epoch": 0.11924009700889249,
      "grad_norm": 2.1626603603363037,
      "learning_rate": 4.9806011492521694e-05,
      "loss": 0.5539,
      "num_input_tokens_seen": 7401232,
      "step": 590,
      "train_runtime": 7589.1609,
      "train_tokens_per_second": 975.237
    },
    {
      "epoch": 0.12025060630557802,
      "grad_norm": 1.448823094367981,
      "learning_rate": 4.980270834728268e-05,
      "loss": 0.5735,
      "num_input_tokens_seen": 7463856,
      "step": 595,
      "train_runtime": 7653.1009,
      "train_tokens_per_second": 975.272
    },
    {
      "epoch": 0.12126111560226355,
      "grad_norm": 1.4811606407165527,
      "learning_rate": 4.979937742813975e-05,
      "loss": 0.4967,
      "num_input_tokens_seen": 7525728,
      "step": 600,
      "train_runtime": 7716.7293,
      "train_tokens_per_second": 975.248
    },
    {
      "epoch": 0.12227162489894908,
      "grad_norm": 1.8654146194458008,
      "learning_rate": 4.9796018738822847e-05,
      "loss": 0.5945,
      "num_input_tokens_seen": 7588480,
      "step": 605,
      "train_runtime": 7781.598,
      "train_tokens_per_second": 975.183
    },
    {
      "epoch": 0.1232821341956346,
      "grad_norm": 1.1722034215927124,
      "learning_rate": 4.979263228309301e-05,
      "loss": 0.4712,
      "num_input_tokens_seen": 7651072,
      "step": 610,
      "train_runtime": 7845.5879,
      "train_tokens_per_second": 975.207
    },
    {
      "epoch": 0.12429264349232012,
      "grad_norm": 2.0283758640289307,
      "learning_rate": 4.978921806474237e-05,
      "loss": 0.6655,
      "num_input_tokens_seen": 7712992,
      "step": 615,
      "train_runtime": 7909.3273,
      "train_tokens_per_second": 975.177
    },
    {
      "epoch": 0.12530315278900567,
      "grad_norm": 2.235264301300049,
      "learning_rate": 4.978577608759415e-05,
      "loss": 0.5773,
      "num_input_tokens_seen": 7775632,
      "step": 620,
      "train_runtime": 7973.4533,
      "train_tokens_per_second": 975.19
    },
    {
      "epoch": 0.1263136620856912,
      "grad_norm": 6.497363567352295,
      "learning_rate": 4.978230635550265e-05,
      "loss": 0.4447,
      "num_input_tokens_seen": 7838400,
      "step": 625,
      "train_runtime": 8037.5581,
      "train_tokens_per_second": 975.222
    },
    {
      "epoch": 0.12732417138237673,
      "grad_norm": 1.3632737398147583,
      "learning_rate": 4.977880887235324e-05,
      "loss": 0.4912,
      "num_input_tokens_seen": 7902096,
      "step": 630,
      "train_runtime": 8102.5895,
      "train_tokens_per_second": 975.256
    },
    {
      "epoch": 0.12833468067906226,
      "grad_norm": 1.1715339422225952,
      "learning_rate": 4.9775283642062397e-05,
      "loss": 0.5265,
      "num_input_tokens_seen": 7965392,
      "step": 635,
      "train_runtime": 8167.3529,
      "train_tokens_per_second": 975.272
    },
    {
      "epoch": 0.1293451899757478,
      "grad_norm": 2.433999538421631,
      "learning_rate": 4.977173066857764e-05,
      "loss": 0.6131,
      "num_input_tokens_seen": 8028672,
      "step": 640,
      "train_runtime": 8232.1596,
      "train_tokens_per_second": 975.281
    },
    {
      "epoch": 0.13035569927243332,
      "grad_norm": 2.575254440307617,
      "learning_rate": 4.9768149955877565e-05,
      "loss": 0.5223,
      "num_input_tokens_seen": 8090864,
      "step": 645,
      "train_runtime": 8295.9121,
      "train_tokens_per_second": 975.283
    },
    {
      "epoch": 0.13136620856911885,
      "grad_norm": 1.740582823753357,
      "learning_rate": 4.9764541507971825e-05,
      "loss": 0.6205,
      "num_input_tokens_seen": 8154432,
      "step": 650,
      "train_runtime": 8360.8813,
      "train_tokens_per_second": 975.308
    },
    {
      "epoch": 0.13237671786580438,
      "grad_norm": 3.2470200061798096,
      "learning_rate": 4.976090532890114e-05,
      "loss": 0.5946,
      "num_input_tokens_seen": 8217360,
      "step": 655,
      "train_runtime": 8425.2248,
      "train_tokens_per_second": 975.328
    },
    {
      "epoch": 0.1333872271624899,
      "grad_norm": 3.1419029235839844,
      "learning_rate": 4.9757241422737275e-05,
      "loss": 0.5508,
      "num_input_tokens_seen": 8279328,
      "step": 660,
      "train_runtime": 8488.6888,
      "train_tokens_per_second": 975.336
    },
    {
      "epoch": 0.13439773645917544,
      "grad_norm": 2.3696303367614746,
      "learning_rate": 4.9753549793583066e-05,
      "loss": 0.6512,
      "num_input_tokens_seen": 8341984,
      "step": 665,
      "train_runtime": 8552.9736,
      "train_tokens_per_second": 975.331
    },
    {
      "epoch": 0.13540824575586097,
      "grad_norm": 1.550959587097168,
      "learning_rate": 4.974983044557235e-05,
      "loss": 0.5431,
      "num_input_tokens_seen": 8404224,
      "step": 670,
      "train_runtime": 8616.8886,
      "train_tokens_per_second": 975.32
    },
    {
      "epoch": 0.13641875505254647,
      "grad_norm": 1.2606433629989624,
      "learning_rate": 4.9746083382870034e-05,
      "loss": 0.4101,
      "num_input_tokens_seen": 8467744,
      "step": 675,
      "train_runtime": 8681.6235,
      "train_tokens_per_second": 975.364
    },
    {
      "epoch": 0.137429264349232,
      "grad_norm": 1.5148366689682007,
      "learning_rate": 4.9742308609672064e-05,
      "loss": 0.3915,
      "num_input_tokens_seen": 8529216,
      "step": 680,
      "train_runtime": 8744.8044,
      "train_tokens_per_second": 975.347
    },
    {
      "epoch": 0.13843977364591753,
      "grad_norm": 4.447848320007324,
      "learning_rate": 4.9738506130205384e-05,
      "loss": 0.6023,
      "num_input_tokens_seen": 8590928,
      "step": 685,
      "train_runtime": 8808.3051,
      "train_tokens_per_second": 975.321
    },
    {
      "epoch": 0.13945028294260306,
      "grad_norm": 1.4885520935058594,
      "learning_rate": 4.9734675948728016e-05,
      "loss": 0.5472,
      "num_input_tokens_seen": 8652432,
      "step": 690,
      "train_runtime": 8871.6817,
      "train_tokens_per_second": 975.287
    },
    {
      "epoch": 0.1404607922392886,
      "grad_norm": 1.5482364892959595,
      "learning_rate": 4.973081806952894e-05,
      "loss": 0.5076,
      "num_input_tokens_seen": 8715696,
      "step": 695,
      "train_runtime": 8936.5448,
      "train_tokens_per_second": 975.287
    },
    {
      "epoch": 0.14147130153597412,
      "grad_norm": 1.3517813682556152,
      "learning_rate": 4.97269324969282e-05,
      "loss": 0.3967,
      "num_input_tokens_seen": 8778944,
      "step": 700,
      "train_runtime": 9001.2685,
      "train_tokens_per_second": 975.301
    },
    {
      "epoch": 0.14248181083265965,
      "grad_norm": 1.6823707818984985,
      "learning_rate": 4.972301923527683e-05,
      "loss": 0.5762,
      "num_input_tokens_seen": 8842416,
      "step": 705,
      "train_runtime": 9066.8732,
      "train_tokens_per_second": 975.244
    },
    {
      "epoch": 0.14349232012934518,
      "grad_norm": 1.6719893217086792,
      "learning_rate": 4.971907828895687e-05,
      "loss": 0.7174,
      "num_input_tokens_seen": 8906528,
      "step": 710,
      "train_runtime": 9132.2294,
      "train_tokens_per_second": 975.285
    },
    {
      "epoch": 0.1445028294260307,
      "grad_norm": 1.5479017496109009,
      "learning_rate": 4.9715109662381377e-05,
      "loss": 0.4741,
      "num_input_tokens_seen": 8969088,
      "step": 715,
      "train_runtime": 9196.4583,
      "train_tokens_per_second": 975.276
    },
    {
      "epoch": 0.14551333872271624,
      "grad_norm": 1.4091579914093018,
      "learning_rate": 4.971111335999438e-05,
      "loss": 0.4334,
      "num_input_tokens_seen": 9032320,
      "step": 720,
      "train_runtime": 9261.0465,
      "train_tokens_per_second": 975.302
    },
    {
      "epoch": 0.14652384801940177,
      "grad_norm": 1.314772367477417,
      "learning_rate": 4.970708938627091e-05,
      "loss": 0.637,
      "num_input_tokens_seen": 9095456,
      "step": 725,
      "train_runtime": 9325.6569,
      "train_tokens_per_second": 975.315
    },
    {
      "epoch": 0.1475343573160873,
      "grad_norm": 1.3346257209777832,
      "learning_rate": 4.970303774571699e-05,
      "loss": 0.4679,
      "num_input_tokens_seen": 9157712,
      "step": 730,
      "train_runtime": 9389.5057,
      "train_tokens_per_second": 975.314
    },
    {
      "epoch": 0.14854486661277283,
      "grad_norm": 1.9148470163345337,
      "learning_rate": 4.969895844286962e-05,
      "loss": 0.5204,
      "num_input_tokens_seen": 9219904,
      "step": 735,
      "train_runtime": 9453.4207,
      "train_tokens_per_second": 975.298
    },
    {
      "epoch": 0.14955537590945836,
      "grad_norm": 0.8981589078903198,
      "learning_rate": 4.9694851482296776e-05,
      "loss": 0.396,
      "num_input_tokens_seen": 9282592,
      "step": 740,
      "train_runtime": 9517.365,
      "train_tokens_per_second": 975.332
    },
    {
      "epoch": 0.1505658852061439,
      "grad_norm": 1.8844726085662842,
      "learning_rate": 4.969071686859741e-05,
      "loss": 0.5262,
      "num_input_tokens_seen": 9344784,
      "step": 745,
      "train_runtime": 9581.3638,
      "train_tokens_per_second": 975.308
    },
    {
      "epoch": 0.15157639450282942,
      "grad_norm": 1.1442819833755493,
      "learning_rate": 4.968655460640141e-05,
      "loss": 0.5145,
      "num_input_tokens_seen": 9408208,
      "step": 750,
      "train_runtime": 9646.0926,
      "train_tokens_per_second": 975.339
    },
    {
      "epoch": 0.15258690379951495,
      "grad_norm": 1.4171738624572754,
      "learning_rate": 4.9682364700369686e-05,
      "loss": 0.6042,
      "num_input_tokens_seen": 9469552,
      "step": 755,
      "train_runtime": 9709.0637,
      "train_tokens_per_second": 975.331
    },
    {
      "epoch": 0.15359741309620048,
      "grad_norm": 1.2637836933135986,
      "learning_rate": 4.9678147155194044e-05,
      "loss": 0.3788,
      "num_input_tokens_seen": 9531744,
      "step": 760,
      "train_runtime": 9772.7002,
      "train_tokens_per_second": 975.344
    },
    {
      "epoch": 0.154607922392886,
      "grad_norm": 1.174815058708191,
      "learning_rate": 4.9673901975597256e-05,
      "loss": 0.5437,
      "num_input_tokens_seen": 9594448,
      "step": 765,
      "train_runtime": 9836.9292,
      "train_tokens_per_second": 975.35
    },
    {
      "epoch": 0.15561843168957154,
      "grad_norm": 1.4299592971801758,
      "learning_rate": 4.966962916633306e-05,
      "loss": 0.5083,
      "num_input_tokens_seen": 9656384,
      "step": 770,
      "train_runtime": 9900.7578,
      "train_tokens_per_second": 975.318
    },
    {
      "epoch": 0.15662894098625707,
      "grad_norm": 1.8838950395584106,
      "learning_rate": 4.966532873218611e-05,
      "loss": 0.4856,
      "num_input_tokens_seen": 9719136,
      "step": 775,
      "train_runtime": 9965.0737,
      "train_tokens_per_second": 975.32
    },
    {
      "epoch": 0.1576394502829426,
      "grad_norm": 1.0754367113113403,
      "learning_rate": 4.9661000677972006e-05,
      "loss": 0.6251,
      "num_input_tokens_seen": 9783280,
      "step": 780,
      "train_runtime": 10030.3963,
      "train_tokens_per_second": 975.363
    },
    {
      "epoch": 0.15864995957962813,
      "grad_norm": 1.0097066164016724,
      "learning_rate": 4.965664500853728e-05,
      "loss": 0.6153,
      "num_input_tokens_seen": 9846448,
      "step": 785,
      "train_runtime": 10094.9216,
      "train_tokens_per_second": 975.386
    },
    {
      "epoch": 0.15966046887631366,
      "grad_norm": 1.1163966655731201,
      "learning_rate": 4.965226172875937e-05,
      "loss": 0.5767,
      "num_input_tokens_seen": 9908928,
      "step": 790,
      "train_runtime": 10158.7787,
      "train_tokens_per_second": 975.405
    },
    {
      "epoch": 0.1606709781729992,
      "grad_norm": 2.197213649749756,
      "learning_rate": 4.9647850843546654e-05,
      "loss": 0.6148,
      "num_input_tokens_seen": 9972016,
      "step": 795,
      "train_runtime": 10223.326,
      "train_tokens_per_second": 975.418
    },
    {
      "epoch": 0.16168148746968472,
      "grad_norm": 1.2278189659118652,
      "learning_rate": 4.964341235783839e-05,
      "loss": 0.5412,
      "num_input_tokens_seen": 10033296,
      "step": 800,
      "train_runtime": 10286.5306,
      "train_tokens_per_second": 975.382
    },
    {
      "epoch": 0.16269199676637025,
      "grad_norm": 1.5702325105667114,
      "learning_rate": 4.963894627660479e-05,
      "loss": 0.6647,
      "num_input_tokens_seen": 10095872,
      "step": 805,
      "train_runtime": 10351.3027,
      "train_tokens_per_second": 975.324
    },
    {
      "epoch": 0.16370250606305578,
      "grad_norm": 1.2177772521972656,
      "learning_rate": 4.9634452604846925e-05,
      "loss": 0.4279,
      "num_input_tokens_seen": 10159184,
      "step": 810,
      "train_runtime": 10416.0041,
      "train_tokens_per_second": 975.344
    },
    {
      "epoch": 0.1647130153597413,
      "grad_norm": 2.477494239807129,
      "learning_rate": 4.9629931347596784e-05,
      "loss": 0.6188,
      "num_input_tokens_seen": 10221888,
      "step": 815,
      "train_runtime": 10480.2357,
      "train_tokens_per_second": 975.349
    },
    {
      "epoch": 0.16572352465642684,
      "grad_norm": 1.0318177938461304,
      "learning_rate": 4.962538250991725e-05,
      "loss": 0.8083,
      "num_input_tokens_seen": 10284512,
      "step": 820,
      "train_runtime": 10544.2168,
      "train_tokens_per_second": 975.37
    },
    {
      "epoch": 0.16673403395311237,
      "grad_norm": 1.3359973430633545,
      "learning_rate": 4.962080609690205e-05,
      "loss": 0.665,
      "num_input_tokens_seen": 10347360,
      "step": 825,
      "train_runtime": 10608.706,
      "train_tokens_per_second": 975.365
    },
    {
      "epoch": 0.1677445432497979,
      "grad_norm": 1.4112993478775024,
      "learning_rate": 4.961620211367586e-05,
      "loss": 0.5295,
      "num_input_tokens_seen": 10409680,
      "step": 830,
      "train_runtime": 10672.8116,
      "train_tokens_per_second": 975.346
    },
    {
      "epoch": 0.16875505254648343,
      "grad_norm": 1.39415442943573,
      "learning_rate": 4.961157056539415e-05,
      "loss": 0.5889,
      "num_input_tokens_seen": 10472640,
      "step": 835,
      "train_runtime": 10737.225,
      "train_tokens_per_second": 975.358
    },
    {
      "epoch": 0.16976556184316896,
      "grad_norm": 1.366027593612671,
      "learning_rate": 4.9606911457243324e-05,
      "loss": 0.4153,
      "num_input_tokens_seen": 10534624,
      "step": 840,
      "train_runtime": 10800.8294,
      "train_tokens_per_second": 975.353
    },
    {
      "epoch": 0.1707760711398545,
      "grad_norm": 1.4660476446151733,
      "learning_rate": 4.9602224794440614e-05,
      "loss": 0.5922,
      "num_input_tokens_seen": 10597248,
      "step": 845,
      "train_runtime": 10865.0952,
      "train_tokens_per_second": 975.348
    },
    {
      "epoch": 0.17178658043654002,
      "grad_norm": 1.3375862836837769,
      "learning_rate": 4.9597510582234105e-05,
      "loss": 0.6029,
      "num_input_tokens_seen": 10659152,
      "step": 850,
      "train_runtime": 10928.6238,
      "train_tokens_per_second": 975.343
    },
    {
      "epoch": 0.17279708973322555,
      "grad_norm": 1.0716784000396729,
      "learning_rate": 4.9592768825902744e-05,
      "loss": 0.5673,
      "num_input_tokens_seen": 10721904,
      "step": 855,
      "train_runtime": 10992.9378,
      "train_tokens_per_second": 975.345
    },
    {
      "epoch": 0.17380759902991108,
      "grad_norm": 1.5033396482467651,
      "learning_rate": 4.9587999530756324e-05,
      "loss": 0.4951,
      "num_input_tokens_seen": 10785440,
      "step": 860,
      "train_runtime": 11057.661,
      "train_tokens_per_second": 975.382
    },
    {
      "epoch": 0.1748181083265966,
      "grad_norm": 2.2859349250793457,
      "learning_rate": 4.958320270213546e-05,
      "loss": 0.6315,
      "num_input_tokens_seen": 10848384,
      "step": 865,
      "train_runtime": 11122.1253,
      "train_tokens_per_second": 975.388
    },
    {
      "epoch": 0.17582861762328214,
      "grad_norm": 2.100872755050659,
      "learning_rate": 4.957837834541161e-05,
      "loss": 0.5582,
      "num_input_tokens_seen": 10910592,
      "step": 870,
      "train_runtime": 11185.9019,
      "train_tokens_per_second": 975.388
    },
    {
      "epoch": 0.17683912691996767,
      "grad_norm": 1.2115658521652222,
      "learning_rate": 4.957352646598706e-05,
      "loss": 0.5539,
      "num_input_tokens_seen": 10973536,
      "step": 875,
      "train_runtime": 11250.536,
      "train_tokens_per_second": 975.379
    },
    {
      "epoch": 0.1778496362166532,
      "grad_norm": 1.5861892700195312,
      "learning_rate": 4.9568647069294905e-05,
      "loss": 0.5056,
      "num_input_tokens_seen": 11036576,
      "step": 880,
      "train_runtime": 11314.9497,
      "train_tokens_per_second": 975.398
    },
    {
      "epoch": 0.17886014551333873,
      "grad_norm": 2.308945894241333,
      "learning_rate": 4.9563740160799065e-05,
      "loss": 0.4853,
      "num_input_tokens_seen": 11098848,
      "step": 885,
      "train_runtime": 11378.9565,
      "train_tokens_per_second": 975.384
    },
    {
      "epoch": 0.17987065481002426,
      "grad_norm": 1.2397212982177734,
      "learning_rate": 4.955880574599426e-05,
      "loss": 0.4136,
      "num_input_tokens_seen": 11161456,
      "step": 890,
      "train_runtime": 11443.1683,
      "train_tokens_per_second": 975.382
    },
    {
      "epoch": 0.1808811641067098,
      "grad_norm": 1.622370958328247,
      "learning_rate": 4.955384383040602e-05,
      "loss": 0.4363,
      "num_input_tokens_seen": 11224848,
      "step": 895,
      "train_runtime": 11507.8194,
      "train_tokens_per_second": 975.411
    },
    {
      "epoch": 0.18189167340339532,
      "grad_norm": 1.4617271423339844,
      "learning_rate": 4.954885441959065e-05,
      "loss": 0.508,
      "num_input_tokens_seen": 11287264,
      "step": 900,
      "train_runtime": 11571.975,
      "train_tokens_per_second": 975.397
    },
    {
      "epoch": 0.18290218270008085,
      "grad_norm": 1.0077873468399048,
      "learning_rate": 4.9543837519135285e-05,
      "loss": 0.7297,
      "num_input_tokens_seen": 11348992,
      "step": 905,
      "train_runtime": 11636.0336,
      "train_tokens_per_second": 975.332
    },
    {
      "epoch": 0.18391269199676638,
      "grad_norm": 1.401862382888794,
      "learning_rate": 4.953879313465778e-05,
      "loss": 0.5134,
      "num_input_tokens_seen": 11412208,
      "step": 910,
      "train_runtime": 11700.5859,
      "train_tokens_per_second": 975.354
    },
    {
      "epoch": 0.1849232012934519,
      "grad_norm": 1.1108022928237915,
      "learning_rate": 4.953372127180683e-05,
      "loss": 0.623,
      "num_input_tokens_seen": 11474752,
      "step": 915,
      "train_runtime": 11764.5861,
      "train_tokens_per_second": 975.364
    },
    {
      "epoch": 0.18593371059013744,
      "grad_norm": 1.7566180229187012,
      "learning_rate": 4.952862193626187e-05,
      "loss": 0.5305,
      "num_input_tokens_seen": 11537552,
      "step": 920,
      "train_runtime": 11828.8495,
      "train_tokens_per_second": 975.374
    },
    {
      "epoch": 0.18694421988682297,
      "grad_norm": 0.7695490121841431,
      "learning_rate": 4.9523495133733086e-05,
      "loss": 0.4943,
      "num_input_tokens_seen": 11601120,
      "step": 925,
      "train_runtime": 11894.0517,
      "train_tokens_per_second": 975.372
    },
    {
      "epoch": 0.1879547291835085,
      "grad_norm": 2.0062143802642822,
      "learning_rate": 4.951834086996146e-05,
      "loss": 0.5239,
      "num_input_tokens_seen": 11663216,
      "step": 930,
      "train_runtime": 11957.7185,
      "train_tokens_per_second": 975.371
    },
    {
      "epoch": 0.18896523848019403,
      "grad_norm": 1.372841477394104,
      "learning_rate": 4.951315915071867e-05,
      "loss": 0.6014,
      "num_input_tokens_seen": 11725168,
      "step": 935,
      "train_runtime": 12021.4292,
      "train_tokens_per_second": 975.356
    },
    {
      "epoch": 0.18997574777687956,
      "grad_norm": 0.9856410622596741,
      "learning_rate": 4.950794998180719e-05,
      "loss": 0.4795,
      "num_input_tokens_seen": 11788336,
      "step": 940,
      "train_runtime": 12086.1283,
      "train_tokens_per_second": 975.361
    },
    {
      "epoch": 0.19098625707356506,
      "grad_norm": 1.1294419765472412,
      "learning_rate": 4.9502713369060216e-05,
      "loss": 0.6804,
      "num_input_tokens_seen": 11851504,
      "step": 945,
      "train_runtime": 12150.797,
      "train_tokens_per_second": 975.368
    },
    {
      "epoch": 0.1919967663702506,
      "grad_norm": 1.8597885370254517,
      "learning_rate": 4.949744931834166e-05,
      "loss": 0.3728,
      "num_input_tokens_seen": 11914544,
      "step": 950,
      "train_runtime": 12215.4618,
      "train_tokens_per_second": 975.366
    },
    {
      "epoch": 0.19300727566693612,
      "grad_norm": 3.3092339038848877,
      "learning_rate": 4.9492157835546173e-05,
      "loss": 0.5787,
      "num_input_tokens_seen": 11976768,
      "step": 955,
      "train_runtime": 12279.3263,
      "train_tokens_per_second": 975.36
    },
    {
      "epoch": 0.19401778496362165,
      "grad_norm": 1.2147915363311768,
      "learning_rate": 4.948683892659913e-05,
      "loss": 0.5004,
      "num_input_tokens_seen": 12039344,
      "step": 960,
      "train_runtime": 12343.5374,
      "train_tokens_per_second": 975.356
    },
    {
      "epoch": 0.19502829426030718,
      "grad_norm": 1.2030134201049805,
      "learning_rate": 4.948149259745659e-05,
      "loss": 0.3879,
      "num_input_tokens_seen": 12102544,
      "step": 965,
      "train_runtime": 12408.2053,
      "train_tokens_per_second": 975.366
    },
    {
      "epoch": 0.1960388035569927,
      "grad_norm": 1.1429862976074219,
      "learning_rate": 4.9476118854105355e-05,
      "loss": 0.563,
      "num_input_tokens_seen": 12164928,
      "step": 970,
      "train_runtime": 12472.2812,
      "train_tokens_per_second": 975.357
    },
    {
      "epoch": 0.19704931285367824,
      "grad_norm": 2.2821457386016846,
      "learning_rate": 4.94707177025629e-05,
      "loss": 0.5075,
      "num_input_tokens_seen": 12229056,
      "step": 975,
      "train_runtime": 12537.6635,
      "train_tokens_per_second": 975.386
    },
    {
      "epoch": 0.19805982215036377,
      "grad_norm": 1.9932829141616821,
      "learning_rate": 4.9465289148877396e-05,
      "loss": 0.6108,
      "num_input_tokens_seen": 12292160,
      "step": 980,
      "train_runtime": 12602.3107,
      "train_tokens_per_second": 975.389
    },
    {
      "epoch": 0.1990703314470493,
      "grad_norm": 0.8735355734825134,
      "learning_rate": 4.94598331991277e-05,
      "loss": 0.4147,
      "num_input_tokens_seen": 12354544,
      "step": 985,
      "train_runtime": 12666.2176,
      "train_tokens_per_second": 975.393
    },
    {
      "epoch": 0.20008084074373483,
      "grad_norm": 1.2199101448059082,
      "learning_rate": 4.945434985942335e-05,
      "loss": 0.4432,
      "num_input_tokens_seen": 12417408,
      "step": 990,
      "train_runtime": 12730.3912,
      "train_tokens_per_second": 975.414
    },
    {
      "epoch": 0.20109135004042036,
      "grad_norm": 1.4074676036834717,
      "learning_rate": 4.944883913590456e-05,
      "loss": 0.573,
      "num_input_tokens_seen": 12480320,
      "step": 995,
      "train_runtime": 12794.7676,
      "train_tokens_per_second": 975.424
    },
    {
      "epoch": 0.2021018593371059,
      "grad_norm": 1.8896433115005493,
      "learning_rate": 4.944330103474217e-05,
      "loss": 0.6542,
      "num_input_tokens_seen": 12543584,
      "step": 1000,
      "train_runtime": 12859.3397,
      "train_tokens_per_second": 975.445
    },
    {
      "epoch": 0.20311236863379142,
      "grad_norm": 1.7716344594955444,
      "learning_rate": 4.9437735562137744e-05,
      "loss": 0.5937,
      "num_input_tokens_seen": 12605776,
      "step": 1005,
      "train_runtime": 12924.0034,
      "train_tokens_per_second": 975.377
    },
    {
      "epoch": 0.20412287793047695,
      "grad_norm": 2.1080756187438965,
      "learning_rate": 4.943214272432345e-05,
      "loss": 0.5868,
      "num_input_tokens_seen": 12668784,
      "step": 1010,
      "train_runtime": 12988.3746,
      "train_tokens_per_second": 975.394
    },
    {
      "epoch": 0.20513338722716248,
      "grad_norm": 1.7267789840698242,
      "learning_rate": 4.9426522527562096e-05,
      "loss": 0.5071,
      "num_input_tokens_seen": 12731792,
      "step": 1015,
      "train_runtime": 13052.7767,
      "train_tokens_per_second": 975.409
    },
    {
      "epoch": 0.206143896523848,
      "grad_norm": 2.080620527267456,
      "learning_rate": 4.942087497814716e-05,
      "loss": 0.6936,
      "num_input_tokens_seen": 12794896,
      "step": 1020,
      "train_runtime": 13117.2494,
      "train_tokens_per_second": 975.425
    },
    {
      "epoch": 0.20715440582053354,
      "grad_norm": 1.613187313079834,
      "learning_rate": 4.941520008240271e-05,
      "loss": 0.4687,
      "num_input_tokens_seen": 12857840,
      "step": 1025,
      "train_runtime": 13181.8523,
      "train_tokens_per_second": 975.42
    },
    {
      "epoch": 0.20816491511721907,
      "grad_norm": 1.7164286375045776,
      "learning_rate": 4.940949784668346e-05,
      "loss": 0.437,
      "num_input_tokens_seen": 12920528,
      "step": 1030,
      "train_runtime": 13245.9639,
      "train_tokens_per_second": 975.431
    },
    {
      "epoch": 0.2091754244139046,
      "grad_norm": 2.189507246017456,
      "learning_rate": 4.940376827737475e-05,
      "loss": 0.5784,
      "num_input_tokens_seen": 12982720,
      "step": 1035,
      "train_runtime": 13309.6967,
      "train_tokens_per_second": 975.433
    },
    {
      "epoch": 0.21018593371059013,
      "grad_norm": 1.4522734880447388,
      "learning_rate": 4.93980113808925e-05,
      "loss": 0.6199,
      "num_input_tokens_seen": 13045264,
      "step": 1040,
      "train_runtime": 13373.7209,
      "train_tokens_per_second": 975.44
    },
    {
      "epoch": 0.21119644300727566,
      "grad_norm": 1.78391432762146,
      "learning_rate": 4.939222716368325e-05,
      "loss": 0.4842,
      "num_input_tokens_seen": 13108352,
      "step": 1045,
      "train_runtime": 13438.028,
      "train_tokens_per_second": 975.467
    },
    {
      "epoch": 0.2122069523039612,
      "grad_norm": 1.6015946865081787,
      "learning_rate": 4.938641563222412e-05,
      "loss": 0.6005,
      "num_input_tokens_seen": 13171952,
      "step": 1050,
      "train_runtime": 13502.9278,
      "train_tokens_per_second": 975.489
    },
    {
      "epoch": 0.21321746160064672,
      "grad_norm": 1.5950475931167603,
      "learning_rate": 4.9380576793022813e-05,
      "loss": 0.5228,
      "num_input_tokens_seen": 13234576,
      "step": 1055,
      "train_runtime": 13567.1594,
      "train_tokens_per_second": 975.486
    },
    {
      "epoch": 0.21422797089733225,
      "grad_norm": 1.063085675239563,
      "learning_rate": 4.937471065261765e-05,
      "loss": 0.5864,
      "num_input_tokens_seen": 13298592,
      "step": 1060,
      "train_runtime": 13632.4498,
      "train_tokens_per_second": 975.51
    },
    {
      "epoch": 0.21523848019401778,
      "grad_norm": 2.2352519035339355,
      "learning_rate": 4.936881721757747e-05,
      "loss": 0.6442,
      "num_input_tokens_seen": 13361088,
      "step": 1065,
      "train_runtime": 13696.356,
      "train_tokens_per_second": 975.521
    },
    {
      "epoch": 0.21624898949070331,
      "grad_norm": 1.4957116842269897,
      "learning_rate": 4.936289649450172e-05,
      "loss": 0.6308,
      "num_input_tokens_seen": 13424304,
      "step": 1070,
      "train_runtime": 13761.0385,
      "train_tokens_per_second": 975.53
    },
    {
      "epoch": 0.21725949878738884,
      "grad_norm": 1.7873773574829102,
      "learning_rate": 4.935694849002037e-05,
      "loss": 0.6283,
      "num_input_tokens_seen": 13486864,
      "step": 1075,
      "train_runtime": 13825.0301,
      "train_tokens_per_second": 975.54
    },
    {
      "epoch": 0.21827000808407437,
      "grad_norm": 1.889017939567566,
      "learning_rate": 4.935097321079396e-05,
      "loss": 0.6495,
      "num_input_tokens_seen": 13550032,
      "step": 1080,
      "train_runtime": 13889.7842,
      "train_tokens_per_second": 975.539
    },
    {
      "epoch": 0.2192805173807599,
      "grad_norm": 1.1631578207015991,
      "learning_rate": 4.9344970663513577e-05,
      "loss": 0.51,
      "num_input_tokens_seen": 13613824,
      "step": 1085,
      "train_runtime": 13954.8454,
      "train_tokens_per_second": 975.563
    },
    {
      "epoch": 0.22029102667744543,
      "grad_norm": 1.3471548557281494,
      "learning_rate": 4.933894085490082e-05,
      "loss": 0.5314,
      "num_input_tokens_seen": 13675296,
      "step": 1090,
      "train_runtime": 14018.1289,
      "train_tokens_per_second": 975.544
    },
    {
      "epoch": 0.22130153597413096,
      "grad_norm": 1.0632174015045166,
      "learning_rate": 4.933288379170782e-05,
      "loss": 0.4928,
      "num_input_tokens_seen": 13738080,
      "step": 1095,
      "train_runtime": 14082.3251,
      "train_tokens_per_second": 975.555
    },
    {
      "epoch": 0.2223120452708165,
      "grad_norm": 0.8830507397651672,
      "learning_rate": 4.932679948071725e-05,
      "loss": 0.5086,
      "num_input_tokens_seen": 13800480,
      "step": 1100,
      "train_runtime": 14146.182,
      "train_tokens_per_second": 975.562
    },
    {
      "epoch": 0.22332255456750202,
      "grad_norm": 1.648896336555481,
      "learning_rate": 4.9320687928742294e-05,
      "loss": 0.565,
      "num_input_tokens_seen": 13862400,
      "step": 1105,
      "train_runtime": 14210.2702,
      "train_tokens_per_second": 975.52
    },
    {
      "epoch": 0.22433306386418755,
      "grad_norm": 3.067739963531494,
      "learning_rate": 4.93145491426266e-05,
      "loss": 0.5693,
      "num_input_tokens_seen": 13926432,
      "step": 1110,
      "train_runtime": 14275.2737,
      "train_tokens_per_second": 975.563
    },
    {
      "epoch": 0.22534357316087308,
      "grad_norm": 1.8760874271392822,
      "learning_rate": 4.930838312924434e-05,
      "loss": 0.5122,
      "num_input_tokens_seen": 13989376,
      "step": 1115,
      "train_runtime": 14339.7976,
      "train_tokens_per_second": 975.563
    },
    {
      "epoch": 0.22635408245755861,
      "grad_norm": 1.390177845954895,
      "learning_rate": 4.9302189895500186e-05,
      "loss": 0.5673,
      "num_input_tokens_seen": 14051056,
      "step": 1120,
      "train_runtime": 14403.1356,
      "train_tokens_per_second": 975.555
    },
    {
      "epoch": 0.22736459175424414,
      "grad_norm": 1.2372604608535767,
      "learning_rate": 4.929596944832928e-05,
      "loss": 0.4628,
      "num_input_tokens_seen": 14113488,
      "step": 1125,
      "train_runtime": 14467.075,
      "train_tokens_per_second": 975.559
    },
    {
      "epoch": 0.22837510105092967,
      "grad_norm": 1.5144269466400146,
      "learning_rate": 4.928972179469722e-05,
      "loss": 0.5511,
      "num_input_tokens_seen": 14176320,
      "step": 1130,
      "train_runtime": 14531.167,
      "train_tokens_per_second": 975.58
    },
    {
      "epoch": 0.2293856103476152,
      "grad_norm": 1.3868563175201416,
      "learning_rate": 4.9283446941600106e-05,
      "loss": 0.4791,
      "num_input_tokens_seen": 14238976,
      "step": 1135,
      "train_runtime": 14595.3874,
      "train_tokens_per_second": 975.581
    },
    {
      "epoch": 0.23039611964430073,
      "grad_norm": 2.2589707374572754,
      "learning_rate": 4.927714489606446e-05,
      "loss": 0.6754,
      "num_input_tokens_seen": 14301296,
      "step": 1140,
      "train_runtime": 14659.1707,
      "train_tokens_per_second": 975.587
    },
    {
      "epoch": 0.23140662894098626,
      "grad_norm": 1.8672808408737183,
      "learning_rate": 4.927081566514728e-05,
      "loss": 0.4545,
      "num_input_tokens_seen": 14363872,
      "step": 1145,
      "train_runtime": 14723.2373,
      "train_tokens_per_second": 975.592
    },
    {
      "epoch": 0.2324171382376718,
      "grad_norm": 1.314982533454895,
      "learning_rate": 4.926445925593599e-05,
      "loss": 0.4916,
      "num_input_tokens_seen": 14426816,
      "step": 1150,
      "train_runtime": 14787.5204,
      "train_tokens_per_second": 975.608
    },
    {
      "epoch": 0.23342764753435732,
      "grad_norm": 1.8125275373458862,
      "learning_rate": 4.9258075675548464e-05,
      "loss": 0.6,
      "num_input_tokens_seen": 14488336,
      "step": 1155,
      "train_runtime": 14850.7679,
      "train_tokens_per_second": 975.595
    },
    {
      "epoch": 0.23443815683104285,
      "grad_norm": 2.217879295349121,
      "learning_rate": 4.925166493113298e-05,
      "loss": 0.5529,
      "num_input_tokens_seen": 14550880,
      "step": 1160,
      "train_runtime": 14915.0838,
      "train_tokens_per_second": 975.582
    },
    {
      "epoch": 0.23544866612772838,
      "grad_norm": 1.3465816974639893,
      "learning_rate": 4.924522702986824e-05,
      "loss": 0.5409,
      "num_input_tokens_seen": 14613248,
      "step": 1165,
      "train_runtime": 14978.8231,
      "train_tokens_per_second": 975.594
    },
    {
      "epoch": 0.23645917542441391,
      "grad_norm": 2.1553943157196045,
      "learning_rate": 4.923876197896338e-05,
      "loss": 0.6399,
      "num_input_tokens_seen": 14677152,
      "step": 1170,
      "train_runtime": 15043.899,
      "train_tokens_per_second": 975.622
    },
    {
      "epoch": 0.23746968472109944,
      "grad_norm": 2.428708553314209,
      "learning_rate": 4.923226978565792e-05,
      "loss": 0.5926,
      "num_input_tokens_seen": 14740528,
      "step": 1175,
      "train_runtime": 15108.6748,
      "train_tokens_per_second": 975.633
    },
    {
      "epoch": 0.23848019401778497,
      "grad_norm": 1.7512344121932983,
      "learning_rate": 4.922575045722174e-05,
      "loss": 0.705,
      "num_input_tokens_seen": 14803200,
      "step": 1180,
      "train_runtime": 15172.7321,
      "train_tokens_per_second": 975.645
    },
    {
      "epoch": 0.2394907033144705,
      "grad_norm": 1.3346056938171387,
      "learning_rate": 4.921920400095519e-05,
      "loss": 0.4949,
      "num_input_tokens_seen": 14866480,
      "step": 1185,
      "train_runtime": 15237.6116,
      "train_tokens_per_second": 975.644
    },
    {
      "epoch": 0.24050121261115603,
      "grad_norm": 2.2078330516815186,
      "learning_rate": 4.921263042418891e-05,
      "loss": 0.5992,
      "num_input_tokens_seen": 14929440,
      "step": 1190,
      "train_runtime": 15302.0341,
      "train_tokens_per_second": 975.651
    },
    {
      "epoch": 0.24151172190784156,
      "grad_norm": 1.57637619972229,
      "learning_rate": 4.920602973428396e-05,
      "loss": 0.6232,
      "num_input_tokens_seen": 14990816,
      "step": 1195,
      "train_runtime": 15365.2608,
      "train_tokens_per_second": 975.63
    },
    {
      "epoch": 0.2425222312045271,
      "grad_norm": 1.4772154092788696,
      "learning_rate": 4.9199401938631745e-05,
      "loss": 0.4862,
      "num_input_tokens_seen": 15053216,
      "step": 1200,
      "train_runtime": 15429.1563,
      "train_tokens_per_second": 975.634
    },
    {
      "epoch": 0.24353274050121262,
      "grad_norm": 1.6226205825805664,
      "learning_rate": 4.919274704465403e-05,
      "loss": 0.5439,
      "num_input_tokens_seen": 15116208,
      "step": 1205,
      "train_runtime": 15494.4334,
      "train_tokens_per_second": 975.59
    },
    {
      "epoch": 0.24454324979789815,
      "grad_norm": 1.9553275108337402,
      "learning_rate": 4.9186065059802916e-05,
      "loss": 0.6832,
      "num_input_tokens_seen": 15179488,
      "step": 1210,
      "train_runtime": 15559.2173,
      "train_tokens_per_second": 975.595
    },
    {
      "epoch": 0.24555375909458366,
      "grad_norm": 1.5311768054962158,
      "learning_rate": 4.9179355991560847e-05,
      "loss": 0.719,
      "num_input_tokens_seen": 15244112,
      "step": 1215,
      "train_runtime": 15624.7309,
      "train_tokens_per_second": 975.64
    },
    {
      "epoch": 0.2465642683912692,
      "grad_norm": 1.4127891063690186,
      "learning_rate": 4.917261984744058e-05,
      "loss": 0.416,
      "num_input_tokens_seen": 15305680,
      "step": 1220,
      "train_runtime": 15688.0247,
      "train_tokens_per_second": 975.628
    },
    {
      "epoch": 0.24757477768795472,
      "grad_norm": 1.7582745552062988,
      "learning_rate": 4.9165856634985224e-05,
      "loss": 0.6377,
      "num_input_tokens_seen": 15369376,
      "step": 1225,
      "train_runtime": 15752.8774,
      "train_tokens_per_second": 975.655
    },
    {
      "epoch": 0.24858528698464025,
      "grad_norm": 1.5937873125076294,
      "learning_rate": 4.915906636176817e-05,
      "loss": 0.653,
      "num_input_tokens_seen": 15432720,
      "step": 1230,
      "train_runtime": 15817.5455,
      "train_tokens_per_second": 975.671
    },
    {
      "epoch": 0.24959579628132578,
      "grad_norm": 1.7157689332962036,
      "learning_rate": 4.91522490353931e-05,
      "loss": 0.6825,
      "num_input_tokens_seen": 15495808,
      "step": 1235,
      "train_runtime": 15882.1573,
      "train_tokens_per_second": 975.674
    },
    {
      "epoch": 0.25060630557801133,
      "grad_norm": 1.5872325897216797,
      "learning_rate": 4.914540466349404e-05,
      "loss": 0.4961,
      "num_input_tokens_seen": 15557760,
      "step": 1240,
      "train_runtime": 15945.758,
      "train_tokens_per_second": 975.668
    },
    {
      "epoch": 0.25161681487469684,
      "grad_norm": 1.4517147541046143,
      "learning_rate": 4.913853325373525e-05,
      "loss": 0.5533,
      "num_input_tokens_seen": 15621056,
      "step": 1245,
      "train_runtime": 16010.3648,
      "train_tokens_per_second": 975.684
    },
    {
      "epoch": 0.2526273241713824,
      "grad_norm": 2.0925111770629883,
      "learning_rate": 4.91316348138113e-05,
      "loss": 0.6095,
      "num_input_tokens_seen": 15683456,
      "step": 1250,
      "train_runtime": 16074.1557,
      "train_tokens_per_second": 975.694
    },
    {
      "epoch": 0.2536378334680679,
      "grad_norm": 2.10046124458313,
      "learning_rate": 4.912470935144702e-05,
      "loss": 0.4827,
      "num_input_tokens_seen": 15746528,
      "step": 1255,
      "train_runtime": 16138.6738,
      "train_tokens_per_second": 975.701
    },
    {
      "epoch": 0.25464834276475345,
      "grad_norm": 2.1173200607299805,
      "learning_rate": 4.911775687439748e-05,
      "loss": 0.5165,
      "num_input_tokens_seen": 15809872,
      "step": 1260,
      "train_runtime": 16203.2649,
      "train_tokens_per_second": 975.721
    },
    {
      "epoch": 0.25565885206143896,
      "grad_norm": 1.6267337799072266,
      "learning_rate": 4.911077739044802e-05,
      "loss": 0.4107,
      "num_input_tokens_seen": 15873696,
      "step": 1265,
      "train_runtime": 16268.3058,
      "train_tokens_per_second": 975.744
    },
    {
      "epoch": 0.2566693613581245,
      "grad_norm": 1.34531569480896,
      "learning_rate": 4.910377090741422e-05,
      "loss": 0.5736,
      "num_input_tokens_seen": 15936224,
      "step": 1270,
      "train_runtime": 16332.2028,
      "train_tokens_per_second": 975.755
    },
    {
      "epoch": 0.25767987065481,
      "grad_norm": 1.1722432374954224,
      "learning_rate": 4.90967374331419e-05,
      "loss": 0.4268,
      "num_input_tokens_seen": 15999632,
      "step": 1275,
      "train_runtime": 16396.8254,
      "train_tokens_per_second": 975.776
    },
    {
      "epoch": 0.2586903799514956,
      "grad_norm": 1.632023572921753,
      "learning_rate": 4.9089676975507096e-05,
      "loss": 0.469,
      "num_input_tokens_seen": 16061296,
      "step": 1280,
      "train_runtime": 16460.1737,
      "train_tokens_per_second": 975.767
    },
    {
      "epoch": 0.2597008892481811,
      "grad_norm": 1.180538535118103,
      "learning_rate": 4.908258954241604e-05,
      "loss": 0.4931,
      "num_input_tokens_seen": 16124368,
      "step": 1285,
      "train_runtime": 16524.5807,
      "train_tokens_per_second": 975.781
    },
    {
      "epoch": 0.26071139854486663,
      "grad_norm": 3.09515380859375,
      "learning_rate": 4.9075475141805214e-05,
      "loss": 0.6722,
      "num_input_tokens_seen": 16187328,
      "step": 1290,
      "train_runtime": 16589.1998,
      "train_tokens_per_second": 975.775
    },
    {
      "epoch": 0.26172190784155214,
      "grad_norm": 1.7424309253692627,
      "learning_rate": 4.9068333781641275e-05,
      "loss": 0.5204,
      "num_input_tokens_seen": 16251072,
      "step": 1295,
      "train_runtime": 16654.1403,
      "train_tokens_per_second": 975.798
    },
    {
      "epoch": 0.2627324171382377,
      "grad_norm": 3.5675551891326904,
      "learning_rate": 4.906116546992105e-05,
      "loss": 0.5581,
      "num_input_tokens_seen": 16312992,
      "step": 1300,
      "train_runtime": 16717.7918,
      "train_tokens_per_second": 975.786
    },
    {
      "epoch": 0.2637429264349232,
      "grad_norm": 1.9308832883834839,
      "learning_rate": 4.905397021467158e-05,
      "loss": 0.502,
      "num_input_tokens_seen": 16375136,
      "step": 1305,
      "train_runtime": 16782.2088,
      "train_tokens_per_second": 975.744
    },
    {
      "epoch": 0.26475343573160875,
      "grad_norm": 1.7173316478729248,
      "learning_rate": 4.904674802395007e-05,
      "loss": 0.4739,
      "num_input_tokens_seen": 16438512,
      "step": 1310,
      "train_runtime": 16846.9521,
      "train_tokens_per_second": 975.756
    },
    {
      "epoch": 0.26576394502829426,
      "grad_norm": 1.4554964303970337,
      "learning_rate": 4.903949890584387e-05,
      "loss": 0.5401,
      "num_input_tokens_seen": 16500640,
      "step": 1315,
      "train_runtime": 16910.8335,
      "train_tokens_per_second": 975.744
    },
    {
      "epoch": 0.2667744543249798,
      "grad_norm": 3.7899701595306396,
      "learning_rate": 4.903222286847049e-05,
      "loss": 0.535,
      "num_input_tokens_seen": 16563056,
      "step": 1320,
      "train_runtime": 16974.72,
      "train_tokens_per_second": 975.748
    },
    {
      "epoch": 0.2677849636216653,
      "grad_norm": 1.602953553199768,
      "learning_rate": 4.902491991997759e-05,
      "loss": 0.4942,
      "num_input_tokens_seen": 16624624,
      "step": 1325,
      "train_runtime": 17037.9956,
      "train_tokens_per_second": 975.738
    },
    {
      "epoch": 0.2687954729183509,
      "grad_norm": 2.2925679683685303,
      "learning_rate": 4.9017590068542966e-05,
      "loss": 0.5941,
      "num_input_tokens_seen": 16687520,
      "step": 1330,
      "train_runtime": 17102.2397,
      "train_tokens_per_second": 975.751
    },
    {
      "epoch": 0.2698059822150364,
      "grad_norm": 1.710546612739563,
      "learning_rate": 4.901023332237453e-05,
      "loss": 0.6074,
      "num_input_tokens_seen": 16749984,
      "step": 1335,
      "train_runtime": 17166.042,
      "train_tokens_per_second": 975.763
    },
    {
      "epoch": 0.27081649151172194,
      "grad_norm": 1.502816915512085,
      "learning_rate": 4.900284968971032e-05,
      "loss": 0.5629,
      "num_input_tokens_seen": 16812896,
      "step": 1340,
      "train_runtime": 17230.6657,
      "train_tokens_per_second": 975.754
    },
    {
      "epoch": 0.27182700080840744,
      "grad_norm": 2.976975679397583,
      "learning_rate": 4.8995439178818476e-05,
      "loss": 0.4743,
      "num_input_tokens_seen": 16875040,
      "step": 1345,
      "train_runtime": 17294.3429,
      "train_tokens_per_second": 975.755
    },
    {
      "epoch": 0.27283751010509294,
      "grad_norm": 1.6818543672561646,
      "learning_rate": 4.898800179799724e-05,
      "loss": 0.4476,
      "num_input_tokens_seen": 16937008,
      "step": 1350,
      "train_runtime": 17357.7555,
      "train_tokens_per_second": 975.76
    },
    {
      "epoch": 0.2738480194017785,
      "grad_norm": 1.939050316810608,
      "learning_rate": 4.898053755557493e-05,
      "loss": 0.5061,
      "num_input_tokens_seen": 16999536,
      "step": 1355,
      "train_runtime": 17421.8649,
      "train_tokens_per_second": 975.759
    },
    {
      "epoch": 0.274858528698464,
      "grad_norm": 1.6756031513214111,
      "learning_rate": 4.8973046459909975e-05,
      "loss": 0.574,
      "num_input_tokens_seen": 17062064,
      "step": 1360,
      "train_runtime": 17486.1249,
      "train_tokens_per_second": 975.749
    },
    {
      "epoch": 0.27586903799514956,
      "grad_norm": 1.6403261423110962,
      "learning_rate": 4.896552851939083e-05,
      "loss": 0.6029,
      "num_input_tokens_seen": 17125728,
      "step": 1365,
      "train_runtime": 17551.3249,
      "train_tokens_per_second": 975.751
    },
    {
      "epoch": 0.27687954729183506,
      "grad_norm": 1.5214020013809204,
      "learning_rate": 4.8957983742436055e-05,
      "loss": 0.5255,
      "num_input_tokens_seen": 17188912,
      "step": 1370,
      "train_runtime": 17615.8995,
      "train_tokens_per_second": 975.761
    },
    {
      "epoch": 0.2778900565885206,
      "grad_norm": 2.233180284500122,
      "learning_rate": 4.895041213749422e-05,
      "loss": 0.5793,
      "num_input_tokens_seen": 17251504,
      "step": 1375,
      "train_runtime": 17680.1303,
      "train_tokens_per_second": 975.757
    },
    {
      "epoch": 0.2789005658852061,
      "grad_norm": 3.6884524822235107,
      "learning_rate": 4.894281371304397e-05,
      "loss": 0.6017,
      "num_input_tokens_seen": 17314528,
      "step": 1380,
      "train_runtime": 17744.5013,
      "train_tokens_per_second": 975.769
    },
    {
      "epoch": 0.2799110751818917,
      "grad_norm": 1.6433534622192383,
      "learning_rate": 4.893518847759396e-05,
      "loss": 0.4902,
      "num_input_tokens_seen": 17377744,
      "step": 1385,
      "train_runtime": 17809.2956,
      "train_tokens_per_second": 975.768
    },
    {
      "epoch": 0.2809215844785772,
      "grad_norm": 1.6057523488998413,
      "learning_rate": 4.8927536439682884e-05,
      "loss": 0.5034,
      "num_input_tokens_seen": 17440928,
      "step": 1390,
      "train_runtime": 17873.979,
      "train_tokens_per_second": 975.772
    },
    {
      "epoch": 0.28193209377526274,
      "grad_norm": 1.4545027017593384,
      "learning_rate": 4.891985760787944e-05,
      "loss": 0.4721,
      "num_input_tokens_seen": 17504224,
      "step": 1395,
      "train_runtime": 17938.5228,
      "train_tokens_per_second": 975.79
    },
    {
      "epoch": 0.28294260307194824,
      "grad_norm": 1.6862480640411377,
      "learning_rate": 4.891215199078233e-05,
      "loss": 0.4965,
      "num_input_tokens_seen": 17568016,
      "step": 1400,
      "train_runtime": 18003.5378,
      "train_tokens_per_second": 975.809
    },
    {
      "epoch": 0.2839531123686338,
      "grad_norm": 2.2852890491485596,
      "learning_rate": 4.8904419597020254e-05,
      "loss": 0.5293,
      "num_input_tokens_seen": 17631152,
      "step": 1405,
      "train_runtime": 18069.0294,
      "train_tokens_per_second": 975.766
    },
    {
      "epoch": 0.2849636216653193,
      "grad_norm": 1.4068840742111206,
      "learning_rate": 4.889666043525189e-05,
      "loss": 0.6887,
      "num_input_tokens_seen": 17693888,
      "step": 1410,
      "train_runtime": 18133.4708,
      "train_tokens_per_second": 975.758
    },
    {
      "epoch": 0.28597413096200486,
      "grad_norm": 1.6920101642608643,
      "learning_rate": 4.888887451416589e-05,
      "loss": 0.4625,
      "num_input_tokens_seen": 17756224,
      "step": 1415,
      "train_runtime": 18197.5487,
      "train_tokens_per_second": 975.748
    },
    {
      "epoch": 0.28698464025869036,
      "grad_norm": 1.8134652376174927,
      "learning_rate": 4.888106184248089e-05,
      "loss": 0.5949,
      "num_input_tokens_seen": 17818864,
      "step": 1420,
      "train_runtime": 18261.8246,
      "train_tokens_per_second": 975.744
    },
    {
      "epoch": 0.2879951495553759,
      "grad_norm": 1.5748082399368286,
      "learning_rate": 4.8873222428945465e-05,
      "loss": 0.5493,
      "num_input_tokens_seen": 17881648,
      "step": 1425,
      "train_runtime": 18326.026,
      "train_tokens_per_second": 975.752
    },
    {
      "epoch": 0.2890056588520614,
      "grad_norm": 2.2755610942840576,
      "learning_rate": 4.886535628233811e-05,
      "loss": 0.5258,
      "num_input_tokens_seen": 17945440,
      "step": 1430,
      "train_runtime": 18391.153,
      "train_tokens_per_second": 975.765
    },
    {
      "epoch": 0.290016168148747,
      "grad_norm": 1.3617759943008423,
      "learning_rate": 4.8857463411467325e-05,
      "loss": 0.565,
      "num_input_tokens_seen": 18009408,
      "step": 1435,
      "train_runtime": 18456.6697,
      "train_tokens_per_second": 975.767
    },
    {
      "epoch": 0.2910266774454325,
      "grad_norm": 1.923537254333496,
      "learning_rate": 4.8849543825171465e-05,
      "loss": 0.5128,
      "num_input_tokens_seen": 18072160,
      "step": 1440,
      "train_runtime": 18520.6588,
      "train_tokens_per_second": 975.784
    },
    {
      "epoch": 0.29203718674211804,
      "grad_norm": 1.5626420974731445,
      "learning_rate": 4.8841597532318827e-05,
      "loss": 0.3803,
      "num_input_tokens_seen": 18134208,
      "step": 1445,
      "train_runtime": 18584.4929,
      "train_tokens_per_second": 975.771
    },
    {
      "epoch": 0.29304769603880354,
      "grad_norm": 1.5253312587738037,
      "learning_rate": 4.8833624541807625e-05,
      "loss": 0.6036,
      "num_input_tokens_seen": 18196800,
      "step": 1450,
      "train_runtime": 18648.6489,
      "train_tokens_per_second": 975.77
    },
    {
      "epoch": 0.2940582053354891,
      "grad_norm": 2.5415525436401367,
      "learning_rate": 4.882562486256596e-05,
      "loss": 0.6562,
      "num_input_tokens_seen": 18259360,
      "step": 1455,
      "train_runtime": 18712.8025,
      "train_tokens_per_second": 975.768
    },
    {
      "epoch": 0.2950687146321746,
      "grad_norm": 1.7288944721221924,
      "learning_rate": 4.8817598503551823e-05,
      "loss": 0.5224,
      "num_input_tokens_seen": 18321200,
      "step": 1460,
      "train_runtime": 18776.5171,
      "train_tokens_per_second": 975.751
    },
    {
      "epoch": 0.29607922392886016,
      "grad_norm": 2.41684627532959,
      "learning_rate": 4.880954547375306e-05,
      "loss": 0.5673,
      "num_input_tokens_seen": 18384864,
      "step": 1465,
      "train_runtime": 18841.3454,
      "train_tokens_per_second": 975.772
    },
    {
      "epoch": 0.29708973322554566,
      "grad_norm": 1.655532717704773,
      "learning_rate": 4.8801465782187406e-05,
      "loss": 0.5455,
      "num_input_tokens_seen": 18447824,
      "step": 1470,
      "train_runtime": 18905.6139,
      "train_tokens_per_second": 975.786
    },
    {
      "epoch": 0.2981002425222312,
      "grad_norm": 1.9508646726608276,
      "learning_rate": 4.8793359437902455e-05,
      "loss": 0.5459,
      "num_input_tokens_seen": 18511504,
      "step": 1475,
      "train_runtime": 18970.6411,
      "train_tokens_per_second": 975.797
    },
    {
      "epoch": 0.2991107518189167,
      "grad_norm": 1.8227324485778809,
      "learning_rate": 4.878522644997562e-05,
      "loss": 0.5789,
      "num_input_tokens_seen": 18574320,
      "step": 1480,
      "train_runtime": 19034.8688,
      "train_tokens_per_second": 975.805
    },
    {
      "epoch": 0.3001212611156023,
      "grad_norm": 1.4144004583358765,
      "learning_rate": 4.877706682751418e-05,
      "loss": 0.4625,
      "num_input_tokens_seen": 18636944,
      "step": 1485,
      "train_runtime": 19099.2786,
      "train_tokens_per_second": 975.793
    },
    {
      "epoch": 0.3011317704122878,
      "grad_norm": 1.8599655628204346,
      "learning_rate": 4.876888057965522e-05,
      "loss": 0.5315,
      "num_input_tokens_seen": 18699312,
      "step": 1490,
      "train_runtime": 19163.0965,
      "train_tokens_per_second": 975.798
    },
    {
      "epoch": 0.30214227970897334,
      "grad_norm": 2.326120376586914,
      "learning_rate": 4.876066771556564e-05,
      "loss": 0.684,
      "num_input_tokens_seen": 18761632,
      "step": 1495,
      "train_runtime": 19226.9641,
      "train_tokens_per_second": 975.798
    },
    {
      "epoch": 0.30315278900565884,
      "grad_norm": 1.7509257793426514,
      "learning_rate": 4.875242824444215e-05,
      "loss": 0.4631,
      "num_input_tokens_seen": 18825360,
      "step": 1500,
      "train_runtime": 19292.0888,
      "train_tokens_per_second": 975.807
    },
    {
      "epoch": 0.3041632983023444,
      "grad_norm": 1.7182096242904663,
      "learning_rate": 4.8744162175511253e-05,
      "loss": 0.598,
      "num_input_tokens_seen": 18888192,
      "step": 1505,
      "train_runtime": 19357.4486,
      "train_tokens_per_second": 975.758
    },
    {
      "epoch": 0.3051738075990299,
      "grad_norm": 1.708662748336792,
      "learning_rate": 4.873586951802925e-05,
      "loss": 0.5534,
      "num_input_tokens_seen": 18950656,
      "step": 1510,
      "train_runtime": 19421.4671,
      "train_tokens_per_second": 975.758
    },
    {
      "epoch": 0.30618431689571546,
      "grad_norm": 1.6756752729415894,
      "learning_rate": 4.872755028128219e-05,
      "loss": 0.6042,
      "num_input_tokens_seen": 19013008,
      "step": 1515,
      "train_runtime": 19485.2788,
      "train_tokens_per_second": 975.763
    },
    {
      "epoch": 0.30719482619240096,
      "grad_norm": 1.7622144222259521,
      "learning_rate": 4.871920447458589e-05,
      "loss": 0.5785,
      "num_input_tokens_seen": 19075776,
      "step": 1520,
      "train_runtime": 19549.4838,
      "train_tokens_per_second": 975.769
    },
    {
      "epoch": 0.3082053354890865,
      "grad_norm": 1.8716739416122437,
      "learning_rate": 4.871083210728594e-05,
      "loss": 0.3639,
      "num_input_tokens_seen": 19139456,
      "step": 1525,
      "train_runtime": 19614.4738,
      "train_tokens_per_second": 975.782
    },
    {
      "epoch": 0.309215844785772,
      "grad_norm": 2.522054672241211,
      "learning_rate": 4.8702433188757653e-05,
      "loss": 0.7146,
      "num_input_tokens_seen": 19201776,
      "step": 1530,
      "train_runtime": 19678.76,
      "train_tokens_per_second": 975.761
    },
    {
      "epoch": 0.3102263540824576,
      "grad_norm": 1.8667172193527222,
      "learning_rate": 4.86940077284061e-05,
      "loss": 0.5227,
      "num_input_tokens_seen": 19264944,
      "step": 1535,
      "train_runtime": 19743.5011,
      "train_tokens_per_second": 975.761
    },
    {
      "epoch": 0.3112368633791431,
      "grad_norm": 1.9262158870697021,
      "learning_rate": 4.868555573566602e-05,
      "loss": 0.5602,
      "num_input_tokens_seen": 19327136,
      "step": 1540,
      "train_runtime": 19807.3363,
      "train_tokens_per_second": 975.756
    },
    {
      "epoch": 0.31224737267582864,
      "grad_norm": 1.4575457572937012,
      "learning_rate": 4.867707722000193e-05,
      "loss": 0.4221,
      "num_input_tokens_seen": 19389792,
      "step": 1545,
      "train_runtime": 19871.6028,
      "train_tokens_per_second": 975.754
    },
    {
      "epoch": 0.31325788197251414,
      "grad_norm": 1.0929795503616333,
      "learning_rate": 4.8668572190908e-05,
      "loss": 0.4242,
      "num_input_tokens_seen": 19452288,
      "step": 1550,
      "train_runtime": 19935.6226,
      "train_tokens_per_second": 975.755
    },
    {
      "epoch": 0.3142683912691997,
      "grad_norm": 1.6170650720596313,
      "learning_rate": 4.866004065790809e-05,
      "loss": 0.5926,
      "num_input_tokens_seen": 19515392,
      "step": 1555,
      "train_runtime": 20000.4836,
      "train_tokens_per_second": 975.746
    },
    {
      "epoch": 0.3152789005658852,
      "grad_norm": 1.6265442371368408,
      "learning_rate": 4.865148263055578e-05,
      "loss": 0.5221,
      "num_input_tokens_seen": 19579552,
      "step": 1560,
      "train_runtime": 20065.7297,
      "train_tokens_per_second": 975.771
    },
    {
      "epoch": 0.31628940986257076,
      "grad_norm": 3.076514482498169,
      "learning_rate": 4.8642898118434264e-05,
      "loss": 0.6174,
      "num_input_tokens_seen": 19641792,
      "step": 1565,
      "train_runtime": 20129.5213,
      "train_tokens_per_second": 975.77
    },
    {
      "epoch": 0.31729991915925626,
      "grad_norm": 2.127314805984497,
      "learning_rate": 4.863428713115644e-05,
      "loss": 0.4708,
      "num_input_tokens_seen": 19704656,
      "step": 1570,
      "train_runtime": 20193.953,
      "train_tokens_per_second": 975.77
    },
    {
      "epoch": 0.3183104284559418,
      "grad_norm": 1.9193264245986938,
      "learning_rate": 4.8625649678364826e-05,
      "loss": 0.7397,
      "num_input_tokens_seen": 19767312,
      "step": 1575,
      "train_runtime": 20258.2886,
      "train_tokens_per_second": 975.764
    },
    {
      "epoch": 0.3193209377526273,
      "grad_norm": 1.7302488088607788,
      "learning_rate": 4.861698576973158e-05,
      "loss": 0.6927,
      "num_input_tokens_seen": 19829968,
      "step": 1580,
      "train_runtime": 20322.7824,
      "train_tokens_per_second": 975.751
    },
    {
      "epoch": 0.3203314470493129,
      "grad_norm": 3.5616636276245117,
      "learning_rate": 4.86082954149585e-05,
      "loss": 0.6162,
      "num_input_tokens_seen": 19892048,
      "step": 1585,
      "train_runtime": 20386.4832,
      "train_tokens_per_second": 975.747
    },
    {
      "epoch": 0.3213419563459984,
      "grad_norm": 2.1608941555023193,
      "learning_rate": 4.8599578623776964e-05,
      "loss": 0.8213,
      "num_input_tokens_seen": 19954256,
      "step": 1590,
      "train_runtime": 20450.2208,
      "train_tokens_per_second": 975.748
    },
    {
      "epoch": 0.32235246564268394,
      "grad_norm": 1.3799066543579102,
      "learning_rate": 4.8590835405948e-05,
      "loss": 0.6739,
      "num_input_tokens_seen": 20017520,
      "step": 1595,
      "train_runtime": 20514.7868,
      "train_tokens_per_second": 975.761
    },
    {
      "epoch": 0.32336297493936944,
      "grad_norm": 1.715981364250183,
      "learning_rate": 4.8582065771262185e-05,
      "loss": 0.5721,
      "num_input_tokens_seen": 20079872,
      "step": 1600,
      "train_runtime": 20578.636,
      "train_tokens_per_second": 975.763
    },
    {
      "epoch": 0.324373484236055,
      "grad_norm": 1.3662761449813843,
      "learning_rate": 4.8573269729539706e-05,
      "loss": 0.5834,
      "num_input_tokens_seen": 20142784,
      "step": 1605,
      "train_runtime": 20643.898,
      "train_tokens_per_second": 975.726
    },
    {
      "epoch": 0.3253839935327405,
      "grad_norm": 2.5306766033172607,
      "learning_rate": 4.85644472906303e-05,
      "loss": 0.4889,
      "num_input_tokens_seen": 20206000,
      "step": 1610,
      "train_runtime": 20708.5793,
      "train_tokens_per_second": 975.731
    },
    {
      "epoch": 0.326394502829426,
      "grad_norm": 2.362455368041992,
      "learning_rate": 4.8555598464413285e-05,
      "loss": 0.6433,
      "num_input_tokens_seen": 20268464,
      "step": 1615,
      "train_runtime": 20772.6363,
      "train_tokens_per_second": 975.729
    },
    {
      "epoch": 0.32740501212611156,
      "grad_norm": 1.7771496772766113,
      "learning_rate": 4.8546723260797525e-05,
      "loss": 0.5764,
      "num_input_tokens_seen": 20330720,
      "step": 1620,
      "train_runtime": 20836.311,
      "train_tokens_per_second": 975.735
    },
    {
      "epoch": 0.32841552142279706,
      "grad_norm": 1.3814650774002075,
      "learning_rate": 4.8537821689721384e-05,
      "loss": 0.6391,
      "num_input_tokens_seen": 20393488,
      "step": 1625,
      "train_runtime": 20900.7019,
      "train_tokens_per_second": 975.732
    },
    {
      "epoch": 0.3294260307194826,
      "grad_norm": 1.7514578104019165,
      "learning_rate": 4.85288937611528e-05,
      "loss": 0.5746,
      "num_input_tokens_seen": 20455728,
      "step": 1630,
      "train_runtime": 20964.6045,
      "train_tokens_per_second": 975.727
    },
    {
      "epoch": 0.3304365400161681,
      "grad_norm": 2.849400520324707,
      "learning_rate": 4.851993948508922e-05,
      "loss": 0.826,
      "num_input_tokens_seen": 20518352,
      "step": 1635,
      "train_runtime": 21028.8286,
      "train_tokens_per_second": 975.725
    },
    {
      "epoch": 0.3314470493128537,
      "grad_norm": 1.6299289464950562,
      "learning_rate": 4.851095887155754e-05,
      "loss": 0.5185,
      "num_input_tokens_seen": 20580496,
      "step": 1640,
      "train_runtime": 21092.6208,
      "train_tokens_per_second": 975.72
    },
    {
      "epoch": 0.3324575586095392,
      "grad_norm": 1.4274083375930786,
      "learning_rate": 4.850195193061423e-05,
      "loss": 0.6423,
      "num_input_tokens_seen": 20642912,
      "step": 1645,
      "train_runtime": 21156.6083,
      "train_tokens_per_second": 975.719
    },
    {
      "epoch": 0.33346806790622474,
      "grad_norm": 1.8098825216293335,
      "learning_rate": 4.849291867234519e-05,
      "loss": 0.6119,
      "num_input_tokens_seen": 20707248,
      "step": 1650,
      "train_runtime": 21222.5749,
      "train_tokens_per_second": 975.718
    },
    {
      "epoch": 0.33447857720291024,
      "grad_norm": 4.092645168304443,
      "learning_rate": 4.848385910686579e-05,
      "loss": 0.5492,
      "num_input_tokens_seen": 20770736,
      "step": 1655,
      "train_runtime": 21287.3805,
      "train_tokens_per_second": 975.73
    },
    {
      "epoch": 0.3354890864995958,
      "grad_norm": 3.8554623126983643,
      "learning_rate": 4.847477324432088e-05,
      "loss": 0.622,
      "num_input_tokens_seen": 20832288,
      "step": 1660,
      "train_runtime": 21350.7313,
      "train_tokens_per_second": 975.718
    },
    {
      "epoch": 0.3364995957962813,
      "grad_norm": 2.663052558898926,
      "learning_rate": 4.8465661094884745e-05,
      "loss": 0.5081,
      "num_input_tokens_seen": 20895040,
      "step": 1665,
      "train_runtime": 21415.043,
      "train_tokens_per_second": 975.718
    },
    {
      "epoch": 0.33751010509296686,
      "grad_norm": 1.9359393119812012,
      "learning_rate": 4.845652266876111e-05,
      "loss": 0.4786,
      "num_input_tokens_seen": 20957136,
      "step": 1670,
      "train_runtime": 21479.0247,
      "train_tokens_per_second": 975.702
    },
    {
      "epoch": 0.33852061438965236,
      "grad_norm": 1.3138741254806519,
      "learning_rate": 4.844735797618311e-05,
      "loss": 0.5061,
      "num_input_tokens_seen": 21020880,
      "step": 1675,
      "train_runtime": 21543.9154,
      "train_tokens_per_second": 975.722
    },
    {
      "epoch": 0.3395311236863379,
      "grad_norm": 1.692830204963684,
      "learning_rate": 4.8438167027413316e-05,
      "loss": 0.5069,
      "num_input_tokens_seen": 21084080,
      "step": 1680,
      "train_runtime": 21608.5561,
      "train_tokens_per_second": 975.728
    },
    {
      "epoch": 0.3405416329830234,
      "grad_norm": 1.8101788759231567,
      "learning_rate": 4.84289498327437e-05,
      "loss": 0.5286,
      "num_input_tokens_seen": 21146640,
      "step": 1685,
      "train_runtime": 21672.6326,
      "train_tokens_per_second": 975.73
    },
    {
      "epoch": 0.341552142279709,
      "grad_norm": 1.1981191635131836,
      "learning_rate": 4.841970640249559e-05,
      "loss": 0.5434,
      "num_input_tokens_seen": 21209136,
      "step": 1690,
      "train_runtime": 21736.7866,
      "train_tokens_per_second": 975.725
    },
    {
      "epoch": 0.3425626515763945,
      "grad_norm": 1.3577014207839966,
      "learning_rate": 4.8410436747019734e-05,
      "loss": 0.4168,
      "num_input_tokens_seen": 21272160,
      "step": 1695,
      "train_runtime": 21801.4136,
      "train_tokens_per_second": 975.724
    },
    {
      "epoch": 0.34357316087308004,
      "grad_norm": 4.692377090454102,
      "learning_rate": 4.840114087669622e-05,
      "loss": 0.5813,
      "num_input_tokens_seen": 21335488,
      "step": 1700,
      "train_runtime": 21866.1354,
      "train_tokens_per_second": 975.732
    },
    {
      "epoch": 0.34458367016976554,
      "grad_norm": 2.2779791355133057,
      "learning_rate": 4.839181880193451e-05,
      "loss": 0.7125,
      "num_input_tokens_seen": 21398784,
      "step": 1705,
      "train_runtime": 21931.5014,
      "train_tokens_per_second": 975.71
    },
    {
      "epoch": 0.3455941794664511,
      "grad_norm": 1.0946977138519287,
      "learning_rate": 4.838247053317339e-05,
      "loss": 0.4778,
      "num_input_tokens_seen": 21461360,
      "step": 1710,
      "train_runtime": 21995.6673,
      "train_tokens_per_second": 975.709
    },
    {
      "epoch": 0.3466046887631366,
      "grad_norm": 2.7339019775390625,
      "learning_rate": 4.837309608088099e-05,
      "loss": 0.7134,
      "num_input_tokens_seen": 21524304,
      "step": 1715,
      "train_runtime": 22060.3843,
      "train_tokens_per_second": 975.699
    },
    {
      "epoch": 0.34761519805982216,
      "grad_norm": 1.1877027750015259,
      "learning_rate": 4.836369545555476e-05,
      "loss": 0.4513,
      "num_input_tokens_seen": 21586608,
      "step": 1720,
      "train_runtime": 22124.154,
      "train_tokens_per_second": 975.703
    },
    {
      "epoch": 0.34862570735650766,
      "grad_norm": 2.464203119277954,
      "learning_rate": 4.835426866772146e-05,
      "loss": 0.5265,
      "num_input_tokens_seen": 21650352,
      "step": 1725,
      "train_runtime": 22189.179,
      "train_tokens_per_second": 975.717
    },
    {
      "epoch": 0.3496362166531932,
      "grad_norm": 2.1910669803619385,
      "learning_rate": 4.834481572793714e-05,
      "loss": 0.5227,
      "num_input_tokens_seen": 21713968,
      "step": 1730,
      "train_runtime": 22254.3092,
      "train_tokens_per_second": 975.72
    },
    {
      "epoch": 0.3506467259498787,
      "grad_norm": 2.6002161502838135,
      "learning_rate": 4.833533664678713e-05,
      "loss": 0.4968,
      "num_input_tokens_seen": 21775088,
      "step": 1735,
      "train_runtime": 22317.4233,
      "train_tokens_per_second": 975.699
    },
    {
      "epoch": 0.3516572352465643,
      "grad_norm": 3.0826144218444824,
      "learning_rate": 4.832583143488605e-05,
      "loss": 0.437,
      "num_input_tokens_seen": 21837392,
      "step": 1740,
      "train_runtime": 22381.2117,
      "train_tokens_per_second": 975.702
    },
    {
      "epoch": 0.3526677445432498,
      "grad_norm": 2.722979784011841,
      "learning_rate": 4.8316300102877766e-05,
      "loss": 0.5511,
      "num_input_tokens_seen": 21900176,
      "step": 1745,
      "train_runtime": 22445.496,
      "train_tokens_per_second": 975.705
    },
    {
      "epoch": 0.35367825383993534,
      "grad_norm": 1.7133607864379883,
      "learning_rate": 4.83067426614354e-05,
      "loss": 0.5074,
      "num_input_tokens_seen": 21962624,
      "step": 1750,
      "train_runtime": 22509.6285,
      "train_tokens_per_second": 975.699
    },
    {
      "epoch": 0.35468876313662084,
      "grad_norm": 1.9941431283950806,
      "learning_rate": 4.8297159121261315e-05,
      "loss": 0.5081,
      "num_input_tokens_seen": 22025360,
      "step": 1755,
      "train_runtime": 22573.9449,
      "train_tokens_per_second": 975.698
    },
    {
      "epoch": 0.3556992724333064,
      "grad_norm": 1.4987860918045044,
      "learning_rate": 4.828754949308708e-05,
      "loss": 0.4241,
      "num_input_tokens_seen": 22088544,
      "step": 1760,
      "train_runtime": 22638.7056,
      "train_tokens_per_second": 975.698
    },
    {
      "epoch": 0.3567097817299919,
      "grad_norm": 1.7710025310516357,
      "learning_rate": 4.827791378767351e-05,
      "loss": 0.5616,
      "num_input_tokens_seen": 22150416,
      "step": 1765,
      "train_runtime": 22702.3045,
      "train_tokens_per_second": 975.69
    },
    {
      "epoch": 0.35772029102667746,
      "grad_norm": 1.935526967048645,
      "learning_rate": 4.826825201581059e-05,
      "loss": 0.5255,
      "num_input_tokens_seen": 22212592,
      "step": 1770,
      "train_runtime": 22766.1247,
      "train_tokens_per_second": 975.686
    },
    {
      "epoch": 0.35873080032336296,
      "grad_norm": 1.295298457145691,
      "learning_rate": 4.8258564188317515e-05,
      "loss": 0.486,
      "num_input_tokens_seen": 22276256,
      "step": 1775,
      "train_runtime": 22831.1155,
      "train_tokens_per_second": 975.697
    },
    {
      "epoch": 0.3597413096200485,
      "grad_norm": 1.7367733716964722,
      "learning_rate": 4.824885031604264e-05,
      "loss": 0.5737,
      "num_input_tokens_seen": 22339104,
      "step": 1780,
      "train_runtime": 22895.3906,
      "train_tokens_per_second": 975.703
    },
    {
      "epoch": 0.360751818916734,
      "grad_norm": 1.8711543083190918,
      "learning_rate": 4.8239110409863486e-05,
      "loss": 0.5011,
      "num_input_tokens_seen": 22401520,
      "step": 1785,
      "train_runtime": 22959.6083,
      "train_tokens_per_second": 975.693
    },
    {
      "epoch": 0.3617623282134196,
      "grad_norm": 2.1925411224365234,
      "learning_rate": 4.8229344480686755e-05,
      "loss": 0.4183,
      "num_input_tokens_seen": 22465600,
      "step": 1790,
      "train_runtime": 23024.9599,
      "train_tokens_per_second": 975.706
    },
    {
      "epoch": 0.3627728375101051,
      "grad_norm": 2.4854328632354736,
      "learning_rate": 4.821955253944826e-05,
      "loss": 0.4896,
      "num_input_tokens_seen": 22528416,
      "step": 1795,
      "train_runtime": 23089.0793,
      "train_tokens_per_second": 975.717
    },
    {
      "epoch": 0.36378334680679064,
      "grad_norm": 1.5440609455108643,
      "learning_rate": 4.8209734597112935e-05,
      "loss": 0.4581,
      "num_input_tokens_seen": 22591152,
      "step": 1800,
      "train_runtime": 23153.3421,
      "train_tokens_per_second": 975.719
    },
    {
      "epoch": 0.36479385610347614,
      "grad_norm": 2.2477779388427734,
      "learning_rate": 4.8199890664674857e-05,
      "loss": 0.5575,
      "num_input_tokens_seen": 22653872,
      "step": 1805,
      "train_runtime": 23218.4933,
      "train_tokens_per_second": 975.682
    },
    {
      "epoch": 0.3658043654001617,
      "grad_norm": 1.755066156387329,
      "learning_rate": 4.819002075315719e-05,
      "loss": 0.4409,
      "num_input_tokens_seen": 22716592,
      "step": 1810,
      "train_runtime": 23282.8002,
      "train_tokens_per_second": 975.681
    },
    {
      "epoch": 0.3668148746968472,
      "grad_norm": 2.7436773777008057,
      "learning_rate": 4.8180124873612206e-05,
      "loss": 0.5409,
      "num_input_tokens_seen": 22779936,
      "step": 1815,
      "train_runtime": 23347.6017,
      "train_tokens_per_second": 975.686
    },
    {
      "epoch": 0.36782538399353276,
      "grad_norm": 1.853756070137024,
      "learning_rate": 4.817020303712124e-05,
      "loss": 0.4438,
      "num_input_tokens_seen": 22843184,
      "step": 1820,
      "train_runtime": 23412.2429,
      "train_tokens_per_second": 975.694
    },
    {
      "epoch": 0.36883589329021826,
      "grad_norm": 1.432997226715088,
      "learning_rate": 4.8160255254794684e-05,
      "loss": 0.6467,
      "num_input_tokens_seen": 22906096,
      "step": 1825,
      "train_runtime": 23476.57,
      "train_tokens_per_second": 975.7
    },
    {
      "epoch": 0.3698464025869038,
      "grad_norm": 1.857452392578125,
      "learning_rate": 4.8150281537772016e-05,
      "loss": 0.5589,
      "num_input_tokens_seen": 22968544,
      "step": 1830,
      "train_runtime": 23540.8468,
      "train_tokens_per_second": 975.689
    },
    {
      "epoch": 0.3708569118835893,
      "grad_norm": 2.0928189754486084,
      "learning_rate": 4.8140281897221715e-05,
      "loss": 0.5667,
      "num_input_tokens_seen": 23031520,
      "step": 1835,
      "train_runtime": 23605.2359,
      "train_tokens_per_second": 975.695
    },
    {
      "epoch": 0.3718674211802749,
      "grad_norm": 2.1340529918670654,
      "learning_rate": 4.813025634434132e-05,
      "loss": 0.5066,
      "num_input_tokens_seen": 23094448,
      "step": 1840,
      "train_runtime": 23669.6228,
      "train_tokens_per_second": 975.7
    },
    {
      "epoch": 0.3728779304769604,
      "grad_norm": 2.3517978191375732,
      "learning_rate": 4.812020489035738e-05,
      "loss": 0.3901,
      "num_input_tokens_seen": 23156880,
      "step": 1845,
      "train_runtime": 23733.8653,
      "train_tokens_per_second": 975.689
    },
    {
      "epoch": 0.37388843977364594,
      "grad_norm": 1.9761152267456055,
      "learning_rate": 4.8110127546525443e-05,
      "loss": 0.5027,
      "num_input_tokens_seen": 23219760,
      "step": 1850,
      "train_runtime": 23798.6301,
      "train_tokens_per_second": 975.676
    },
    {
      "epoch": 0.37489894907033144,
      "grad_norm": 2.174785852432251,
      "learning_rate": 4.810002432413005e-05,
      "loss": 0.5955,
      "num_input_tokens_seen": 23281008,
      "step": 1855,
      "train_runtime": 23861.6463,
      "train_tokens_per_second": 975.666
    },
    {
      "epoch": 0.375909458367017,
      "grad_norm": 1.4305042028427124,
      "learning_rate": 4.8089895234484715e-05,
      "loss": 0.4503,
      "num_input_tokens_seen": 23342992,
      "step": 1860,
      "train_runtime": 23925.5467,
      "train_tokens_per_second": 975.651
    },
    {
      "epoch": 0.3769199676637025,
      "grad_norm": 1.6115118265151978,
      "learning_rate": 4.807974028893194e-05,
      "loss": 0.4637,
      "num_input_tokens_seen": 23405728,
      "step": 1865,
      "train_runtime": 23989.8937,
      "train_tokens_per_second": 975.65
    },
    {
      "epoch": 0.37793047696038806,
      "grad_norm": 2.1996607780456543,
      "learning_rate": 4.806955949884314e-05,
      "loss": 0.574,
      "num_input_tokens_seen": 23468496,
      "step": 1870,
      "train_runtime": 24054.3376,
      "train_tokens_per_second": 975.645
    },
    {
      "epoch": 0.37894098625707356,
      "grad_norm": 2.3207812309265137,
      "learning_rate": 4.805935287561872e-05,
      "loss": 0.5196,
      "num_input_tokens_seen": 23532240,
      "step": 1875,
      "train_runtime": 24119.4497,
      "train_tokens_per_second": 975.654
    },
    {
      "epoch": 0.3799514955537591,
      "grad_norm": 1.6301788091659546,
      "learning_rate": 4.8049120430687965e-05,
      "loss": 0.4862,
      "num_input_tokens_seen": 23594320,
      "step": 1880,
      "train_runtime": 24183.2579,
      "train_tokens_per_second": 975.647
    },
    {
      "epoch": 0.3809620048504446,
      "grad_norm": 1.0526773929595947,
      "learning_rate": 4.803886217550911e-05,
      "loss": 0.4978,
      "num_input_tokens_seen": 23656032,
      "step": 1885,
      "train_runtime": 24246.9614,
      "train_tokens_per_second": 975.629
    },
    {
      "epoch": 0.3819725141471301,
      "grad_norm": 2.7206509113311768,
      "learning_rate": 4.8028578121569275e-05,
      "loss": 0.7475,
      "num_input_tokens_seen": 23717888,
      "step": 1890,
      "train_runtime": 24310.4218,
      "train_tokens_per_second": 975.626
    },
    {
      "epoch": 0.3829830234438157,
      "grad_norm": 2.2876992225646973,
      "learning_rate": 4.801826828038447e-05,
      "loss": 0.5009,
      "num_input_tokens_seen": 23780464,
      "step": 1895,
      "train_runtime": 24374.5894,
      "train_tokens_per_second": 975.625
    },
    {
      "epoch": 0.3839935327405012,
      "grad_norm": 2.5558865070343018,
      "learning_rate": 4.8007932663499596e-05,
      "loss": 0.6105,
      "num_input_tokens_seen": 23841968,
      "step": 1900,
      "train_runtime": 24438.0365,
      "train_tokens_per_second": 975.609
    },
    {
      "epoch": 0.38500404203718674,
      "grad_norm": 1.1633703708648682,
      "learning_rate": 4.79975712824884e-05,
      "loss": 0.4993,
      "num_input_tokens_seen": 23905728,
      "step": 1905,
      "train_runtime": 24503.7076,
      "train_tokens_per_second": 975.596
    },
    {
      "epoch": 0.38601455133387225,
      "grad_norm": 2.794238328933716,
      "learning_rate": 4.798718414895348e-05,
      "loss": 0.4953,
      "num_input_tokens_seen": 23969360,
      "step": 1910,
      "train_runtime": 24568.8256,
      "train_tokens_per_second": 975.601
    },
    {
      "epoch": 0.3870250606305578,
      "grad_norm": 1.7181602716445923,
      "learning_rate": 4.797677127452628e-05,
      "loss": 0.5682,
      "num_input_tokens_seen": 24032208,
      "step": 1915,
      "train_runtime": 24633.167,
      "train_tokens_per_second": 975.604
    },
    {
      "epoch": 0.3880355699272433,
      "grad_norm": 1.363006353378296,
      "learning_rate": 4.796633267086708e-05,
      "loss": 0.4616,
      "num_input_tokens_seen": 24093776,
      "step": 1920,
      "train_runtime": 24696.7684,
      "train_tokens_per_second": 975.584
    },
    {
      "epoch": 0.38904607922392886,
      "grad_norm": 1.4504339694976807,
      "learning_rate": 4.795586834966494e-05,
      "loss": 0.5134,
      "num_input_tokens_seen": 24155520,
      "step": 1925,
      "train_runtime": 24760.4007,
      "train_tokens_per_second": 975.571
    },
    {
      "epoch": 0.39005658852061437,
      "grad_norm": 1.557482123374939,
      "learning_rate": 4.794537832263773e-05,
      "loss": 0.4406,
      "num_input_tokens_seen": 24219264,
      "step": 1930,
      "train_runtime": 24825.7586,
      "train_tokens_per_second": 975.57
    },
    {
      "epoch": 0.3910670978172999,
      "grad_norm": 1.9616872072219849,
      "learning_rate": 4.793486260153214e-05,
      "loss": 0.6329,
      "num_input_tokens_seen": 24282544,
      "step": 1935,
      "train_runtime": 24890.5345,
      "train_tokens_per_second": 975.573
    },
    {
      "epoch": 0.3920776071139854,
      "grad_norm": 1.4244763851165771,
      "learning_rate": 4.7924321198123577e-05,
      "loss": 0.3978,
      "num_input_tokens_seen": 24345472,
      "step": 1940,
      "train_runtime": 24955.3681,
      "train_tokens_per_second": 975.561
    },
    {
      "epoch": 0.393088116410671,
      "grad_norm": 1.4616844654083252,
      "learning_rate": 4.791375412421624e-05,
      "loss": 0.4754,
      "num_input_tokens_seen": 24407712,
      "step": 1945,
      "train_runtime": 25019.4382,
      "train_tokens_per_second": 975.55
    },
    {
      "epoch": 0.3940986257073565,
      "grad_norm": 1.8156288862228394,
      "learning_rate": 4.790316139164307e-05,
      "loss": 0.491,
      "num_input_tokens_seen": 24470144,
      "step": 1950,
      "train_runtime": 25083.4729,
      "train_tokens_per_second": 975.548
    },
    {
      "epoch": 0.39510913500404204,
      "grad_norm": 1.825763463973999,
      "learning_rate": 4.7892543012265735e-05,
      "loss": 0.5409,
      "num_input_tokens_seen": 24533664,
      "step": 1955,
      "train_runtime": 25148.4235,
      "train_tokens_per_second": 975.555
    },
    {
      "epoch": 0.39611964430072755,
      "grad_norm": 1.1889376640319824,
      "learning_rate": 4.7881898997974626e-05,
      "loss": 0.5543,
      "num_input_tokens_seen": 24595952,
      "step": 1960,
      "train_runtime": 25212.3041,
      "train_tokens_per_second": 975.554
    },
    {
      "epoch": 0.3971301535974131,
      "grad_norm": 1.8799630403518677,
      "learning_rate": 4.787122936068883e-05,
      "loss": 0.4477,
      "num_input_tokens_seen": 24658928,
      "step": 1965,
      "train_runtime": 25276.8146,
      "train_tokens_per_second": 975.555
    },
    {
      "epoch": 0.3981406628940986,
      "grad_norm": 1.7844159603118896,
      "learning_rate": 4.786053411235614e-05,
      "loss": 0.516,
      "num_input_tokens_seen": 24721088,
      "step": 1970,
      "train_runtime": 25340.6818,
      "train_tokens_per_second": 975.549
    },
    {
      "epoch": 0.39915117219078416,
      "grad_norm": 1.5311998128890991,
      "learning_rate": 4.784981326495302e-05,
      "loss": 0.4801,
      "num_input_tokens_seen": 24783648,
      "step": 1975,
      "train_runtime": 25404.8858,
      "train_tokens_per_second": 975.547
    },
    {
      "epoch": 0.40016168148746967,
      "grad_norm": 3.650291681289673,
      "learning_rate": 4.7839066830484615e-05,
      "loss": 0.5473,
      "num_input_tokens_seen": 24846272,
      "step": 1980,
      "train_runtime": 25469.0362,
      "train_tokens_per_second": 975.548
    },
    {
      "epoch": 0.4011721907841552,
      "grad_norm": 2.021848201751709,
      "learning_rate": 4.7828294820984686e-05,
      "loss": 0.5177,
      "num_input_tokens_seen": 24908112,
      "step": 1985,
      "train_runtime": 25532.776,
      "train_tokens_per_second": 975.535
    },
    {
      "epoch": 0.4021827000808407,
      "grad_norm": 1.3244876861572266,
      "learning_rate": 4.781749724851567e-05,
      "loss": 0.4074,
      "num_input_tokens_seen": 24971024,
      "step": 1990,
      "train_runtime": 25597.4172,
      "train_tokens_per_second": 975.529
    },
    {
      "epoch": 0.4031932093775263,
      "grad_norm": 1.401671051979065,
      "learning_rate": 4.780667412516861e-05,
      "loss": 0.6098,
      "num_input_tokens_seen": 25033904,
      "step": 1995,
      "train_runtime": 25661.843,
      "train_tokens_per_second": 975.53
    },
    {
      "epoch": 0.4042037186742118,
      "grad_norm": 1.4706215858459473,
      "learning_rate": 4.779582546306317e-05,
      "loss": 0.4273,
      "num_input_tokens_seen": 25096080,
      "step": 2000,
      "train_runtime": 25725.631,
      "train_tokens_per_second": 975.528
    },
    {
      "epoch": 0.40521422797089734,
      "grad_norm": 4.067756652832031,
      "learning_rate": 4.778495127434761e-05,
      "loss": 0.4951,
      "num_input_tokens_seen": 25158448,
      "step": 2005,
      "train_runtime": 25790.6416,
      "train_tokens_per_second": 975.487
    },
    {
      "epoch": 0.40622473726758285,
      "grad_norm": 1.5422651767730713,
      "learning_rate": 4.777405157119876e-05,
      "loss": 0.4534,
      "num_input_tokens_seen": 25220688,
      "step": 2010,
      "train_runtime": 25854.5161,
      "train_tokens_per_second": 975.485
    },
    {
      "epoch": 0.4072352465642684,
      "grad_norm": 1.3048373460769653,
      "learning_rate": 4.776312636582205e-05,
      "loss": 0.479,
      "num_input_tokens_seen": 25283280,
      "step": 2015,
      "train_runtime": 25918.7929,
      "train_tokens_per_second": 975.481
    },
    {
      "epoch": 0.4082457558609539,
      "grad_norm": 2.1042211055755615,
      "learning_rate": 4.775217567045144e-05,
      "loss": 0.555,
      "num_input_tokens_seen": 25345712,
      "step": 2020,
      "train_runtime": 25982.7188,
      "train_tokens_per_second": 975.483
    },
    {
      "epoch": 0.40925626515763947,
      "grad_norm": 2.247394561767578,
      "learning_rate": 4.774119949734945e-05,
      "loss": 0.4853,
      "num_input_tokens_seen": 25408976,
      "step": 2025,
      "train_runtime": 26047.4576,
      "train_tokens_per_second": 975.488
    },
    {
      "epoch": 0.41026677445432497,
      "grad_norm": 1.3181017637252808,
      "learning_rate": 4.7730197858807116e-05,
      "loss": 0.3631,
      "num_input_tokens_seen": 25472208,
      "step": 2030,
      "train_runtime": 26112.233,
      "train_tokens_per_second": 975.489
    },
    {
      "epoch": 0.4112772837510105,
      "grad_norm": 1.2973747253417969,
      "learning_rate": 4.7719170767144005e-05,
      "loss": 0.6286,
      "num_input_tokens_seen": 25535008,
      "step": 2035,
      "train_runtime": 26176.6034,
      "train_tokens_per_second": 975.49
    },
    {
      "epoch": 0.412287793047696,
      "grad_norm": 1.6887165307998657,
      "learning_rate": 4.7708118234708175e-05,
      "loss": 0.4919,
      "num_input_tokens_seen": 25598464,
      "step": 2040,
      "train_runtime": 26241.5012,
      "train_tokens_per_second": 975.495
    },
    {
      "epoch": 0.4132983023443816,
      "grad_norm": 1.5379546880722046,
      "learning_rate": 4.769704027387618e-05,
      "loss": 0.4936,
      "num_input_tokens_seen": 25660880,
      "step": 2045,
      "train_runtime": 26305.6984,
      "train_tokens_per_second": 975.488
    },
    {
      "epoch": 0.4143088116410671,
      "grad_norm": 2.5711703300476074,
      "learning_rate": 4.7685936897053044e-05,
      "loss": 0.5301,
      "num_input_tokens_seen": 25723648,
      "step": 2050,
      "train_runtime": 26369.8337,
      "train_tokens_per_second": 975.495
    },
    {
      "epoch": 0.41531932093775265,
      "grad_norm": 2.426265239715576,
      "learning_rate": 4.767480811667225e-05,
      "loss": 0.5323,
      "num_input_tokens_seen": 25787472,
      "step": 2055,
      "train_runtime": 26435.449,
      "train_tokens_per_second": 975.488
    },
    {
      "epoch": 0.41632983023443815,
      "grad_norm": 1.6281192302703857,
      "learning_rate": 4.766365394519573e-05,
      "loss": 0.4304,
      "num_input_tokens_seen": 25849968,
      "step": 2060,
      "train_runtime": 26499.3766,
      "train_tokens_per_second": 975.493
    },
    {
      "epoch": 0.4173403395311237,
      "grad_norm": 2.193288564682007,
      "learning_rate": 4.765247439511386e-05,
      "loss": 0.4391,
      "num_input_tokens_seen": 25912432,
      "step": 2065,
      "train_runtime": 26563.2321,
      "train_tokens_per_second": 975.5
    },
    {
      "epoch": 0.4183508488278092,
      "grad_norm": 1.1912521123886108,
      "learning_rate": 4.764126947894541e-05,
      "loss": 0.5253,
      "num_input_tokens_seen": 25975040,
      "step": 2070,
      "train_runtime": 26627.4873,
      "train_tokens_per_second": 975.497
    },
    {
      "epoch": 0.41936135812449477,
      "grad_norm": 1.589071273803711,
      "learning_rate": 4.763003920923758e-05,
      "loss": 0.5516,
      "num_input_tokens_seen": 26037344,
      "step": 2075,
      "train_runtime": 26691.3873,
      "train_tokens_per_second": 975.496
    },
    {
      "epoch": 0.42037186742118027,
      "grad_norm": 1.3028534650802612,
      "learning_rate": 4.761878359856594e-05,
      "loss": 0.6885,
      "num_input_tokens_seen": 26100272,
      "step": 2080,
      "train_runtime": 26755.9141,
      "train_tokens_per_second": 975.495
    },
    {
      "epoch": 0.4213823767178658,
      "grad_norm": 2.253370523452759,
      "learning_rate": 4.760750265953445e-05,
      "loss": 0.5867,
      "num_input_tokens_seen": 26163584,
      "step": 2085,
      "train_runtime": 26820.5277,
      "train_tokens_per_second": 975.506
    },
    {
      "epoch": 0.4223928860145513,
      "grad_norm": 1.5055052042007446,
      "learning_rate": 4.759619640477543e-05,
      "loss": 0.4956,
      "num_input_tokens_seen": 26226848,
      "step": 2090,
      "train_runtime": 26885.2252,
      "train_tokens_per_second": 975.512
    },
    {
      "epoch": 0.4234033953112369,
      "grad_norm": 1.7898956537246704,
      "learning_rate": 4.758486484694955e-05,
      "loss": 0.4537,
      "num_input_tokens_seen": 26289744,
      "step": 2095,
      "train_runtime": 26949.5576,
      "train_tokens_per_second": 975.517
    },
    {
      "epoch": 0.4244139046079224,
      "grad_norm": 2.495243787765503,
      "learning_rate": 4.7573507998745805e-05,
      "loss": 0.5164,
      "num_input_tokens_seen": 26352512,
      "step": 2100,
      "train_runtime": 27013.8639,
      "train_tokens_per_second": 975.518
    },
    {
      "epoch": 0.42542441390460795,
      "grad_norm": 1.5315089225769043,
      "learning_rate": 4.756212587288151e-05,
      "loss": 0.5086,
      "num_input_tokens_seen": 26415536,
      "step": 2105,
      "train_runtime": 27079.2426,
      "train_tokens_per_second": 975.49
    },
    {
      "epoch": 0.42643492320129345,
      "grad_norm": 2.1081645488739014,
      "learning_rate": 4.7550718482102315e-05,
      "loss": 0.55,
      "num_input_tokens_seen": 26477424,
      "step": 2110,
      "train_runtime": 27142.8388,
      "train_tokens_per_second": 975.485
    },
    {
      "epoch": 0.427445432497979,
      "grad_norm": 1.6387313604354858,
      "learning_rate": 4.753928583918212e-05,
      "loss": 0.5103,
      "num_input_tokens_seen": 26540784,
      "step": 2115,
      "train_runtime": 27207.5567,
      "train_tokens_per_second": 975.493
    },
    {
      "epoch": 0.4284559417946645,
      "grad_norm": 1.9973325729370117,
      "learning_rate": 4.752782795692312e-05,
      "loss": 0.5923,
      "num_input_tokens_seen": 26604016,
      "step": 2120,
      "train_runtime": 27272.3357,
      "train_tokens_per_second": 975.495
    },
    {
      "epoch": 0.42946645109135007,
      "grad_norm": 1.3017288446426392,
      "learning_rate": 4.7516344848155786e-05,
      "loss": 0.4133,
      "num_input_tokens_seen": 26668000,
      "step": 2125,
      "train_runtime": 27337.9288,
      "train_tokens_per_second": 975.495
    },
    {
      "epoch": 0.43047696038803557,
      "grad_norm": 1.950661063194275,
      "learning_rate": 4.750483652573881e-05,
      "loss": 0.5133,
      "num_input_tokens_seen": 26730352,
      "step": 2130,
      "train_runtime": 27401.7842,
      "train_tokens_per_second": 975.497
    },
    {
      "epoch": 0.4314874696847211,
      "grad_norm": 1.4122635126113892,
      "learning_rate": 4.749330300255915e-05,
      "loss": 0.5442,
      "num_input_tokens_seen": 26793088,
      "step": 2135,
      "train_runtime": 27466.137,
      "train_tokens_per_second": 975.495
    },
    {
      "epoch": 0.43249797898140663,
      "grad_norm": 1.6474499702453613,
      "learning_rate": 4.7481744291531946e-05,
      "loss": 0.521,
      "num_input_tokens_seen": 26855168,
      "step": 2140,
      "train_runtime": 27529.9092,
      "train_tokens_per_second": 975.491
    },
    {
      "epoch": 0.4335084882780922,
      "grad_norm": 1.4681315422058105,
      "learning_rate": 4.7470160405600584e-05,
      "loss": 0.4092,
      "num_input_tokens_seen": 26918112,
      "step": 2145,
      "train_runtime": 27594.104,
      "train_tokens_per_second": 975.502
    },
    {
      "epoch": 0.4345189975747777,
      "grad_norm": 1.8733352422714233,
      "learning_rate": 4.745855135773661e-05,
      "loss": 0.7534,
      "num_input_tokens_seen": 26982048,
      "step": 2150,
      "train_runtime": 27659.548,
      "train_tokens_per_second": 975.506
    },
    {
      "epoch": 0.4355295068714632,
      "grad_norm": 1.7108174562454224,
      "learning_rate": 4.7446917160939765e-05,
      "loss": 0.5005,
      "num_input_tokens_seen": 27045520,
      "step": 2155,
      "train_runtime": 27724.4029,
      "train_tokens_per_second": 975.513
    },
    {
      "epoch": 0.43654001616814875,
      "grad_norm": 1.6778833866119385,
      "learning_rate": 4.743525782823793e-05,
      "loss": 0.6064,
      "num_input_tokens_seen": 27108960,
      "step": 2160,
      "train_runtime": 27789.2442,
      "train_tokens_per_second": 975.52
    },
    {
      "epoch": 0.43755052546483425,
      "grad_norm": 2.462682008743286,
      "learning_rate": 4.7423573372687155e-05,
      "loss": 0.4338,
      "num_input_tokens_seen": 27171984,
      "step": 2165,
      "train_runtime": 27853.6566,
      "train_tokens_per_second": 975.527
    },
    {
      "epoch": 0.4385610347615198,
      "grad_norm": 2.6549901962280273,
      "learning_rate": 4.74118638073716e-05,
      "loss": 0.5409,
      "num_input_tokens_seen": 27234096,
      "step": 2170,
      "train_runtime": 27917.4147,
      "train_tokens_per_second": 975.524
    },
    {
      "epoch": 0.4395715440582053,
      "grad_norm": 1.7239668369293213,
      "learning_rate": 4.7400129145403586e-05,
      "loss": 0.4957,
      "num_input_tokens_seen": 27296944,
      "step": 2175,
      "train_runtime": 27981.678,
      "train_tokens_per_second": 975.529
    },
    {
      "epoch": 0.44058205335489087,
      "grad_norm": 1.802200198173523,
      "learning_rate": 4.738836939992348e-05,
      "loss": 0.599,
      "num_input_tokens_seen": 27359776,
      "step": 2180,
      "train_runtime": 28046.066,
      "train_tokens_per_second": 975.53
    },
    {
      "epoch": 0.44159256265157637,
      "grad_norm": 1.5378830432891846,
      "learning_rate": 4.7376584584099784e-05,
      "loss": 0.545,
      "num_input_tokens_seen": 27421520,
      "step": 2185,
      "train_runtime": 28109.3191,
      "train_tokens_per_second": 975.531
    },
    {
      "epoch": 0.44260307194826193,
      "grad_norm": 1.9057164192199707,
      "learning_rate": 4.7364774711129045e-05,
      "loss": 0.516,
      "num_input_tokens_seen": 27484064,
      "step": 2190,
      "train_runtime": 28173.5976,
      "train_tokens_per_second": 975.526
    },
    {
      "epoch": 0.44361358124494743,
      "grad_norm": 1.90581214427948,
      "learning_rate": 4.735293979423587e-05,
      "loss": 0.5472,
      "num_input_tokens_seen": 27546832,
      "step": 2195,
      "train_runtime": 28237.7927,
      "train_tokens_per_second": 975.531
    },
    {
      "epoch": 0.444624090541633,
      "grad_norm": 1.5852165222167969,
      "learning_rate": 4.7341079846672934e-05,
      "loss": 0.51,
      "num_input_tokens_seen": 27609328,
      "step": 2200,
      "train_runtime": 28301.6371,
      "train_tokens_per_second": 975.538
    },
    {
      "epoch": 0.4456345998383185,
      "grad_norm": 1.0686362981796265,
      "learning_rate": 4.7329194881720916e-05,
      "loss": 0.4835,
      "num_input_tokens_seen": 27672480,
      "step": 2205,
      "train_runtime": 28367.1785,
      "train_tokens_per_second": 975.51
    },
    {
      "epoch": 0.44664510913500405,
      "grad_norm": 1.7081990242004395,
      "learning_rate": 4.731728491268852e-05,
      "loss": 0.4915,
      "num_input_tokens_seen": 27736704,
      "step": 2210,
      "train_runtime": 28432.6383,
      "train_tokens_per_second": 975.523
    },
    {
      "epoch": 0.44765561843168955,
      "grad_norm": 1.7609047889709473,
      "learning_rate": 4.730534995291246e-05,
      "loss": 0.5154,
      "num_input_tokens_seen": 27799872,
      "step": 2215,
      "train_runtime": 28497.3478,
      "train_tokens_per_second": 975.525
    },
    {
      "epoch": 0.4486661277283751,
      "grad_norm": 1.5803484916687012,
      "learning_rate": 4.72933900157574e-05,
      "loss": 0.7146,
      "num_input_tokens_seen": 27862352,
      "step": 2220,
      "train_runtime": 28561.5185,
      "train_tokens_per_second": 975.521
    },
    {
      "epoch": 0.4496766370250606,
      "grad_norm": 2.2223308086395264,
      "learning_rate": 4.728140511461602e-05,
      "loss": 0.5732,
      "num_input_tokens_seen": 27925872,
      "step": 2225,
      "train_runtime": 28626.9615,
      "train_tokens_per_second": 975.509
    },
    {
      "epoch": 0.45068714632174617,
      "grad_norm": 1.39057195186615,
      "learning_rate": 4.726939526290891e-05,
      "loss": 0.4246,
      "num_input_tokens_seen": 27989648,
      "step": 2230,
      "train_runtime": 28691.9922,
      "train_tokens_per_second": 975.521
    },
    {
      "epoch": 0.45169765561843167,
      "grad_norm": 1.5333452224731445,
      "learning_rate": 4.7257360474084635e-05,
      "loss": 0.4603,
      "num_input_tokens_seen": 28052560,
      "step": 2235,
      "train_runtime": 28756.3482,
      "train_tokens_per_second": 975.526
    },
    {
      "epoch": 0.45270816491511723,
      "grad_norm": 1.2734196186065674,
      "learning_rate": 4.724530076161966e-05,
      "loss": 0.5149,
      "num_input_tokens_seen": 28115376,
      "step": 2240,
      "train_runtime": 28820.6294,
      "train_tokens_per_second": 975.53
    },
    {
      "epoch": 0.45371867421180273,
      "grad_norm": 1.544837236404419,
      "learning_rate": 4.723321613901838e-05,
      "loss": 0.5244,
      "num_input_tokens_seen": 28178176,
      "step": 2245,
      "train_runtime": 28885.3442,
      "train_tokens_per_second": 975.518
    },
    {
      "epoch": 0.4547291835084883,
      "grad_norm": 1.5800353288650513,
      "learning_rate": 4.722110661981306e-05,
      "loss": 0.4403,
      "num_input_tokens_seen": 28242496,
      "step": 2250,
      "train_runtime": 28950.8896,
      "train_tokens_per_second": 975.531
    },
    {
      "epoch": 0.4557396928051738,
      "grad_norm": 2.2507569789886475,
      "learning_rate": 4.720897221756386e-05,
      "loss": 0.489,
      "num_input_tokens_seen": 28304848,
      "step": 2255,
      "train_runtime": 29014.6803,
      "train_tokens_per_second": 975.535
    },
    {
      "epoch": 0.45675020210185935,
      "grad_norm": 2.0613365173339844,
      "learning_rate": 4.719681294585882e-05,
      "loss": 0.5446,
      "num_input_tokens_seen": 28368144,
      "step": 2260,
      "train_runtime": 29079.4702,
      "train_tokens_per_second": 975.539
    },
    {
      "epoch": 0.45776071139854485,
      "grad_norm": 2.597219944000244,
      "learning_rate": 4.718462881831378e-05,
      "loss": 0.5499,
      "num_input_tokens_seen": 28430864,
      "step": 2265,
      "train_runtime": 29143.5791,
      "train_tokens_per_second": 975.545
    },
    {
      "epoch": 0.4587712206952304,
      "grad_norm": 1.7665842771530151,
      "learning_rate": 4.7172419848572455e-05,
      "loss": 0.6148,
      "num_input_tokens_seen": 28493856,
      "step": 2270,
      "train_runtime": 29208.2138,
      "train_tokens_per_second": 975.543
    },
    {
      "epoch": 0.4597817299919159,
      "grad_norm": 1.629071593284607,
      "learning_rate": 4.7160186050306376e-05,
      "loss": 0.3501,
      "num_input_tokens_seen": 28557504,
      "step": 2275,
      "train_runtime": 29273.0926,
      "train_tokens_per_second": 975.555
    },
    {
      "epoch": 0.46079223928860147,
      "grad_norm": 1.3741531372070312,
      "learning_rate": 4.714792743721486e-05,
      "loss": 0.4605,
      "num_input_tokens_seen": 28619808,
      "step": 2280,
      "train_runtime": 29337.0864,
      "train_tokens_per_second": 975.55
    },
    {
      "epoch": 0.46180274858528697,
      "grad_norm": 1.440600872039795,
      "learning_rate": 4.7135644023025015e-05,
      "loss": 0.5604,
      "num_input_tokens_seen": 28683120,
      "step": 2285,
      "train_runtime": 29401.8838,
      "train_tokens_per_second": 975.554
    },
    {
      "epoch": 0.46281325788197253,
      "grad_norm": 2.0044264793395996,
      "learning_rate": 4.712333582149172e-05,
      "loss": 0.4614,
      "num_input_tokens_seen": 28746608,
      "step": 2290,
      "train_runtime": 29466.7052,
      "train_tokens_per_second": 975.562
    },
    {
      "epoch": 0.46382376717865803,
      "grad_norm": 2.030933380126953,
      "learning_rate": 4.7111002846397634e-05,
      "loss": 0.5438,
      "num_input_tokens_seen": 28809296,
      "step": 2295,
      "train_runtime": 29531.2531,
      "train_tokens_per_second": 975.553
    },
    {
      "epoch": 0.4648342764753436,
      "grad_norm": 1.8356366157531738,
      "learning_rate": 4.709864511155312e-05,
      "loss": 0.5635,
      "num_input_tokens_seen": 28871168,
      "step": 2300,
      "train_runtime": 29594.9858,
      "train_tokens_per_second": 975.543
    },
    {
      "epoch": 0.4658447857720291,
      "grad_norm": 1.2378754615783691,
      "learning_rate": 4.70862626307963e-05,
      "loss": 0.5136,
      "num_input_tokens_seen": 28933280,
      "step": 2305,
      "train_runtime": 29659.459,
      "train_tokens_per_second": 975.516
    },
    {
      "epoch": 0.46685529506871465,
      "grad_norm": 1.772410273551941,
      "learning_rate": 4.707385541799297e-05,
      "loss": 0.4814,
      "num_input_tokens_seen": 28995472,
      "step": 2310,
      "train_runtime": 29723.3849,
      "train_tokens_per_second": 975.51
    },
    {
      "epoch": 0.46786580436540015,
      "grad_norm": 2.277204751968384,
      "learning_rate": 4.706142348703667e-05,
      "loss": 0.4239,
      "num_input_tokens_seen": 29058032,
      "step": 2315,
      "train_runtime": 29787.6801,
      "train_tokens_per_second": 975.505
    },
    {
      "epoch": 0.4688763136620857,
      "grad_norm": 1.7986656427383423,
      "learning_rate": 4.704896685184858e-05,
      "loss": 0.4966,
      "num_input_tokens_seen": 29120944,
      "step": 2320,
      "train_runtime": 29852.0436,
      "train_tokens_per_second": 975.509
    },
    {
      "epoch": 0.4698868229587712,
      "grad_norm": 3.2173988819122314,
      "learning_rate": 4.703648552637754e-05,
      "loss": 0.4631,
      "num_input_tokens_seen": 29183424,
      "step": 2325,
      "train_runtime": 29915.9628,
      "train_tokens_per_second": 975.513
    },
    {
      "epoch": 0.47089733225545677,
      "grad_norm": 1.8889480829238892,
      "learning_rate": 4.702397952460007e-05,
      "loss": 0.4906,
      "num_input_tokens_seen": 29246880,
      "step": 2330,
      "train_runtime": 29980.9747,
      "train_tokens_per_second": 975.515
    },
    {
      "epoch": 0.47190784155214227,
      "grad_norm": 2.027590036392212,
      "learning_rate": 4.7011448860520314e-05,
      "loss": 0.5924,
      "num_input_tokens_seen": 29309408,
      "step": 2335,
      "train_runtime": 30045.3128,
      "train_tokens_per_second": 975.507
    },
    {
      "epoch": 0.47291835084882783,
      "grad_norm": 1.3290995359420776,
      "learning_rate": 4.699889354817e-05,
      "loss": 0.5455,
      "num_input_tokens_seen": 29371920,
      "step": 2340,
      "train_runtime": 30109.5214,
      "train_tokens_per_second": 975.503
    },
    {
      "epoch": 0.47392886014551333,
      "grad_norm": 2.1695868968963623,
      "learning_rate": 4.69863136016085e-05,
      "loss": 0.604,
      "num_input_tokens_seen": 29435344,
      "step": 2345,
      "train_runtime": 30174.4334,
      "train_tokens_per_second": 975.506
    },
    {
      "epoch": 0.4749393694421989,
      "grad_norm": 1.8551337718963623,
      "learning_rate": 4.697370903492274e-05,
      "loss": 0.5471,
      "num_input_tokens_seen": 29498288,
      "step": 2350,
      "train_runtime": 30238.9561,
      "train_tokens_per_second": 975.506
    },
    {
      "epoch": 0.4759498787388844,
      "grad_norm": 1.7974225282669067,
      "learning_rate": 4.6961079862227244e-05,
      "loss": 0.5097,
      "num_input_tokens_seen": 29561136,
      "step": 2355,
      "train_runtime": 30303.4179,
      "train_tokens_per_second": 975.505
    },
    {
      "epoch": 0.47696038803556995,
      "grad_norm": 1.4540064334869385,
      "learning_rate": 4.6948426097664055e-05,
      "loss": 0.6165,
      "num_input_tokens_seen": 29623936,
      "step": 2360,
      "train_runtime": 30367.9553,
      "train_tokens_per_second": 975.5
    },
    {
      "epoch": 0.47797089733225545,
      "grad_norm": 1.5756468772888184,
      "learning_rate": 4.693574775540278e-05,
      "loss": 0.5125,
      "num_input_tokens_seen": 29687088,
      "step": 2365,
      "train_runtime": 30432.5067,
      "train_tokens_per_second": 975.506
    },
    {
      "epoch": 0.478981406628941,
      "grad_norm": 1.0236455202102661,
      "learning_rate": 4.692304484964055e-05,
      "loss": 0.5101,
      "num_input_tokens_seen": 29750672,
      "step": 2370,
      "train_runtime": 30497.5431,
      "train_tokens_per_second": 975.51
    },
    {
      "epoch": 0.4799919159256265,
      "grad_norm": 1.060908317565918,
      "learning_rate": 4.691031739460198e-05,
      "loss": 0.4812,
      "num_input_tokens_seen": 29812096,
      "step": 2375,
      "train_runtime": 30560.6114,
      "train_tokens_per_second": 975.507
    },
    {
      "epoch": 0.48100242522231207,
      "grad_norm": 1.5550264120101929,
      "learning_rate": 4.689756540453919e-05,
      "loss": 0.5097,
      "num_input_tokens_seen": 29875408,
      "step": 2380,
      "train_runtime": 30625.6118,
      "train_tokens_per_second": 975.504
    },
    {
      "epoch": 0.48201293451899757,
      "grad_norm": 1.568332314491272,
      "learning_rate": 4.6884788893731774e-05,
      "loss": 0.5585,
      "num_input_tokens_seen": 29938336,
      "step": 2385,
      "train_runtime": 30690.1219,
      "train_tokens_per_second": 975.504
    },
    {
      "epoch": 0.48302344381568313,
      "grad_norm": 1.4687817096710205,
      "learning_rate": 4.6871987876486775e-05,
      "loss": 0.4781,
      "num_input_tokens_seen": 30001616,
      "step": 2390,
      "train_runtime": 30754.8935,
      "train_tokens_per_second": 975.507
    },
    {
      "epoch": 0.48403395311236863,
      "grad_norm": 2.152364730834961,
      "learning_rate": 4.68591623671387e-05,
      "loss": 0.7114,
      "num_input_tokens_seen": 30064208,
      "step": 2395,
      "train_runtime": 30818.9636,
      "train_tokens_per_second": 975.51
    },
    {
      "epoch": 0.4850444624090542,
      "grad_norm": 1.5502197742462158,
      "learning_rate": 4.684631238004946e-05,
      "loss": 0.5849,
      "num_input_tokens_seen": 30127456,
      "step": 2400,
      "train_runtime": 30883.7524,
      "train_tokens_per_second": 975.512
    },
    {
      "epoch": 0.4860549717057397,
      "grad_norm": 1.8168343305587769,
      "learning_rate": 4.683343792960837e-05,
      "loss": 0.4209,
      "num_input_tokens_seen": 30190160,
      "step": 2405,
      "train_runtime": 30949.2639,
      "train_tokens_per_second": 975.473
    },
    {
      "epoch": 0.48706548100242525,
      "grad_norm": 2.2491774559020996,
      "learning_rate": 4.682053903023217e-05,
      "loss": 0.5801,
      "num_input_tokens_seen": 30252688,
      "step": 2410,
      "train_runtime": 31013.4975,
      "train_tokens_per_second": 975.468
    },
    {
      "epoch": 0.48807599029911075,
      "grad_norm": 2.5923244953155518,
      "learning_rate": 4.6807615696364946e-05,
      "loss": 0.5048,
      "num_input_tokens_seen": 30314832,
      "step": 2415,
      "train_runtime": 31077.3121,
      "train_tokens_per_second": 975.465
    },
    {
      "epoch": 0.4890864995957963,
      "grad_norm": 1.7420611381530762,
      "learning_rate": 4.6794667942478176e-05,
      "loss": 0.4749,
      "num_input_tokens_seen": 30377216,
      "step": 2420,
      "train_runtime": 31141.2752,
      "train_tokens_per_second": 975.465
    },
    {
      "epoch": 0.4900970088924818,
      "grad_norm": 1.3037309646606445,
      "learning_rate": 4.678169578307065e-05,
      "loss": 0.4067,
      "num_input_tokens_seen": 30440160,
      "step": 2425,
      "train_runtime": 31206.0675,
      "train_tokens_per_second": 975.456
    },
    {
      "epoch": 0.4911075181891673,
      "grad_norm": 1.529101014137268,
      "learning_rate": 4.6768699232668515e-05,
      "loss": 0.545,
      "num_input_tokens_seen": 30501344,
      "step": 2430,
      "train_runtime": 31268.9577,
      "train_tokens_per_second": 975.451
    },
    {
      "epoch": 0.49211802748585287,
      "grad_norm": 1.9383490085601807,
      "learning_rate": 4.6755678305825206e-05,
      "loss": 0.5898,
      "num_input_tokens_seen": 30564768,
      "step": 2435,
      "train_runtime": 31333.9718,
      "train_tokens_per_second": 975.451
    },
    {
      "epoch": 0.4931285367825384,
      "grad_norm": 2.477189302444458,
      "learning_rate": 4.674263301712148e-05,
      "loss": 0.5125,
      "num_input_tokens_seen": 30628256,
      "step": 2440,
      "train_runtime": 31398.903,
      "train_tokens_per_second": 975.456
    },
    {
      "epoch": 0.49413904607922393,
      "grad_norm": 1.755456566810608,
      "learning_rate": 4.672956338116536e-05,
      "loss": 0.3533,
      "num_input_tokens_seen": 30690640,
      "step": 2445,
      "train_runtime": 31463.1367,
      "train_tokens_per_second": 975.448
    },
    {
      "epoch": 0.49514955537590943,
      "grad_norm": 1.1445114612579346,
      "learning_rate": 4.6716469412592135e-05,
      "loss": 0.3625,
      "num_input_tokens_seen": 30753760,
      "step": 2450,
      "train_runtime": 31527.5883,
      "train_tokens_per_second": 975.456
    },
    {
      "epoch": 0.496160064672595,
      "grad_norm": 1.419999599456787,
      "learning_rate": 4.6703351126064344e-05,
      "loss": 0.4545,
      "num_input_tokens_seen": 30816416,
      "step": 2455,
      "train_runtime": 31591.977,
      "train_tokens_per_second": 975.451
    },
    {
      "epoch": 0.4971705739692805,
      "grad_norm": 2.250549554824829,
      "learning_rate": 4.669020853627175e-05,
      "loss": 0.5927,
      "num_input_tokens_seen": 30879488,
      "step": 2460,
      "train_runtime": 31656.596,
      "train_tokens_per_second": 975.452
    },
    {
      "epoch": 0.49818108326596605,
      "grad_norm": 1.3699820041656494,
      "learning_rate": 4.6677041657931344e-05,
      "loss": 0.4768,
      "num_input_tokens_seen": 30942304,
      "step": 2465,
      "train_runtime": 31721.0231,
      "train_tokens_per_second": 975.451
    },
    {
      "epoch": 0.49919159256265155,
      "grad_norm": 1.2449700832366943,
      "learning_rate": 4.6663850505787295e-05,
      "loss": 0.5423,
      "num_input_tokens_seen": 31005424,
      "step": 2470,
      "train_runtime": 31785.8601,
      "train_tokens_per_second": 975.447
    },
    {
      "epoch": 0.5002021018593371,
      "grad_norm": 3.34438419342041,
      "learning_rate": 4.665063509461097e-05,
      "loss": 0.5605,
      "num_input_tokens_seen": 31070096,
      "step": 2475,
      "train_runtime": 31851.7286,
      "train_tokens_per_second": 975.46
    },
    {
      "epoch": 0.5012126111560227,
      "grad_norm": 1.2801159620285034,
      "learning_rate": 4.6637395439200896e-05,
      "loss": 0.4929,
      "num_input_tokens_seen": 31133616,
      "step": 2480,
      "train_runtime": 31916.7243,
      "train_tokens_per_second": 975.464
    },
    {
      "epoch": 0.5022231204527081,
      "grad_norm": 1.3320257663726807,
      "learning_rate": 4.662413155438276e-05,
      "loss": 0.4919,
      "num_input_tokens_seen": 31196320,
      "step": 2485,
      "train_runtime": 31980.9452,
      "train_tokens_per_second": 975.466
    },
    {
      "epoch": 0.5032336297493937,
      "grad_norm": 1.8907841444015503,
      "learning_rate": 4.661084345500935e-05,
      "loss": 0.5366,
      "num_input_tokens_seen": 31259376,
      "step": 2490,
      "train_runtime": 32045.954,
      "train_tokens_per_second": 975.455
    },
    {
      "epoch": 0.5042441390460792,
      "grad_norm": 1.8293025493621826,
      "learning_rate": 4.6597531155960606e-05,
      "loss": 0.4494,
      "num_input_tokens_seen": 31322656,
      "step": 2495,
      "train_runtime": 32110.5149,
      "train_tokens_per_second": 975.464
    },
    {
      "epoch": 0.5052546483427648,
      "grad_norm": 2.0753278732299805,
      "learning_rate": 4.6584194672143545e-05,
      "loss": 0.6334,
      "num_input_tokens_seen": 31386752,
      "step": 2500,
      "train_runtime": 32175.9491,
      "train_tokens_per_second": 975.472
    },
    {
      "epoch": 0.5062651576394502,
      "grad_norm": 1.0583356618881226,
      "learning_rate": 4.657083401849226e-05,
      "loss": 0.5018,
      "num_input_tokens_seen": 31449328,
      "step": 2505,
      "train_runtime": 32240.8235,
      "train_tokens_per_second": 975.451
    },
    {
      "epoch": 0.5072756669361358,
      "grad_norm": 1.332647681236267,
      "learning_rate": 4.655744920996794e-05,
      "loss": 0.5086,
      "num_input_tokens_seen": 31512544,
      "step": 2510,
      "train_runtime": 32305.666,
      "train_tokens_per_second": 975.449
    },
    {
      "epoch": 0.5082861762328214,
      "grad_norm": 1.5890825986862183,
      "learning_rate": 4.654404026155878e-05,
      "loss": 0.465,
      "num_input_tokens_seen": 31576336,
      "step": 2515,
      "train_runtime": 32370.7505,
      "train_tokens_per_second": 975.459
    },
    {
      "epoch": 0.5092966855295069,
      "grad_norm": 0.9230080246925354,
      "learning_rate": 4.653060718828005e-05,
      "loss": 0.5698,
      "num_input_tokens_seen": 31639104,
      "step": 2520,
      "train_runtime": 32435.1087,
      "train_tokens_per_second": 975.459
    },
    {
      "epoch": 0.5103071948261924,
      "grad_norm": 2.464129686355591,
      "learning_rate": 4.6517150005174e-05,
      "loss": 0.5736,
      "num_input_tokens_seen": 31702048,
      "step": 2525,
      "train_runtime": 32499.7509,
      "train_tokens_per_second": 975.455
    },
    {
      "epoch": 0.5113177041228779,
      "grad_norm": 2.387725830078125,
      "learning_rate": 4.65036687273099e-05,
      "loss": 0.4447,
      "num_input_tokens_seen": 31765088,
      "step": 2530,
      "train_runtime": 32564.1878,
      "train_tokens_per_second": 975.461
    },
    {
      "epoch": 0.5123282134195635,
      "grad_norm": 1.2119756937026978,
      "learning_rate": 4.649016336978399e-05,
      "loss": 0.3838,
      "num_input_tokens_seen": 31827840,
      "step": 2535,
      "train_runtime": 32628.583,
      "train_tokens_per_second": 975.459
    },
    {
      "epoch": 0.513338722716249,
      "grad_norm": 2.8147783279418945,
      "learning_rate": 4.647663394771948e-05,
      "loss": 0.5738,
      "num_input_tokens_seen": 31889200,
      "step": 2540,
      "train_runtime": 32691.7959,
      "train_tokens_per_second": 975.45
    },
    {
      "epoch": 0.5143492320129345,
      "grad_norm": 1.9508857727050781,
      "learning_rate": 4.646308047626652e-05,
      "loss": 0.4432,
      "num_input_tokens_seen": 31951328,
      "step": 2545,
      "train_runtime": 32755.7363,
      "train_tokens_per_second": 975.442
    },
    {
      "epoch": 0.51535974130962,
      "grad_norm": 2.0493216514587402,
      "learning_rate": 4.644950297060219e-05,
      "loss": 0.4359,
      "num_input_tokens_seen": 32015216,
      "step": 2550,
      "train_runtime": 32820.8642,
      "train_tokens_per_second": 975.453
    },
    {
      "epoch": 0.5163702506063056,
      "grad_norm": 1.4022619724273682,
      "learning_rate": 4.64359014459305e-05,
      "loss": 0.53,
      "num_input_tokens_seen": 32078272,
      "step": 2555,
      "train_runtime": 32885.2566,
      "train_tokens_per_second": 975.461
    },
    {
      "epoch": 0.5173807599029911,
      "grad_norm": 2.100835084915161,
      "learning_rate": 4.642227591748234e-05,
      "loss": 0.507,
      "num_input_tokens_seen": 32139904,
      "step": 2560,
      "train_runtime": 32948.7302,
      "train_tokens_per_second": 975.452
    },
    {
      "epoch": 0.5183912691996766,
      "grad_norm": 2.3838157653808594,
      "learning_rate": 4.640862640051549e-05,
      "loss": 0.715,
      "num_input_tokens_seen": 32201328,
      "step": 2565,
      "train_runtime": 33011.9034,
      "train_tokens_per_second": 975.446
    },
    {
      "epoch": 0.5194017784963622,
      "grad_norm": 2.49159836769104,
      "learning_rate": 4.639495291031457e-05,
      "loss": 0.5495,
      "num_input_tokens_seen": 32263728,
      "step": 2570,
      "train_runtime": 33075.8779,
      "train_tokens_per_second": 975.446
    },
    {
      "epoch": 0.5204122877930477,
      "grad_norm": 1.8363938331604004,
      "learning_rate": 4.6381255462191076e-05,
      "loss": 0.5454,
      "num_input_tokens_seen": 32326592,
      "step": 2575,
      "train_runtime": 33140.1028,
      "train_tokens_per_second": 975.452
    },
    {
      "epoch": 0.5214227970897333,
      "grad_norm": 1.9562805891036987,
      "learning_rate": 4.636753407148331e-05,
      "loss": 0.6341,
      "num_input_tokens_seen": 32388048,
      "step": 2580,
      "train_runtime": 33203.2642,
      "train_tokens_per_second": 975.448
    },
    {
      "epoch": 0.5224333063864187,
      "grad_norm": 1.6112689971923828,
      "learning_rate": 4.63537887535564e-05,
      "loss": 0.6041,
      "num_input_tokens_seen": 32450368,
      "step": 2585,
      "train_runtime": 33267.5753,
      "train_tokens_per_second": 975.435
    },
    {
      "epoch": 0.5234438156831043,
      "grad_norm": 2.120760679244995,
      "learning_rate": 4.634001952380226e-05,
      "loss": 0.4931,
      "num_input_tokens_seen": 32513456,
      "step": 2590,
      "train_runtime": 33332.0546,
      "train_tokens_per_second": 975.441
    },
    {
      "epoch": 0.5244543249797898,
      "grad_norm": 1.756439208984375,
      "learning_rate": 4.632622639763956e-05,
      "loss": 0.711,
      "num_input_tokens_seen": 32576752,
      "step": 2595,
      "train_runtime": 33396.8716,
      "train_tokens_per_second": 975.443
    },
    {
      "epoch": 0.5254648342764754,
      "grad_norm": 2.2229573726654053,
      "learning_rate": 4.6312409390513765e-05,
      "loss": 0.5865,
      "num_input_tokens_seen": 32638704,
      "step": 2600,
      "train_runtime": 33460.5426,
      "train_tokens_per_second": 975.439
    },
    {
      "epoch": 0.5264753435731608,
      "grad_norm": 1.6432154178619385,
      "learning_rate": 4.629856851789707e-05,
      "loss": 0.4531,
      "num_input_tokens_seen": 32701600,
      "step": 2605,
      "train_runtime": 33526.1469,
      "train_tokens_per_second": 975.406
    },
    {
      "epoch": 0.5274858528698464,
      "grad_norm": 2.810499906539917,
      "learning_rate": 4.628470379528838e-05,
      "loss": 0.56,
      "num_input_tokens_seen": 32764096,
      "step": 2610,
      "train_runtime": 33590.2071,
      "train_tokens_per_second": 975.406
    },
    {
      "epoch": 0.528496362166532,
      "grad_norm": 1.911418080329895,
      "learning_rate": 4.627081523821332e-05,
      "loss": 0.4666,
      "num_input_tokens_seen": 32826736,
      "step": 2615,
      "train_runtime": 33654.2874,
      "train_tokens_per_second": 975.41
    },
    {
      "epoch": 0.5295068714632175,
      "grad_norm": 1.4071186780929565,
      "learning_rate": 4.625690286222419e-05,
      "loss": 0.6233,
      "num_input_tokens_seen": 32889680,
      "step": 2620,
      "train_runtime": 33718.7989,
      "train_tokens_per_second": 975.411
    },
    {
      "epoch": 0.530517380759903,
      "grad_norm": 1.980558156967163,
      "learning_rate": 4.62429666829e-05,
      "loss": 0.6152,
      "num_input_tokens_seen": 32952704,
      "step": 2625,
      "train_runtime": 33783.2567,
      "train_tokens_per_second": 975.415
    },
    {
      "epoch": 0.5315278900565885,
      "grad_norm": 2.0979042053222656,
      "learning_rate": 4.6229006715846365e-05,
      "loss": 0.6268,
      "num_input_tokens_seen": 33014928,
      "step": 2630,
      "train_runtime": 33847.4317,
      "train_tokens_per_second": 975.404
    },
    {
      "epoch": 0.5325383993532741,
      "grad_norm": 2.38376522064209,
      "learning_rate": 4.6215022976695576e-05,
      "loss": 0.6963,
      "num_input_tokens_seen": 33077232,
      "step": 2635,
      "train_runtime": 33911.502,
      "train_tokens_per_second": 975.399
    },
    {
      "epoch": 0.5335489086499596,
      "grad_norm": 1.4423748254776,
      "learning_rate": 4.620101548110652e-05,
      "loss": 0.5297,
      "num_input_tokens_seen": 33139200,
      "step": 2640,
      "train_runtime": 33975.0949,
      "train_tokens_per_second": 975.397
    },
    {
      "epoch": 0.5345594179466451,
      "grad_norm": 1.5236417055130005,
      "learning_rate": 4.61869842447647e-05,
      "loss": 0.4329,
      "num_input_tokens_seen": 33202752,
      "step": 2645,
      "train_runtime": 34039.9372,
      "train_tokens_per_second": 975.406
    },
    {
      "epoch": 0.5355699272433306,
      "grad_norm": 1.3566206693649292,
      "learning_rate": 4.61729292833822e-05,
      "loss": 0.4424,
      "num_input_tokens_seen": 33265168,
      "step": 2650,
      "train_runtime": 34103.976,
      "train_tokens_per_second": 975.404
    },
    {
      "epoch": 0.5365804365400162,
      "grad_norm": 2.108513593673706,
      "learning_rate": 4.6158850612697656e-05,
      "loss": 0.6151,
      "num_input_tokens_seen": 33328032,
      "step": 2655,
      "train_runtime": 34168.5637,
      "train_tokens_per_second": 975.4
    },
    {
      "epoch": 0.5375909458367018,
      "grad_norm": 1.8718950748443604,
      "learning_rate": 4.6144748248476285e-05,
      "loss": 0.4386,
      "num_input_tokens_seen": 33389840,
      "step": 2660,
      "train_runtime": 34232.0861,
      "train_tokens_per_second": 975.396
    },
    {
      "epoch": 0.5386014551333872,
      "grad_norm": 2.0942890644073486,
      "learning_rate": 4.613062220650981e-05,
      "loss": 0.5386,
      "num_input_tokens_seen": 33452544,
      "step": 2665,
      "train_runtime": 34296.2184,
      "train_tokens_per_second": 975.4
    },
    {
      "epoch": 0.5396119644300728,
      "grad_norm": 1.8542861938476562,
      "learning_rate": 4.6116472502616484e-05,
      "loss": 0.5576,
      "num_input_tokens_seen": 33515056,
      "step": 2670,
      "train_runtime": 34360.3111,
      "train_tokens_per_second": 975.4
    },
    {
      "epoch": 0.5406224737267583,
      "grad_norm": 1.1600099802017212,
      "learning_rate": 4.610229915264104e-05,
      "loss": 0.4653,
      "num_input_tokens_seen": 33576896,
      "step": 2675,
      "train_runtime": 34424.0242,
      "train_tokens_per_second": 975.391
    },
    {
      "epoch": 0.5416329830234439,
      "grad_norm": 1.566375970840454,
      "learning_rate": 4.6088102172454704e-05,
      "loss": 0.4069,
      "num_input_tokens_seen": 33639392,
      "step": 2680,
      "train_runtime": 34488.2045,
      "train_tokens_per_second": 975.388
    },
    {
      "epoch": 0.5426434923201293,
      "grad_norm": 1.9079339504241943,
      "learning_rate": 4.607388157795516e-05,
      "loss": 0.6491,
      "num_input_tokens_seen": 33702848,
      "step": 2685,
      "train_runtime": 34553.056,
      "train_tokens_per_second": 975.394
    },
    {
      "epoch": 0.5436540016168149,
      "grad_norm": 2.940500259399414,
      "learning_rate": 4.605963738506652e-05,
      "loss": 0.4786,
      "num_input_tokens_seen": 33765488,
      "step": 2690,
      "train_runtime": 34617.4027,
      "train_tokens_per_second": 975.391
    },
    {
      "epoch": 0.5446645109135004,
      "grad_norm": 1.8192803859710693,
      "learning_rate": 4.604536960973935e-05,
      "loss": 0.5764,
      "num_input_tokens_seen": 33828128,
      "step": 2695,
      "train_runtime": 34681.7148,
      "train_tokens_per_second": 975.388
    },
    {
      "epoch": 0.5456750202101859,
      "grad_norm": 1.287667989730835,
      "learning_rate": 4.60310782679506e-05,
      "loss": 0.5692,
      "num_input_tokens_seen": 33892064,
      "step": 2700,
      "train_runtime": 34747.237,
      "train_tokens_per_second": 975.389
    },
    {
      "epoch": 0.5466855295068714,
      "grad_norm": 2.119638442993164,
      "learning_rate": 4.601676337570362e-05,
      "loss": 0.5673,
      "num_input_tokens_seen": 33954048,
      "step": 2705,
      "train_runtime": 34811.7171,
      "train_tokens_per_second": 975.363
    },
    {
      "epoch": 0.547696038803557,
      "grad_norm": 2.0242919921875,
      "learning_rate": 4.600242494902812e-05,
      "loss": 0.5105,
      "num_input_tokens_seen": 34016976,
      "step": 2710,
      "train_runtime": 34876.1319,
      "train_tokens_per_second": 975.366
    },
    {
      "epoch": 0.5487065481002426,
      "grad_norm": 2.519746780395508,
      "learning_rate": 4.598806300398019e-05,
      "loss": 0.4981,
      "num_input_tokens_seen": 34079504,
      "step": 2715,
      "train_runtime": 34940.2128,
      "train_tokens_per_second": 975.366
    },
    {
      "epoch": 0.549717057396928,
      "grad_norm": 1.3155981302261353,
      "learning_rate": 4.5973677556642216e-05,
      "loss": 0.5109,
      "num_input_tokens_seen": 34141744,
      "step": 2720,
      "train_runtime": 35004.4464,
      "train_tokens_per_second": 975.354
    },
    {
      "epoch": 0.5507275666936136,
      "grad_norm": 1.8946806192398071,
      "learning_rate": 4.595926862312294e-05,
      "loss": 0.4194,
      "num_input_tokens_seen": 34204624,
      "step": 2725,
      "train_runtime": 35068.7373,
      "train_tokens_per_second": 975.359
    },
    {
      "epoch": 0.5517380759902991,
      "grad_norm": 1.8397178649902344,
      "learning_rate": 4.5944836219557384e-05,
      "loss": 0.5475,
      "num_input_tokens_seen": 34266416,
      "step": 2730,
      "train_runtime": 35132.1367,
      "train_tokens_per_second": 975.358
    },
    {
      "epoch": 0.5527485852869847,
      "grad_norm": 2.299304723739624,
      "learning_rate": 4.593038036210686e-05,
      "loss": 0.685,
      "num_input_tokens_seen": 34329520,
      "step": 2735,
      "train_runtime": 35196.88,
      "train_tokens_per_second": 975.357
    },
    {
      "epoch": 0.5537590945836701,
      "grad_norm": 1.3390100002288818,
      "learning_rate": 4.5915901066958925e-05,
      "loss": 0.5645,
      "num_input_tokens_seen": 34391344,
      "step": 2740,
      "train_runtime": 35260.3326,
      "train_tokens_per_second": 975.355
    },
    {
      "epoch": 0.5547696038803557,
      "grad_norm": 2.4369945526123047,
      "learning_rate": 4.590139835032741e-05,
      "loss": 0.6381,
      "num_input_tokens_seen": 34454784,
      "step": 2745,
      "train_runtime": 35325.3742,
      "train_tokens_per_second": 975.355
    },
    {
      "epoch": 0.5557801131770412,
      "grad_norm": 1.2973344326019287,
      "learning_rate": 4.588687222845235e-05,
      "loss": 0.5177,
      "num_input_tokens_seen": 34517008,
      "step": 2750,
      "train_runtime": 35389.2637,
      "train_tokens_per_second": 975.353
    },
    {
      "epoch": 0.5567906224737268,
      "grad_norm": 3.808028221130371,
      "learning_rate": 4.587232271759999e-05,
      "loss": 0.4138,
      "num_input_tokens_seen": 34580624,
      "step": 2755,
      "train_runtime": 35454.5646,
      "train_tokens_per_second": 975.35
    },
    {
      "epoch": 0.5578011317704122,
      "grad_norm": 1.6525832414627075,
      "learning_rate": 4.585774983406279e-05,
      "loss": 0.5336,
      "num_input_tokens_seen": 34643136,
      "step": 2760,
      "train_runtime": 35518.6157,
      "train_tokens_per_second": 975.352
    },
    {
      "epoch": 0.5588116410670978,
      "grad_norm": 1.8483145236968994,
      "learning_rate": 4.584315359415936e-05,
      "loss": 0.6298,
      "num_input_tokens_seen": 34705136,
      "step": 2765,
      "train_runtime": 35582.5443,
      "train_tokens_per_second": 975.342
    },
    {
      "epoch": 0.5598221503637834,
      "grad_norm": 1.6289176940917969,
      "learning_rate": 4.582853401423447e-05,
      "loss": 0.417,
      "num_input_tokens_seen": 34768032,
      "step": 2770,
      "train_runtime": 35647.029,
      "train_tokens_per_second": 975.342
    },
    {
      "epoch": 0.5608326596604689,
      "grad_norm": 1.4267842769622803,
      "learning_rate": 4.581389111065902e-05,
      "loss": 0.625,
      "num_input_tokens_seen": 34830432,
      "step": 2775,
      "train_runtime": 35710.986,
      "train_tokens_per_second": 975.342
    },
    {
      "epoch": 0.5618431689571544,
      "grad_norm": 3.667694330215454,
      "learning_rate": 4.579922489983004e-05,
      "loss": 0.5238,
      "num_input_tokens_seen": 34892944,
      "step": 2780,
      "train_runtime": 35775.0918,
      "train_tokens_per_second": 975.342
    },
    {
      "epoch": 0.5628536782538399,
      "grad_norm": 1.6978460550308228,
      "learning_rate": 4.578453539817065e-05,
      "loss": 0.4085,
      "num_input_tokens_seen": 34954960,
      "step": 2785,
      "train_runtime": 35838.7789,
      "train_tokens_per_second": 975.339
    },
    {
      "epoch": 0.5638641875505255,
      "grad_norm": 1.6142058372497559,
      "learning_rate": 4.576982262213006e-05,
      "loss": 0.494,
      "num_input_tokens_seen": 35019216,
      "step": 2790,
      "train_runtime": 35904.4989,
      "train_tokens_per_second": 975.343
    },
    {
      "epoch": 0.564874696847211,
      "grad_norm": 2.515331745147705,
      "learning_rate": 4.575508658818352e-05,
      "loss": 0.5971,
      "num_input_tokens_seen": 35081168,
      "step": 2795,
      "train_runtime": 35968.2526,
      "train_tokens_per_second": 975.337
    },
    {
      "epoch": 0.5658852061438965,
      "grad_norm": 1.4227374792099,
      "learning_rate": 4.574032731283236e-05,
      "loss": 0.5092,
      "num_input_tokens_seen": 35143328,
      "step": 2800,
      "train_runtime": 36032.0768,
      "train_tokens_per_second": 975.335
    },
    {
      "epoch": 0.566895715440582,
      "grad_norm": 1.486804723739624,
      "learning_rate": 4.5725544812603906e-05,
      "loss": 0.6951,
      "num_input_tokens_seen": 35206720,
      "step": 2805,
      "train_runtime": 36097.8771,
      "train_tokens_per_second": 975.313
    },
    {
      "epoch": 0.5679062247372676,
      "grad_norm": 2.368466854095459,
      "learning_rate": 4.57107391040515e-05,
      "loss": 0.6022,
      "num_input_tokens_seen": 35269072,
      "step": 2810,
      "train_runtime": 36162.0393,
      "train_tokens_per_second": 975.307
    },
    {
      "epoch": 0.5689167340339532,
      "grad_norm": 1.750137448310852,
      "learning_rate": 4.5695910203754485e-05,
      "loss": 0.5589,
      "num_input_tokens_seen": 35331792,
      "step": 2815,
      "train_runtime": 36226.5214,
      "train_tokens_per_second": 975.302
    },
    {
      "epoch": 0.5699272433306386,
      "grad_norm": 1.7691633701324463,
      "learning_rate": 4.568105812831814e-05,
      "loss": 0.5023,
      "num_input_tokens_seen": 35394368,
      "step": 2820,
      "train_runtime": 36290.8479,
      "train_tokens_per_second": 975.297
    },
    {
      "epoch": 0.5709377526273242,
      "grad_norm": 2.209713935852051,
      "learning_rate": 4.5666182894373745e-05,
      "loss": 0.7919,
      "num_input_tokens_seen": 35456352,
      "step": 2825,
      "train_runtime": 36354.5393,
      "train_tokens_per_second": 975.294
    },
    {
      "epoch": 0.5719482619240097,
      "grad_norm": 1.9949334859848022,
      "learning_rate": 4.565128451857847e-05,
      "loss": 0.6509,
      "num_input_tokens_seen": 35519600,
      "step": 2830,
      "train_runtime": 36419.5944,
      "train_tokens_per_second": 975.288
    },
    {
      "epoch": 0.5729587712206953,
      "grad_norm": 1.8798409700393677,
      "learning_rate": 4.563636301761541e-05,
      "loss": 0.5949,
      "num_input_tokens_seen": 35581472,
      "step": 2835,
      "train_runtime": 36483.2342,
      "train_tokens_per_second": 975.283
    },
    {
      "epoch": 0.5739692805173807,
      "grad_norm": 1.7878588438034058,
      "learning_rate": 4.5621418408193586e-05,
      "loss": 0.3366,
      "num_input_tokens_seen": 35643376,
      "step": 2840,
      "train_runtime": 36546.8876,
      "train_tokens_per_second": 975.278
    },
    {
      "epoch": 0.5749797898140663,
      "grad_norm": 1.6998740434646606,
      "learning_rate": 4.560645070704783e-05,
      "loss": 0.5144,
      "num_input_tokens_seen": 35705520,
      "step": 2845,
      "train_runtime": 36610.5488,
      "train_tokens_per_second": 975.28
    },
    {
      "epoch": 0.5759902991107518,
      "grad_norm": 2.173035144805908,
      "learning_rate": 4.5591459930938906e-05,
      "loss": 0.4381,
      "num_input_tokens_seen": 35768304,
      "step": 2850,
      "train_runtime": 36674.8075,
      "train_tokens_per_second": 975.283
    },
    {
      "epoch": 0.5770008084074374,
      "grad_norm": 1.5707606077194214,
      "learning_rate": 4.557644609665337e-05,
      "loss": 0.5191,
      "num_input_tokens_seen": 35830992,
      "step": 2855,
      "train_runtime": 36739.3447,
      "train_tokens_per_second": 975.276
    },
    {
      "epoch": 0.5780113177041228,
      "grad_norm": 1.4168833494186401,
      "learning_rate": 4.556140922100361e-05,
      "loss": 0.4246,
      "num_input_tokens_seen": 35893008,
      "step": 2860,
      "train_runtime": 36802.8847,
      "train_tokens_per_second": 975.277
    },
    {
      "epoch": 0.5790218270008084,
      "grad_norm": 1.4998137950897217,
      "learning_rate": 4.5546349320827805e-05,
      "loss": 0.5375,
      "num_input_tokens_seen": 35955136,
      "step": 2865,
      "train_runtime": 36866.7017,
      "train_tokens_per_second": 975.274
    },
    {
      "epoch": 0.580032336297494,
      "grad_norm": 2.243381977081299,
      "learning_rate": 4.553126641298996e-05,
      "loss": 0.5418,
      "num_input_tokens_seen": 36017328,
      "step": 2870,
      "train_runtime": 36930.4636,
      "train_tokens_per_second": 975.274
    },
    {
      "epoch": 0.5810428455941795,
      "grad_norm": 1.6582964658737183,
      "learning_rate": 4.551616051437979e-05,
      "loss": 0.4839,
      "num_input_tokens_seen": 36079136,
      "step": 2875,
      "train_runtime": 36994.187,
      "train_tokens_per_second": 975.265
    },
    {
      "epoch": 0.582053354890865,
      "grad_norm": 1.6492221355438232,
      "learning_rate": 4.550103164191278e-05,
      "loss": 0.5039,
      "num_input_tokens_seen": 36142544,
      "step": 2880,
      "train_runtime": 37058.9155,
      "train_tokens_per_second": 975.273
    },
    {
      "epoch": 0.5830638641875505,
      "grad_norm": 2.195403814315796,
      "learning_rate": 4.548587981253014e-05,
      "loss": 0.398,
      "num_input_tokens_seen": 36205104,
      "step": 2885,
      "train_runtime": 37122.9153,
      "train_tokens_per_second": 975.276
    },
    {
      "epoch": 0.5840743734842361,
      "grad_norm": 1.647976040840149,
      "learning_rate": 4.5470705043198794e-05,
      "loss": 0.4834,
      "num_input_tokens_seen": 36267552,
      "step": 2890,
      "train_runtime": 37187.1504,
      "train_tokens_per_second": 975.271
    },
    {
      "epoch": 0.5850848827809216,
      "grad_norm": 1.43458890914917,
      "learning_rate": 4.545550735091133e-05,
      "loss": 0.3716,
      "num_input_tokens_seen": 36329904,
      "step": 2895,
      "train_runtime": 37251.4039,
      "train_tokens_per_second": 975.263
    },
    {
      "epoch": 0.5860953920776071,
      "grad_norm": 1.7377612590789795,
      "learning_rate": 4.544028675268603e-05,
      "loss": 0.5308,
      "num_input_tokens_seen": 36392096,
      "step": 2900,
      "train_runtime": 37315.0656,
      "train_tokens_per_second": 975.265
    },
    {
      "epoch": 0.5871059013742926,
      "grad_norm": 1.869408130645752,
      "learning_rate": 4.5425043265566815e-05,
      "loss": 0.5821,
      "num_input_tokens_seen": 36454720,
      "step": 2905,
      "train_runtime": 37380.0081,
      "train_tokens_per_second": 975.246
    },
    {
      "epoch": 0.5881164106709782,
      "grad_norm": 1.7362003326416016,
      "learning_rate": 4.5409776906623234e-05,
      "loss": 0.4484,
      "num_input_tokens_seen": 36518400,
      "step": 2910,
      "train_runtime": 37445.0946,
      "train_tokens_per_second": 975.252
    },
    {
      "epoch": 0.5891269199676638,
      "grad_norm": 2.689328670501709,
      "learning_rate": 4.5394487692950456e-05,
      "loss": 0.6254,
      "num_input_tokens_seen": 36581632,
      "step": 2915,
      "train_runtime": 37509.5678,
      "train_tokens_per_second": 975.261
    },
    {
      "epoch": 0.5901374292643492,
      "grad_norm": 2.204850435256958,
      "learning_rate": 4.537917564166924e-05,
      "loss": 0.5558,
      "num_input_tokens_seen": 36643952,
      "step": 2920,
      "train_runtime": 37570.787,
      "train_tokens_per_second": 975.331
    },
    {
      "epoch": 0.5911479385610348,
      "grad_norm": 1.784748911857605,
      "learning_rate": 4.53638407699259e-05,
      "loss": 0.3872,
      "num_input_tokens_seen": 36704592,
      "step": 2925,
      "train_runtime": 37622.9098,
      "train_tokens_per_second": 975.592
    },
    {
      "epoch": 0.5921584478577203,
      "grad_norm": 2.5550882816314697,
      "learning_rate": 4.534848309489235e-05,
      "loss": 0.5934,
      "num_input_tokens_seen": 36766304,
      "step": 2930,
      "train_runtime": 37675.3921,
      "train_tokens_per_second": 975.871
    },
    {
      "epoch": 0.5931689571544059,
      "grad_norm": 1.6945290565490723,
      "learning_rate": 4.533310263376599e-05,
      "loss": 0.6051,
      "num_input_tokens_seen": 36828128,
      "step": 2935,
      "train_runtime": 37728.1375,
      "train_tokens_per_second": 976.145
    },
    {
      "epoch": 0.5941794664510913,
      "grad_norm": 1.6258769035339355,
      "learning_rate": 4.531769940376976e-05,
      "loss": 0.4431,
      "num_input_tokens_seen": 36890752,
      "step": 2940,
      "train_runtime": 37781.3197,
      "train_tokens_per_second": 976.428
    },
    {
      "epoch": 0.5951899757477769,
      "grad_norm": 1.4493860006332397,
      "learning_rate": 4.5302273422152094e-05,
      "loss": 0.5224,
      "num_input_tokens_seen": 36953232,
      "step": 2945,
      "train_runtime": 37834.3863,
      "train_tokens_per_second": 976.71
    },
    {
      "epoch": 0.5962004850444624,
      "grad_norm": 1.7887884378433228,
      "learning_rate": 4.5286824706186907e-05,
      "loss": 0.4685,
      "num_input_tokens_seen": 37015664,
      "step": 2950,
      "train_runtime": 37887.4422,
      "train_tokens_per_second": 976.99
    },
    {
      "epoch": 0.597210994341148,
      "grad_norm": 1.945711612701416,
      "learning_rate": 4.5271353273173546e-05,
      "loss": 0.5868,
      "num_input_tokens_seen": 37078272,
      "step": 2955,
      "train_runtime": 37940.7952,
      "train_tokens_per_second": 977.267
    },
    {
      "epoch": 0.5982215036378334,
      "grad_norm": 3.792614221572876,
      "learning_rate": 4.525585914043683e-05,
      "loss": 0.6172,
      "num_input_tokens_seen": 37141344,
      "step": 2960,
      "train_runtime": 37994.3769,
      "train_tokens_per_second": 977.548
    },
    {
      "epoch": 0.599232012934519,
      "grad_norm": 2.4165585041046143,
      "learning_rate": 4.524034232532699e-05,
      "loss": 0.5026,
      "num_input_tokens_seen": 37203936,
      "step": 2965,
      "train_runtime": 38047.6196,
      "train_tokens_per_second": 977.826
    },
    {
      "epoch": 0.6002425222312046,
      "grad_norm": 1.6405607461929321,
      "learning_rate": 4.522480284521964e-05,
      "loss": 0.4932,
      "num_input_tokens_seen": 37265456,
      "step": 2970,
      "train_runtime": 38100.2265,
      "train_tokens_per_second": 978.09
    },
    {
      "epoch": 0.60125303152789,
      "grad_norm": 1.4852241277694702,
      "learning_rate": 4.520924071751578e-05,
      "loss": 0.5461,
      "num_input_tokens_seen": 37328304,
      "step": 2975,
      "train_runtime": 38153.6364,
      "train_tokens_per_second": 978.368
    },
    {
      "epoch": 0.6022635408245756,
      "grad_norm": 1.1702618598937988,
      "learning_rate": 4.519365595964179e-05,
      "loss": 0.4493,
      "num_input_tokens_seen": 37390976,
      "step": 2980,
      "train_runtime": 38206.8609,
      "train_tokens_per_second": 978.646
    },
    {
      "epoch": 0.6032740501212611,
      "grad_norm": 1.8091017007827759,
      "learning_rate": 4.517804858904936e-05,
      "loss": 0.5654,
      "num_input_tokens_seen": 37453280,
      "step": 2985,
      "train_runtime": 38259.979,
      "train_tokens_per_second": 978.915
    },
    {
      "epoch": 0.6042845594179467,
      "grad_norm": 2.424670934677124,
      "learning_rate": 4.516241862321552e-05,
      "loss": 0.5247,
      "num_input_tokens_seen": 37516864,
      "step": 2990,
      "train_runtime": 38313.9347,
      "train_tokens_per_second": 979.196
    },
    {
      "epoch": 0.6052950687146321,
      "grad_norm": 1.8165916204452515,
      "learning_rate": 4.514676607964261e-05,
      "loss": 0.4227,
      "num_input_tokens_seen": 37580432,
      "step": 2995,
      "train_runtime": 38367.7905,
      "train_tokens_per_second": 979.479
    },
    {
      "epoch": 0.6063055780113177,
      "grad_norm": 1.7896876335144043,
      "learning_rate": 4.5131090975858234e-05,
      "loss": 0.4754,
      "num_input_tokens_seen": 37643664,
      "step": 3000,
      "train_runtime": 38421.6082,
      "train_tokens_per_second": 979.752
    },
    {
      "epoch": 0.6073160873080032,
      "grad_norm": 2.542463779449463,
      "learning_rate": 4.5115393329415266e-05,
      "loss": 0.6961,
      "num_input_tokens_seen": 37705952,
      "step": 3005,
      "train_runtime": 38475.2453,
      "train_tokens_per_second": 980.005
    },
    {
      "epoch": 0.6083265966046888,
      "grad_norm": 1.483755350112915,
      "learning_rate": 4.5099673157891824e-05,
      "loss": 0.4995,
      "num_input_tokens_seen": 37769024,
      "step": 3010,
      "train_runtime": 38528.8163,
      "train_tokens_per_second": 980.28
    },
    {
      "epoch": 0.6093371059013742,
      "grad_norm": 2.3448333740234375,
      "learning_rate": 4.508393047889126e-05,
      "loss": 0.4651,
      "num_input_tokens_seen": 37831232,
      "step": 3015,
      "train_runtime": 38581.7888,
      "train_tokens_per_second": 980.546
    },
    {
      "epoch": 0.6103476151980598,
      "grad_norm": 1.2622820138931274,
      "learning_rate": 4.506816531004209e-05,
      "loss": 0.7191,
      "num_input_tokens_seen": 37893824,
      "step": 3020,
      "train_runtime": 38634.9487,
      "train_tokens_per_second": 980.817
    },
    {
      "epoch": 0.6113581244947454,
      "grad_norm": 1.5217092037200928,
      "learning_rate": 4.505237766899807e-05,
      "loss": 0.5428,
      "num_input_tokens_seen": 37956816,
      "step": 3025,
      "train_runtime": 38688.3923,
      "train_tokens_per_second": 981.091
    },
    {
      "epoch": 0.6123686337914309,
      "grad_norm": 1.9336577653884888,
      "learning_rate": 4.503656757343809e-05,
      "loss": 0.6536,
      "num_input_tokens_seen": 38018384,
      "step": 3030,
      "train_runtime": 38740.9952,
      "train_tokens_per_second": 981.348
    },
    {
      "epoch": 0.6133791430881164,
      "grad_norm": 1.3856720924377441,
      "learning_rate": 4.502073504106617e-05,
      "loss": 0.4659,
      "num_input_tokens_seen": 38081264,
      "step": 3035,
      "train_runtime": 38794.4464,
      "train_tokens_per_second": 981.616
    },
    {
      "epoch": 0.6143896523848019,
      "grad_norm": 1.4326910972595215,
      "learning_rate": 4.5004880089611504e-05,
      "loss": 0.3888,
      "num_input_tokens_seen": 38143744,
      "step": 3040,
      "train_runtime": 38847.482,
      "train_tokens_per_second": 981.885
    },
    {
      "epoch": 0.6154001616814875,
      "grad_norm": 2.420412063598633,
      "learning_rate": 4.498900273682832e-05,
      "loss": 0.4761,
      "num_input_tokens_seen": 38206352,
      "step": 3045,
      "train_runtime": 38900.6509,
      "train_tokens_per_second": 982.152
    },
    {
      "epoch": 0.616410670978173,
      "grad_norm": 2.0287036895751953,
      "learning_rate": 4.4973103000495995e-05,
      "loss": 0.4892,
      "num_input_tokens_seen": 38269760,
      "step": 3050,
      "train_runtime": 38954.5257,
      "train_tokens_per_second": 982.421
    },
    {
      "epoch": 0.6174211802748585,
      "grad_norm": 2.1132664680480957,
      "learning_rate": 4.4957180898418935e-05,
      "loss": 0.5311,
      "num_input_tokens_seen": 38331856,
      "step": 3055,
      "train_runtime": 39007.4359,
      "train_tokens_per_second": 982.681
    },
    {
      "epoch": 0.618431689571544,
      "grad_norm": 2.6910531520843506,
      "learning_rate": 4.49412364484266e-05,
      "loss": 0.5044,
      "num_input_tokens_seen": 38394672,
      "step": 3060,
      "train_runtime": 39060.7449,
      "train_tokens_per_second": 982.948
    },
    {
      "epoch": 0.6194421988682296,
      "grad_norm": 1.2593311071395874,
      "learning_rate": 4.4925269668373495e-05,
      "loss": 0.3682,
      "num_input_tokens_seen": 38458400,
      "step": 3065,
      "train_runtime": 39114.8288,
      "train_tokens_per_second": 983.218
    },
    {
      "epoch": 0.6204527081649152,
      "grad_norm": 1.9570966958999634,
      "learning_rate": 4.490928057613909e-05,
      "loss": 0.4379,
      "num_input_tokens_seen": 38520832,
      "step": 3070,
      "train_runtime": 39167.8882,
      "train_tokens_per_second": 983.48
    },
    {
      "epoch": 0.6214632174616006,
      "grad_norm": 2.1933131217956543,
      "learning_rate": 4.4893269189627876e-05,
      "loss": 0.5184,
      "num_input_tokens_seen": 38584080,
      "step": 3075,
      "train_runtime": 39221.6749,
      "train_tokens_per_second": 983.744
    },
    {
      "epoch": 0.6224737267582862,
      "grad_norm": 3.133838415145874,
      "learning_rate": 4.487723552676928e-05,
      "loss": 0.5484,
      "num_input_tokens_seen": 38646112,
      "step": 3080,
      "train_runtime": 39274.4866,
      "train_tokens_per_second": 984.0
    },
    {
      "epoch": 0.6234842360549717,
      "grad_norm": 1.372136116027832,
      "learning_rate": 4.4861179605517704e-05,
      "loss": 0.3659,
      "num_input_tokens_seen": 38709920,
      "step": 3085,
      "train_runtime": 39328.5646,
      "train_tokens_per_second": 984.27
    },
    {
      "epoch": 0.6244947453516573,
      "grad_norm": 2.2683238983154297,
      "learning_rate": 4.4845101443852445e-05,
      "loss": 0.5038,
      "num_input_tokens_seen": 38771632,
      "step": 3090,
      "train_runtime": 39381.2389,
      "train_tokens_per_second": 984.52
    },
    {
      "epoch": 0.6255052546483427,
      "grad_norm": 2.060194253921509,
      "learning_rate": 4.482900105977772e-05,
      "loss": 0.6551,
      "num_input_tokens_seen": 38834912,
      "step": 3095,
      "train_runtime": 39435.1179,
      "train_tokens_per_second": 984.78
    },
    {
      "epoch": 0.6265157639450283,
      "grad_norm": 2.633361339569092,
      "learning_rate": 4.481287847132265e-05,
      "loss": 0.36,
      "num_input_tokens_seen": 38897584,
      "step": 3100,
      "train_runtime": 39488.4939,
      "train_tokens_per_second": 985.036
    },
    {
      "epoch": 0.6275262732417138,
      "grad_norm": 1.0628435611724854,
      "learning_rate": 4.4796733696541174e-05,
      "loss": 0.523,
      "num_input_tokens_seen": 38958928,
      "step": 3105,
      "train_runtime": 39541.4155,
      "train_tokens_per_second": 985.269
    },
    {
      "epoch": 0.6285367825383994,
      "grad_norm": 1.7778185606002808,
      "learning_rate": 4.478056675351212e-05,
      "loss": 0.4472,
      "num_input_tokens_seen": 39021392,
      "step": 3110,
      "train_runtime": 39594.5152,
      "train_tokens_per_second": 985.525
    },
    {
      "epoch": 0.6295472918350848,
      "grad_norm": 2.245917797088623,
      "learning_rate": 4.4764377660339105e-05,
      "loss": 0.4361,
      "num_input_tokens_seen": 39084464,
      "step": 3115,
      "train_runtime": 39648.1118,
      "train_tokens_per_second": 985.784
    },
    {
      "epoch": 0.6305578011317704,
      "grad_norm": 2.3242673873901367,
      "learning_rate": 4.474816643515057e-05,
      "loss": 0.5768,
      "num_input_tokens_seen": 39147728,
      "step": 3120,
      "train_runtime": 39701.7147,
      "train_tokens_per_second": 986.046
    },
    {
      "epoch": 0.631568310428456,
      "grad_norm": 2.0666425228118896,
      "learning_rate": 4.473193309609975e-05,
      "loss": 0.4069,
      "num_input_tokens_seen": 39210912,
      "step": 3125,
      "train_runtime": 39755.4908,
      "train_tokens_per_second": 986.302
    },
    {
      "epoch": 0.6325788197251415,
      "grad_norm": 2.4778025150299072,
      "learning_rate": 4.4715677661364606e-05,
      "loss": 0.5414,
      "num_input_tokens_seen": 39272944,
      "step": 3130,
      "train_runtime": 39808.2449,
      "train_tokens_per_second": 986.553
    },
    {
      "epoch": 0.633589329021827,
      "grad_norm": 1.7759780883789062,
      "learning_rate": 4.469940014914787e-05,
      "loss": 0.499,
      "num_input_tokens_seen": 39335536,
      "step": 3135,
      "train_runtime": 39861.4546,
      "train_tokens_per_second": 986.806
    },
    {
      "epoch": 0.6345998383185125,
      "grad_norm": 2.516711950302124,
      "learning_rate": 4.4683100577677e-05,
      "loss": 0.5392,
      "num_input_tokens_seen": 39398304,
      "step": 3140,
      "train_runtime": 39914.8278,
      "train_tokens_per_second": 987.059
    },
    {
      "epoch": 0.6356103476151981,
      "grad_norm": 1.6481728553771973,
      "learning_rate": 4.466677896520414e-05,
      "loss": 0.5422,
      "num_input_tokens_seen": 39461456,
      "step": 3145,
      "train_runtime": 39968.4069,
      "train_tokens_per_second": 987.316
    },
    {
      "epoch": 0.6366208569118836,
      "grad_norm": 2.0796918869018555,
      "learning_rate": 4.46504353300061e-05,
      "loss": 0.5795,
      "num_input_tokens_seen": 39523360,
      "step": 3150,
      "train_runtime": 40021.0177,
      "train_tokens_per_second": 987.565
    },
    {
      "epoch": 0.6376313662085691,
      "grad_norm": 1.6268665790557861,
      "learning_rate": 4.4634069690384404e-05,
      "loss": 0.6902,
      "num_input_tokens_seen": 39585648,
      "step": 3155,
      "train_runtime": 40074.0551,
      "train_tokens_per_second": 987.812
    },
    {
      "epoch": 0.6386418755052546,
      "grad_norm": 3.0555264949798584,
      "learning_rate": 4.461768206466516e-05,
      "loss": 0.63,
      "num_input_tokens_seen": 39648720,
      "step": 3160,
      "train_runtime": 40127.587,
      "train_tokens_per_second": 988.066
    },
    {
      "epoch": 0.6396523848019402,
      "grad_norm": 2.588101863861084,
      "learning_rate": 4.460127247119913e-05,
      "loss": 0.4919,
      "num_input_tokens_seen": 39711008,
      "step": 3165,
      "train_runtime": 40180.5791,
      "train_tokens_per_second": 988.313
    },
    {
      "epoch": 0.6406628940986258,
      "grad_norm": 1.590399980545044,
      "learning_rate": 4.458484092836167e-05,
      "loss": 0.4809,
      "num_input_tokens_seen": 39772224,
      "step": 3170,
      "train_runtime": 40232.8355,
      "train_tokens_per_second": 988.551
    },
    {
      "epoch": 0.6416734033953112,
      "grad_norm": 1.5387052297592163,
      "learning_rate": 4.45683874545527e-05,
      "loss": 0.4968,
      "num_input_tokens_seen": 39835200,
      "step": 3175,
      "train_runtime": 40286.2237,
      "train_tokens_per_second": 988.805
    },
    {
      "epoch": 0.6426839126919968,
      "grad_norm": 1.6103140115737915,
      "learning_rate": 4.45519120681967e-05,
      "loss": 0.5758,
      "num_input_tokens_seen": 39898016,
      "step": 3180,
      "train_runtime": 40339.6434,
      "train_tokens_per_second": 989.052
    },
    {
      "epoch": 0.6436944219886823,
      "grad_norm": 2.472770929336548,
      "learning_rate": 4.453541478774271e-05,
      "loss": 0.5289,
      "num_input_tokens_seen": 39961440,
      "step": 3185,
      "train_runtime": 40393.4986,
      "train_tokens_per_second": 989.304
    },
    {
      "epoch": 0.6447049312853679,
      "grad_norm": 1.5453792810440063,
      "learning_rate": 4.4518895631664274e-05,
      "loss": 0.4312,
      "num_input_tokens_seen": 40024464,
      "step": 3190,
      "train_runtime": 40447.1485,
      "train_tokens_per_second": 989.55
    },
    {
      "epoch": 0.6457154405820533,
      "grad_norm": 2.5392251014709473,
      "learning_rate": 4.450235461845942e-05,
      "loss": 0.5801,
      "num_input_tokens_seen": 40086112,
      "step": 3195,
      "train_runtime": 40499.5216,
      "train_tokens_per_second": 989.792
    },
    {
      "epoch": 0.6467259498787389,
      "grad_norm": 3.3328640460968018,
      "learning_rate": 4.4485791766650675e-05,
      "loss": 0.5723,
      "num_input_tokens_seen": 40149792,
      "step": 3200,
      "train_runtime": 40553.4431,
      "train_tokens_per_second": 990.046
    },
    {
      "epoch": 0.6477364591754244,
      "grad_norm": 1.6479299068450928,
      "learning_rate": 4.446920709478499e-05,
      "loss": 0.5676,
      "num_input_tokens_seen": 40211936,
      "step": 3205,
      "train_runtime": 40607.1282,
      "train_tokens_per_second": 990.268
    },
    {
      "epoch": 0.64874696847211,
      "grad_norm": 7.275776386260986,
      "learning_rate": 4.445260062143378e-05,
      "loss": 0.5694,
      "num_input_tokens_seen": 40275008,
      "step": 3210,
      "train_runtime": 40660.6155,
      "train_tokens_per_second": 990.516
    },
    {
      "epoch": 0.6497574777687954,
      "grad_norm": 1.4987236261367798,
      "learning_rate": 4.4435972365192864e-05,
      "loss": 0.3461,
      "num_input_tokens_seen": 40338128,
      "step": 3215,
      "train_runtime": 40714.331,
      "train_tokens_per_second": 990.76
    },
    {
      "epoch": 0.650767987065481,
      "grad_norm": 2.126138210296631,
      "learning_rate": 4.4419322344682436e-05,
      "loss": 0.5518,
      "num_input_tokens_seen": 40401008,
      "step": 3220,
      "train_runtime": 40767.7386,
      "train_tokens_per_second": 991.004
    },
    {
      "epoch": 0.6517784963621666,
      "grad_norm": 2.021162509918213,
      "learning_rate": 4.440265057854709e-05,
      "loss": 0.4984,
      "num_input_tokens_seen": 40463104,
      "step": 3225,
      "train_runtime": 40820.5512,
      "train_tokens_per_second": 991.243
    },
    {
      "epoch": 0.652789005658852,
      "grad_norm": 1.628983497619629,
      "learning_rate": 4.4385957085455745e-05,
      "loss": 0.3877,
      "num_input_tokens_seen": 40526208,
      "step": 3230,
      "train_runtime": 40874.2668,
      "train_tokens_per_second": 991.485
    },
    {
      "epoch": 0.6537995149555376,
      "grad_norm": 2.4302570819854736,
      "learning_rate": 4.436924188410167e-05,
      "loss": 0.5974,
      "num_input_tokens_seen": 40590624,
      "step": 3235,
      "train_runtime": 40928.7385,
      "train_tokens_per_second": 991.739
    },
    {
      "epoch": 0.6548100242522231,
      "grad_norm": 2.4842031002044678,
      "learning_rate": 4.4352504993202425e-05,
      "loss": 0.5399,
      "num_input_tokens_seen": 40654256,
      "step": 3240,
      "train_runtime": 40982.6404,
      "train_tokens_per_second": 991.987
    },
    {
      "epoch": 0.6558205335489087,
      "grad_norm": 1.822931170463562,
      "learning_rate": 4.433574643149988e-05,
      "loss": 0.5915,
      "num_input_tokens_seen": 40717424,
      "step": 3245,
      "train_runtime": 41036.4241,
      "train_tokens_per_second": 992.226
    },
    {
      "epoch": 0.6568310428455941,
      "grad_norm": 3.1506946086883545,
      "learning_rate": 4.431896621776014e-05,
      "loss": 0.4062,
      "num_input_tokens_seen": 40779328,
      "step": 3250,
      "train_runtime": 41089.1148,
      "train_tokens_per_second": 992.461
    },
    {
      "epoch": 0.6578415521422797,
      "grad_norm": 2.835491180419922,
      "learning_rate": 4.430216437077358e-05,
      "loss": 0.5776,
      "num_input_tokens_seen": 40841552,
      "step": 3255,
      "train_runtime": 41141.9576,
      "train_tokens_per_second": 992.698
    },
    {
      "epoch": 0.6588520614389652,
      "grad_norm": 1.5423635244369507,
      "learning_rate": 4.42853409093548e-05,
      "loss": 0.5311,
      "num_input_tokens_seen": 40904048,
      "step": 3260,
      "train_runtime": 41195.3724,
      "train_tokens_per_second": 992.928
    },
    {
      "epoch": 0.6598625707356508,
      "grad_norm": 2.3012945652008057,
      "learning_rate": 4.42684958523426e-05,
      "loss": 0.4897,
      "num_input_tokens_seen": 40967168,
      "step": 3265,
      "train_runtime": 41249.0172,
      "train_tokens_per_second": 993.167
    },
    {
      "epoch": 0.6608730800323362,
      "grad_norm": 2.2071893215179443,
      "learning_rate": 4.425162921859995e-05,
      "loss": 0.5326,
      "num_input_tokens_seen": 41030432,
      "step": 3270,
      "train_runtime": 41302.7111,
      "train_tokens_per_second": 993.408
    },
    {
      "epoch": 0.6618835893290218,
      "grad_norm": 1.7949693202972412,
      "learning_rate": 4.4234741027014006e-05,
      "loss": 0.4872,
      "num_input_tokens_seen": 41092784,
      "step": 3275,
      "train_runtime": 41355.8817,
      "train_tokens_per_second": 993.638
    },
    {
      "epoch": 0.6628940986257074,
      "grad_norm": 1.9676374197006226,
      "learning_rate": 4.421783129649602e-05,
      "loss": 0.4673,
      "num_input_tokens_seen": 41154464,
      "step": 3280,
      "train_runtime": 41408.521,
      "train_tokens_per_second": 993.865
    },
    {
      "epoch": 0.6639046079223929,
      "grad_norm": 2.8055801391601562,
      "learning_rate": 4.420090004598143e-05,
      "loss": 0.6207,
      "num_input_tokens_seen": 41218656,
      "step": 3285,
      "train_runtime": 41462.7551,
      "train_tokens_per_second": 994.113
    },
    {
      "epoch": 0.6649151172190784,
      "grad_norm": 3.9156229496002197,
      "learning_rate": 4.418394729442972e-05,
      "loss": 0.6759,
      "num_input_tokens_seen": 41281472,
      "step": 3290,
      "train_runtime": 41516.0307,
      "train_tokens_per_second": 994.35
    },
    {
      "epoch": 0.6659256265157639,
      "grad_norm": 1.7649062871932983,
      "learning_rate": 4.416697306082446e-05,
      "loss": 0.4657,
      "num_input_tokens_seen": 41342704,
      "step": 3295,
      "train_runtime": 41568.2506,
      "train_tokens_per_second": 994.574
    },
    {
      "epoch": 0.6669361358124495,
      "grad_norm": 1.959588885307312,
      "learning_rate": 4.414997736417328e-05,
      "loss": 0.4073,
      "num_input_tokens_seen": 41406384,
      "step": 3300,
      "train_runtime": 41622.2768,
      "train_tokens_per_second": 994.813
    },
    {
      "epoch": 0.667946645109135,
      "grad_norm": 1.6653803586959839,
      "learning_rate": 4.413296022350786e-05,
      "loss": 0.6225,
      "num_input_tokens_seen": 41468464,
      "step": 3305,
      "train_runtime": 41675.7988,
      "train_tokens_per_second": 995.025
    },
    {
      "epoch": 0.6689571544058205,
      "grad_norm": 1.9138998985290527,
      "learning_rate": 4.4115921657883856e-05,
      "loss": 0.5273,
      "num_input_tokens_seen": 41531072,
      "step": 3310,
      "train_runtime": 41729.0105,
      "train_tokens_per_second": 995.257
    },
    {
      "epoch": 0.669967663702506,
      "grad_norm": 2.338444709777832,
      "learning_rate": 4.409886168638096e-05,
      "loss": 0.4255,
      "num_input_tokens_seen": 41593424,
      "step": 3315,
      "train_runtime": 41782.1158,
      "train_tokens_per_second": 995.484
    },
    {
      "epoch": 0.6709781729991916,
      "grad_norm": 2.605226755142212,
      "learning_rate": 4.40817803281028e-05,
      "loss": 0.4759,
      "num_input_tokens_seen": 41656352,
      "step": 3320,
      "train_runtime": 41835.6381,
      "train_tokens_per_second": 995.715
    },
    {
      "epoch": 0.6719886822958772,
      "grad_norm": 1.781409740447998,
      "learning_rate": 4.406467760217697e-05,
      "loss": 0.6114,
      "num_input_tokens_seen": 41717888,
      "step": 3325,
      "train_runtime": 41888.1704,
      "train_tokens_per_second": 995.935
    },
    {
      "epoch": 0.6729991915925626,
      "grad_norm": 1.8070117235183716,
      "learning_rate": 4.4047553527754984e-05,
      "loss": 0.4626,
      "num_input_tokens_seen": 41781632,
      "step": 3330,
      "train_runtime": 41942.1695,
      "train_tokens_per_second": 996.172
    },
    {
      "epoch": 0.6740097008892482,
      "grad_norm": 2.459141731262207,
      "learning_rate": 4.403040812401226e-05,
      "loss": 0.5208,
      "num_input_tokens_seen": 41845280,
      "step": 3335,
      "train_runtime": 41996.0531,
      "train_tokens_per_second": 996.41
    },
    {
      "epoch": 0.6750202101859337,
      "grad_norm": 1.8798739910125732,
      "learning_rate": 4.401324141014811e-05,
      "loss": 0.4704,
      "num_input_tokens_seen": 41908640,
      "step": 3340,
      "train_runtime": 42049.9446,
      "train_tokens_per_second": 996.64
    },
    {
      "epoch": 0.6760307194826193,
      "grad_norm": 2.183347225189209,
      "learning_rate": 4.3996053405385695e-05,
      "loss": 0.5468,
      "num_input_tokens_seen": 41972368,
      "step": 3345,
      "train_runtime": 42103.904,
      "train_tokens_per_second": 996.876
    },
    {
      "epoch": 0.6770412287793047,
      "grad_norm": 1.7675809860229492,
      "learning_rate": 4.3978844128972035e-05,
      "loss": 0.4111,
      "num_input_tokens_seen": 42034416,
      "step": 3350,
      "train_runtime": 42156.7687,
      "train_tokens_per_second": 997.098
    },
    {
      "epoch": 0.6780517380759903,
      "grad_norm": 3.7889842987060547,
      "learning_rate": 4.3961613600177966e-05,
      "loss": 0.5303,
      "num_input_tokens_seen": 42096848,
      "step": 3355,
      "train_runtime": 42209.8086,
      "train_tokens_per_second": 997.324
    },
    {
      "epoch": 0.6790622473726758,
      "grad_norm": 1.8546557426452637,
      "learning_rate": 4.39443618382981e-05,
      "loss": 0.5025,
      "num_input_tokens_seen": 42159648,
      "step": 3360,
      "train_runtime": 42263.3453,
      "train_tokens_per_second": 997.546
    },
    {
      "epoch": 0.6800727566693614,
      "grad_norm": 1.7415767908096313,
      "learning_rate": 4.392708886265085e-05,
      "loss": 0.6663,
      "num_input_tokens_seen": 42221968,
      "step": 3365,
      "train_runtime": 42316.2949,
      "train_tokens_per_second": 997.771
    },
    {
      "epoch": 0.6810832659660468,
      "grad_norm": 3.034219741821289,
      "learning_rate": 4.3909794692578374e-05,
      "loss": 0.363,
      "num_input_tokens_seen": 42285872,
      "step": 3370,
      "train_runtime": 42370.4642,
      "train_tokens_per_second": 998.004
    },
    {
      "epoch": 0.6820937752627324,
      "grad_norm": 1.6969319581985474,
      "learning_rate": 4.389247934744657e-05,
      "loss": 0.4712,
      "num_input_tokens_seen": 42348384,
      "step": 3375,
      "train_runtime": 42423.7309,
      "train_tokens_per_second": 998.224
    },
    {
      "epoch": 0.683104284559418,
      "grad_norm": 2.782670021057129,
      "learning_rate": 4.3875142846645044e-05,
      "loss": 0.626,
      "num_input_tokens_seen": 42411648,
      "step": 3380,
      "train_runtime": 42477.4772,
      "train_tokens_per_second": 998.45
    },
    {
      "epoch": 0.6841147938561035,
      "grad_norm": 1.6446385383605957,
      "learning_rate": 4.385778520958709e-05,
      "loss": 0.4025,
      "num_input_tokens_seen": 42474064,
      "step": 3385,
      "train_runtime": 42530.4707,
      "train_tokens_per_second": 998.674
    },
    {
      "epoch": 0.685125303152789,
      "grad_norm": 2.230085611343384,
      "learning_rate": 4.384040645570967e-05,
      "loss": 0.5407,
      "num_input_tokens_seen": 42536944,
      "step": 3390,
      "train_runtime": 42584.0171,
      "train_tokens_per_second": 998.895
    },
    {
      "epoch": 0.6861358124494745,
      "grad_norm": 2.1712422370910645,
      "learning_rate": 4.382300660447341e-05,
      "loss": 0.5798,
      "num_input_tokens_seen": 42598976,
      "step": 3395,
      "train_runtime": 42636.8514,
      "train_tokens_per_second": 999.112
    },
    {
      "epoch": 0.6871463217461601,
      "grad_norm": 2.0577709674835205,
      "learning_rate": 4.3805585675362515e-05,
      "loss": 0.5504,
      "num_input_tokens_seen": 42661520,
      "step": 3400,
      "train_runtime": 42689.8782,
      "train_tokens_per_second": 999.336
    },
    {
      "epoch": 0.6881568310428456,
      "grad_norm": 2.3572490215301514,
      "learning_rate": 4.378814368788484e-05,
      "loss": 0.4125,
      "num_input_tokens_seen": 42723312,
      "step": 3405,
      "train_runtime": 42743.1933,
      "train_tokens_per_second": 999.535
    },
    {
      "epoch": 0.6891673403395311,
      "grad_norm": 1.4062126874923706,
      "learning_rate": 4.377068066157181e-05,
      "loss": 0.3908,
      "num_input_tokens_seen": 42785776,
      "step": 3410,
      "train_runtime": 42796.3842,
      "train_tokens_per_second": 999.752
    },
    {
      "epoch": 0.6901778496362166,
      "grad_norm": 1.7911561727523804,
      "learning_rate": 4.375319661597839e-05,
      "loss": 0.5168,
      "num_input_tokens_seen": 42848880,
      "step": 3415,
      "train_runtime": 42849.9864,
      "train_tokens_per_second": 999.974
    },
    {
      "epoch": 0.6911883589329022,
      "grad_norm": 1.6952661275863647,
      "learning_rate": 4.37356915706831e-05,
      "loss": 0.5516,
      "num_input_tokens_seen": 42910928,
      "step": 3420,
      "train_runtime": 42902.7444,
      "train_tokens_per_second": 1000.191
    },
    {
      "epoch": 0.6921988682295878,
      "grad_norm": 1.7608380317687988,
      "learning_rate": 4.371816554528797e-05,
      "loss": 0.3707,
      "num_input_tokens_seen": 42972960,
      "step": 3425,
      "train_runtime": 42955.6835,
      "train_tokens_per_second": 1000.402
    },
    {
      "epoch": 0.6932093775262732,
      "grad_norm": 2.3441274166107178,
      "learning_rate": 4.370061855941853e-05,
      "loss": 0.4163,
      "num_input_tokens_seen": 43034928,
      "step": 3430,
      "train_runtime": 43008.3137,
      "train_tokens_per_second": 1000.619
    },
    {
      "epoch": 0.6942198868229588,
      "grad_norm": 2.5999510288238525,
      "learning_rate": 4.3683050632723764e-05,
      "loss": 0.5804,
      "num_input_tokens_seen": 43098128,
      "step": 3435,
      "train_runtime": 43061.9755,
      "train_tokens_per_second": 1000.84
    },
    {
      "epoch": 0.6952303961196443,
      "grad_norm": 1.7553446292877197,
      "learning_rate": 4.366546178487613e-05,
      "loss": 0.4184,
      "num_input_tokens_seen": 43161200,
      "step": 3440,
      "train_runtime": 43115.5095,
      "train_tokens_per_second": 1001.06
    },
    {
      "epoch": 0.6962409054163299,
      "grad_norm": 2.0835981369018555,
      "learning_rate": 4.364785203557149e-05,
      "loss": 0.4786,
      "num_input_tokens_seen": 43223872,
      "step": 3445,
      "train_runtime": 43168.728,
      "train_tokens_per_second": 1001.277
    },
    {
      "epoch": 0.6972514147130153,
      "grad_norm": 2.6174943447113037,
      "learning_rate": 4.3630221404529124e-05,
      "loss": 0.5572,
      "num_input_tokens_seen": 43287056,
      "step": 3450,
      "train_runtime": 43222.3585,
      "train_tokens_per_second": 1001.497
    },
    {
      "epoch": 0.6982619240097009,
      "grad_norm": 1.4975402355194092,
      "learning_rate": 4.36125699114917e-05,
      "loss": 0.5518,
      "num_input_tokens_seen": 43349680,
      "step": 3455,
      "train_runtime": 43275.6219,
      "train_tokens_per_second": 1001.711
    },
    {
      "epoch": 0.6992724333063864,
      "grad_norm": 1.7864329814910889,
      "learning_rate": 4.359489757622522e-05,
      "loss": 0.4945,
      "num_input_tokens_seen": 43411536,
      "step": 3460,
      "train_runtime": 43328.0672,
      "train_tokens_per_second": 1001.926
    },
    {
      "epoch": 0.700282942603072,
      "grad_norm": 2.8835484981536865,
      "learning_rate": 4.357720441851907e-05,
      "loss": 0.5326,
      "num_input_tokens_seen": 43473968,
      "step": 3465,
      "train_runtime": 43381.1879,
      "train_tokens_per_second": 1002.139
    },
    {
      "epoch": 0.7012934518997574,
      "grad_norm": 3.797377824783325,
      "learning_rate": 4.3559490458185906e-05,
      "loss": 0.5759,
      "num_input_tokens_seen": 43537296,
      "step": 3470,
      "train_runtime": 43434.7907,
      "train_tokens_per_second": 1002.36
    },
    {
      "epoch": 0.702303961196443,
      "grad_norm": 1.9245448112487793,
      "learning_rate": 4.354175571506171e-05,
      "loss": 0.7076,
      "num_input_tokens_seen": 43600016,
      "step": 3475,
      "train_runtime": 43488.1959,
      "train_tokens_per_second": 1002.571
    },
    {
      "epoch": 0.7033144704931286,
      "grad_norm": 2.116738796234131,
      "learning_rate": 4.352400020900573e-05,
      "loss": 0.5985,
      "num_input_tokens_seen": 43662768,
      "step": 3480,
      "train_runtime": 43541.5037,
      "train_tokens_per_second": 1002.785
    },
    {
      "epoch": 0.7043249797898141,
      "grad_norm": 2.244004726409912,
      "learning_rate": 4.350622395990045e-05,
      "loss": 0.5798,
      "num_input_tokens_seen": 43725456,
      "step": 3485,
      "train_runtime": 43594.8305,
      "train_tokens_per_second": 1002.996
    },
    {
      "epoch": 0.7053354890864996,
      "grad_norm": 2.0891902446746826,
      "learning_rate": 4.348842698765161e-05,
      "loss": 0.4816,
      "num_input_tokens_seen": 43788880,
      "step": 3490,
      "train_runtime": 43648.4638,
      "train_tokens_per_second": 1003.217
    },
    {
      "epoch": 0.7063459983831851,
      "grad_norm": 1.5886051654815674,
      "learning_rate": 4.3470609312188116e-05,
      "loss": 0.3959,
      "num_input_tokens_seen": 43852176,
      "step": 3495,
      "train_runtime": 43702.1836,
      "train_tokens_per_second": 1003.432
    },
    {
      "epoch": 0.7073565076798707,
      "grad_norm": 2.0969035625457764,
      "learning_rate": 4.34527709534621e-05,
      "loss": 0.5913,
      "num_input_tokens_seen": 43915360,
      "step": 3500,
      "train_runtime": 43755.7679,
      "train_tokens_per_second": 1003.647
    },
    {
      "epoch": 0.7083670169765561,
      "grad_norm": 1.5966018438339233,
      "learning_rate": 4.3434911931448816e-05,
      "loss": 0.4783,
      "num_input_tokens_seen": 43978048,
      "step": 3505,
      "train_runtime": 43809.7303,
      "train_tokens_per_second": 1003.842
    },
    {
      "epoch": 0.7093775262732417,
      "grad_norm": 2.141709327697754,
      "learning_rate": 4.34170322661467e-05,
      "loss": 0.6088,
      "num_input_tokens_seen": 44040992,
      "step": 3510,
      "train_runtime": 43863.2472,
      "train_tokens_per_second": 1004.052
    },
    {
      "epoch": 0.7103880355699272,
      "grad_norm": 3.6045877933502197,
      "learning_rate": 4.3399131977577246e-05,
      "loss": 0.5611,
      "num_input_tokens_seen": 44104064,
      "step": 3515,
      "train_runtime": 43916.7734,
      "train_tokens_per_second": 1004.265
    },
    {
      "epoch": 0.7113985448666128,
      "grad_norm": 1.5452955961227417,
      "learning_rate": 4.33812110857851e-05,
      "loss": 0.6411,
      "num_input_tokens_seen": 44167456,
      "step": 3520,
      "train_runtime": 43970.4835,
      "train_tokens_per_second": 1004.48
    },
    {
      "epoch": 0.7124090541632982,
      "grad_norm": 3.0392954349517822,
      "learning_rate": 4.336326961083795e-05,
      "loss": 0.3991,
      "num_input_tokens_seen": 44230272,
      "step": 3525,
      "train_runtime": 44023.8309,
      "train_tokens_per_second": 1004.689
    },
    {
      "epoch": 0.7134195634599838,
      "grad_norm": 3.0012803077697754,
      "learning_rate": 4.334530757282654e-05,
      "loss": 0.4094,
      "num_input_tokens_seen": 44294576,
      "step": 3530,
      "train_runtime": 44078.1964,
      "train_tokens_per_second": 1004.909
    },
    {
      "epoch": 0.7144300727566694,
      "grad_norm": 2.5342037677764893,
      "learning_rate": 4.332732499186463e-05,
      "loss": 0.4571,
      "num_input_tokens_seen": 44356208,
      "step": 3535,
      "train_runtime": 44130.7614,
      "train_tokens_per_second": 1005.109
    },
    {
      "epoch": 0.7154405820533549,
      "grad_norm": 1.6977217197418213,
      "learning_rate": 4.3309321888089e-05,
      "loss": 0.6579,
      "num_input_tokens_seen": 44419712,
      "step": 3540,
      "train_runtime": 44184.582,
      "train_tokens_per_second": 1005.322
    },
    {
      "epoch": 0.7164510913500404,
      "grad_norm": 2.5160956382751465,
      "learning_rate": 4.329129828165939e-05,
      "loss": 0.552,
      "num_input_tokens_seen": 44482096,
      "step": 3545,
      "train_runtime": 44237.6795,
      "train_tokens_per_second": 1005.525
    },
    {
      "epoch": 0.7174616006467259,
      "grad_norm": 2.0284831523895264,
      "learning_rate": 4.327325419275853e-05,
      "loss": 0.6812,
      "num_input_tokens_seen": 44545072,
      "step": 3550,
      "train_runtime": 44291.1129,
      "train_tokens_per_second": 1005.734
    },
    {
      "epoch": 0.7184721099434115,
      "grad_norm": 1.6575517654418945,
      "learning_rate": 4.325518964159205e-05,
      "loss": 0.6096,
      "num_input_tokens_seen": 44608016,
      "step": 3555,
      "train_runtime": 44344.5992,
      "train_tokens_per_second": 1005.94
    },
    {
      "epoch": 0.719482619240097,
      "grad_norm": 1.8106058835983276,
      "learning_rate": 4.323710464838852e-05,
      "loss": 0.5898,
      "num_input_tokens_seen": 44671216,
      "step": 3560,
      "train_runtime": 44398.2137,
      "train_tokens_per_second": 1006.149
    },
    {
      "epoch": 0.7204931285367825,
      "grad_norm": 2.446876287460327,
      "learning_rate": 4.321899923339939e-05,
      "loss": 0.473,
      "num_input_tokens_seen": 44732736,
      "step": 3565,
      "train_runtime": 44450.687,
      "train_tokens_per_second": 1006.345
    },
    {
      "epoch": 0.721503637833468,
      "grad_norm": 2.192195177078247,
      "learning_rate": 4.320087341689899e-05,
      "loss": 0.4422,
      "num_input_tokens_seen": 44794896,
      "step": 3570,
      "train_runtime": 44503.6062,
      "train_tokens_per_second": 1006.545
    },
    {
      "epoch": 0.7225141471301536,
      "grad_norm": 2.76629638671875,
      "learning_rate": 4.318272721918447e-05,
      "loss": 0.6451,
      "num_input_tokens_seen": 44857296,
      "step": 3575,
      "train_runtime": 44556.6274,
      "train_tokens_per_second": 1006.748
    },
    {
      "epoch": 0.7235246564268392,
      "grad_norm": 3.5075936317443848,
      "learning_rate": 4.316456066057583e-05,
      "loss": 0.4409,
      "num_input_tokens_seen": 44919584,
      "step": 3580,
      "train_runtime": 44609.664,
      "train_tokens_per_second": 1006.947
    },
    {
      "epoch": 0.7245351657235246,
      "grad_norm": 1.597316861152649,
      "learning_rate": 4.314637376141586e-05,
      "loss": 0.5422,
      "num_input_tokens_seen": 44982256,
      "step": 3585,
      "train_runtime": 44662.9322,
      "train_tokens_per_second": 1007.15
    },
    {
      "epoch": 0.7255456750202102,
      "grad_norm": 2.0533690452575684,
      "learning_rate": 4.312816654207012e-05,
      "loss": 0.4277,
      "num_input_tokens_seen": 45044688,
      "step": 3590,
      "train_runtime": 44716.0381,
      "train_tokens_per_second": 1007.35
    },
    {
      "epoch": 0.7265561843168957,
      "grad_norm": 4.096959114074707,
      "learning_rate": 4.310993902292692e-05,
      "loss": 0.6317,
      "num_input_tokens_seen": 45107408,
      "step": 3595,
      "train_runtime": 44769.5169,
      "train_tokens_per_second": 1007.547
    },
    {
      "epoch": 0.7275666936135813,
      "grad_norm": 1.9291629791259766,
      "learning_rate": 4.309169122439733e-05,
      "loss": 0.4295,
      "num_input_tokens_seen": 45171200,
      "step": 3600,
      "train_runtime": 44823.4337,
      "train_tokens_per_second": 1007.759
    },
    {
      "epoch": 0.7285772029102667,
      "grad_norm": 2.268563985824585,
      "learning_rate": 4.30734231669151e-05,
      "loss": 0.6033,
      "num_input_tokens_seen": 45234464,
      "step": 3605,
      "train_runtime": 44877.8257,
      "train_tokens_per_second": 1007.947
    },
    {
      "epoch": 0.7295877122069523,
      "grad_norm": 1.8172075748443604,
      "learning_rate": 4.3055134870936685e-05,
      "loss": 0.4966,
      "num_input_tokens_seen": 45296160,
      "step": 3610,
      "train_runtime": 44930.4536,
      "train_tokens_per_second": 1008.139
    },
    {
      "epoch": 0.7305982215036378,
      "grad_norm": 2.177926778793335,
      "learning_rate": 4.303682635694119e-05,
      "loss": 0.6236,
      "num_input_tokens_seen": 45358640,
      "step": 3615,
      "train_runtime": 44983.6175,
      "train_tokens_per_second": 1008.337
    },
    {
      "epoch": 0.7316087308003234,
      "grad_norm": 5.089705467224121,
      "learning_rate": 4.3018497645430364e-05,
      "loss": 0.4249,
      "num_input_tokens_seen": 45420704,
      "step": 3620,
      "train_runtime": 45036.4174,
      "train_tokens_per_second": 1008.533
    },
    {
      "epoch": 0.7326192400970089,
      "grad_norm": 1.5158629417419434,
      "learning_rate": 4.300014875692857e-05,
      "loss": 0.5582,
      "num_input_tokens_seen": 45483488,
      "step": 3625,
      "train_runtime": 45089.7845,
      "train_tokens_per_second": 1008.732
    },
    {
      "epoch": 0.7336297493936944,
      "grad_norm": 1.605587124824524,
      "learning_rate": 4.298177971198277e-05,
      "loss": 0.5401,
      "num_input_tokens_seen": 45546208,
      "step": 3630,
      "train_runtime": 45142.9412,
      "train_tokens_per_second": 1008.933
    },
    {
      "epoch": 0.73464025869038,
      "grad_norm": 2.0000710487365723,
      "learning_rate": 4.29633905311625e-05,
      "loss": 0.5205,
      "num_input_tokens_seen": 45609264,
      "step": 3635,
      "train_runtime": 45196.4196,
      "train_tokens_per_second": 1009.134
    },
    {
      "epoch": 0.7356507679870655,
      "grad_norm": 1.4068832397460938,
      "learning_rate": 4.294498123505983e-05,
      "loss": 0.4193,
      "num_input_tokens_seen": 45671504,
      "step": 3640,
      "train_runtime": 45249.3544,
      "train_tokens_per_second": 1009.329
    },
    {
      "epoch": 0.736661277283751,
      "grad_norm": 2.291558027267456,
      "learning_rate": 4.2926551844289376e-05,
      "loss": 0.5388,
      "num_input_tokens_seen": 45733904,
      "step": 3645,
      "train_runtime": 45302.3847,
      "train_tokens_per_second": 1009.525
    },
    {
      "epoch": 0.7376717865804365,
      "grad_norm": 1.6232393980026245,
      "learning_rate": 4.290810237948824e-05,
      "loss": 0.3786,
      "num_input_tokens_seen": 45797104,
      "step": 3650,
      "train_runtime": 45355.9232,
      "train_tokens_per_second": 1009.727
    },
    {
      "epoch": 0.7386822958771221,
      "grad_norm": 1.2897249460220337,
      "learning_rate": 4.2889632861316e-05,
      "loss": 0.4606,
      "num_input_tokens_seen": 45859296,
      "step": 3655,
      "train_runtime": 45408.6888,
      "train_tokens_per_second": 1009.923
    },
    {
      "epoch": 0.7396928051738076,
      "grad_norm": 2.9088804721832275,
      "learning_rate": 4.287114331045471e-05,
      "loss": 0.4636,
      "num_input_tokens_seen": 45921424,
      "step": 3660,
      "train_runtime": 45461.5037,
      "train_tokens_per_second": 1010.117
    },
    {
      "epoch": 0.7407033144704931,
      "grad_norm": 2.1120221614837646,
      "learning_rate": 4.285263374760883e-05,
      "loss": 0.4389,
      "num_input_tokens_seen": 45983792,
      "step": 3665,
      "train_runtime": 45514.3245,
      "train_tokens_per_second": 1010.315
    },
    {
      "epoch": 0.7417138237671786,
      "grad_norm": 2.1836631298065186,
      "learning_rate": 4.2834104193505254e-05,
      "loss": 0.407,
      "num_input_tokens_seen": 46046128,
      "step": 3670,
      "train_runtime": 45567.3822,
      "train_tokens_per_second": 1010.506
    },
    {
      "epoch": 0.7427243330638642,
      "grad_norm": 2.605895519256592,
      "learning_rate": 4.2815554668893236e-05,
      "loss": 0.4758,
      "num_input_tokens_seen": 46108464,
      "step": 3675,
      "train_runtime": 45620.4424,
      "train_tokens_per_second": 1010.697
    },
    {
      "epoch": 0.7437348423605498,
      "grad_norm": 1.5430845022201538,
      "learning_rate": 4.279698519454442e-05,
      "loss": 0.5177,
      "num_input_tokens_seen": 46170816,
      "step": 3680,
      "train_runtime": 45673.6005,
      "train_tokens_per_second": 1010.886
    },
    {
      "epoch": 0.7447453516572352,
      "grad_norm": 2.092679023742676,
      "learning_rate": 4.277839579125278e-05,
      "loss": 0.51,
      "num_input_tokens_seen": 46233120,
      "step": 3685,
      "train_runtime": 45726.4941,
      "train_tokens_per_second": 1011.079
    },
    {
      "epoch": 0.7457558609539208,
      "grad_norm": 2.163539171218872,
      "learning_rate": 4.275978647983459e-05,
      "loss": 0.5504,
      "num_input_tokens_seen": 46296544,
      "step": 3690,
      "train_runtime": 45780.2528,
      "train_tokens_per_second": 1011.278
    },
    {
      "epoch": 0.7467663702506063,
      "grad_norm": 2.5955240726470947,
      "learning_rate": 4.274115728112844e-05,
      "loss": 0.6433,
      "num_input_tokens_seen": 46359664,
      "step": 3695,
      "train_runtime": 45833.9224,
      "train_tokens_per_second": 1011.471
    },
    {
      "epoch": 0.7477768795472919,
      "grad_norm": 2.047301769256592,
      "learning_rate": 4.2722508215995175e-05,
      "loss": 0.5225,
      "num_input_tokens_seen": 46423024,
      "step": 3700,
      "train_runtime": 45887.6446,
      "train_tokens_per_second": 1011.667
    },
    {
      "epoch": 0.7487873888439773,
      "grad_norm": 2.046248435974121,
      "learning_rate": 4.270383930531788e-05,
      "loss": 0.5568,
      "num_input_tokens_seen": 46485984,
      "step": 3705,
      "train_runtime": 45941.6998,
      "train_tokens_per_second": 1011.847
    },
    {
      "epoch": 0.7497978981406629,
      "grad_norm": 2.020413398742676,
      "learning_rate": 4.2685150570001897e-05,
      "loss": 0.3805,
      "num_input_tokens_seen": 46548736,
      "step": 3710,
      "train_runtime": 45994.985,
      "train_tokens_per_second": 1012.039
    },
    {
      "epoch": 0.7508084074373484,
      "grad_norm": 2.390204668045044,
      "learning_rate": 4.266644203097474e-05,
      "loss": 0.5043,
      "num_input_tokens_seen": 46610800,
      "step": 3715,
      "train_runtime": 46047.7454,
      "train_tokens_per_second": 1012.228
    },
    {
      "epoch": 0.751818916734034,
      "grad_norm": 5.067107200622559,
      "learning_rate": 4.264771370918609e-05,
      "loss": 0.8888,
      "num_input_tokens_seen": 46673200,
      "step": 3720,
      "train_runtime": 46100.8515,
      "train_tokens_per_second": 1012.415
    },
    {
      "epoch": 0.7528294260307195,
      "grad_norm": 2.3455328941345215,
      "learning_rate": 4.26289656256078e-05,
      "loss": 0.5251,
      "num_input_tokens_seen": 46734720,
      "step": 3725,
      "train_runtime": 46153.0233,
      "train_tokens_per_second": 1012.604
    },
    {
      "epoch": 0.753839935327405,
      "grad_norm": 2.6291897296905518,
      "learning_rate": 4.2610197801233855e-05,
      "loss": 0.5901,
      "num_input_tokens_seen": 46797264,
      "step": 3730,
      "train_runtime": 46206.1693,
      "train_tokens_per_second": 1012.793
    },
    {
      "epoch": 0.7548504446240906,
      "grad_norm": 1.5301520824432373,
      "learning_rate": 4.259141025708032e-05,
      "loss": 0.6426,
      "num_input_tokens_seen": 46859072,
      "step": 3735,
      "train_runtime": 46258.7344,
      "train_tokens_per_second": 1012.978
    },
    {
      "epoch": 0.7558609539207761,
      "grad_norm": 1.3708456754684448,
      "learning_rate": 4.2572603014185384e-05,
      "loss": 0.4617,
      "num_input_tokens_seen": 46922048,
      "step": 3740,
      "train_runtime": 46312.2102,
      "train_tokens_per_second": 1013.168
    },
    {
      "epoch": 0.7568714632174616,
      "grad_norm": 1.7119519710540771,
      "learning_rate": 4.255377609360924e-05,
      "loss": 0.4512,
      "num_input_tokens_seen": 46984288,
      "step": 3745,
      "train_runtime": 46365.0846,
      "train_tokens_per_second": 1013.355
    },
    {
      "epoch": 0.7578819725141471,
      "grad_norm": 2.337019681930542,
      "learning_rate": 4.2534929516434176e-05,
      "loss": 0.4014,
      "num_input_tokens_seen": 47046944,
      "step": 3750,
      "train_runtime": 46418.3035,
      "train_tokens_per_second": 1013.543
    },
    {
      "epoch": 0.7588924818108327,
      "grad_norm": 3.81901216506958,
      "learning_rate": 4.251606330376443e-05,
      "loss": 0.4966,
      "num_input_tokens_seen": 47109184,
      "step": 3755,
      "train_runtime": 46471.2126,
      "train_tokens_per_second": 1013.728
    },
    {
      "epoch": 0.7599029911075182,
      "grad_norm": 2.251197338104248,
      "learning_rate": 4.249717747672628e-05,
      "loss": 0.5043,
      "num_input_tokens_seen": 47172336,
      "step": 3760,
      "train_runtime": 46524.6773,
      "train_tokens_per_second": 1013.921
    },
    {
      "epoch": 0.7609135004042037,
      "grad_norm": 2.7644262313842773,
      "learning_rate": 4.247827205646794e-05,
      "loss": 0.6536,
      "num_input_tokens_seen": 47234512,
      "step": 3765,
      "train_runtime": 46577.67,
      "train_tokens_per_second": 1014.102
    },
    {
      "epoch": 0.7619240097008892,
      "grad_norm": 2.709610939025879,
      "learning_rate": 4.2459347064159565e-05,
      "loss": 0.6159,
      "num_input_tokens_seen": 47297312,
      "step": 3770,
      "train_runtime": 46630.9515,
      "train_tokens_per_second": 1014.29
    },
    {
      "epoch": 0.7629345189975748,
      "grad_norm": 1.9093852043151855,
      "learning_rate": 4.244040252099324e-05,
      "loss": 0.5486,
      "num_input_tokens_seen": 47359760,
      "step": 3775,
      "train_runtime": 46684.189,
      "train_tokens_per_second": 1014.471
    },
    {
      "epoch": 0.7639450282942603,
      "grad_norm": 2.498148202896118,
      "learning_rate": 4.242143844818294e-05,
      "loss": 0.4887,
      "num_input_tokens_seen": 47422752,
      "step": 3780,
      "train_runtime": 46737.6637,
      "train_tokens_per_second": 1014.658
    },
    {
      "epoch": 0.7649555375909458,
      "grad_norm": 1.9407976865768433,
      "learning_rate": 4.240245486696449e-05,
      "loss": 0.5059,
      "num_input_tokens_seen": 47485472,
      "step": 3785,
      "train_runtime": 46790.9252,
      "train_tokens_per_second": 1014.844
    },
    {
      "epoch": 0.7659660468876314,
      "grad_norm": 2.0491464138031006,
      "learning_rate": 4.238345179859559e-05,
      "loss": 0.499,
      "num_input_tokens_seen": 47547680,
      "step": 3790,
      "train_runtime": 46843.8852,
      "train_tokens_per_second": 1015.024
    },
    {
      "epoch": 0.7669765561843169,
      "grad_norm": 2.4807515144348145,
      "learning_rate": 4.2364429264355733e-05,
      "loss": 0.3771,
      "num_input_tokens_seen": 47610816,
      "step": 3795,
      "train_runtime": 46897.4004,
      "train_tokens_per_second": 1015.212
    },
    {
      "epoch": 0.7679870654810024,
      "grad_norm": 2.31081485748291,
      "learning_rate": 4.234538728554624e-05,
      "loss": 0.4398,
      "num_input_tokens_seen": 47673200,
      "step": 3800,
      "train_runtime": 46950.6087,
      "train_tokens_per_second": 1015.39
    },
    {
      "epoch": 0.7689975747776879,
      "grad_norm": 2.577385187149048,
      "learning_rate": 4.232632588349017e-05,
      "loss": 0.473,
      "num_input_tokens_seen": 47735472,
      "step": 3805,
      "train_runtime": 47004.253,
      "train_tokens_per_second": 1015.556
    },
    {
      "epoch": 0.7700080840743735,
      "grad_norm": 2.024951696395874,
      "learning_rate": 4.2307245079532365e-05,
      "loss": 0.6298,
      "num_input_tokens_seen": 47797376,
      "step": 3810,
      "train_runtime": 47057.0478,
      "train_tokens_per_second": 1015.733
    },
    {
      "epoch": 0.771018593371059,
      "grad_norm": 2.384866714477539,
      "learning_rate": 4.228814489503937e-05,
      "loss": 0.5478,
      "num_input_tokens_seen": 47859600,
      "step": 3815,
      "train_runtime": 47109.9674,
      "train_tokens_per_second": 1015.912
    },
    {
      "epoch": 0.7720291026677445,
      "grad_norm": 1.4928040504455566,
      "learning_rate": 4.226902535139946e-05,
      "loss": 0.3771,
      "num_input_tokens_seen": 47922800,
      "step": 3820,
      "train_runtime": 47163.5735,
      "train_tokens_per_second": 1016.098
    },
    {
      "epoch": 0.77303961196443,
      "grad_norm": 1.8612140417099,
      "learning_rate": 4.224988647002254e-05,
      "loss": 0.6617,
      "num_input_tokens_seen": 47984976,
      "step": 3825,
      "train_runtime": 47216.4629,
      "train_tokens_per_second": 1016.276
    },
    {
      "epoch": 0.7740501212611156,
      "grad_norm": 2.2502241134643555,
      "learning_rate": 4.223072827234023e-05,
      "loss": 0.5416,
      "num_input_tokens_seen": 48047072,
      "step": 3830,
      "train_runtime": 47269.2995,
      "train_tokens_per_second": 1016.454
    },
    {
      "epoch": 0.7750606305578012,
      "grad_norm": 1.8064229488372803,
      "learning_rate": 4.221155077980572e-05,
      "loss": 0.5872,
      "num_input_tokens_seen": 48109456,
      "step": 3835,
      "train_runtime": 47322.3284,
      "train_tokens_per_second": 1016.633
    },
    {
      "epoch": 0.7760711398544866,
      "grad_norm": 2.48117995262146,
      "learning_rate": 4.2192354013893855e-05,
      "loss": 0.5713,
      "num_input_tokens_seen": 48171376,
      "step": 3840,
      "train_runtime": 47374.9058,
      "train_tokens_per_second": 1016.812
    },
    {
      "epoch": 0.7770816491511722,
      "grad_norm": 2.283531904220581,
      "learning_rate": 4.217313799610104e-05,
      "loss": 0.56,
      "num_input_tokens_seen": 48234496,
      "step": 3845,
      "train_runtime": 47428.4592,
      "train_tokens_per_second": 1016.995
    },
    {
      "epoch": 0.7780921584478577,
      "grad_norm": 2.5415995121002197,
      "learning_rate": 4.215390274794524e-05,
      "loss": 0.5064,
      "num_input_tokens_seen": 48297280,
      "step": 3850,
      "train_runtime": 47481.859,
      "train_tokens_per_second": 1017.173
    },
    {
      "epoch": 0.7791026677445433,
      "grad_norm": 1.846784234046936,
      "learning_rate": 4.213464829096595e-05,
      "loss": 0.5354,
      "num_input_tokens_seen": 48359440,
      "step": 3855,
      "train_runtime": 47534.6244,
      "train_tokens_per_second": 1017.352
    },
    {
      "epoch": 0.7801131770412287,
      "grad_norm": 1.8942805528640747,
      "learning_rate": 4.211537464672418e-05,
      "loss": 0.4877,
      "num_input_tokens_seen": 48423328,
      "step": 3860,
      "train_runtime": 47588.8106,
      "train_tokens_per_second": 1017.536
    },
    {
      "epoch": 0.7811236863379143,
      "grad_norm": 2.973477840423584,
      "learning_rate": 4.2096081836802436e-05,
      "loss": 0.5099,
      "num_input_tokens_seen": 48486496,
      "step": 3865,
      "train_runtime": 47642.4698,
      "train_tokens_per_second": 1017.716
    },
    {
      "epoch": 0.7821341956345998,
      "grad_norm": 1.8667057752609253,
      "learning_rate": 4.2076769882804665e-05,
      "loss": 0.622,
      "num_input_tokens_seen": 48548864,
      "step": 3870,
      "train_runtime": 47695.5406,
      "train_tokens_per_second": 1017.891
    },
    {
      "epoch": 0.7831447049312854,
      "grad_norm": 1.342407464981079,
      "learning_rate": 4.205743880635626e-05,
      "loss": 0.4241,
      "num_input_tokens_seen": 48611216,
      "step": 3875,
      "train_runtime": 47748.3592,
      "train_tokens_per_second": 1018.071
    },
    {
      "epoch": 0.7841552142279709,
      "grad_norm": 2.1424481868743896,
      "learning_rate": 4.203808862910404e-05,
      "loss": 0.6003,
      "num_input_tokens_seen": 48672912,
      "step": 3880,
      "train_runtime": 47800.7207,
      "train_tokens_per_second": 1018.246
    },
    {
      "epoch": 0.7851657235246564,
      "grad_norm": 1.7488077878952026,
      "learning_rate": 4.20187193727162e-05,
      "loss": 0.5831,
      "num_input_tokens_seen": 48735120,
      "step": 3885,
      "train_runtime": 47853.5036,
      "train_tokens_per_second": 1018.423
    },
    {
      "epoch": 0.786176232821342,
      "grad_norm": 2.9778149127960205,
      "learning_rate": 4.199933105888229e-05,
      "loss": 0.4293,
      "num_input_tokens_seen": 48796864,
      "step": 3890,
      "train_runtime": 47906.0299,
      "train_tokens_per_second": 1018.595
    },
    {
      "epoch": 0.7871867421180275,
      "grad_norm": 2.7023606300354004,
      "learning_rate": 4.197992370931322e-05,
      "loss": 0.45,
      "num_input_tokens_seen": 48860016,
      "step": 3895,
      "train_runtime": 47959.612,
      "train_tokens_per_second": 1018.774
    },
    {
      "epoch": 0.788197251414713,
      "grad_norm": 3.4939401149749756,
      "learning_rate": 4.196049734574121e-05,
      "loss": 0.4905,
      "num_input_tokens_seen": 48923088,
      "step": 3900,
      "train_runtime": 48013.1002,
      "train_tokens_per_second": 1018.953
    },
    {
      "epoch": 0.7892077607113985,
      "grad_norm": 2.3977408409118652,
      "learning_rate": 4.194105198991976e-05,
      "loss": 0.4905,
      "num_input_tokens_seen": 48986288,
      "step": 3905,
      "train_runtime": 48067.461,
      "train_tokens_per_second": 1019.115
    },
    {
      "epoch": 0.7902182700080841,
      "grad_norm": 3.368034839630127,
      "learning_rate": 4.192158766362365e-05,
      "loss": 0.4971,
      "num_input_tokens_seen": 49050464,
      "step": 3910,
      "train_runtime": 48121.7291,
      "train_tokens_per_second": 1019.3
    },
    {
      "epoch": 0.7912287793047696,
      "grad_norm": 3.5667874813079834,
      "learning_rate": 4.19021043886489e-05,
      "loss": 0.4787,
      "num_input_tokens_seen": 49113296,
      "step": 3915,
      "train_runtime": 48175.075,
      "train_tokens_per_second": 1019.475
    },
    {
      "epoch": 0.7922392886014551,
      "grad_norm": 2.2630043029785156,
      "learning_rate": 4.188260218681277e-05,
      "loss": 0.4368,
      "num_input_tokens_seen": 49176352,
      "step": 3920,
      "train_runtime": 48228.5673,
      "train_tokens_per_second": 1019.652
    },
    {
      "epoch": 0.7932497978981407,
      "grad_norm": 2.0136122703552246,
      "learning_rate": 4.186308107995366e-05,
      "loss": 0.5168,
      "num_input_tokens_seen": 49238896,
      "step": 3925,
      "train_runtime": 48281.8324,
      "train_tokens_per_second": 1019.822
    },
    {
      "epoch": 0.7942603071948262,
      "grad_norm": 6.596989154815674,
      "learning_rate": 4.1843541089931195e-05,
      "loss": 0.6332,
      "num_input_tokens_seen": 49301840,
      "step": 3930,
      "train_runtime": 48335.1953,
      "train_tokens_per_second": 1019.999
    },
    {
      "epoch": 0.7952708164915118,
      "grad_norm": 2.3638150691986084,
      "learning_rate": 4.1823982238626104e-05,
      "loss": 0.4854,
      "num_input_tokens_seen": 49364240,
      "step": 3935,
      "train_runtime": 48388.5235,
      "train_tokens_per_second": 1020.164
    },
    {
      "epoch": 0.7962813257881972,
      "grad_norm": 1.8217140436172485,
      "learning_rate": 4.1804404547940274e-05,
      "loss": 0.6378,
      "num_input_tokens_seen": 49427888,
      "step": 3940,
      "train_runtime": 48442.3259,
      "train_tokens_per_second": 1020.345
    },
    {
      "epoch": 0.7972918350848828,
      "grad_norm": 2.171380043029785,
      "learning_rate": 4.178480803979666e-05,
      "loss": 0.5552,
      "num_input_tokens_seen": 49490224,
      "step": 3945,
      "train_runtime": 48495.1924,
      "train_tokens_per_second": 1020.518
    },
    {
      "epoch": 0.7983023443815683,
      "grad_norm": 1.905605435371399,
      "learning_rate": 4.17651927361393e-05,
      "loss": 0.5488,
      "num_input_tokens_seen": 49552416,
      "step": 3950,
      "train_runtime": 48548.102,
      "train_tokens_per_second": 1020.687
    },
    {
      "epoch": 0.7993128536782539,
      "grad_norm": 1.4563610553741455,
      "learning_rate": 4.174555865893328e-05,
      "loss": 0.3769,
      "num_input_tokens_seen": 49615536,
      "step": 3955,
      "train_runtime": 48601.601,
      "train_tokens_per_second": 1020.862
    },
    {
      "epoch": 0.8003233629749393,
      "grad_norm": 2.045550584793091,
      "learning_rate": 4.17259058301647e-05,
      "loss": 0.4216,
      "num_input_tokens_seen": 49677536,
      "step": 3960,
      "train_runtime": 48654.4103,
      "train_tokens_per_second": 1021.028
    },
    {
      "epoch": 0.8013338722716249,
      "grad_norm": 2.221576452255249,
      "learning_rate": 4.170623427184067e-05,
      "loss": 0.6094,
      "num_input_tokens_seen": 49739808,
      "step": 3965,
      "train_runtime": 48707.3466,
      "train_tokens_per_second": 1021.197
    },
    {
      "epoch": 0.8023443815683104,
      "grad_norm": 1.7563120126724243,
      "learning_rate": 4.168654400598927e-05,
      "loss": 0.4877,
      "num_input_tokens_seen": 49800400,
      "step": 3970,
      "train_runtime": 48758.972,
      "train_tokens_per_second": 1021.359
    },
    {
      "epoch": 0.803354890864996,
      "grad_norm": 2.30084490776062,
      "learning_rate": 4.166683505465951e-05,
      "loss": 0.5115,
      "num_input_tokens_seen": 49863600,
      "step": 3975,
      "train_runtime": 48812.6822,
      "train_tokens_per_second": 1021.53
    },
    {
      "epoch": 0.8043654001616815,
      "grad_norm": 2.2606091499328613,
      "learning_rate": 4.164710743992136e-05,
      "loss": 0.4921,
      "num_input_tokens_seen": 49926560,
      "step": 3980,
      "train_runtime": 48866.0751,
      "train_tokens_per_second": 1021.702
    },
    {
      "epoch": 0.805375909458367,
      "grad_norm": 3.198673963546753,
      "learning_rate": 4.1627361183865646e-05,
      "loss": 0.5396,
      "num_input_tokens_seen": 49990096,
      "step": 3985,
      "train_runtime": 48919.8596,
      "train_tokens_per_second": 1021.877
    },
    {
      "epoch": 0.8063864187550526,
      "grad_norm": 6.917028427124023,
      "learning_rate": 4.1607596308604115e-05,
      "loss": 0.6498,
      "num_input_tokens_seen": 50052784,
      "step": 3990,
      "train_runtime": 48973.1023,
      "train_tokens_per_second": 1022.046
    },
    {
      "epoch": 0.8073969280517381,
      "grad_norm": 1.9554572105407715,
      "learning_rate": 4.158781283626932e-05,
      "loss": 0.4407,
      "num_input_tokens_seen": 50115632,
      "step": 3995,
      "train_runtime": 49026.3725,
      "train_tokens_per_second": 1022.218
    },
    {
      "epoch": 0.8084074373484236,
      "grad_norm": 3.005791425704956,
      "learning_rate": 4.156801078901468e-05,
      "loss": 0.4595,
      "num_input_tokens_seen": 50178528,
      "step": 4000,
      "train_runtime": 49079.6519,
      "train_tokens_per_second": 1022.39
    },
    {
      "epoch": 0.8094179466451091,
      "grad_norm": 2.2493834495544434,
      "learning_rate": 4.154819018901437e-05,
      "loss": 0.5878,
      "num_input_tokens_seen": 50240288,
      "step": 4005,
      "train_runtime": 49132.7821,
      "train_tokens_per_second": 1022.541
    },
    {
      "epoch": 0.8104284559417947,
      "grad_norm": 1.7571550607681274,
      "learning_rate": 4.1528351058463366e-05,
      "loss": 0.5316,
      "num_input_tokens_seen": 50302848,
      "step": 4010,
      "train_runtime": 49185.9172,
      "train_tokens_per_second": 1022.708
    },
    {
      "epoch": 0.8114389652384802,
      "grad_norm": 1.8312500715255737,
      "learning_rate": 4.150849341957741e-05,
      "loss": 0.5974,
      "num_input_tokens_seen": 50366192,
      "step": 4015,
      "train_runtime": 49239.626,
      "train_tokens_per_second": 1022.879
    },
    {
      "epoch": 0.8124494745351657,
      "grad_norm": 1.5523344278335571,
      "learning_rate": 4.148861729459294e-05,
      "loss": 0.543,
      "num_input_tokens_seen": 50428208,
      "step": 4020,
      "train_runtime": 49292.4948,
      "train_tokens_per_second": 1023.04
    },
    {
      "epoch": 0.8134599838318513,
      "grad_norm": 2.238966226577759,
      "learning_rate": 4.146872270576709e-05,
      "loss": 0.4786,
      "num_input_tokens_seen": 50490384,
      "step": 4025,
      "train_runtime": 49345.3424,
      "train_tokens_per_second": 1023.205
    },
    {
      "epoch": 0.8144704931285368,
      "grad_norm": 6.718408107757568,
      "learning_rate": 4.144880967537771e-05,
      "loss": 0.6348,
      "num_input_tokens_seen": 50554608,
      "step": 4030,
      "train_runtime": 49399.7066,
      "train_tokens_per_second": 1023.379
    },
    {
      "epoch": 0.8154810024252223,
      "grad_norm": 25.852441787719727,
      "learning_rate": 4.1428878225723254e-05,
      "loss": 0.4308,
      "num_input_tokens_seen": 50617664,
      "step": 4035,
      "train_runtime": 49453.3448,
      "train_tokens_per_second": 1023.544
    },
    {
      "epoch": 0.8164915117219078,
      "grad_norm": 2.716392993927002,
      "learning_rate": 4.1408928379122846e-05,
      "loss": 0.447,
      "num_input_tokens_seen": 50680496,
      "step": 4040,
      "train_runtime": 49506.7157,
      "train_tokens_per_second": 1023.71
    },
    {
      "epoch": 0.8175020210185934,
      "grad_norm": 2.6844046115875244,
      "learning_rate": 4.138896015791617e-05,
      "loss": 0.7395,
      "num_input_tokens_seen": 50743760,
      "step": 4045,
      "train_runtime": 49560.2964,
      "train_tokens_per_second": 1023.879
    },
    {
      "epoch": 0.8185125303152789,
      "grad_norm": 3.3638346195220947,
      "learning_rate": 4.136897358446351e-05,
      "loss": 0.474,
      "num_input_tokens_seen": 50806032,
      "step": 4050,
      "train_runtime": 49613.2043,
      "train_tokens_per_second": 1024.043
    },
    {
      "epoch": 0.8195230396119644,
      "grad_norm": 1.7049438953399658,
      "learning_rate": 4.13489686811457e-05,
      "loss": 0.5308,
      "num_input_tokens_seen": 50869312,
      "step": 4055,
      "train_runtime": 49666.8554,
      "train_tokens_per_second": 1024.21
    },
    {
      "epoch": 0.8205335489086499,
      "grad_norm": 1.3196289539337158,
      "learning_rate": 4.132894547036409e-05,
      "loss": 0.3998,
      "num_input_tokens_seen": 50930928,
      "step": 4060,
      "train_runtime": 49719.1737,
      "train_tokens_per_second": 1024.372
    },
    {
      "epoch": 0.8215440582053355,
      "grad_norm": 3.057187557220459,
      "learning_rate": 4.130890397454054e-05,
      "loss": 0.5473,
      "num_input_tokens_seen": 50993328,
      "step": 4065,
      "train_runtime": 49772.2609,
      "train_tokens_per_second": 1024.533
    },
    {
      "epoch": 0.822554567502021,
      "grad_norm": 2.215963125228882,
      "learning_rate": 4.1288844216117364e-05,
      "loss": 0.517,
      "num_input_tokens_seen": 51055936,
      "step": 4070,
      "train_runtime": 49825.4138,
      "train_tokens_per_second": 1024.697
    },
    {
      "epoch": 0.8235650767987065,
      "grad_norm": 1.9275766611099243,
      "learning_rate": 4.1268766217557364e-05,
      "loss": 0.478,
      "num_input_tokens_seen": 51118144,
      "step": 4075,
      "train_runtime": 49878.2884,
      "train_tokens_per_second": 1024.858
    },
    {
      "epoch": 0.824575586095392,
      "grad_norm": 1.608491063117981,
      "learning_rate": 4.1248670001343735e-05,
      "loss": 0.5462,
      "num_input_tokens_seen": 51180656,
      "step": 4080,
      "train_runtime": 49931.5102,
      "train_tokens_per_second": 1025.017
    },
    {
      "epoch": 0.8255860953920776,
      "grad_norm": 2.2808218002319336,
      "learning_rate": 4.122855558998008e-05,
      "loss": 0.5436,
      "num_input_tokens_seen": 51242864,
      "step": 4085,
      "train_runtime": 49984.3474,
      "train_tokens_per_second": 1025.178
    },
    {
      "epoch": 0.8265966046887632,
      "grad_norm": 2.442976951599121,
      "learning_rate": 4.120842300599039e-05,
      "loss": 0.5096,
      "num_input_tokens_seen": 51305056,
      "step": 4090,
      "train_runtime": 50037.1872,
      "train_tokens_per_second": 1025.339
    },
    {
      "epoch": 0.8276071139854486,
      "grad_norm": 3.218945264816284,
      "learning_rate": 4.118827227191899e-05,
      "loss": 0.4902,
      "num_input_tokens_seen": 51368384,
      "step": 4095,
      "train_runtime": 50090.941,
      "train_tokens_per_second": 1025.502
    },
    {
      "epoch": 0.8286176232821342,
      "grad_norm": 1.8047066926956177,
      "learning_rate": 4.116810341033054e-05,
      "loss": 0.6261,
      "num_input_tokens_seen": 51431584,
      "step": 4100,
      "train_runtime": 50144.4421,
      "train_tokens_per_second": 1025.669
    },
    {
      "epoch": 0.8296281325788197,
      "grad_norm": 4.956545352935791,
      "learning_rate": 4.114791644380998e-05,
      "loss": 0.6003,
      "num_input_tokens_seen": 51495056,
      "step": 4105,
      "train_runtime": 50199.0848,
      "train_tokens_per_second": 1025.817
    },
    {
      "epoch": 0.8306386418755053,
      "grad_norm": 1.3285131454467773,
      "learning_rate": 4.112771139496255e-05,
      "loss": 0.3258,
      "num_input_tokens_seen": 51557248,
      "step": 4110,
      "train_runtime": 50251.9728,
      "train_tokens_per_second": 1025.975
    },
    {
      "epoch": 0.8316491511721907,
      "grad_norm": 2.3606321811676025,
      "learning_rate": 4.110748828641372e-05,
      "loss": 0.4689,
      "num_input_tokens_seen": 51619040,
      "step": 4115,
      "train_runtime": 50304.5219,
      "train_tokens_per_second": 1026.131
    },
    {
      "epoch": 0.8326596604688763,
      "grad_norm": 2.453813314437866,
      "learning_rate": 4.1087247140809196e-05,
      "loss": 0.65,
      "num_input_tokens_seen": 51682000,
      "step": 4120,
      "train_runtime": 50357.8679,
      "train_tokens_per_second": 1026.294
    },
    {
      "epoch": 0.8336701697655619,
      "grad_norm": 4.244381904602051,
      "learning_rate": 4.106698798081487e-05,
      "loss": 0.5363,
      "num_input_tokens_seen": 51744256,
      "step": 4125,
      "train_runtime": 50410.837,
      "train_tokens_per_second": 1026.451
    },
    {
      "epoch": 0.8346806790622474,
      "grad_norm": 2.419301748275757,
      "learning_rate": 4.104671082911681e-05,
      "loss": 0.5236,
      "num_input_tokens_seen": 51807600,
      "step": 4130,
      "train_runtime": 50464.4775,
      "train_tokens_per_second": 1026.615
    },
    {
      "epoch": 0.8356911883589329,
      "grad_norm": 2.3377315998077393,
      "learning_rate": 4.102641570842124e-05,
      "loss": 0.4351,
      "num_input_tokens_seen": 51871360,
      "step": 4135,
      "train_runtime": 50518.4428,
      "train_tokens_per_second": 1026.781
    },
    {
      "epoch": 0.8367016976556184,
      "grad_norm": 2.276008129119873,
      "learning_rate": 4.100610264145449e-05,
      "loss": 0.5584,
      "num_input_tokens_seen": 51933920,
      "step": 4140,
      "train_runtime": 50571.5409,
      "train_tokens_per_second": 1026.94
    },
    {
      "epoch": 0.837712206952304,
      "grad_norm": 1.8687609434127808,
      "learning_rate": 4.0985771650963e-05,
      "loss": 0.5123,
      "num_input_tokens_seen": 51996592,
      "step": 4145,
      "train_runtime": 50624.6618,
      "train_tokens_per_second": 1027.1
    },
    {
      "epoch": 0.8387227162489895,
      "grad_norm": 1.6782805919647217,
      "learning_rate": 4.096542275971327e-05,
      "loss": 0.5888,
      "num_input_tokens_seen": 52059168,
      "step": 4150,
      "train_runtime": 50677.8362,
      "train_tokens_per_second": 1027.257
    },
    {
      "epoch": 0.839733225545675,
      "grad_norm": 2.6153199672698975,
      "learning_rate": 4.094505599049184e-05,
      "loss": 0.5416,
      "num_input_tokens_seen": 52121504,
      "step": 4155,
      "train_runtime": 50730.7926,
      "train_tokens_per_second": 1027.414
    },
    {
      "epoch": 0.8407437348423605,
      "grad_norm": 2.8362531661987305,
      "learning_rate": 4.0924671366105304e-05,
      "loss": 0.5021,
      "num_input_tokens_seen": 52183248,
      "step": 4160,
      "train_runtime": 50783.341,
      "train_tokens_per_second": 1027.566
    },
    {
      "epoch": 0.8417542441390461,
      "grad_norm": 3.0674219131469727,
      "learning_rate": 4.09042689093802e-05,
      "loss": 0.5692,
      "num_input_tokens_seen": 52245792,
      "step": 4165,
      "train_runtime": 50836.4782,
      "train_tokens_per_second": 1027.722
    },
    {
      "epoch": 0.8427647534357317,
      "grad_norm": 1.8192342519760132,
      "learning_rate": 4.088384864316309e-05,
      "loss": 0.4817,
      "num_input_tokens_seen": 52307824,
      "step": 4170,
      "train_runtime": 50889.3587,
      "train_tokens_per_second": 1027.874
    },
    {
      "epoch": 0.8437752627324171,
      "grad_norm": 2.132341146469116,
      "learning_rate": 4.0863410590320415e-05,
      "loss": 0.5357,
      "num_input_tokens_seen": 52370608,
      "step": 4175,
      "train_runtime": 50942.7052,
      "train_tokens_per_second": 1028.03
    },
    {
      "epoch": 0.8447857720291027,
      "grad_norm": 2.879599094390869,
      "learning_rate": 4.084295477373859e-05,
      "loss": 0.4601,
      "num_input_tokens_seen": 52434000,
      "step": 4180,
      "train_runtime": 50996.3553,
      "train_tokens_per_second": 1028.191
    },
    {
      "epoch": 0.8457962813257882,
      "grad_norm": 1.476905345916748,
      "learning_rate": 4.0822481216323906e-05,
      "loss": 0.4855,
      "num_input_tokens_seen": 52495872,
      "step": 4185,
      "train_runtime": 51048.9388,
      "train_tokens_per_second": 1028.344
    },
    {
      "epoch": 0.8468067906224738,
      "grad_norm": 2.286463737487793,
      "learning_rate": 4.080198994100248e-05,
      "loss": 0.4067,
      "num_input_tokens_seen": 52559888,
      "step": 4190,
      "train_runtime": 51103.1047,
      "train_tokens_per_second": 1028.507
    },
    {
      "epoch": 0.8478172999191592,
      "grad_norm": 2.555731773376465,
      "learning_rate": 4.078148097072034e-05,
      "loss": 0.5307,
      "num_input_tokens_seen": 52622736,
      "step": 4195,
      "train_runtime": 51156.5038,
      "train_tokens_per_second": 1028.662
    },
    {
      "epoch": 0.8488278092158448,
      "grad_norm": 2.2541887760162354,
      "learning_rate": 4.0760954328443265e-05,
      "loss": 0.4758,
      "num_input_tokens_seen": 52685584,
      "step": 4200,
      "train_runtime": 51209.881,
      "train_tokens_per_second": 1028.817
    },
    {
      "epoch": 0.8498383185125303,
      "grad_norm": 2.1356563568115234,
      "learning_rate": 4.0740410037156856e-05,
      "loss": 0.4621,
      "num_input_tokens_seen": 52748112,
      "step": 4205,
      "train_runtime": 51263.6781,
      "train_tokens_per_second": 1028.957
    },
    {
      "epoch": 0.8508488278092159,
      "grad_norm": 2.52983021736145,
      "learning_rate": 4.071984811986648e-05,
      "loss": 0.4628,
      "num_input_tokens_seen": 52810784,
      "step": 4210,
      "train_runtime": 51316.7889,
      "train_tokens_per_second": 1029.113
    },
    {
      "epoch": 0.8518593371059013,
      "grad_norm": 1.6105778217315674,
      "learning_rate": 4.069926859959722e-05,
      "loss": 0.5246,
      "num_input_tokens_seen": 52873520,
      "step": 4215,
      "train_runtime": 51370.0082,
      "train_tokens_per_second": 1029.268
    },
    {
      "epoch": 0.8528698464025869,
      "grad_norm": 1.9530906677246094,
      "learning_rate": 4.067867149939389e-05,
      "loss": 0.4736,
      "num_input_tokens_seen": 52936384,
      "step": 4220,
      "train_runtime": 51423.3255,
      "train_tokens_per_second": 1029.424
    },
    {
      "epoch": 0.8538803556992725,
      "grad_norm": 3.2954728603363037,
      "learning_rate": 4.065805684232097e-05,
      "loss": 0.6847,
      "num_input_tokens_seen": 52998336,
      "step": 4225,
      "train_runtime": 51475.8828,
      "train_tokens_per_second": 1029.576
    },
    {
      "epoch": 0.854890864995958,
      "grad_norm": 1.5382277965545654,
      "learning_rate": 4.063742465146264e-05,
      "loss": 0.4952,
      "num_input_tokens_seen": 53060368,
      "step": 4230,
      "train_runtime": 51528.598,
      "train_tokens_per_second": 1029.727
    },
    {
      "epoch": 0.8559013742926435,
      "grad_norm": 1.9011530876159668,
      "learning_rate": 4.061677494992265e-05,
      "loss": 0.66,
      "num_input_tokens_seen": 53122848,
      "step": 4235,
      "train_runtime": 51581.6336,
      "train_tokens_per_second": 1029.879
    },
    {
      "epoch": 0.856911883589329,
      "grad_norm": 2.5433883666992188,
      "learning_rate": 4.059610776082443e-05,
      "loss": 0.4808,
      "num_input_tokens_seen": 53186544,
      "step": 4240,
      "train_runtime": 51635.5522,
      "train_tokens_per_second": 1030.037
    },
    {
      "epoch": 0.8579223928860146,
      "grad_norm": 3.129316568374634,
      "learning_rate": 4.057542310731094e-05,
      "loss": 0.4625,
      "num_input_tokens_seen": 53248912,
      "step": 4245,
      "train_runtime": 51688.5676,
      "train_tokens_per_second": 1030.187
    },
    {
      "epoch": 0.8589329021827001,
      "grad_norm": 1.9907660484313965,
      "learning_rate": 4.055472101254472e-05,
      "loss": 0.7242,
      "num_input_tokens_seen": 53311184,
      "step": 4250,
      "train_runtime": 51741.4629,
      "train_tokens_per_second": 1030.338
    },
    {
      "epoch": 0.8599434114793856,
      "grad_norm": 2.729736804962158,
      "learning_rate": 4.053400149970782e-05,
      "loss": 0.4742,
      "num_input_tokens_seen": 53373520,
      "step": 4255,
      "train_runtime": 51794.5541,
      "train_tokens_per_second": 1030.485
    },
    {
      "epoch": 0.8609539207760711,
      "grad_norm": 5.5506134033203125,
      "learning_rate": 4.051326459200182e-05,
      "loss": 0.6671,
      "num_input_tokens_seen": 53435936,
      "step": 4260,
      "train_runtime": 51847.6448,
      "train_tokens_per_second": 1030.634
    },
    {
      "epoch": 0.8619644300727567,
      "grad_norm": 2.6532163619995117,
      "learning_rate": 4.0492510312647775e-05,
      "loss": 0.6215,
      "num_input_tokens_seen": 53498528,
      "step": 4265,
      "train_runtime": 51900.8268,
      "train_tokens_per_second": 1030.784
    },
    {
      "epoch": 0.8629749393694423,
      "grad_norm": 3.397986888885498,
      "learning_rate": 4.047173868488618e-05,
      "loss": 0.5935,
      "num_input_tokens_seen": 53562560,
      "step": 4270,
      "train_runtime": 51954.9847,
      "train_tokens_per_second": 1030.942
    },
    {
      "epoch": 0.8639854486661277,
      "grad_norm": 2.4828431606292725,
      "learning_rate": 4.0450949731976955e-05,
      "loss": 0.4426,
      "num_input_tokens_seen": 53624400,
      "step": 4275,
      "train_runtime": 52007.8541,
      "train_tokens_per_second": 1031.083
    },
    {
      "epoch": 0.8649959579628133,
      "grad_norm": 2.0460076332092285,
      "learning_rate": 4.0430143477199445e-05,
      "loss": 0.5552,
      "num_input_tokens_seen": 53687504,
      "step": 4280,
      "train_runtime": 52061.2769,
      "train_tokens_per_second": 1031.237
    },
    {
      "epoch": 0.8660064672594988,
      "grad_norm": 5.038049697875977,
      "learning_rate": 4.040931994385233e-05,
      "loss": 0.546,
      "num_input_tokens_seen": 53750624,
      "step": 4285,
      "train_runtime": 52114.7121,
      "train_tokens_per_second": 1031.391
    },
    {
      "epoch": 0.8670169765561844,
      "grad_norm": 3.362572193145752,
      "learning_rate": 4.038847915525368e-05,
      "loss": 0.5631,
      "num_input_tokens_seen": 53812304,
      "step": 4290,
      "train_runtime": 52167.2013,
      "train_tokens_per_second": 1031.535
    },
    {
      "epoch": 0.8680274858528698,
      "grad_norm": 1.6319624185562134,
      "learning_rate": 4.0367621134740865e-05,
      "loss": 0.6394,
      "num_input_tokens_seen": 53875584,
      "step": 4295,
      "train_runtime": 52220.8284,
      "train_tokens_per_second": 1031.688
    },
    {
      "epoch": 0.8690379951495554,
      "grad_norm": 2.464627504348755,
      "learning_rate": 4.034674590567054e-05,
      "loss": 0.4586,
      "num_input_tokens_seen": 53938016,
      "step": 4300,
      "train_runtime": 52273.9175,
      "train_tokens_per_second": 1031.834
    },
    {
      "epoch": 0.8700485044462409,
      "grad_norm": 3.653299331665039,
      "learning_rate": 4.0325853491418664e-05,
      "loss": 0.5432,
      "num_input_tokens_seen": 54000176,
      "step": 4305,
      "train_runtime": 52327.4302,
      "train_tokens_per_second": 1031.967
    },
    {
      "epoch": 0.8710590137429264,
      "grad_norm": 2.774017572402954,
      "learning_rate": 4.03049439153804e-05,
      "loss": 0.5597,
      "num_input_tokens_seen": 54063744,
      "step": 4310,
      "train_runtime": 52381.2598,
      "train_tokens_per_second": 1032.12
    },
    {
      "epoch": 0.8720695230396119,
      "grad_norm": 4.985434055328369,
      "learning_rate": 4.028401720097017e-05,
      "loss": 0.5635,
      "num_input_tokens_seen": 54126272,
      "step": 4315,
      "train_runtime": 52434.4247,
      "train_tokens_per_second": 1032.266
    },
    {
      "epoch": 0.8730800323362975,
      "grad_norm": 1.8484939336776733,
      "learning_rate": 4.0263073371621554e-05,
      "loss": 0.4578,
      "num_input_tokens_seen": 54190192,
      "step": 4320,
      "train_runtime": 52488.5239,
      "train_tokens_per_second": 1032.42
    },
    {
      "epoch": 0.874090541632983,
      "grad_norm": 2.353998899459839,
      "learning_rate": 4.024211245078731e-05,
      "loss": 0.534,
      "num_input_tokens_seen": 54252960,
      "step": 4325,
      "train_runtime": 52541.8438,
      "train_tokens_per_second": 1032.567
    },
    {
      "epoch": 0.8751010509296685,
      "grad_norm": 2.5791101455688477,
      "learning_rate": 4.022113446193933e-05,
      "loss": 0.488,
      "num_input_tokens_seen": 54315968,
      "step": 4330,
      "train_runtime": 52595.3244,
      "train_tokens_per_second": 1032.715
    },
    {
      "epoch": 0.8761115602263541,
      "grad_norm": 2.5182952880859375,
      "learning_rate": 4.020013942856864e-05,
      "loss": 0.5762,
      "num_input_tokens_seen": 54379376,
      "step": 4335,
      "train_runtime": 52648.9542,
      "train_tokens_per_second": 1032.867
    },
    {
      "epoch": 0.8771220695230396,
      "grad_norm": 2.699636697769165,
      "learning_rate": 4.017912737418532e-05,
      "loss": 0.4879,
      "num_input_tokens_seen": 54442624,
      "step": 4340,
      "train_runtime": 52702.6895,
      "train_tokens_per_second": 1033.014
    },
    {
      "epoch": 0.8781325788197252,
      "grad_norm": 2.2089765071868896,
      "learning_rate": 4.015809832231852e-05,
      "loss": 0.4608,
      "num_input_tokens_seen": 54504944,
      "step": 4345,
      "train_runtime": 52755.6967,
      "train_tokens_per_second": 1033.158
    },
    {
      "epoch": 0.8791430881164106,
      "grad_norm": 2.0359549522399902,
      "learning_rate": 4.0137052296516454e-05,
      "loss": 0.5812,
      "num_input_tokens_seen": 54568176,
      "step": 4350,
      "train_runtime": 52809.3152,
      "train_tokens_per_second": 1033.306
    },
    {
      "epoch": 0.8801535974130962,
      "grad_norm": 2.420830488204956,
      "learning_rate": 4.011598932034629e-05,
      "loss": 0.4389,
      "num_input_tokens_seen": 54630656,
      "step": 4355,
      "train_runtime": 52862.4715,
      "train_tokens_per_second": 1033.449
    },
    {
      "epoch": 0.8811641067097817,
      "grad_norm": 2.966799020767212,
      "learning_rate": 4.009490941739423e-05,
      "loss": 0.4783,
      "num_input_tokens_seen": 54693328,
      "step": 4360,
      "train_runtime": 52915.7504,
      "train_tokens_per_second": 1033.593
    },
    {
      "epoch": 0.8821746160064673,
      "grad_norm": 2.4117369651794434,
      "learning_rate": 4.007381261126539e-05,
      "loss": 0.4512,
      "num_input_tokens_seen": 54757200,
      "step": 4365,
      "train_runtime": 52969.6832,
      "train_tokens_per_second": 1033.746
    },
    {
      "epoch": 0.8831851253031527,
      "grad_norm": 2.927680730819702,
      "learning_rate": 4.005269892558383e-05,
      "loss": 0.4716,
      "num_input_tokens_seen": 54819376,
      "step": 4370,
      "train_runtime": 53022.4954,
      "train_tokens_per_second": 1033.889
    },
    {
      "epoch": 0.8841956345998383,
      "grad_norm": 2.149078130722046,
      "learning_rate": 4.003156838399252e-05,
      "loss": 0.5299,
      "num_input_tokens_seen": 54882352,
      "step": 4375,
      "train_runtime": 53075.8569,
      "train_tokens_per_second": 1034.036
    },
    {
      "epoch": 0.8852061438965239,
      "grad_norm": 1.882054328918457,
      "learning_rate": 4.0010421010153285e-05,
      "loss": 0.6183,
      "num_input_tokens_seen": 54945328,
      "step": 4380,
      "train_runtime": 53129.3277,
      "train_tokens_per_second": 1034.181
    },
    {
      "epoch": 0.8862166531932094,
      "grad_norm": 1.8714667558670044,
      "learning_rate": 3.998925682774683e-05,
      "loss": 0.4609,
      "num_input_tokens_seen": 55008672,
      "step": 4385,
      "train_runtime": 53183.0225,
      "train_tokens_per_second": 1034.328
    },
    {
      "epoch": 0.8872271624898949,
      "grad_norm": 2.1281089782714844,
      "learning_rate": 3.996807586047264e-05,
      "loss": 0.4403,
      "num_input_tokens_seen": 55072048,
      "step": 4390,
      "train_runtime": 53236.7622,
      "train_tokens_per_second": 1034.474
    },
    {
      "epoch": 0.8882376717865804,
      "grad_norm": 33.98843002319336,
      "learning_rate": 3.994687813204903e-05,
      "loss": 0.4968,
      "num_input_tokens_seen": 55134896,
      "step": 4395,
      "train_runtime": 53290.1458,
      "train_tokens_per_second": 1034.617
    },
    {
      "epoch": 0.889248181083266,
      "grad_norm": 3.422545909881592,
      "learning_rate": 3.992566366621307e-05,
      "loss": 0.4843,
      "num_input_tokens_seen": 55197600,
      "step": 4400,
      "train_runtime": 53343.3423,
      "train_tokens_per_second": 1034.761
    },
    {
      "epoch": 0.8902586903799515,
      "grad_norm": 2.036436080932617,
      "learning_rate": 3.990443248672058e-05,
      "loss": 0.5641,
      "num_input_tokens_seen": 55260448,
      "step": 4405,
      "train_runtime": 53397.3957,
      "train_tokens_per_second": 1034.89
    },
    {
      "epoch": 0.891269199676637,
      "grad_norm": 1.9309730529785156,
      "learning_rate": 3.988318461734607e-05,
      "loss": 0.4885,
      "num_input_tokens_seen": 55323552,
      "step": 4410,
      "train_runtime": 53450.8389,
      "train_tokens_per_second": 1035.036
    },
    {
      "epoch": 0.8922797089733225,
      "grad_norm": 2.5203914642333984,
      "learning_rate": 3.9861920081882783e-05,
      "loss": 0.5356,
      "num_input_tokens_seen": 55387504,
      "step": 4415,
      "train_runtime": 53504.9525,
      "train_tokens_per_second": 1035.185
    },
    {
      "epoch": 0.8932902182700081,
      "grad_norm": 1.7756476402282715,
      "learning_rate": 3.984063890414259e-05,
      "loss": 0.4647,
      "num_input_tokens_seen": 55448960,
      "step": 4420,
      "train_runtime": 53557.171,
      "train_tokens_per_second": 1035.323
    },
    {
      "epoch": 0.8943007275666937,
      "grad_norm": 2.9435126781463623,
      "learning_rate": 3.981934110795601e-05,
      "loss": 0.4625,
      "num_input_tokens_seen": 55512256,
      "step": 4425,
      "train_runtime": 53610.7781,
      "train_tokens_per_second": 1035.468
    },
    {
      "epoch": 0.8953112368633791,
      "grad_norm": 1.6444402933120728,
      "learning_rate": 3.979802671717217e-05,
      "loss": 0.4414,
      "num_input_tokens_seen": 55575472,
      "step": 4430,
      "train_runtime": 53664.4338,
      "train_tokens_per_second": 1035.611
    },
    {
      "epoch": 0.8963217461600647,
      "grad_norm": 2.473832368850708,
      "learning_rate": 3.977669575565879e-05,
      "loss": 0.5721,
      "num_input_tokens_seen": 55638640,
      "step": 4435,
      "train_runtime": 53717.8499,
      "train_tokens_per_second": 1035.757
    },
    {
      "epoch": 0.8973322554567502,
      "grad_norm": 1.2993834018707275,
      "learning_rate": 3.975534824730211e-05,
      "loss": 0.6013,
      "num_input_tokens_seen": 55701280,
      "step": 4440,
      "train_runtime": 53770.8631,
      "train_tokens_per_second": 1035.901
    },
    {
      "epoch": 0.8983427647534358,
      "grad_norm": 2.8045084476470947,
      "learning_rate": 3.9733984216006946e-05,
      "loss": 0.43,
      "num_input_tokens_seen": 55764144,
      "step": 4445,
      "train_runtime": 53824.3653,
      "train_tokens_per_second": 1036.039
    },
    {
      "epoch": 0.8993532740501212,
      "grad_norm": 2.343620538711548,
      "learning_rate": 3.9712603685696594e-05,
      "loss": 0.6016,
      "num_input_tokens_seen": 55826736,
      "step": 4450,
      "train_runtime": 53877.65,
      "train_tokens_per_second": 1036.176
    },
    {
      "epoch": 0.9003637833468068,
      "grad_norm": 2.3364877700805664,
      "learning_rate": 3.969120668031282e-05,
      "loss": 0.6479,
      "num_input_tokens_seen": 55889168,
      "step": 4455,
      "train_runtime": 53930.6234,
      "train_tokens_per_second": 1036.316
    },
    {
      "epoch": 0.9013742926434923,
      "grad_norm": 2.337895154953003,
      "learning_rate": 3.9669793223815844e-05,
      "loss": 0.6088,
      "num_input_tokens_seen": 55951840,
      "step": 4460,
      "train_runtime": 53983.7322,
      "train_tokens_per_second": 1036.457
    },
    {
      "epoch": 0.9023848019401779,
      "grad_norm": 2.287299871444702,
      "learning_rate": 3.964836334018431e-05,
      "loss": 0.5345,
      "num_input_tokens_seen": 56014560,
      "step": 4465,
      "train_runtime": 54037.1468,
      "train_tokens_per_second": 1036.594
    },
    {
      "epoch": 0.9033953112368633,
      "grad_norm": 4.105304718017578,
      "learning_rate": 3.962691705341526e-05,
      "loss": 0.4541,
      "num_input_tokens_seen": 56076640,
      "step": 4470,
      "train_runtime": 54089.8955,
      "train_tokens_per_second": 1036.73
    },
    {
      "epoch": 0.9044058205335489,
      "grad_norm": 1.7126332521438599,
      "learning_rate": 3.960545438752409e-05,
      "loss": 0.4144,
      "num_input_tokens_seen": 56139712,
      "step": 4475,
      "train_runtime": 54143.3656,
      "train_tokens_per_second": 1036.871
    },
    {
      "epoch": 0.9054163298302345,
      "grad_norm": 2.0385470390319824,
      "learning_rate": 3.9583975366544554e-05,
      "loss": 0.4356,
      "num_input_tokens_seen": 56203216,
      "step": 4480,
      "train_runtime": 54197.107,
      "train_tokens_per_second": 1037.015
    },
    {
      "epoch": 0.90642683912692,
      "grad_norm": 2.3789799213409424,
      "learning_rate": 3.956248001452873e-05,
      "loss": 0.5287,
      "num_input_tokens_seen": 56266064,
      "step": 4485,
      "train_runtime": 54250.4524,
      "train_tokens_per_second": 1037.154
    },
    {
      "epoch": 0.9074373484236055,
      "grad_norm": 2.361480474472046,
      "learning_rate": 3.9540968355546923e-05,
      "loss": 0.5098,
      "num_input_tokens_seen": 56328784,
      "step": 4490,
      "train_runtime": 54303.7529,
      "train_tokens_per_second": 1037.291
    },
    {
      "epoch": 0.908447857720291,
      "grad_norm": 2.2616870403289795,
      "learning_rate": 3.951944041368779e-05,
      "loss": 0.5002,
      "num_input_tokens_seen": 56390576,
      "step": 4495,
      "train_runtime": 54356.3884,
      "train_tokens_per_second": 1037.423
    },
    {
      "epoch": 0.9094583670169766,
      "grad_norm": 2.0945980548858643,
      "learning_rate": 3.949789621305815e-05,
      "loss": 0.5726,
      "num_input_tokens_seen": 56453840,
      "step": 4500,
      "train_runtime": 54410.0309,
      "train_tokens_per_second": 1037.563
    },
    {
      "epoch": 0.9104688763136621,
      "grad_norm": 2.1266613006591797,
      "learning_rate": 3.9476335777783046e-05,
      "loss": 0.5611,
      "num_input_tokens_seen": 56518352,
      "step": 4505,
      "train_runtime": 54465.2924,
      "train_tokens_per_second": 1037.695
    },
    {
      "epoch": 0.9114793856103476,
      "grad_norm": 9.120553016662598,
      "learning_rate": 3.945475913200572e-05,
      "loss": 0.5681,
      "num_input_tokens_seen": 56581040,
      "step": 4510,
      "train_runtime": 54518.4265,
      "train_tokens_per_second": 1037.833
    },
    {
      "epoch": 0.9124898949070331,
      "grad_norm": 1.6878447532653809,
      "learning_rate": 3.943316629988756e-05,
      "loss": 0.6045,
      "num_input_tokens_seen": 56644944,
      "step": 4515,
      "train_runtime": 54572.5266,
      "train_tokens_per_second": 1037.975
    },
    {
      "epoch": 0.9135004042037187,
      "grad_norm": 2.2848563194274902,
      "learning_rate": 3.941155730560804e-05,
      "loss": 0.4985,
      "num_input_tokens_seen": 56708976,
      "step": 4520,
      "train_runtime": 54626.732,
      "train_tokens_per_second": 1038.118
    },
    {
      "epoch": 0.9145109135004043,
      "grad_norm": 1.6974833011627197,
      "learning_rate": 3.93899321733648e-05,
      "loss": 0.3921,
      "num_input_tokens_seen": 56771040,
      "step": 4525,
      "train_runtime": 54679.5236,
      "train_tokens_per_second": 1038.25
    },
    {
      "epoch": 0.9155214227970897,
      "grad_norm": 2.3680670261383057,
      "learning_rate": 3.9368290927373494e-05,
      "loss": 0.5642,
      "num_input_tokens_seen": 56833296,
      "step": 4530,
      "train_runtime": 54732.4154,
      "train_tokens_per_second": 1038.385
    },
    {
      "epoch": 0.9165319320937753,
      "grad_norm": 3.522139549255371,
      "learning_rate": 3.934663359186786e-05,
      "loss": 0.5829,
      "num_input_tokens_seen": 56895760,
      "step": 4535,
      "train_runtime": 54785.4765,
      "train_tokens_per_second": 1038.519
    },
    {
      "epoch": 0.9175424413904608,
      "grad_norm": 2.9872617721557617,
      "learning_rate": 3.9324960191099616e-05,
      "loss": 0.5644,
      "num_input_tokens_seen": 56958144,
      "step": 4540,
      "train_runtime": 54838.4002,
      "train_tokens_per_second": 1038.654
    },
    {
      "epoch": 0.9185529506871464,
      "grad_norm": 2.7378079891204834,
      "learning_rate": 3.93032707493385e-05,
      "loss": 0.4868,
      "num_input_tokens_seen": 57020688,
      "step": 4545,
      "train_runtime": 54891.4321,
      "train_tokens_per_second": 1038.79
    },
    {
      "epoch": 0.9195634599838318,
      "grad_norm": 1.962058186531067,
      "learning_rate": 3.9281565290872196e-05,
      "loss": 0.4939,
      "num_input_tokens_seen": 57083264,
      "step": 4550,
      "train_runtime": 54944.6144,
      "train_tokens_per_second": 1038.924
    },
    {
      "epoch": 0.9205739692805174,
      "grad_norm": 2.481644868850708,
      "learning_rate": 3.925984384000633e-05,
      "loss": 0.5992,
      "num_input_tokens_seen": 57146592,
      "step": 4555,
      "train_runtime": 54998.2226,
      "train_tokens_per_second": 1039.063
    },
    {
      "epoch": 0.9215844785772029,
      "grad_norm": 2.2578587532043457,
      "learning_rate": 3.923810642106444e-05,
      "loss": 0.5607,
      "num_input_tokens_seen": 57209776,
      "step": 4560,
      "train_runtime": 55051.8014,
      "train_tokens_per_second": 1039.199
    },
    {
      "epoch": 0.9225949878738885,
      "grad_norm": 10.093400955200195,
      "learning_rate": 3.921635305838793e-05,
      "loss": 0.5038,
      "num_input_tokens_seen": 57272320,
      "step": 4565,
      "train_runtime": 55104.9628,
      "train_tokens_per_second": 1039.331
    },
    {
      "epoch": 0.9236054971705739,
      "grad_norm": 2.803948402404785,
      "learning_rate": 3.919458377633608e-05,
      "loss": 0.5139,
      "num_input_tokens_seen": 57334464,
      "step": 4570,
      "train_runtime": 55157.8751,
      "train_tokens_per_second": 1039.461
    },
    {
      "epoch": 0.9246160064672595,
      "grad_norm": 3.04189395904541,
      "learning_rate": 3.917279859928597e-05,
      "loss": 0.6485,
      "num_input_tokens_seen": 57397536,
      "step": 4575,
      "train_runtime": 55211.2836,
      "train_tokens_per_second": 1039.598
    },
    {
      "epoch": 0.9256265157639451,
      "grad_norm": 2.803323745727539,
      "learning_rate": 3.9150997551632494e-05,
      "loss": 0.4648,
      "num_input_tokens_seen": 57460304,
      "step": 4580,
      "train_runtime": 55264.5085,
      "train_tokens_per_second": 1039.732
    },
    {
      "epoch": 0.9266370250606305,
      "grad_norm": 1.7505314350128174,
      "learning_rate": 3.9129180657788335e-05,
      "loss": 0.4679,
      "num_input_tokens_seen": 57521792,
      "step": 4585,
      "train_runtime": 55316.6901,
      "train_tokens_per_second": 1039.863
    },
    {
      "epoch": 0.9276475343573161,
      "grad_norm": 1.8972257375717163,
      "learning_rate": 3.910734794218388e-05,
      "loss": 0.5926,
      "num_input_tokens_seen": 57584320,
      "step": 4590,
      "train_runtime": 55369.7808,
      "train_tokens_per_second": 1039.995
    },
    {
      "epoch": 0.9286580436540016,
      "grad_norm": 2.4559314250946045,
      "learning_rate": 3.9085499429267266e-05,
      "loss": 0.5385,
      "num_input_tokens_seen": 57647472,
      "step": 4595,
      "train_runtime": 55423.2989,
      "train_tokens_per_second": 1040.131
    },
    {
      "epoch": 0.9296685529506872,
      "grad_norm": 2.4988672733306885,
      "learning_rate": 3.906363514350431e-05,
      "loss": 0.6416,
      "num_input_tokens_seen": 57708576,
      "step": 4600,
      "train_runtime": 55475.2728,
      "train_tokens_per_second": 1040.258
    },
    {
      "epoch": 0.9306790622473726,
      "grad_norm": 3.7148070335388184,
      "learning_rate": 3.904175510937848e-05,
      "loss": 0.669,
      "num_input_tokens_seen": 57770864,
      "step": 4605,
      "train_runtime": 55528.9359,
      "train_tokens_per_second": 1040.374
    },
    {
      "epoch": 0.9316895715440582,
      "grad_norm": 2.2063040733337402,
      "learning_rate": 3.9019859351390897e-05,
      "loss": 0.4299,
      "num_input_tokens_seen": 57832336,
      "step": 4610,
      "train_runtime": 55581.3292,
      "train_tokens_per_second": 1040.499
    },
    {
      "epoch": 0.9327000808407437,
      "grad_norm": 1.1304970979690552,
      "learning_rate": 3.8997947894060294e-05,
      "loss": 0.5213,
      "num_input_tokens_seen": 57894368,
      "step": 4615,
      "train_runtime": 55634.0822,
      "train_tokens_per_second": 1040.628
    },
    {
      "epoch": 0.9337105901374293,
      "grad_norm": 3.481020927429199,
      "learning_rate": 3.897602076192295e-05,
      "loss": 0.4476,
      "num_input_tokens_seen": 57957008,
      "step": 4620,
      "train_runtime": 55687.2203,
      "train_tokens_per_second": 1040.76
    },
    {
      "epoch": 0.9347210994341147,
      "grad_norm": 1.9522442817687988,
      "learning_rate": 3.895407797953273e-05,
      "loss": 0.5109,
      "num_input_tokens_seen": 58020528,
      "step": 4625,
      "train_runtime": 55741.0335,
      "train_tokens_per_second": 1040.894
    },
    {
      "epoch": 0.9357316087308003,
      "grad_norm": 3.211536407470703,
      "learning_rate": 3.893211957146101e-05,
      "loss": 0.5957,
      "num_input_tokens_seen": 58083200,
      "step": 4630,
      "train_runtime": 55794.189,
      "train_tokens_per_second": 1041.026
    },
    {
      "epoch": 0.9367421180274859,
      "grad_norm": 8.661966323852539,
      "learning_rate": 3.8910145562296664e-05,
      "loss": 0.4821,
      "num_input_tokens_seen": 58145152,
      "step": 4635,
      "train_runtime": 55846.7757,
      "train_tokens_per_second": 1041.155
    },
    {
      "epoch": 0.9377526273241714,
      "grad_norm": 1.4505726099014282,
      "learning_rate": 3.888815597664603e-05,
      "loss": 0.6078,
      "num_input_tokens_seen": 58206656,
      "step": 4640,
      "train_runtime": 55898.9122,
      "train_tokens_per_second": 1041.284
    },
    {
      "epoch": 0.9387631366208569,
      "grad_norm": 2.6066884994506836,
      "learning_rate": 3.886615083913292e-05,
      "loss": 0.4315,
      "num_input_tokens_seen": 58269600,
      "step": 4645,
      "train_runtime": 55952.2278,
      "train_tokens_per_second": 1041.417
    },
    {
      "epoch": 0.9397736459175424,
      "grad_norm": 2.072139024734497,
      "learning_rate": 3.884413017439852e-05,
      "loss": 0.5606,
      "num_input_tokens_seen": 58332992,
      "step": 4650,
      "train_runtime": 56005.9061,
      "train_tokens_per_second": 1041.551
    },
    {
      "epoch": 0.940784155214228,
      "grad_norm": 3.6480355262756348,
      "learning_rate": 3.882209400710143e-05,
      "loss": 0.4991,
      "num_input_tokens_seen": 58395696,
      "step": 4655,
      "train_runtime": 56059.0176,
      "train_tokens_per_second": 1041.682
    },
    {
      "epoch": 0.9417946645109135,
      "grad_norm": 1.2796025276184082,
      "learning_rate": 3.880004236191758e-05,
      "loss": 0.4818,
      "num_input_tokens_seen": 58457760,
      "step": 4660,
      "train_runtime": 56111.6672,
      "train_tokens_per_second": 1041.811
    },
    {
      "epoch": 0.942805173807599,
      "grad_norm": 2.346433401107788,
      "learning_rate": 3.877797526354027e-05,
      "loss": 0.4524,
      "num_input_tokens_seen": 58519824,
      "step": 4665,
      "train_runtime": 56164.2521,
      "train_tokens_per_second": 1041.941
    },
    {
      "epoch": 0.9438156831042845,
      "grad_norm": 2.376281499862671,
      "learning_rate": 3.875589273668008e-05,
      "loss": 0.5234,
      "num_input_tokens_seen": 58582912,
      "step": 4670,
      "train_runtime": 56217.7258,
      "train_tokens_per_second": 1042.072
    },
    {
      "epoch": 0.9448261924009701,
      "grad_norm": 2.929840564727783,
      "learning_rate": 3.873379480606488e-05,
      "loss": 0.5906,
      "num_input_tokens_seen": 58646304,
      "step": 4675,
      "train_runtime": 56271.5208,
      "train_tokens_per_second": 1042.202
    },
    {
      "epoch": 0.9458367016976557,
      "grad_norm": 2.762417793273926,
      "learning_rate": 3.8711681496439775e-05,
      "loss": 0.5419,
      "num_input_tokens_seen": 58709056,
      "step": 4680,
      "train_runtime": 56324.6884,
      "train_tokens_per_second": 1042.333
    },
    {
      "epoch": 0.9468472109943411,
      "grad_norm": 2.1553409099578857,
      "learning_rate": 3.868955283256711e-05,
      "loss": 0.504,
      "num_input_tokens_seen": 58771248,
      "step": 4685,
      "train_runtime": 56377.5628,
      "train_tokens_per_second": 1042.458
    },
    {
      "epoch": 0.9478577202910267,
      "grad_norm": 2.075336456298828,
      "learning_rate": 3.866740883922639e-05,
      "loss": 0.7064,
      "num_input_tokens_seen": 58834656,
      "step": 4690,
      "train_runtime": 56431.3251,
      "train_tokens_per_second": 1042.589
    },
    {
      "epoch": 0.9488682295877122,
      "grad_norm": 1.6377204656600952,
      "learning_rate": 3.8645249541214325e-05,
      "loss": 0.4451,
      "num_input_tokens_seen": 58897568,
      "step": 4695,
      "train_runtime": 56484.5571,
      "train_tokens_per_second": 1042.72
    },
    {
      "epoch": 0.9498787388843978,
      "grad_norm": 1.940409541130066,
      "learning_rate": 3.862307496334474e-05,
      "loss": 0.6174,
      "num_input_tokens_seen": 58960512,
      "step": 4700,
      "train_runtime": 56537.8984,
      "train_tokens_per_second": 1042.849
    },
    {
      "epoch": 0.9508892481810832,
      "grad_norm": 2.2130281925201416,
      "learning_rate": 3.860088513044858e-05,
      "loss": 0.5833,
      "num_input_tokens_seen": 59022592,
      "step": 4705,
      "train_runtime": 56591.301,
      "train_tokens_per_second": 1042.962
    },
    {
      "epoch": 0.9518997574777688,
      "grad_norm": 2.0039749145507812,
      "learning_rate": 3.857868006737387e-05,
      "loss": 0.5692,
      "num_input_tokens_seen": 59086432,
      "step": 4710,
      "train_runtime": 56645.2505,
      "train_tokens_per_second": 1043.096
    },
    {
      "epoch": 0.9529102667744543,
      "grad_norm": 1.7287758588790894,
      "learning_rate": 3.8556459798985685e-05,
      "loss": 0.5811,
      "num_input_tokens_seen": 59149168,
      "step": 4715,
      "train_runtime": 56698.4972,
      "train_tokens_per_second": 1043.223
    },
    {
      "epoch": 0.9539207760711399,
      "grad_norm": 1.7209349870681763,
      "learning_rate": 3.8534224350166124e-05,
      "loss": 0.633,
      "num_input_tokens_seen": 59211584,
      "step": 4720,
      "train_runtime": 56751.3213,
      "train_tokens_per_second": 1043.352
    },
    {
      "epoch": 0.9549312853678253,
      "grad_norm": 1.5876020193099976,
      "learning_rate": 3.851197374581431e-05,
      "loss": 0.4274,
      "num_input_tokens_seen": 59274816,
      "step": 4725,
      "train_runtime": 56804.8348,
      "train_tokens_per_second": 1043.482
    },
    {
      "epoch": 0.9559417946645109,
      "grad_norm": 2.3966259956359863,
      "learning_rate": 3.848970801084629e-05,
      "loss": 0.4848,
      "num_input_tokens_seen": 59337296,
      "step": 4730,
      "train_runtime": 56857.8473,
      "train_tokens_per_second": 1043.608
    },
    {
      "epoch": 0.9569523039611965,
      "grad_norm": 3.7419610023498535,
      "learning_rate": 3.8467427170195114e-05,
      "loss": 0.5108,
      "num_input_tokens_seen": 59400768,
      "step": 4735,
      "train_runtime": 56911.5078,
      "train_tokens_per_second": 1043.739
    },
    {
      "epoch": 0.957962813257882,
      "grad_norm": 1.6289347410202026,
      "learning_rate": 3.84451312488107e-05,
      "loss": 0.4017,
      "num_input_tokens_seen": 59462848,
      "step": 4740,
      "train_runtime": 56964.2262,
      "train_tokens_per_second": 1043.863
    },
    {
      "epoch": 0.9589733225545675,
      "grad_norm": 2.888622283935547,
      "learning_rate": 3.842282027165987e-05,
      "loss": 0.5471,
      "num_input_tokens_seen": 59526928,
      "step": 4745,
      "train_runtime": 57018.2395,
      "train_tokens_per_second": 1043.998
    },
    {
      "epoch": 0.959983831851253,
      "grad_norm": 1.8701590299606323,
      "learning_rate": 3.8400494263726286e-05,
      "loss": 0.5035,
      "num_input_tokens_seen": 59589616,
      "step": 4750,
      "train_runtime": 57071.4164,
      "train_tokens_per_second": 1044.124
    },
    {
      "epoch": 0.9609943411479386,
      "grad_norm": 2.5234904289245605,
      "learning_rate": 3.837815325001049e-05,
      "loss": 0.3585,
      "num_input_tokens_seen": 59652880,
      "step": 4755,
      "train_runtime": 57124.9709,
      "train_tokens_per_second": 1044.252
    },
    {
      "epoch": 0.9620048504446241,
      "grad_norm": 1.96832275390625,
      "learning_rate": 3.835579725552978e-05,
      "loss": 0.6593,
      "num_input_tokens_seen": 59716256,
      "step": 4760,
      "train_runtime": 57178.6428,
      "train_tokens_per_second": 1044.38
    },
    {
      "epoch": 0.9630153597413096,
      "grad_norm": 3.2504048347473145,
      "learning_rate": 3.8333426305318256e-05,
      "loss": 0.5946,
      "num_input_tokens_seen": 59778688,
      "step": 4765,
      "train_runtime": 57231.6668,
      "train_tokens_per_second": 1044.504
    },
    {
      "epoch": 0.9640258690379951,
      "grad_norm": 1.910295844078064,
      "learning_rate": 3.8311040424426745e-05,
      "loss": 0.5391,
      "num_input_tokens_seen": 59841408,
      "step": 4770,
      "train_runtime": 57284.9159,
      "train_tokens_per_second": 1044.628
    },
    {
      "epoch": 0.9650363783346807,
      "grad_norm": 2.693141460418701,
      "learning_rate": 3.8288639637922804e-05,
      "loss": 0.4878,
      "num_input_tokens_seen": 59904160,
      "step": 4775,
      "train_runtime": 57338.091,
      "train_tokens_per_second": 1044.753
    },
    {
      "epoch": 0.9660468876313663,
      "grad_norm": 2.3923609256744385,
      "learning_rate": 3.82662239708907e-05,
      "loss": 0.5272,
      "num_input_tokens_seen": 59966704,
      "step": 4780,
      "train_runtime": 57391.3364,
      "train_tokens_per_second": 1044.874
    },
    {
      "epoch": 0.9670573969280517,
      "grad_norm": 2.537780523300171,
      "learning_rate": 3.8243793448431327e-05,
      "loss": 0.6182,
      "num_input_tokens_seen": 60029408,
      "step": 4785,
      "train_runtime": 57444.7036,
      "train_tokens_per_second": 1044.995
    },
    {
      "epoch": 0.9680679062247373,
      "grad_norm": 2.3842616081237793,
      "learning_rate": 3.8221348095662225e-05,
      "loss": 0.6267,
      "num_input_tokens_seen": 60091648,
      "step": 4790,
      "train_runtime": 57497.5872,
      "train_tokens_per_second": 1045.116
    },
    {
      "epoch": 0.9690784155214228,
      "grad_norm": 1.8012906312942505,
      "learning_rate": 3.8198887937717564e-05,
      "loss": 0.4823,
      "num_input_tokens_seen": 60152672,
      "step": 4795,
      "train_runtime": 57549.7003,
      "train_tokens_per_second": 1045.23
    },
    {
      "epoch": 0.9700889248181084,
      "grad_norm": 1.7601834535598755,
      "learning_rate": 3.817641299974807e-05,
      "loss": 0.5923,
      "num_input_tokens_seen": 60215952,
      "step": 4800,
      "train_runtime": 57603.1564,
      "train_tokens_per_second": 1045.359
    },
    {
      "epoch": 0.9710994341147938,
      "grad_norm": 2.617309093475342,
      "learning_rate": 3.815392330692102e-05,
      "loss": 0.5535,
      "num_input_tokens_seen": 60280384,
      "step": 4805,
      "train_runtime": 57658.2165,
      "train_tokens_per_second": 1045.478
    },
    {
      "epoch": 0.9721099434114794,
      "grad_norm": 2.0808956623077393,
      "learning_rate": 3.813141888442023e-05,
      "loss": 0.5913,
      "num_input_tokens_seen": 60341856,
      "step": 4810,
      "train_runtime": 57710.4446,
      "train_tokens_per_second": 1045.597
    },
    {
      "epoch": 0.9731204527081649,
      "grad_norm": 5.122744560241699,
      "learning_rate": 3.8108899757445994e-05,
      "loss": 0.5799,
      "num_input_tokens_seen": 60404336,
      "step": 4815,
      "train_runtime": 57763.5905,
      "train_tokens_per_second": 1045.716
    },
    {
      "epoch": 0.9741309620048505,
      "grad_norm": 2.76849365234375,
      "learning_rate": 3.808636595121507e-05,
      "loss": 0.5382,
      "num_input_tokens_seen": 60466496,
      "step": 4820,
      "train_runtime": 57816.3687,
      "train_tokens_per_second": 1045.837
    },
    {
      "epoch": 0.975141471301536,
      "grad_norm": 1.6150476932525635,
      "learning_rate": 3.806381749096067e-05,
      "loss": 0.4933,
      "num_input_tokens_seen": 60530016,
      "step": 4825,
      "train_runtime": 57870.1531,
      "train_tokens_per_second": 1045.963
    },
    {
      "epoch": 0.9761519805982215,
      "grad_norm": 1.8821994066238403,
      "learning_rate": 3.80412544019324e-05,
      "loss": 0.5837,
      "num_input_tokens_seen": 60591952,
      "step": 4830,
      "train_runtime": 57922.7494,
      "train_tokens_per_second": 1046.082
    },
    {
      "epoch": 0.9771624898949071,
      "grad_norm": 1.832108497619629,
      "learning_rate": 3.801867670939626e-05,
      "loss": 0.4188,
      "num_input_tokens_seen": 60654000,
      "step": 4835,
      "train_runtime": 57975.5749,
      "train_tokens_per_second": 1046.199
    },
    {
      "epoch": 0.9781729991915926,
      "grad_norm": 3.6380577087402344,
      "learning_rate": 3.7996084438634585e-05,
      "loss": 0.4633,
      "num_input_tokens_seen": 60717200,
      "step": 4840,
      "train_runtime": 58029.278,
      "train_tokens_per_second": 1046.32
    },
    {
      "epoch": 0.9791835084882781,
      "grad_norm": 2.049696922302246,
      "learning_rate": 3.7973477614946064e-05,
      "loss": 0.4085,
      "num_input_tokens_seen": 60780496,
      "step": 4845,
      "train_runtime": 58082.8892,
      "train_tokens_per_second": 1046.444
    },
    {
      "epoch": 0.9801940177849636,
      "grad_norm": 1.6429425477981567,
      "learning_rate": 3.795085626364564e-05,
      "loss": 0.3644,
      "num_input_tokens_seen": 60843616,
      "step": 4850,
      "train_runtime": 58136.5049,
      "train_tokens_per_second": 1046.565
    },
    {
      "epoch": 0.9812045270816492,
      "grad_norm": 2.0341100692749023,
      "learning_rate": 3.7928220410064576e-05,
      "loss": 0.5068,
      "num_input_tokens_seen": 60905824,
      "step": 4855,
      "train_runtime": 58189.2964,
      "train_tokens_per_second": 1046.684
    },
    {
      "epoch": 0.9822150363783346,
      "grad_norm": 2.491884708404541,
      "learning_rate": 3.7905570079550315e-05,
      "loss": 0.513,
      "num_input_tokens_seen": 60967296,
      "step": 4860,
      "train_runtime": 58241.5339,
      "train_tokens_per_second": 1046.801
    },
    {
      "epoch": 0.9832255456750202,
      "grad_norm": 4.195850372314453,
      "learning_rate": 3.7882905297466577e-05,
      "loss": 0.6945,
      "num_input_tokens_seen": 61030384,
      "step": 4865,
      "train_runtime": 58294.9636,
      "train_tokens_per_second": 1046.924
    },
    {
      "epoch": 0.9842360549717057,
      "grad_norm": 2.1009206771850586,
      "learning_rate": 3.7860226089193205e-05,
      "loss": 0.485,
      "num_input_tokens_seen": 61093712,
      "step": 4870,
      "train_runtime": 58348.5629,
      "train_tokens_per_second": 1047.047
    },
    {
      "epoch": 0.9852465642683913,
      "grad_norm": 2.0129706859588623,
      "learning_rate": 3.783753248012624e-05,
      "loss": 0.495,
      "num_input_tokens_seen": 61156480,
      "step": 4875,
      "train_runtime": 58401.8159,
      "train_tokens_per_second": 1047.167
    },
    {
      "epoch": 0.9862570735650767,
      "grad_norm": 2.325829267501831,
      "learning_rate": 3.78148244956778e-05,
      "loss": 0.7462,
      "num_input_tokens_seen": 61218976,
      "step": 4880,
      "train_runtime": 58454.8397,
      "train_tokens_per_second": 1047.287
    },
    {
      "epoch": 0.9872675828617623,
      "grad_norm": 1.8466428518295288,
      "learning_rate": 3.779210216127618e-05,
      "loss": 0.5005,
      "num_input_tokens_seen": 61280896,
      "step": 4885,
      "train_runtime": 58507.4964,
      "train_tokens_per_second": 1047.402
    },
    {
      "epoch": 0.9882780921584479,
      "grad_norm": 2.0206520557403564,
      "learning_rate": 3.776936550236565e-05,
      "loss": 0.6437,
      "num_input_tokens_seen": 61342592,
      "step": 4890,
      "train_runtime": 58559.895,
      "train_tokens_per_second": 1047.519
    },
    {
      "epoch": 0.9892886014551334,
      "grad_norm": 1.6188017129898071,
      "learning_rate": 3.77466145444066e-05,
      "loss": 0.3949,
      "num_input_tokens_seen": 61404912,
      "step": 4895,
      "train_runtime": 58612.7533,
      "train_tokens_per_second": 1047.637
    },
    {
      "epoch": 0.9902991107518189,
      "grad_norm": 2.601254940032959,
      "learning_rate": 3.772384931287536e-05,
      "loss": 0.6835,
      "num_input_tokens_seen": 61468000,
      "step": 4900,
      "train_runtime": 58666.3406,
      "train_tokens_per_second": 1047.756
    },
    {
      "epoch": 0.9913096200485044,
      "grad_norm": 1.6797138452529907,
      "learning_rate": 3.7701069833264324e-05,
      "loss": 0.4061,
      "num_input_tokens_seen": 61531696,
      "step": 4905,
      "train_runtime": 58720.9665,
      "train_tokens_per_second": 1047.866
    },
    {
      "epoch": 0.99232012934519,
      "grad_norm": 2.753499984741211,
      "learning_rate": 3.7678276131081766e-05,
      "loss": 0.7935,
      "num_input_tokens_seen": 61593408,
      "step": 4910,
      "train_runtime": 58773.3619,
      "train_tokens_per_second": 1047.982
    },
    {
      "epoch": 0.9933306386418755,
      "grad_norm": 2.121089220046997,
      "learning_rate": 3.7655468231851943e-05,
      "loss": 0.4764,
      "num_input_tokens_seen": 61657104,
      "step": 4915,
      "train_runtime": 58827.3531,
      "train_tokens_per_second": 1048.103
    },
    {
      "epoch": 0.994341147938561,
      "grad_norm": 2.1080703735351562,
      "learning_rate": 3.763264616111496e-05,
      "loss": 0.6318,
      "num_input_tokens_seen": 61718784,
      "step": 4920,
      "train_runtime": 58879.6202,
      "train_tokens_per_second": 1048.22
    },
    {
      "epoch": 0.9953516572352465,
      "grad_norm": 1.678442120552063,
      "learning_rate": 3.7609809944426833e-05,
      "loss": 0.6433,
      "num_input_tokens_seen": 61782144,
      "step": 4925,
      "train_runtime": 58933.1972,
      "train_tokens_per_second": 1048.342
    },
    {
      "epoch": 0.9963621665319321,
      "grad_norm": 3.9483802318573,
      "learning_rate": 3.75869596073594e-05,
      "loss": 0.7554,
      "num_input_tokens_seen": 61845104,
      "step": 4930,
      "train_runtime": 58986.6679,
      "train_tokens_per_second": 1048.459
    },
    {
      "epoch": 0.9973726758286177,
      "grad_norm": 8.200407028198242,
      "learning_rate": 3.756409517550031e-05,
      "loss": 0.5585,
      "num_input_tokens_seen": 61906128,
      "step": 4935,
      "train_runtime": 59038.6932,
      "train_tokens_per_second": 1048.569
    },
    {
      "epoch": 0.9983831851253031,
      "grad_norm": 1.9866267442703247,
      "learning_rate": 3.7541216674452996e-05,
      "loss": 0.5027,
      "num_input_tokens_seen": 61967440,
      "step": 4940,
      "train_runtime": 59090.7958,
      "train_tokens_per_second": 1048.682
    },
    {
      "epoch": 0.9993936944219887,
      "grad_norm": 2.2618370056152344,
      "learning_rate": 3.751832412983665e-05,
      "loss": 0.5626,
      "num_input_tokens_seen": 62030848,
      "step": 4945,
      "train_runtime": 59144.5381,
      "train_tokens_per_second": 1048.801
    },
    {
      "epoch": 1.0004042037186742,
      "grad_norm": 1.7832496166229248,
      "learning_rate": 3.749541756728618e-05,
      "loss": 0.4789,
      "num_input_tokens_seen": 62093536,
      "step": 4950,
      "train_runtime": 59197.7543,
      "train_tokens_per_second": 1048.917
    },
    {
      "epoch": 1.0014147130153597,
      "grad_norm": 1.628530502319336,
      "learning_rate": 3.747249701245222e-05,
      "loss": 0.5321,
      "num_input_tokens_seen": 62155760,
      "step": 4955,
      "train_runtime": 59250.6245,
      "train_tokens_per_second": 1049.031
    },
    {
      "epoch": 1.0024252223120453,
      "grad_norm": 1.6251755952835083,
      "learning_rate": 3.7449562491001036e-05,
      "loss": 0.4101,
      "num_input_tokens_seen": 62219424,
      "step": 4960,
      "train_runtime": 59304.579,
      "train_tokens_per_second": 1049.15
    },
    {
      "epoch": 1.0034357316087308,
      "grad_norm": 1.4418164491653442,
      "learning_rate": 3.7426614028614545e-05,
      "loss": 0.4525,
      "num_input_tokens_seen": 62281744,
      "step": 4965,
      "train_runtime": 59357.4274,
      "train_tokens_per_second": 1049.266
    },
    {
      "epoch": 1.0044462409054162,
      "grad_norm": 3.01492977142334,
      "learning_rate": 3.7403651650990284e-05,
      "loss": 0.3625,
      "num_input_tokens_seen": 62344176,
      "step": 4970,
      "train_runtime": 59410.4868,
      "train_tokens_per_second": 1049.38
    },
    {
      "epoch": 1.005456750202102,
      "grad_norm": 2.3329484462738037,
      "learning_rate": 3.7380675383841373e-05,
      "loss": 0.3294,
      "num_input_tokens_seen": 62406768,
      "step": 4975,
      "train_runtime": 59463.5822,
      "train_tokens_per_second": 1049.496
    },
    {
      "epoch": 1.0064672594987873,
      "grad_norm": 1.3007605075836182,
      "learning_rate": 3.735768525289647e-05,
      "loss": 0.6111,
      "num_input_tokens_seen": 62468848,
      "step": 4980,
      "train_runtime": 59516.3618,
      "train_tokens_per_second": 1049.608
    },
    {
      "epoch": 1.007477768795473,
      "grad_norm": 3.4463260173797607,
      "learning_rate": 3.733468128389978e-05,
      "loss": 0.3448,
      "num_input_tokens_seen": 62531680,
      "step": 4985,
      "train_runtime": 59569.5101,
      "train_tokens_per_second": 1049.726
    },
    {
      "epoch": 1.0084882780921585,
      "grad_norm": 1.7767937183380127,
      "learning_rate": 3.731166350261097e-05,
      "loss": 0.4309,
      "num_input_tokens_seen": 62594656,
      "step": 4990,
      "train_runtime": 59622.9322,
      "train_tokens_per_second": 1049.842
    },
    {
      "epoch": 1.009498787388844,
      "grad_norm": 2.4945690631866455,
      "learning_rate": 3.728863193480521e-05,
      "loss": 0.5213,
      "num_input_tokens_seen": 62658672,
      "step": 4995,
      "train_runtime": 59676.9605,
      "train_tokens_per_second": 1049.964
    },
    {
      "epoch": 1.0105092966855296,
      "grad_norm": 2.5983407497406006,
      "learning_rate": 3.7265586606273095e-05,
      "loss": 0.5163,
      "num_input_tokens_seen": 62721424,
      "step": 5000,
      "train_runtime": 59730.1428,
      "train_tokens_per_second": 1050.08
    },
    {
      "epoch": 1.011519805982215,
      "grad_norm": 2.1024818420410156,
      "learning_rate": 3.724252754282062e-05,
      "loss": 0.395,
      "num_input_tokens_seen": 62783120,
      "step": 5005,
      "train_runtime": 59783.4162,
      "train_tokens_per_second": 1050.176
    },
    {
      "epoch": 1.0125303152789005,
      "grad_norm": 4.258815288543701,
      "learning_rate": 3.721945477026917e-05,
      "loss": 0.5294,
      "num_input_tokens_seen": 62845232,
      "step": 5010,
      "train_runtime": 59836.1883,
      "train_tokens_per_second": 1050.288
    },
    {
      "epoch": 1.0135408245755861,
      "grad_norm": 29.87835121154785,
      "learning_rate": 3.7196368314455474e-05,
      "loss": 0.4993,
      "num_input_tokens_seen": 62908432,
      "step": 5015,
      "train_runtime": 59889.7078,
      "train_tokens_per_second": 1050.405
    },
    {
      "epoch": 1.0145513338722716,
      "grad_norm": 3.6661994457244873,
      "learning_rate": 3.7173268201231594e-05,
      "loss": 0.4356,
      "num_input_tokens_seen": 62970816,
      "step": 5020,
      "train_runtime": 59942.4623,
      "train_tokens_per_second": 1050.521
    },
    {
      "epoch": 1.0155618431689573,
      "grad_norm": 3.7425472736358643,
      "learning_rate": 3.715015445646487e-05,
      "loss": 0.3739,
      "num_input_tokens_seen": 63032704,
      "step": 5025,
      "train_runtime": 59995.0905,
      "train_tokens_per_second": 1050.631
    },
    {
      "epoch": 1.0165723524656427,
      "grad_norm": 3.2214362621307373,
      "learning_rate": 3.712702710603793e-05,
      "loss": 0.4355,
      "num_input_tokens_seen": 63096096,
      "step": 5030,
      "train_runtime": 60048.682,
      "train_tokens_per_second": 1050.749
    },
    {
      "epoch": 1.0175828617623282,
      "grad_norm": 1.3729512691497803,
      "learning_rate": 3.710388617584862e-05,
      "loss": 0.4493,
      "num_input_tokens_seen": 63158960,
      "step": 5035,
      "train_runtime": 60102.0745,
      "train_tokens_per_second": 1050.862
    },
    {
      "epoch": 1.0185933710590138,
      "grad_norm": 1.9793461561203003,
      "learning_rate": 3.708073169180998e-05,
      "loss": 0.4387,
      "num_input_tokens_seen": 63222480,
      "step": 5040,
      "train_runtime": 60155.7164,
      "train_tokens_per_second": 1050.98
    },
    {
      "epoch": 1.0196038803556993,
      "grad_norm": 2.056666374206543,
      "learning_rate": 3.7057563679850275e-05,
      "loss": 0.4627,
      "num_input_tokens_seen": 63285472,
      "step": 5045,
      "train_runtime": 60208.9485,
      "train_tokens_per_second": 1051.097
    },
    {
      "epoch": 1.0206143896523847,
      "grad_norm": 2.1111350059509277,
      "learning_rate": 3.703438216591285e-05,
      "loss": 0.5299,
      "num_input_tokens_seen": 63346928,
      "step": 5050,
      "train_runtime": 60261.1996,
      "train_tokens_per_second": 1051.206
    },
    {
      "epoch": 1.0216248989490704,
      "grad_norm": 2.1361472606658936,
      "learning_rate": 3.701118717595623e-05,
      "loss": 0.4974,
      "num_input_tokens_seen": 63409968,
      "step": 5055,
      "train_runtime": 60314.6324,
      "train_tokens_per_second": 1051.32
    },
    {
      "epoch": 1.0226354082457558,
      "grad_norm": 2.1817374229431152,
      "learning_rate": 3.698797873595401e-05,
      "loss": 0.3598,
      "num_input_tokens_seen": 63472320,
      "step": 5060,
      "train_runtime": 60367.6014,
      "train_tokens_per_second": 1051.43
    },
    {
      "epoch": 1.0236459175424415,
      "grad_norm": 1.725956678390503,
      "learning_rate": 3.696475687189482e-05,
      "loss": 0.35,
      "num_input_tokens_seen": 63535376,
      "step": 5065,
      "train_runtime": 60421.0855,
      "train_tokens_per_second": 1051.543
    },
    {
      "epoch": 1.024656426839127,
      "grad_norm": 1.4698119163513184,
      "learning_rate": 3.694152160978237e-05,
      "loss": 0.3281,
      "num_input_tokens_seen": 63597408,
      "step": 5070,
      "train_runtime": 60473.7542,
      "train_tokens_per_second": 1051.653
    },
    {
      "epoch": 1.0256669361358124,
      "grad_norm": 2.7805469036102295,
      "learning_rate": 3.6918272975635336e-05,
      "loss": 0.4797,
      "num_input_tokens_seen": 63661552,
      "step": 5075,
      "train_runtime": 60527.8871,
      "train_tokens_per_second": 1051.772
    },
    {
      "epoch": 1.026677445432498,
      "grad_norm": 2.8573920726776123,
      "learning_rate": 3.6895010995487376e-05,
      "loss": 0.2865,
      "num_input_tokens_seen": 63724768,
      "step": 5080,
      "train_runtime": 60581.2893,
      "train_tokens_per_second": 1051.889
    },
    {
      "epoch": 1.0276879547291835,
      "grad_norm": 2.115859270095825,
      "learning_rate": 3.687173569538711e-05,
      "loss": 0.4418,
      "num_input_tokens_seen": 63786944,
      "step": 5085,
      "train_runtime": 60634.0635,
      "train_tokens_per_second": 1051.999
    },
    {
      "epoch": 1.028698464025869,
      "grad_norm": 1.7604470252990723,
      "learning_rate": 3.6848447101398056e-05,
      "loss": 0.3953,
      "num_input_tokens_seen": 63849344,
      "step": 5090,
      "train_runtime": 60687.006,
      "train_tokens_per_second": 1052.109
    },
    {
      "epoch": 1.0297089733225546,
      "grad_norm": 2.0340847969055176,
      "learning_rate": 3.682514523959862e-05,
      "loss": 0.4531,
      "num_input_tokens_seen": 63912224,
      "step": 5095,
      "train_runtime": 60740.3216,
      "train_tokens_per_second": 1052.221
    },
    {
      "epoch": 1.03071948261924,
      "grad_norm": 1.5928900241851807,
      "learning_rate": 3.680183013608206e-05,
      "loss": 0.3589,
      "num_input_tokens_seen": 63974640,
      "step": 5100,
      "train_runtime": 60793.3598,
      "train_tokens_per_second": 1052.329
    },
    {
      "epoch": 1.0317299919159257,
      "grad_norm": 1.9714279174804688,
      "learning_rate": 3.677850181695649e-05,
      "loss": 0.3016,
      "num_input_tokens_seen": 64038176,
      "step": 5105,
      "train_runtime": 60847.9628,
      "train_tokens_per_second": 1052.429
    },
    {
      "epoch": 1.0327405012126112,
      "grad_norm": 1.0390301942825317,
      "learning_rate": 3.675516030834478e-05,
      "loss": 0.3599,
      "num_input_tokens_seen": 64100736,
      "step": 5110,
      "train_runtime": 60900.9852,
      "train_tokens_per_second": 1052.54
    },
    {
      "epoch": 1.0337510105092966,
      "grad_norm": 2.087892532348633,
      "learning_rate": 3.6731805636384615e-05,
      "loss": 0.4286,
      "num_input_tokens_seen": 64162704,
      "step": 5115,
      "train_runtime": 60953.7698,
      "train_tokens_per_second": 1052.645
    },
    {
      "epoch": 1.0347615198059823,
      "grad_norm": 1.7603695392608643,
      "learning_rate": 3.670843782722838e-05,
      "loss": 0.4638,
      "num_input_tokens_seen": 64224608,
      "step": 5120,
      "train_runtime": 61006.4513,
      "train_tokens_per_second": 1052.751
    },
    {
      "epoch": 1.0357720291026677,
      "grad_norm": 2.3435065746307373,
      "learning_rate": 3.6685056907043204e-05,
      "loss": 0.4395,
      "num_input_tokens_seen": 64287568,
      "step": 5125,
      "train_runtime": 61059.9084,
      "train_tokens_per_second": 1052.861
    },
    {
      "epoch": 1.0367825383993532,
      "grad_norm": 2.139986753463745,
      "learning_rate": 3.6661662902010865e-05,
      "loss": 0.4128,
      "num_input_tokens_seen": 64349984,
      "step": 5130,
      "train_runtime": 61113.0559,
      "train_tokens_per_second": 1052.966
    },
    {
      "epoch": 1.0377930476960389,
      "grad_norm": 4.684333324432373,
      "learning_rate": 3.663825583832782e-05,
      "loss": 0.4899,
      "num_input_tokens_seen": 64412208,
      "step": 5135,
      "train_runtime": 61165.8238,
      "train_tokens_per_second": 1053.075
    },
    {
      "epoch": 1.0388035569927243,
      "grad_norm": 2.6025149822235107,
      "learning_rate": 3.661483574220514e-05,
      "loss": 0.427,
      "num_input_tokens_seen": 64474608,
      "step": 5140,
      "train_runtime": 61218.6298,
      "train_tokens_per_second": 1053.186
    },
    {
      "epoch": 1.0398140662894098,
      "grad_norm": 2.247562885284424,
      "learning_rate": 3.659140263986849e-05,
      "loss": 0.3719,
      "num_input_tokens_seen": 64536400,
      "step": 5145,
      "train_runtime": 61271.184,
      "train_tokens_per_second": 1053.291
    },
    {
      "epoch": 1.0408245755860954,
      "grad_norm": 3.706169605255127,
      "learning_rate": 3.65679565575581e-05,
      "loss": 0.4679,
      "num_input_tokens_seen": 64599600,
      "step": 5150,
      "train_runtime": 61324.8165,
      "train_tokens_per_second": 1053.401
    },
    {
      "epoch": 1.0418350848827809,
      "grad_norm": 2.5884528160095215,
      "learning_rate": 3.654449752152872e-05,
      "loss": 0.4128,
      "num_input_tokens_seen": 64661088,
      "step": 5155,
      "train_runtime": 61376.9944,
      "train_tokens_per_second": 1053.507
    },
    {
      "epoch": 1.0428455941794665,
      "grad_norm": 2.030867099761963,
      "learning_rate": 3.652102555804964e-05,
      "loss": 0.3903,
      "num_input_tokens_seen": 64724080,
      "step": 5160,
      "train_runtime": 61430.36,
      "train_tokens_per_second": 1053.617
    },
    {
      "epoch": 1.043856103476152,
      "grad_norm": 2.1048760414123535,
      "learning_rate": 3.6497540693404586e-05,
      "loss": 0.6297,
      "num_input_tokens_seen": 64787136,
      "step": 5165,
      "train_runtime": 61483.7894,
      "train_tokens_per_second": 1053.727
    },
    {
      "epoch": 1.0448666127728374,
      "grad_norm": 1.7360237836837769,
      "learning_rate": 3.647404295389176e-05,
      "loss": 0.4863,
      "num_input_tokens_seen": 64849264,
      "step": 5170,
      "train_runtime": 61536.5697,
      "train_tokens_per_second": 1053.833
    },
    {
      "epoch": 1.045877122069523,
      "grad_norm": 2.061236619949341,
      "learning_rate": 3.6450532365823766e-05,
      "loss": 0.3879,
      "num_input_tokens_seen": 64911616,
      "step": 5175,
      "train_runtime": 61589.5337,
      "train_tokens_per_second": 1053.939
    },
    {
      "epoch": 1.0468876313662085,
      "grad_norm": 2.207628011703491,
      "learning_rate": 3.6427008955527606e-05,
      "loss": 0.4867,
      "num_input_tokens_seen": 64974784,
      "step": 5180,
      "train_runtime": 61643.0845,
      "train_tokens_per_second": 1054.048
    },
    {
      "epoch": 1.047898140662894,
      "grad_norm": 2.284269094467163,
      "learning_rate": 3.640347274934462e-05,
      "loss": 0.6123,
      "num_input_tokens_seen": 65037024,
      "step": 5185,
      "train_runtime": 61695.8752,
      "train_tokens_per_second": 1054.155
    },
    {
      "epoch": 1.0489086499595797,
      "grad_norm": 3.5178236961364746,
      "learning_rate": 3.637992377363051e-05,
      "loss": 0.3906,
      "num_input_tokens_seen": 65099088,
      "step": 5190,
      "train_runtime": 61748.6458,
      "train_tokens_per_second": 1054.259
    },
    {
      "epoch": 1.049919159256265,
      "grad_norm": 2.5209219455718994,
      "learning_rate": 3.635636205475525e-05,
      "loss": 0.5193,
      "num_input_tokens_seen": 65162336,
      "step": 5195,
      "train_runtime": 61802.1668,
      "train_tokens_per_second": 1054.37
    },
    {
      "epoch": 1.0509296685529508,
      "grad_norm": 1.845930576324463,
      "learning_rate": 3.633278761910309e-05,
      "loss": 0.4235,
      "num_input_tokens_seen": 65225024,
      "step": 5200,
      "train_runtime": 61855.2201,
      "train_tokens_per_second": 1054.479
    },
    {
      "epoch": 1.0519401778496362,
      "grad_norm": 2.764909267425537,
      "learning_rate": 3.630920049307253e-05,
      "loss": 0.6852,
      "num_input_tokens_seen": 65287088,
      "step": 5205,
      "train_runtime": 61908.7357,
      "train_tokens_per_second": 1054.57
    },
    {
      "epoch": 1.0529506871463217,
      "grad_norm": 1.9237942695617676,
      "learning_rate": 3.6285600703076266e-05,
      "loss": 0.4083,
      "num_input_tokens_seen": 65348896,
      "step": 5210,
      "train_runtime": 61961.294,
      "train_tokens_per_second": 1054.673
    },
    {
      "epoch": 1.0539611964430073,
      "grad_norm": 3.2783455848693848,
      "learning_rate": 3.6261988275541186e-05,
      "loss": 0.484,
      "num_input_tokens_seen": 65411520,
      "step": 5215,
      "train_runtime": 62014.4611,
      "train_tokens_per_second": 1054.778
    },
    {
      "epoch": 1.0549717057396928,
      "grad_norm": 2.3812179565429688,
      "learning_rate": 3.6238363236908326e-05,
      "loss": 0.4801,
      "num_input_tokens_seen": 65474640,
      "step": 5220,
      "train_runtime": 62067.7685,
      "train_tokens_per_second": 1054.89
    },
    {
      "epoch": 1.0559822150363782,
      "grad_norm": 2.0787432193756104,
      "learning_rate": 3.621472561363284e-05,
      "loss": 0.4788,
      "num_input_tokens_seen": 65537488,
      "step": 5225,
      "train_runtime": 62120.8808,
      "train_tokens_per_second": 1054.999
    },
    {
      "epoch": 1.056992724333064,
      "grad_norm": 1.6334563493728638,
      "learning_rate": 3.619107543218398e-05,
      "loss": 0.4841,
      "num_input_tokens_seen": 65600048,
      "step": 5230,
      "train_runtime": 62173.8999,
      "train_tokens_per_second": 1055.106
    },
    {
      "epoch": 1.0580032336297494,
      "grad_norm": 2.140934705734253,
      "learning_rate": 3.6167412719045044e-05,
      "loss": 0.4183,
      "num_input_tokens_seen": 65660496,
      "step": 5235,
      "train_runtime": 62225.5541,
      "train_tokens_per_second": 1055.201
    },
    {
      "epoch": 1.059013742926435,
      "grad_norm": 2.4419589042663574,
      "learning_rate": 3.614373750071341e-05,
      "loss": 0.4251,
      "num_input_tokens_seen": 65724000,
      "step": 5240,
      "train_runtime": 62279.2691,
      "train_tokens_per_second": 1055.311
    },
    {
      "epoch": 1.0600242522231205,
      "grad_norm": 2.1546173095703125,
      "learning_rate": 3.6120049803700396e-05,
      "loss": 0.386,
      "num_input_tokens_seen": 65788000,
      "step": 5245,
      "train_runtime": 62333.2802,
      "train_tokens_per_second": 1055.423
    },
    {
      "epoch": 1.061034761519806,
      "grad_norm": 2.69389009475708,
      "learning_rate": 3.6096349654531336e-05,
      "loss": 0.524,
      "num_input_tokens_seen": 65850544,
      "step": 5250,
      "train_runtime": 62386.227,
      "train_tokens_per_second": 1055.53
    },
    {
      "epoch": 1.0620452708164916,
      "grad_norm": 2.2144689559936523,
      "learning_rate": 3.6072637079745494e-05,
      "loss": 0.5609,
      "num_input_tokens_seen": 65912784,
      "step": 5255,
      "train_runtime": 62439.1311,
      "train_tokens_per_second": 1055.633
    },
    {
      "epoch": 1.063055780113177,
      "grad_norm": 1.9775751829147339,
      "learning_rate": 3.604891210589605e-05,
      "loss": 0.3606,
      "num_input_tokens_seen": 65975008,
      "step": 5260,
      "train_runtime": 62492.0068,
      "train_tokens_per_second": 1055.735
    },
    {
      "epoch": 1.0640662894098625,
      "grad_norm": 3.2085859775543213,
      "learning_rate": 3.6025174759550074e-05,
      "loss": 0.4887,
      "num_input_tokens_seen": 66037776,
      "step": 5265,
      "train_runtime": 62545.1589,
      "train_tokens_per_second": 1055.842
    },
    {
      "epoch": 1.0650767987065481,
      "grad_norm": 1.9084718227386475,
      "learning_rate": 3.600142506728847e-05,
      "loss": 0.295,
      "num_input_tokens_seen": 66100112,
      "step": 5270,
      "train_runtime": 62598.1065,
      "train_tokens_per_second": 1055.944
    },
    {
      "epoch": 1.0660873080032336,
      "grad_norm": 2.4601755142211914,
      "learning_rate": 3.5977663055706e-05,
      "loss": 0.49,
      "num_input_tokens_seen": 66162080,
      "step": 5275,
      "train_runtime": 62650.6855,
      "train_tokens_per_second": 1056.047
    },
    {
      "epoch": 1.0670978172999193,
      "grad_norm": 4.017998218536377,
      "learning_rate": 3.595388875141119e-05,
      "loss": 0.4402,
      "num_input_tokens_seen": 66224608,
      "step": 5280,
      "train_runtime": 62703.7854,
      "train_tokens_per_second": 1056.15
    },
    {
      "epoch": 1.0681083265966047,
      "grad_norm": 2.1017978191375732,
      "learning_rate": 3.5930102181026346e-05,
      "loss": 0.362,
      "num_input_tokens_seen": 66287504,
      "step": 5285,
      "train_runtime": 62756.9648,
      "train_tokens_per_second": 1056.257
    },
    {
      "epoch": 1.0691188358932902,
      "grad_norm": 2.1352343559265137,
      "learning_rate": 3.590630337118751e-05,
      "loss": 0.3546,
      "num_input_tokens_seen": 66350000,
      "step": 5290,
      "train_runtime": 62809.9,
      "train_tokens_per_second": 1056.362
    },
    {
      "epoch": 1.0701293451899758,
      "grad_norm": 1.8458629846572876,
      "learning_rate": 3.588249234854443e-05,
      "loss": 0.4137,
      "num_input_tokens_seen": 66413184,
      "step": 5295,
      "train_runtime": 62863.4182,
      "train_tokens_per_second": 1056.468
    },
    {
      "epoch": 1.0711398544866613,
      "grad_norm": 2.190701961517334,
      "learning_rate": 3.5858669139760516e-05,
      "loss": 0.3813,
      "num_input_tokens_seen": 66476496,
      "step": 5300,
      "train_runtime": 62916.9626,
      "train_tokens_per_second": 1056.575
    },
    {
      "epoch": 1.0721503637833467,
      "grad_norm": 2.721944808959961,
      "learning_rate": 3.5834833771512846e-05,
      "loss": 0.4713,
      "num_input_tokens_seen": 66539760,
      "step": 5305,
      "train_runtime": 62971.5416,
      "train_tokens_per_second": 1056.664
    },
    {
      "epoch": 1.0731608730800324,
      "grad_norm": 2.6653213500976562,
      "learning_rate": 3.58109862704921e-05,
      "loss": 0.4634,
      "num_input_tokens_seen": 66603152,
      "step": 5310,
      "train_runtime": 63025.2054,
      "train_tokens_per_second": 1056.77
    },
    {
      "epoch": 1.0741713823767178,
      "grad_norm": 3.3557820320129395,
      "learning_rate": 3.578712666340254e-05,
      "loss": 0.3682,
      "num_input_tokens_seen": 66665872,
      "step": 5315,
      "train_runtime": 63078.4384,
      "train_tokens_per_second": 1056.873
    },
    {
      "epoch": 1.0751818916734033,
      "grad_norm": 1.9459317922592163,
      "learning_rate": 3.5763254976962e-05,
      "loss": 0.4622,
      "num_input_tokens_seen": 66728288,
      "step": 5320,
      "train_runtime": 63131.3921,
      "train_tokens_per_second": 1056.975
    },
    {
      "epoch": 1.0751818916734033,
      "num_input_tokens_seen": 66728288,
      "step": 5320,
      "total_flos": 2.580229026866135e+17,
      "train_loss": 0.5386726873261588,
      "train_runtime": 63131.3948,
      "train_samples_per_second": 3.762,
      "train_steps_per_second": 0.235
    }
  ],
  "logging_steps": 5,
  "max_steps": 14844,
  "num_input_tokens_seen": 66728288,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.580229026866135e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
