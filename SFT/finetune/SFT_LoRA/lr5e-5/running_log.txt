[INFO|2025-10-20 22:09:11] tokenization_utils_base.py:2093 >> loading file vocab.json
[INFO|2025-10-20 22:09:11] tokenization_utils_base.py:2093 >> loading file merges.txt
[INFO|2025-10-20 22:09:11] tokenization_utils_base.py:2093 >> loading file tokenizer.json
[INFO|2025-10-20 22:09:11] tokenization_utils_base.py:2093 >> loading file added_tokens.json
[INFO|2025-10-20 22:09:11] tokenization_utils_base.py:2093 >> loading file special_tokens_map.json
[INFO|2025-10-20 22:09:11] tokenization_utils_base.py:2093 >> loading file tokenizer_config.json
[INFO|2025-10-20 22:09:11] tokenization_utils_base.py:2093 >> loading file chat_template.jinja
[INFO|2025-10-20 22:09:12] tokenization_utils_base.py:2364 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-10-20 22:09:12] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-20 22:09:12] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-20 22:09:12] tokenization_utils_base.py:2093 >> loading file vocab.json
[INFO|2025-10-20 22:09:12] tokenization_utils_base.py:2093 >> loading file merges.txt
[INFO|2025-10-20 22:09:12] tokenization_utils_base.py:2093 >> loading file tokenizer.json
[INFO|2025-10-20 22:09:12] tokenization_utils_base.py:2093 >> loading file added_tokens.json
[INFO|2025-10-20 22:09:12] tokenization_utils_base.py:2093 >> loading file special_tokens_map.json
[INFO|2025-10-20 22:09:12] tokenization_utils_base.py:2093 >> loading file tokenizer_config.json
[INFO|2025-10-20 22:09:12] tokenization_utils_base.py:2093 >> loading file chat_template.jinja
[INFO|2025-10-20 22:09:12] tokenization_utils_base.py:2364 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-10-20 22:09:12] logging.py:143 >> Loading dataset /media/data/users/liqz/llama_factory/LLaMA-Factory/data/train_top5_alpaca.jsonl...
[INFO|2025-10-20 22:09:14] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-20 22:09:14] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-20 22:09:14] logging.py:143 >> Loading 4-bit AWQ-quantized model.
[INFO|2025-10-20 22:09:14] logging.py:143 >> KV cache is disabled during training.
[WARNING|2025-10-20 22:09:14] logging.py:328 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|2025-10-20 22:09:14] auto.py:242 >> 
[WARNING|2025-10-20 22:09:14] quantizer_awq.py:102 >> `torch.bfloat16` is not supported for AWQ CUDA/XPU kernels yet. Casting to `torch.float16`.
[INFO|2025-10-20 22:09:14] modeling_utils.py:1169 >> loading weights file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/model.safetensors.index.json
[INFO|2025-10-20 22:09:14] modeling_utils.py:2341 >> Instantiating Qwen3ForCausalLM model under default dtype torch.float16.
[INFO|2025-10-20 22:09:14] configuration_utils.py:986 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

[INFO|2025-10-20 22:09:16] configuration_utils.py:939 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/generation_config.json
[INFO|2025-10-20 22:09:16] configuration_utils.py:986 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

[INFO|2025-10-20 22:09:16] dynamic_module_utils.py:423 >> Could not locate the custom_generate/generate.py inside /media/data/users/liqz/Qwen/Qwen3-8B-AWQ.
[INFO|2025-10-20 22:09:16] logging.py:143 >> Gradient checkpointing enabled.
[INFO|2025-10-20 22:09:16] logging.py:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-10-20 22:09:16] logging.py:143 >> Upcasting trainable params to float32.
[INFO|2025-10-20 22:09:16] logging.py:143 >> Fine-tuning method: LoRA
[INFO|2025-10-20 22:09:16] logging.py:143 >> Found linear modules: up_proj,o_proj,gate_proj,q_proj,v_proj,down_proj,k_proj
[INFO|2025-10-20 22:09:17] logging.py:143 >> trainable params: 21,823,488 || all params: 1,266,791,424 || trainable%: 1.7227
[WARNING|2025-10-20 22:09:17] trainer.py:906 >> The model is already on multiple devices. Skipping the move to device specified in `args`.
[INFO|2025-10-20 22:09:17] trainer.py:749 >> Using auto half precision backend
[WARNING|2025-10-20 22:09:17] trainer.py:982 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|2025-10-20 22:09:17] trainer.py:2519 >> ***** Running training *****
[INFO|2025-10-20 22:09:17] trainer.py:2520 >>   Num examples = 79,168
[INFO|2025-10-20 22:09:17] trainer.py:2521 >>   Num Epochs = 3
[INFO|2025-10-20 22:09:17] trainer.py:2522 >>   Instantaneous batch size per device = 2
[INFO|2025-10-20 22:09:17] trainer.py:2525 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|2025-10-20 22:09:17] trainer.py:2526 >>   Gradient Accumulation steps = 8
[INFO|2025-10-20 22:09:17] trainer.py:2527 >>   Total optimization steps = 14,844
[INFO|2025-10-20 22:09:17] trainer.py:2528 >>   Number of trainable parameters = 21,823,488
[INFO|2025-10-20 22:10:22] logging.py:143 >> {'loss': 6.3574, 'learning_rate': 5.0000e-05, 'epoch': 0.00, 'throughput': 961.34}
[INFO|2025-10-20 22:11:26] logging.py:143 >> {'loss': 2.4250, 'learning_rate': 5.0000e-05, 'epoch': 0.00, 'throughput': 968.80}
[INFO|2025-10-20 22:12:31] logging.py:143 >> {'loss': 1.2831, 'learning_rate': 5.0000e-05, 'epoch': 0.00, 'throughput': 972.09}
[INFO|2025-10-20 22:13:35] logging.py:143 >> {'loss': 0.8713, 'learning_rate': 5.0000e-05, 'epoch': 0.00, 'throughput': 971.65}
[INFO|2025-10-20 22:14:40] logging.py:143 >> {'loss': 0.8078, 'learning_rate': 5.0000e-05, 'epoch': 0.01, 'throughput': 972.43}
[INFO|2025-10-20 22:15:44] logging.py:143 >> {'loss': 0.6283, 'learning_rate': 5.0000e-05, 'epoch': 0.01, 'throughput': 973.01}
[INFO|2025-10-20 22:16:47] logging.py:143 >> {'loss': 0.7267, 'learning_rate': 4.9999e-05, 'epoch': 0.01, 'throughput': 973.15}
[INFO|2025-10-20 22:17:52] logging.py:143 >> {'loss': 0.5722, 'learning_rate': 4.9999e-05, 'epoch': 0.01, 'throughput': 973.58}
[INFO|2025-10-20 22:18:55] logging.py:143 >> {'loss': 0.6271, 'learning_rate': 4.9999e-05, 'epoch': 0.01, 'throughput': 973.69}
[INFO|2025-10-20 22:20:00] logging.py:143 >> {'loss': 0.7295, 'learning_rate': 4.9999e-05, 'epoch': 0.01, 'throughput': 973.64}
[INFO|2025-10-20 22:21:05] logging.py:143 >> {'loss': 0.7394, 'learning_rate': 4.9998e-05, 'epoch': 0.01, 'throughput': 974.01}
[INFO|2025-10-20 22:22:10] logging.py:143 >> {'loss': 0.7971, 'learning_rate': 4.9998e-05, 'epoch': 0.01, 'throughput': 974.00}
[INFO|2025-10-20 22:23:14] logging.py:143 >> {'loss': 0.7130, 'learning_rate': 4.9998e-05, 'epoch': 0.01, 'throughput': 974.40}
[INFO|2025-10-20 22:24:18] logging.py:143 >> {'loss': 0.6217, 'learning_rate': 4.9997e-05, 'epoch': 0.01, 'throughput': 974.37}
[INFO|2025-10-20 22:25:23] logging.py:143 >> {'loss': 0.6900, 'learning_rate': 4.9997e-05, 'epoch': 0.02, 'throughput': 974.48}
[INFO|2025-10-20 22:26:27] logging.py:143 >> {'loss': 0.8035, 'learning_rate': 4.9997e-05, 'epoch': 0.02, 'throughput': 974.59}
[INFO|2025-10-20 22:27:31] logging.py:143 >> {'loss': 0.5863, 'learning_rate': 4.9996e-05, 'epoch': 0.02, 'throughput': 974.63}
[INFO|2025-10-20 22:28:36] logging.py:143 >> {'loss': 0.6009, 'learning_rate': 4.9996e-05, 'epoch': 0.02, 'throughput': 974.81}
[INFO|2025-10-20 22:29:40] logging.py:143 >> {'loss': 0.5479, 'learning_rate': 4.9995e-05, 'epoch': 0.02, 'throughput': 974.93}
[INFO|2025-10-20 22:30:45] logging.py:143 >> {'loss': 0.5294, 'learning_rate': 4.9995e-05, 'epoch': 0.02, 'throughput': 974.83}
[INFO|2025-10-20 22:30:45] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-100
[INFO|2025-10-20 22:30:45] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-20 22:30:45] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-20 22:30:45] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-100/chat_template.jinja
[INFO|2025-10-20 22:30:45] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-100/tokenizer_config.json
[INFO|2025-10-20 22:30:45] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-100/special_tokens_map.json
[INFO|2025-10-20 22:31:50] logging.py:143 >> {'loss': 0.6036, 'learning_rate': 4.9994e-05, 'epoch': 0.02, 'throughput': 974.34}
[INFO|2025-10-20 22:32:54] logging.py:143 >> {'loss': 0.5934, 'learning_rate': 4.9993e-05, 'epoch': 0.02, 'throughput': 974.44}
[INFO|2025-10-20 22:33:58] logging.py:143 >> {'loss': 0.5414, 'learning_rate': 4.9993e-05, 'epoch': 0.02, 'throughput': 974.64}
[INFO|2025-10-20 22:35:03] logging.py:143 >> {'loss': 0.6259, 'learning_rate': 4.9992e-05, 'epoch': 0.02, 'throughput': 974.64}
[INFO|2025-10-20 22:36:07] logging.py:143 >> {'loss': 0.5068, 'learning_rate': 4.9991e-05, 'epoch': 0.03, 'throughput': 974.46}
[INFO|2025-10-20 22:37:10] logging.py:143 >> {'loss': 0.7494, 'learning_rate': 4.9991e-05, 'epoch': 0.03, 'throughput': 974.46}
[INFO|2025-10-20 22:38:14] logging.py:143 >> {'loss': 0.6420, 'learning_rate': 4.9990e-05, 'epoch': 0.03, 'throughput': 974.59}
[INFO|2025-10-20 22:39:18] logging.py:143 >> {'loss': 0.5296, 'learning_rate': 4.9989e-05, 'epoch': 0.03, 'throughput': 974.54}
[INFO|2025-10-20 22:40:23] logging.py:143 >> {'loss': 0.5160, 'learning_rate': 4.9988e-05, 'epoch': 0.03, 'throughput': 974.65}
[INFO|2025-10-20 22:41:27] logging.py:143 >> {'loss': 0.6169, 'learning_rate': 4.9988e-05, 'epoch': 0.03, 'throughput': 974.66}
[INFO|2025-10-20 22:42:31] logging.py:143 >> {'loss': 0.7399, 'learning_rate': 4.9987e-05, 'epoch': 0.03, 'throughput': 974.57}
[INFO|2025-10-20 22:43:35] logging.py:143 >> {'loss': 0.4702, 'learning_rate': 4.9986e-05, 'epoch': 0.03, 'throughput': 974.59}
[INFO|2025-10-20 22:44:39] logging.py:143 >> {'loss': 0.5427, 'learning_rate': 4.9985e-05, 'epoch': 0.03, 'throughput': 974.64}
[INFO|2025-10-20 22:45:44] logging.py:143 >> {'loss': 0.8503, 'learning_rate': 4.9984e-05, 'epoch': 0.03, 'throughput': 974.72}
[INFO|2025-10-20 22:46:48] logging.py:143 >> {'loss': 0.5302, 'learning_rate': 4.9983e-05, 'epoch': 0.04, 'throughput': 974.68}
[INFO|2025-10-20 22:47:53] logging.py:143 >> {'loss': 0.5587, 'learning_rate': 4.9982e-05, 'epoch': 0.04, 'throughput': 974.71}
[INFO|2025-10-20 22:48:58] logging.py:143 >> {'loss': 0.5160, 'learning_rate': 4.9981e-05, 'epoch': 0.04, 'throughput': 974.76}
[INFO|2025-10-20 22:50:01] logging.py:143 >> {'loss': 0.5756, 'learning_rate': 4.9980e-05, 'epoch': 0.04, 'throughput': 974.75}
[INFO|2025-10-20 22:51:05] logging.py:143 >> {'loss': 0.6142, 'learning_rate': 4.9979e-05, 'epoch': 0.04, 'throughput': 974.78}
[INFO|2025-10-20 22:52:10] logging.py:143 >> {'loss': 0.4879, 'learning_rate': 4.9978e-05, 'epoch': 0.04, 'throughput': 974.86}
[INFO|2025-10-20 22:52:10] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-200
[INFO|2025-10-20 22:52:10] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-20 22:52:10] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-20 22:52:10] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-200/chat_template.jinja
[INFO|2025-10-20 22:52:10] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-200/tokenizer_config.json
[INFO|2025-10-20 22:52:10] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-200/special_tokens_map.json
[INFO|2025-10-20 22:53:16] logging.py:143 >> {'loss': 0.5648, 'learning_rate': 4.9977e-05, 'epoch': 0.04, 'throughput': 974.63}
[INFO|2025-10-20 22:54:19] logging.py:143 >> {'loss': 0.6527, 'learning_rate': 4.9976e-05, 'epoch': 0.04, 'throughput': 974.58}
[INFO|2025-10-20 22:55:24] logging.py:143 >> {'loss': 0.5608, 'learning_rate': 4.9974e-05, 'epoch': 0.04, 'throughput': 974.56}
[INFO|2025-10-20 22:56:29] logging.py:143 >> {'loss': 0.5701, 'learning_rate': 4.9973e-05, 'epoch': 0.04, 'throughput': 974.62}
[INFO|2025-10-20 22:57:33] logging.py:143 >> {'loss': 0.6124, 'learning_rate': 4.9972e-05, 'epoch': 0.05, 'throughput': 974.66}
[INFO|2025-10-20 22:58:37] logging.py:143 >> {'loss': 0.7043, 'learning_rate': 4.9971e-05, 'epoch': 0.05, 'throughput': 974.68}
[INFO|2025-10-20 22:59:42] logging.py:143 >> {'loss': 0.4950, 'learning_rate': 4.9969e-05, 'epoch': 0.05, 'throughput': 974.78}
[INFO|2025-10-20 23:00:46] logging.py:143 >> {'loss': 0.5300, 'learning_rate': 4.9968e-05, 'epoch': 0.05, 'throughput': 974.79}
[INFO|2025-10-20 23:01:50] logging.py:143 >> {'loss': 0.4652, 'learning_rate': 4.9967e-05, 'epoch': 0.05, 'throughput': 974.86}
[INFO|2025-10-20 23:02:54] logging.py:143 >> {'loss': 0.5296, 'learning_rate': 4.9965e-05, 'epoch': 0.05, 'throughput': 974.90}
[INFO|2025-10-20 23:03:58] logging.py:143 >> {'loss': 0.7164, 'learning_rate': 4.9964e-05, 'epoch': 0.05, 'throughput': 974.93}
[INFO|2025-10-20 23:05:02] logging.py:143 >> {'loss': 0.5224, 'learning_rate': 4.9962e-05, 'epoch': 0.05, 'throughput': 974.87}
[INFO|2025-10-20 23:06:06] logging.py:143 >> {'loss': 0.4562, 'learning_rate': 4.9961e-05, 'epoch': 0.05, 'throughput': 974.90}
[INFO|2025-10-20 23:07:10] logging.py:143 >> {'loss': 0.6413, 'learning_rate': 4.9959e-05, 'epoch': 0.05, 'throughput': 974.91}
[INFO|2025-10-20 23:08:14] logging.py:143 >> {'loss': 0.5532, 'learning_rate': 4.9958e-05, 'epoch': 0.06, 'throughput': 974.97}
[INFO|2025-10-20 23:09:18] logging.py:143 >> {'loss': 0.5885, 'learning_rate': 4.9956e-05, 'epoch': 0.06, 'throughput': 975.01}
[INFO|2025-10-20 23:10:22] logging.py:143 >> {'loss': 0.6056, 'learning_rate': 4.9955e-05, 'epoch': 0.06, 'throughput': 974.99}
[INFO|2025-10-20 23:11:27] logging.py:143 >> {'loss': 0.6405, 'learning_rate': 4.9953e-05, 'epoch': 0.06, 'throughput': 975.07}
[INFO|2025-10-20 23:12:31] logging.py:143 >> {'loss': 0.5401, 'learning_rate': 4.9952e-05, 'epoch': 0.06, 'throughput': 975.08}
[INFO|2025-10-20 23:13:35] logging.py:143 >> {'loss': 0.6403, 'learning_rate': 4.9950e-05, 'epoch': 0.06, 'throughput': 975.09}
[INFO|2025-10-20 23:13:35] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-300
[INFO|2025-10-20 23:13:35] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-20 23:13:35] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-20 23:13:35] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-300/chat_template.jinja
[INFO|2025-10-20 23:13:35] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-300/tokenizer_config.json
[INFO|2025-10-20 23:13:35] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-300/special_tokens_map.json
[INFO|2025-10-20 23:14:39] logging.py:143 >> {'loss': 0.5131, 'learning_rate': 4.9948e-05, 'epoch': 0.06, 'throughput': 974.86}
[INFO|2025-10-20 23:15:43] logging.py:143 >> {'loss': 0.5537, 'learning_rate': 4.9947e-05, 'epoch': 0.06, 'throughput': 974.82}
[INFO|2025-10-20 23:16:47] logging.py:143 >> {'loss': 0.6277, 'learning_rate': 4.9945e-05, 'epoch': 0.06, 'throughput': 974.86}
[INFO|2025-10-20 23:17:51] logging.py:143 >> {'loss': 0.5596, 'learning_rate': 4.9943e-05, 'epoch': 0.06, 'throughput': 974.87}
[INFO|2025-10-20 23:18:55] logging.py:143 >> {'loss': 0.5751, 'learning_rate': 4.9941e-05, 'epoch': 0.07, 'throughput': 974.84}
[INFO|2025-10-20 23:19:59] logging.py:143 >> {'loss': 0.7230, 'learning_rate': 4.9939e-05, 'epoch': 0.07, 'throughput': 974.81}
[INFO|2025-10-20 23:21:04] logging.py:143 >> {'loss': 0.5975, 'learning_rate': 4.9938e-05, 'epoch': 0.07, 'throughput': 974.79}
[INFO|2025-10-20 23:22:08] logging.py:143 >> {'loss': 0.6019, 'learning_rate': 4.9936e-05, 'epoch': 0.07, 'throughput': 974.83}
[INFO|2025-10-20 23:23:12] logging.py:143 >> {'loss': 0.5583, 'learning_rate': 4.9934e-05, 'epoch': 0.07, 'throughput': 974.83}
[INFO|2025-10-20 23:24:16] logging.py:143 >> {'loss': 0.5001, 'learning_rate': 4.9932e-05, 'epoch': 0.07, 'throughput': 974.84}
[INFO|2025-10-20 23:25:21] logging.py:143 >> {'loss': 0.5462, 'learning_rate': 4.9930e-05, 'epoch': 0.07, 'throughput': 974.88}
[INFO|2025-10-20 23:26:26] logging.py:143 >> {'loss': 0.5845, 'learning_rate': 4.9928e-05, 'epoch': 0.07, 'throughput': 974.83}
[INFO|2025-10-20 23:27:30] logging.py:143 >> {'loss': 0.3801, 'learning_rate': 4.9926e-05, 'epoch': 0.07, 'throughput': 974.83}
[INFO|2025-10-20 23:28:34] logging.py:143 >> {'loss': 0.6770, 'learning_rate': 4.9924e-05, 'epoch': 0.07, 'throughput': 974.82}
[INFO|2025-10-20 23:29:39] logging.py:143 >> {'loss': 0.4761, 'learning_rate': 4.9922e-05, 'epoch': 0.08, 'throughput': 974.85}
[INFO|2025-10-20 23:30:43] logging.py:143 >> {'loss': 0.5752, 'learning_rate': 4.9920e-05, 'epoch': 0.08, 'throughput': 974.90}
[INFO|2025-10-20 23:31:46] logging.py:143 >> {'loss': 0.5808, 'learning_rate': 4.9917e-05, 'epoch': 0.08, 'throughput': 974.89}
[INFO|2025-10-20 23:32:51] logging.py:143 >> {'loss': 0.5848, 'learning_rate': 4.9915e-05, 'epoch': 0.08, 'throughput': 974.88}
[INFO|2025-10-20 23:33:54] logging.py:143 >> {'loss': 0.5560, 'learning_rate': 4.9913e-05, 'epoch': 0.08, 'throughput': 974.89}
[INFO|2025-10-20 23:34:59] logging.py:143 >> {'loss': 0.5620, 'learning_rate': 4.9911e-05, 'epoch': 0.08, 'throughput': 974.94}
[INFO|2025-10-20 23:34:59] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-400
[INFO|2025-10-20 23:34:59] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-20 23:34:59] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-20 23:35:00] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-400/chat_template.jinja
[INFO|2025-10-20 23:35:00] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-400/tokenizer_config.json
[INFO|2025-10-20 23:35:00] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-400/special_tokens_map.json
[INFO|2025-10-20 23:36:04] logging.py:143 >> {'loss': 0.4689, 'learning_rate': 4.9909e-05, 'epoch': 0.08, 'throughput': 974.80}
[INFO|2025-10-20 23:37:09] logging.py:143 >> {'loss': 0.4076, 'learning_rate': 4.9906e-05, 'epoch': 0.08, 'throughput': 974.78}
[INFO|2025-10-20 23:38:13] logging.py:143 >> {'loss': 0.7865, 'learning_rate': 4.9904e-05, 'epoch': 0.08, 'throughput': 974.80}
[INFO|2025-10-20 23:39:17] logging.py:143 >> {'loss': 0.7636, 'learning_rate': 4.9902e-05, 'epoch': 0.08, 'throughput': 974.80}
[INFO|2025-10-20 23:40:21] logging.py:143 >> {'loss': 0.5496, 'learning_rate': 4.9899e-05, 'epoch': 0.09, 'throughput': 974.80}
[INFO|2025-10-20 23:41:25] logging.py:143 >> {'loss': 0.5746, 'learning_rate': 4.9897e-05, 'epoch': 0.09, 'throughput': 974.81}
[INFO|2025-10-20 23:42:29] logging.py:143 >> {'loss': 0.5676, 'learning_rate': 4.9895e-05, 'epoch': 0.09, 'throughput': 974.79}
[INFO|2025-10-20 23:43:33] logging.py:143 >> {'loss': 0.5473, 'learning_rate': 4.9892e-05, 'epoch': 0.09, 'throughput': 974.83}
[INFO|2025-10-20 23:44:39] logging.py:143 >> {'loss': 0.6022, 'learning_rate': 4.9890e-05, 'epoch': 0.09, 'throughput': 974.92}
[INFO|2025-10-20 23:45:42] logging.py:143 >> {'loss': 0.5299, 'learning_rate': 4.9887e-05, 'epoch': 0.09, 'throughput': 974.92}
[INFO|2025-10-20 23:46:46] logging.py:143 >> {'loss': 0.4328, 'learning_rate': 4.9885e-05, 'epoch': 0.09, 'throughput': 974.91}
[INFO|2025-10-20 23:47:51] logging.py:143 >> {'loss': 0.4719, 'learning_rate': 4.9882e-05, 'epoch': 0.09, 'throughput': 974.95}
[INFO|2025-10-20 23:48:55] logging.py:143 >> {'loss': 0.6382, 'learning_rate': 4.9880e-05, 'epoch': 0.09, 'throughput': 974.90}
[INFO|2025-10-20 23:49:59] logging.py:143 >> {'loss': 0.6810, 'learning_rate': 4.9877e-05, 'epoch': 0.09, 'throughput': 974.93}
[INFO|2025-10-20 23:51:03] logging.py:143 >> {'loss': 0.5577, 'learning_rate': 4.9874e-05, 'epoch': 0.10, 'throughput': 974.92}
[INFO|2025-10-20 23:52:07] logging.py:143 >> {'loss': 0.5848, 'learning_rate': 4.9872e-05, 'epoch': 0.10, 'throughput': 974.97}
[INFO|2025-10-20 23:53:12] logging.py:143 >> {'loss': 0.4477, 'learning_rate': 4.9869e-05, 'epoch': 0.10, 'throughput': 975.01}
[INFO|2025-10-20 23:54:16] logging.py:143 >> {'loss': 0.5869, 'learning_rate': 4.9866e-05, 'epoch': 0.10, 'throughput': 975.02}
[INFO|2025-10-20 23:55:21] logging.py:143 >> {'loss': 0.6464, 'learning_rate': 4.9863e-05, 'epoch': 0.10, 'throughput': 975.05}
[INFO|2025-10-20 23:56:25] logging.py:143 >> {'loss': 0.5652, 'learning_rate': 4.9861e-05, 'epoch': 0.10, 'throughput': 975.07}
[INFO|2025-10-20 23:56:25] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-500
[INFO|2025-10-20 23:56:25] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-20 23:56:25] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-20 23:56:25] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-500/chat_template.jinja
[INFO|2025-10-20 23:56:25] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-500/tokenizer_config.json
[INFO|2025-10-20 23:56:25] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-500/special_tokens_map.json
[INFO|2025-10-20 23:57:30] logging.py:143 >> {'loss': 0.5224, 'learning_rate': 4.9858e-05, 'epoch': 0.10, 'throughput': 974.98}
[INFO|2025-10-20 23:58:34] logging.py:143 >> {'loss': 0.4803, 'learning_rate': 4.9855e-05, 'epoch': 0.10, 'throughput': 974.98}
[INFO|2025-10-20 23:59:38] logging.py:143 >> {'loss': 0.7243, 'learning_rate': 4.9852e-05, 'epoch': 0.10, 'throughput': 974.94}
[INFO|2025-10-21 00:00:42] logging.py:143 >> {'loss': 0.5130, 'learning_rate': 4.9849e-05, 'epoch': 0.11, 'throughput': 974.97}
[INFO|2025-10-21 00:01:46] logging.py:143 >> {'loss': 0.5966, 'learning_rate': 4.9846e-05, 'epoch': 0.11, 'throughput': 974.95}
[INFO|2025-10-21 00:02:51] logging.py:143 >> {'loss': 0.4807, 'learning_rate': 4.9843e-05, 'epoch': 0.11, 'throughput': 975.01}
[INFO|2025-10-21 00:03:54] logging.py:143 >> {'loss': 0.6098, 'learning_rate': 4.9841e-05, 'epoch': 0.11, 'throughput': 975.03}
[INFO|2025-10-21 00:04:59] logging.py:143 >> {'loss': 0.5284, 'learning_rate': 4.9838e-05, 'epoch': 0.11, 'throughput': 975.01}
[INFO|2025-10-21 00:06:04] logging.py:143 >> {'loss': 0.4674, 'learning_rate': 4.9834e-05, 'epoch': 0.11, 'throughput': 975.06}
[INFO|2025-10-21 00:07:10] logging.py:143 >> {'loss': 0.5032, 'learning_rate': 4.9831e-05, 'epoch': 0.11, 'throughput': 975.13}
[INFO|2025-10-21 00:08:14] logging.py:143 >> {'loss': 0.5402, 'learning_rate': 4.9828e-05, 'epoch': 0.11, 'throughput': 975.12}
[INFO|2025-10-21 00:09:18] logging.py:143 >> {'loss': 0.5421, 'learning_rate': 4.9825e-05, 'epoch': 0.11, 'throughput': 975.16}
[INFO|2025-10-21 00:10:23] logging.py:143 >> {'loss': 0.5577, 'learning_rate': 4.9822e-05, 'epoch': 0.11, 'throughput': 975.13}
[INFO|2025-10-21 00:11:27] logging.py:143 >> {'loss': 0.6090, 'learning_rate': 4.9819e-05, 'epoch': 0.12, 'throughput': 975.14}
[INFO|2025-10-21 00:12:32] logging.py:143 >> {'loss': 0.5708, 'learning_rate': 4.9816e-05, 'epoch': 0.12, 'throughput': 975.17}
[INFO|2025-10-21 00:13:37] logging.py:143 >> {'loss': 0.5243, 'learning_rate': 4.9813e-05, 'epoch': 0.12, 'throughput': 975.19}
[INFO|2025-10-21 00:14:41] logging.py:143 >> {'loss': 0.4952, 'learning_rate': 4.9809e-05, 'epoch': 0.12, 'throughput': 975.21}
[INFO|2025-10-21 00:15:46] logging.py:143 >> {'loss': 0.5539, 'learning_rate': 4.9806e-05, 'epoch': 0.12, 'throughput': 975.24}
[INFO|2025-10-21 00:16:50] logging.py:143 >> {'loss': 0.5735, 'learning_rate': 4.9803e-05, 'epoch': 0.12, 'throughput': 975.27}
[INFO|2025-10-21 00:17:54] logging.py:143 >> {'loss': 0.4967, 'learning_rate': 4.9799e-05, 'epoch': 0.12, 'throughput': 975.25}
[INFO|2025-10-21 00:17:54] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-600
[INFO|2025-10-21 00:17:54] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 00:17:54] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 00:17:54] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-600/chat_template.jinja
[INFO|2025-10-21 00:17:54] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-600/tokenizer_config.json
[INFO|2025-10-21 00:17:54] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-600/special_tokens_map.json
[INFO|2025-10-21 00:18:59] logging.py:143 >> {'loss': 0.5945, 'learning_rate': 4.9796e-05, 'epoch': 0.12, 'throughput': 975.18}
[INFO|2025-10-21 00:20:03] logging.py:143 >> {'loss': 0.4712, 'learning_rate': 4.9793e-05, 'epoch': 0.12, 'throughput': 975.21}
[INFO|2025-10-21 00:21:06] logging.py:143 >> {'loss': 0.6655, 'learning_rate': 4.9789e-05, 'epoch': 0.12, 'throughput': 975.18}
[INFO|2025-10-21 00:22:11] logging.py:143 >> {'loss': 0.5773, 'learning_rate': 4.9786e-05, 'epoch': 0.13, 'throughput': 975.19}
[INFO|2025-10-21 00:23:15] logging.py:143 >> {'loss': 0.4447, 'learning_rate': 4.9782e-05, 'epoch': 0.13, 'throughput': 975.22}
[INFO|2025-10-21 00:24:20] logging.py:143 >> {'loss': 0.4912, 'learning_rate': 4.9779e-05, 'epoch': 0.13, 'throughput': 975.26}
[INFO|2025-10-21 00:25:24] logging.py:143 >> {'loss': 0.5265, 'learning_rate': 4.9775e-05, 'epoch': 0.13, 'throughput': 975.27}
[INFO|2025-10-21 00:26:29] logging.py:143 >> {'loss': 0.6131, 'learning_rate': 4.9772e-05, 'epoch': 0.13, 'throughput': 975.28}
[INFO|2025-10-21 00:27:33] logging.py:143 >> {'loss': 0.5223, 'learning_rate': 4.9768e-05, 'epoch': 0.13, 'throughput': 975.28}
[INFO|2025-10-21 00:28:38] logging.py:143 >> {'loss': 0.6205, 'learning_rate': 4.9765e-05, 'epoch': 0.13, 'throughput': 975.31}
[INFO|2025-10-21 00:29:42] logging.py:143 >> {'loss': 0.5946, 'learning_rate': 4.9761e-05, 'epoch': 0.13, 'throughput': 975.33}
[INFO|2025-10-21 00:30:46] logging.py:143 >> {'loss': 0.5508, 'learning_rate': 4.9757e-05, 'epoch': 0.13, 'throughput': 975.34}
[INFO|2025-10-21 00:31:50] logging.py:143 >> {'loss': 0.6512, 'learning_rate': 4.9754e-05, 'epoch': 0.13, 'throughput': 975.33}
[INFO|2025-10-21 00:32:54] logging.py:143 >> {'loss': 0.5431, 'learning_rate': 4.9750e-05, 'epoch': 0.14, 'throughput': 975.32}
[INFO|2025-10-21 00:33:59] logging.py:143 >> {'loss': 0.4101, 'learning_rate': 4.9746e-05, 'epoch': 0.14, 'throughput': 975.36}
[INFO|2025-10-21 00:35:02] logging.py:143 >> {'loss': 0.3915, 'learning_rate': 4.9742e-05, 'epoch': 0.14, 'throughput': 975.35}
[INFO|2025-10-21 00:36:05] logging.py:143 >> {'loss': 0.6023, 'learning_rate': 4.9739e-05, 'epoch': 0.14, 'throughput': 975.32}
[INFO|2025-10-21 00:37:09] logging.py:143 >> {'loss': 0.5472, 'learning_rate': 4.9735e-05, 'epoch': 0.14, 'throughput': 975.29}
[INFO|2025-10-21 00:38:14] logging.py:143 >> {'loss': 0.5076, 'learning_rate': 4.9731e-05, 'epoch': 0.14, 'throughput': 975.29}
[INFO|2025-10-21 00:39:18] logging.py:143 >> {'loss': 0.3967, 'learning_rate': 4.9727e-05, 'epoch': 0.14, 'throughput': 975.30}
[INFO|2025-10-21 00:39:18] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-700
[INFO|2025-10-21 00:39:18] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 00:39:18] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 00:39:19] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-700/chat_template.jinja
[INFO|2025-10-21 00:39:19] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-700/tokenizer_config.json
[INFO|2025-10-21 00:39:19] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-700/special_tokens_map.json
[INFO|2025-10-21 00:40:24] logging.py:143 >> {'loss': 0.5762, 'learning_rate': 4.9723e-05, 'epoch': 0.14, 'throughput': 975.24}
[INFO|2025-10-21 00:41:29] logging.py:143 >> {'loss': 0.7174, 'learning_rate': 4.9719e-05, 'epoch': 0.14, 'throughput': 975.29}
[INFO|2025-10-21 00:42:34] logging.py:143 >> {'loss': 0.4741, 'learning_rate': 4.9715e-05, 'epoch': 0.14, 'throughput': 975.28}
[INFO|2025-10-21 00:43:38] logging.py:143 >> {'loss': 0.4334, 'learning_rate': 4.9711e-05, 'epoch': 0.15, 'throughput': 975.30}
[INFO|2025-10-21 00:44:43] logging.py:143 >> {'loss': 0.6370, 'learning_rate': 4.9707e-05, 'epoch': 0.15, 'throughput': 975.32}
[INFO|2025-10-21 00:45:47] logging.py:143 >> {'loss': 0.4679, 'learning_rate': 4.9703e-05, 'epoch': 0.15, 'throughput': 975.31}
[INFO|2025-10-21 00:46:50] logging.py:143 >> {'loss': 0.5204, 'learning_rate': 4.9699e-05, 'epoch': 0.15, 'throughput': 975.30}
[INFO|2025-10-21 00:47:54] logging.py:143 >> {'loss': 0.3960, 'learning_rate': 4.9695e-05, 'epoch': 0.15, 'throughput': 975.33}
[INFO|2025-10-21 00:48:58] logging.py:143 >> {'loss': 0.5262, 'learning_rate': 4.9691e-05, 'epoch': 0.15, 'throughput': 975.31}
[INFO|2025-10-21 00:50:03] logging.py:143 >> {'loss': 0.5145, 'learning_rate': 4.9687e-05, 'epoch': 0.15, 'throughput': 975.34}
[INFO|2025-10-21 00:51:06] logging.py:143 >> {'loss': 0.6042, 'learning_rate': 4.9682e-05, 'epoch': 0.15, 'throughput': 975.33}
[INFO|2025-10-21 00:52:10] logging.py:143 >> {'loss': 0.3788, 'learning_rate': 4.9678e-05, 'epoch': 0.15, 'throughput': 975.34}
[INFO|2025-10-21 00:53:14] logging.py:143 >> {'loss': 0.5437, 'learning_rate': 4.9674e-05, 'epoch': 0.15, 'throughput': 975.35}
[INFO|2025-10-21 00:54:18] logging.py:143 >> {'loss': 0.5083, 'learning_rate': 4.9670e-05, 'epoch': 0.16, 'throughput': 975.32}
[INFO|2025-10-21 00:55:22] logging.py:143 >> {'loss': 0.4856, 'learning_rate': 4.9665e-05, 'epoch': 0.16, 'throughput': 975.32}
[INFO|2025-10-21 00:56:27] logging.py:143 >> {'loss': 0.6251, 'learning_rate': 4.9661e-05, 'epoch': 0.16, 'throughput': 975.36}
[INFO|2025-10-21 00:57:32] logging.py:143 >> {'loss': 0.6153, 'learning_rate': 4.9657e-05, 'epoch': 0.16, 'throughput': 975.39}
[INFO|2025-10-21 00:58:36] logging.py:143 >> {'loss': 0.5767, 'learning_rate': 4.9652e-05, 'epoch': 0.16, 'throughput': 975.41}
[INFO|2025-10-21 00:59:40] logging.py:143 >> {'loss': 0.6148, 'learning_rate': 4.9648e-05, 'epoch': 0.16, 'throughput': 975.42}
[INFO|2025-10-21 01:00:44] logging.py:143 >> {'loss': 0.5412, 'learning_rate': 4.9643e-05, 'epoch': 0.16, 'throughput': 975.38}
[INFO|2025-10-21 01:00:44] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-800
[INFO|2025-10-21 01:00:44] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 01:00:44] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 01:00:44] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-800/chat_template.jinja
[INFO|2025-10-21 01:00:44] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-800/tokenizer_config.json
[INFO|2025-10-21 01:00:44] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-800/special_tokens_map.json
[INFO|2025-10-21 01:01:48] logging.py:143 >> {'loss': 0.6647, 'learning_rate': 4.9639e-05, 'epoch': 0.16, 'throughput': 975.32}
[INFO|2025-10-21 01:02:53] logging.py:143 >> {'loss': 0.4279, 'learning_rate': 4.9634e-05, 'epoch': 0.16, 'throughput': 975.34}
[INFO|2025-10-21 01:03:57] logging.py:143 >> {'loss': 0.6188, 'learning_rate': 4.9630e-05, 'epoch': 0.16, 'throughput': 975.35}
[INFO|2025-10-21 01:05:01] logging.py:143 >> {'loss': 0.8083, 'learning_rate': 4.9625e-05, 'epoch': 0.17, 'throughput': 975.37}
[INFO|2025-10-21 01:06:06] logging.py:143 >> {'loss': 0.6650, 'learning_rate': 4.9621e-05, 'epoch': 0.17, 'throughput': 975.37}
[INFO|2025-10-21 01:07:10] logging.py:143 >> {'loss': 0.5295, 'learning_rate': 4.9616e-05, 'epoch': 0.17, 'throughput': 975.35}
[INFO|2025-10-21 01:08:14] logging.py:143 >> {'loss': 0.5889, 'learning_rate': 4.9612e-05, 'epoch': 0.17, 'throughput': 975.36}
[INFO|2025-10-21 01:09:18] logging.py:143 >> {'loss': 0.4153, 'learning_rate': 4.9607e-05, 'epoch': 0.17, 'throughput': 975.35}
[INFO|2025-10-21 01:10:22] logging.py:143 >> {'loss': 0.5922, 'learning_rate': 4.9602e-05, 'epoch': 0.17, 'throughput': 975.35}
[INFO|2025-10-21 01:11:26] logging.py:143 >> {'loss': 0.6029, 'learning_rate': 4.9598e-05, 'epoch': 0.17, 'throughput': 975.34}
[INFO|2025-10-21 01:12:30] logging.py:143 >> {'loss': 0.5673, 'learning_rate': 4.9593e-05, 'epoch': 0.17, 'throughput': 975.35}
[INFO|2025-10-21 01:13:35] logging.py:143 >> {'loss': 0.4951, 'learning_rate': 4.9588e-05, 'epoch': 0.17, 'throughput': 975.38}
[INFO|2025-10-21 01:14:39] logging.py:143 >> {'loss': 0.6315, 'learning_rate': 4.9583e-05, 'epoch': 0.17, 'throughput': 975.39}
[INFO|2025-10-21 01:15:43] logging.py:143 >> {'loss': 0.5582, 'learning_rate': 4.9578e-05, 'epoch': 0.18, 'throughput': 975.39}
[INFO|2025-10-21 01:16:48] logging.py:143 >> {'loss': 0.5539, 'learning_rate': 4.9574e-05, 'epoch': 0.18, 'throughput': 975.38}
[INFO|2025-10-21 01:17:52] logging.py:143 >> {'loss': 0.5056, 'learning_rate': 4.9569e-05, 'epoch': 0.18, 'throughput': 975.40}
[INFO|2025-10-21 01:18:56] logging.py:143 >> {'loss': 0.4853, 'learning_rate': 4.9564e-05, 'epoch': 0.18, 'throughput': 975.38}
[INFO|2025-10-21 01:20:00] logging.py:143 >> {'loss': 0.4136, 'learning_rate': 4.9559e-05, 'epoch': 0.18, 'throughput': 975.38}
[INFO|2025-10-21 01:21:05] logging.py:143 >> {'loss': 0.4363, 'learning_rate': 4.9554e-05, 'epoch': 0.18, 'throughput': 975.41}
[INFO|2025-10-21 01:22:09] logging.py:143 >> {'loss': 0.5080, 'learning_rate': 4.9549e-05, 'epoch': 0.18, 'throughput': 975.40}
[INFO|2025-10-21 01:22:09] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-900
[INFO|2025-10-21 01:22:09] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 01:22:09] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 01:22:09] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-900/chat_template.jinja
[INFO|2025-10-21 01:22:09] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-900/tokenizer_config.json
[INFO|2025-10-21 01:22:09] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-900/special_tokens_map.json
[INFO|2025-10-21 01:23:13] logging.py:143 >> {'loss': 0.7297, 'learning_rate': 4.9544e-05, 'epoch': 0.18, 'throughput': 975.33}
[INFO|2025-10-21 01:24:18] logging.py:143 >> {'loss': 0.5134, 'learning_rate': 4.9539e-05, 'epoch': 0.18, 'throughput': 975.35}
[INFO|2025-10-21 01:25:22] logging.py:143 >> {'loss': 0.6230, 'learning_rate': 4.9534e-05, 'epoch': 0.18, 'throughput': 975.36}
[INFO|2025-10-21 01:26:26] logging.py:143 >> {'loss': 0.5305, 'learning_rate': 4.9529e-05, 'epoch': 0.19, 'throughput': 975.37}
[INFO|2025-10-21 01:27:31] logging.py:143 >> {'loss': 0.4943, 'learning_rate': 4.9523e-05, 'epoch': 0.19, 'throughput': 975.37}
[INFO|2025-10-21 01:28:35] logging.py:143 >> {'loss': 0.5239, 'learning_rate': 4.9518e-05, 'epoch': 0.19, 'throughput': 975.37}
[INFO|2025-10-21 01:29:39] logging.py:143 >> {'loss': 0.6014, 'learning_rate': 4.9513e-05, 'epoch': 0.19, 'throughput': 975.36}
[INFO|2025-10-21 01:30:43] logging.py:143 >> {'loss': 0.4795, 'learning_rate': 4.9508e-05, 'epoch': 0.19, 'throughput': 975.36}
[INFO|2025-10-21 01:31:48] logging.py:143 >> {'loss': 0.6804, 'learning_rate': 4.9503e-05, 'epoch': 0.19, 'throughput': 975.37}
[INFO|2025-10-21 01:32:53] logging.py:143 >> {'loss': 0.3728, 'learning_rate': 4.9497e-05, 'epoch': 0.19, 'throughput': 975.37}
[INFO|2025-10-21 01:33:56] logging.py:143 >> {'loss': 0.5787, 'learning_rate': 4.9492e-05, 'epoch': 0.19, 'throughput': 975.36}
[INFO|2025-10-21 01:35:01] logging.py:143 >> {'loss': 0.5004, 'learning_rate': 4.9487e-05, 'epoch': 0.19, 'throughput': 975.36}
[INFO|2025-10-21 01:36:05] logging.py:143 >> {'loss': 0.3879, 'learning_rate': 4.9481e-05, 'epoch': 0.20, 'throughput': 975.37}
[INFO|2025-10-21 01:37:09] logging.py:143 >> {'loss': 0.5630, 'learning_rate': 4.9476e-05, 'epoch': 0.20, 'throughput': 975.36}
[INFO|2025-10-21 01:38:15] logging.py:143 >> {'loss': 0.5075, 'learning_rate': 4.9471e-05, 'epoch': 0.20, 'throughput': 975.39}
[INFO|2025-10-21 01:39:19] logging.py:143 >> {'loss': 0.6108, 'learning_rate': 4.9465e-05, 'epoch': 0.20, 'throughput': 975.39}
[INFO|2025-10-21 01:40:23] logging.py:143 >> {'loss': 0.4147, 'learning_rate': 4.9460e-05, 'epoch': 0.20, 'throughput': 975.39}
[INFO|2025-10-21 01:41:27] logging.py:143 >> {'loss': 0.4432, 'learning_rate': 4.9454e-05, 'epoch': 0.20, 'throughput': 975.41}
[INFO|2025-10-21 01:42:32] logging.py:143 >> {'loss': 0.5730, 'learning_rate': 4.9449e-05, 'epoch': 0.20, 'throughput': 975.42}
[INFO|2025-10-21 01:43:36] logging.py:143 >> {'loss': 0.6542, 'learning_rate': 4.9443e-05, 'epoch': 0.20, 'throughput': 975.45}
[INFO|2025-10-21 01:43:36] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1000
[INFO|2025-10-21 01:43:36] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 01:43:36] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 01:43:37] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1000/chat_template.jinja
[INFO|2025-10-21 01:43:37] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1000/tokenizer_config.json
[INFO|2025-10-21 01:43:37] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1000/special_tokens_map.json
[INFO|2025-10-21 01:44:41] logging.py:143 >> {'loss': 0.5937, 'learning_rate': 4.9438e-05, 'epoch': 0.20, 'throughput': 975.38}
[INFO|2025-10-21 01:45:45] logging.py:143 >> {'loss': 0.5868, 'learning_rate': 4.9432e-05, 'epoch': 0.20, 'throughput': 975.39}
[INFO|2025-10-21 01:46:50] logging.py:143 >> {'loss': 0.5071, 'learning_rate': 4.9427e-05, 'epoch': 0.21, 'throughput': 975.41}
[INFO|2025-10-21 01:47:54] logging.py:143 >> {'loss': 0.6936, 'learning_rate': 4.9421e-05, 'epoch': 0.21, 'throughput': 975.43}
[INFO|2025-10-21 01:48:59] logging.py:143 >> {'loss': 0.4687, 'learning_rate': 4.9415e-05, 'epoch': 0.21, 'throughput': 975.42}
[INFO|2025-10-21 01:50:03] logging.py:143 >> {'loss': 0.4370, 'learning_rate': 4.9409e-05, 'epoch': 0.21, 'throughput': 975.43}
[INFO|2025-10-21 01:51:07] logging.py:143 >> {'loss': 0.5784, 'learning_rate': 4.9404e-05, 'epoch': 0.21, 'throughput': 975.43}
[INFO|2025-10-21 01:52:11] logging.py:143 >> {'loss': 0.6199, 'learning_rate': 4.9398e-05, 'epoch': 0.21, 'throughput': 975.44}
[INFO|2025-10-21 01:53:15] logging.py:143 >> {'loss': 0.4842, 'learning_rate': 4.9392e-05, 'epoch': 0.21, 'throughput': 975.47}
[INFO|2025-10-21 01:54:20] logging.py:143 >> {'loss': 0.6005, 'learning_rate': 4.9386e-05, 'epoch': 0.21, 'throughput': 975.49}
[INFO|2025-10-21 01:55:24] logging.py:143 >> {'loss': 0.5228, 'learning_rate': 4.9381e-05, 'epoch': 0.21, 'throughput': 975.49}
[INFO|2025-10-21 01:56:30] logging.py:143 >> {'loss': 0.5864, 'learning_rate': 4.9375e-05, 'epoch': 0.21, 'throughput': 975.51}
[INFO|2025-10-21 01:57:33] logging.py:143 >> {'loss': 0.6442, 'learning_rate': 4.9369e-05, 'epoch': 0.22, 'throughput': 975.52}
[INFO|2025-10-21 01:58:38] logging.py:143 >> {'loss': 0.6308, 'learning_rate': 4.9363e-05, 'epoch': 0.22, 'throughput': 975.53}
[INFO|2025-10-21 01:59:42] logging.py:143 >> {'loss': 0.6283, 'learning_rate': 4.9357e-05, 'epoch': 0.22, 'throughput': 975.54}
[INFO|2025-10-21 02:00:47] logging.py:143 >> {'loss': 0.6495, 'learning_rate': 4.9351e-05, 'epoch': 0.22, 'throughput': 975.54}
[INFO|2025-10-21 02:01:52] logging.py:143 >> {'loss': 0.5100, 'learning_rate': 4.9345e-05, 'epoch': 0.22, 'throughput': 975.56}
[INFO|2025-10-21 02:02:55] logging.py:143 >> {'loss': 0.5314, 'learning_rate': 4.9339e-05, 'epoch': 0.22, 'throughput': 975.54}
[INFO|2025-10-21 02:03:59] logging.py:143 >> {'loss': 0.4928, 'learning_rate': 4.9333e-05, 'epoch': 0.22, 'throughput': 975.56}
[INFO|2025-10-21 02:05:03] logging.py:143 >> {'loss': 0.5086, 'learning_rate': 4.9327e-05, 'epoch': 0.22, 'throughput': 975.56}
[INFO|2025-10-21 02:05:03] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1100
[INFO|2025-10-21 02:05:03] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 02:05:03] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 02:05:03] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1100/chat_template.jinja
[INFO|2025-10-21 02:05:03] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1100/tokenizer_config.json
[INFO|2025-10-21 02:05:03] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1100/special_tokens_map.json
[INFO|2025-10-21 02:06:07] logging.py:143 >> {'loss': 0.5650, 'learning_rate': 4.9321e-05, 'epoch': 0.22, 'throughput': 975.52}
[INFO|2025-10-21 02:07:12] logging.py:143 >> {'loss': 0.5693, 'learning_rate': 4.9315e-05, 'epoch': 0.22, 'throughput': 975.56}
[INFO|2025-10-21 02:08:17] logging.py:143 >> {'loss': 0.5122, 'learning_rate': 4.9308e-05, 'epoch': 0.23, 'throughput': 975.56}
[INFO|2025-10-21 02:09:20] logging.py:143 >> {'loss': 0.5673, 'learning_rate': 4.9302e-05, 'epoch': 0.23, 'throughput': 975.56}
[INFO|2025-10-21 02:10:24] logging.py:143 >> {'loss': 0.4628, 'learning_rate': 4.9296e-05, 'epoch': 0.23, 'throughput': 975.56}
[INFO|2025-10-21 02:11:28] logging.py:143 >> {'loss': 0.5511, 'learning_rate': 4.9290e-05, 'epoch': 0.23, 'throughput': 975.58}
[INFO|2025-10-21 02:12:32] logging.py:143 >> {'loss': 0.4791, 'learning_rate': 4.9283e-05, 'epoch': 0.23, 'throughput': 975.58}
[INFO|2025-10-21 02:13:36] logging.py:143 >> {'loss': 0.6754, 'learning_rate': 4.9277e-05, 'epoch': 0.23, 'throughput': 975.59}
[INFO|2025-10-21 02:14:40] logging.py:143 >> {'loss': 0.4545, 'learning_rate': 4.9271e-05, 'epoch': 0.23, 'throughput': 975.59}
[INFO|2025-10-21 02:15:45] logging.py:143 >> {'loss': 0.4916, 'learning_rate': 4.9264e-05, 'epoch': 0.23, 'throughput': 975.61}
[INFO|2025-10-21 02:16:48] logging.py:143 >> {'loss': 0.6000, 'learning_rate': 4.9258e-05, 'epoch': 0.23, 'throughput': 975.60}
[INFO|2025-10-21 02:17:52] logging.py:143 >> {'loss': 0.5529, 'learning_rate': 4.9252e-05, 'epoch': 0.23, 'throughput': 975.58}
[INFO|2025-10-21 02:18:56] logging.py:143 >> {'loss': 0.5409, 'learning_rate': 4.9245e-05, 'epoch': 0.24, 'throughput': 975.59}
[INFO|2025-10-21 02:20:01] logging.py:143 >> {'loss': 0.6399, 'learning_rate': 4.9239e-05, 'epoch': 0.24, 'throughput': 975.62}
[INFO|2025-10-21 02:21:06] logging.py:143 >> {'loss': 0.5926, 'learning_rate': 4.9232e-05, 'epoch': 0.24, 'throughput': 975.63}
[INFO|2025-10-21 02:22:10] logging.py:143 >> {'loss': 0.7050, 'learning_rate': 4.9226e-05, 'epoch': 0.24, 'throughput': 975.65}
[INFO|2025-10-21 02:23:15] logging.py:143 >> {'loss': 0.4949, 'learning_rate': 4.9219e-05, 'epoch': 0.24, 'throughput': 975.64}
[INFO|2025-10-21 02:24:19] logging.py:143 >> {'loss': 0.5992, 'learning_rate': 4.9213e-05, 'epoch': 0.24, 'throughput': 975.65}
[INFO|2025-10-21 02:25:22] logging.py:143 >> {'loss': 0.6232, 'learning_rate': 4.9206e-05, 'epoch': 0.24, 'throughput': 975.63}
[INFO|2025-10-21 02:26:26] logging.py:143 >> {'loss': 0.4862, 'learning_rate': 4.9199e-05, 'epoch': 0.24, 'throughput': 975.63}
[INFO|2025-10-21 02:26:26] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1200
[INFO|2025-10-21 02:26:26] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 02:26:26] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 02:26:26] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1200/chat_template.jinja
[INFO|2025-10-21 02:26:26] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1200/tokenizer_config.json
[INFO|2025-10-21 02:26:26] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1200/special_tokens_map.json
[INFO|2025-10-21 02:27:32] logging.py:143 >> {'loss': 0.5439, 'learning_rate': 4.9193e-05, 'epoch': 0.24, 'throughput': 975.59}
[INFO|2025-10-21 02:28:36] logging.py:143 >> {'loss': 0.6832, 'learning_rate': 4.9186e-05, 'epoch': 0.24, 'throughput': 975.59}
[INFO|2025-10-21 02:29:42] logging.py:143 >> {'loss': 0.7190, 'learning_rate': 4.9179e-05, 'epoch': 0.25, 'throughput': 975.64}
[INFO|2025-10-21 02:30:45] logging.py:143 >> {'loss': 0.4160, 'learning_rate': 4.9173e-05, 'epoch': 0.25, 'throughput': 975.63}
[INFO|2025-10-21 02:31:50] logging.py:143 >> {'loss': 0.6377, 'learning_rate': 4.9166e-05, 'epoch': 0.25, 'throughput': 975.66}
[INFO|2025-10-21 02:32:55] logging.py:143 >> {'loss': 0.6530, 'learning_rate': 4.9159e-05, 'epoch': 0.25, 'throughput': 975.67}
[INFO|2025-10-21 02:33:59] logging.py:143 >> {'loss': 0.6825, 'learning_rate': 4.9152e-05, 'epoch': 0.25, 'throughput': 975.67}
[INFO|2025-10-21 02:35:03] logging.py:143 >> {'loss': 0.4961, 'learning_rate': 4.9145e-05, 'epoch': 0.25, 'throughput': 975.67}
[INFO|2025-10-21 02:36:07] logging.py:143 >> {'loss': 0.5533, 'learning_rate': 4.9139e-05, 'epoch': 0.25, 'throughput': 975.68}
[INFO|2025-10-21 02:37:11] logging.py:143 >> {'loss': 0.6095, 'learning_rate': 4.9132e-05, 'epoch': 0.25, 'throughput': 975.69}
[INFO|2025-10-21 02:38:16] logging.py:143 >> {'loss': 0.4827, 'learning_rate': 4.9125e-05, 'epoch': 0.25, 'throughput': 975.70}
[INFO|2025-10-21 02:39:20] logging.py:143 >> {'loss': 0.5165, 'learning_rate': 4.9118e-05, 'epoch': 0.25, 'throughput': 975.72}
[INFO|2025-10-21 02:40:25] logging.py:143 >> {'loss': 0.4107, 'learning_rate': 4.9111e-05, 'epoch': 0.26, 'throughput': 975.74}
[INFO|2025-10-21 02:41:29] logging.py:143 >> {'loss': 0.5736, 'learning_rate': 4.9104e-05, 'epoch': 0.26, 'throughput': 975.76}
[INFO|2025-10-21 02:42:34] logging.py:143 >> {'loss': 0.4268, 'learning_rate': 4.9097e-05, 'epoch': 0.26, 'throughput': 975.78}
[INFO|2025-10-21 02:43:37] logging.py:143 >> {'loss': 0.4690, 'learning_rate': 4.9090e-05, 'epoch': 0.26, 'throughput': 975.77}
[INFO|2025-10-21 02:44:42] logging.py:143 >> {'loss': 0.4931, 'learning_rate': 4.9083e-05, 'epoch': 0.26, 'throughput': 975.78}
[INFO|2025-10-21 02:45:46] logging.py:143 >> {'loss': 0.6722, 'learning_rate': 4.9075e-05, 'epoch': 0.26, 'throughput': 975.78}
[INFO|2025-10-21 02:46:51] logging.py:143 >> {'loss': 0.5204, 'learning_rate': 4.9068e-05, 'epoch': 0.26, 'throughput': 975.80}
[INFO|2025-10-21 02:47:55] logging.py:143 >> {'loss': 0.5581, 'learning_rate': 4.9061e-05, 'epoch': 0.26, 'throughput': 975.79}
[INFO|2025-10-21 02:47:55] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1300
[INFO|2025-10-21 02:47:55] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 02:47:55] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 02:47:55] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1300/chat_template.jinja
[INFO|2025-10-21 02:47:55] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1300/tokenizer_config.json
[INFO|2025-10-21 02:47:55] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1300/special_tokens_map.json
[INFO|2025-10-21 02:48:59] logging.py:143 >> {'loss': 0.5020, 'learning_rate': 4.9054e-05, 'epoch': 0.26, 'throughput': 975.74}
[INFO|2025-10-21 02:50:04] logging.py:143 >> {'loss': 0.4739, 'learning_rate': 4.9047e-05, 'epoch': 0.26, 'throughput': 975.76}
[INFO|2025-10-21 02:51:08] logging.py:143 >> {'loss': 0.5401, 'learning_rate': 4.9039e-05, 'epoch': 0.27, 'throughput': 975.74}
[INFO|2025-10-21 02:52:12] logging.py:143 >> {'loss': 0.5350, 'learning_rate': 4.9032e-05, 'epoch': 0.27, 'throughput': 975.75}
[INFO|2025-10-21 02:53:15] logging.py:143 >> {'loss': 0.4942, 'learning_rate': 4.9025e-05, 'epoch': 0.27, 'throughput': 975.74}
[INFO|2025-10-21 02:54:19] logging.py:143 >> {'loss': 0.5941, 'learning_rate': 4.9018e-05, 'epoch': 0.27, 'throughput': 975.75}
[INFO|2025-10-21 02:55:23] logging.py:143 >> {'loss': 0.6074, 'learning_rate': 4.9010e-05, 'epoch': 0.27, 'throughput': 975.76}
[INFO|2025-10-21 02:56:28] logging.py:143 >> {'loss': 0.5629, 'learning_rate': 4.9003e-05, 'epoch': 0.27, 'throughput': 975.75}
[INFO|2025-10-21 02:57:31] logging.py:143 >> {'loss': 0.4743, 'learning_rate': 4.8995e-05, 'epoch': 0.27, 'throughput': 975.76}
[INFO|2025-10-21 02:58:35] logging.py:143 >> {'loss': 0.4476, 'learning_rate': 4.8988e-05, 'epoch': 0.27, 'throughput': 975.76}
[INFO|2025-10-21 02:59:39] logging.py:143 >> {'loss': 0.5061, 'learning_rate': 4.8981e-05, 'epoch': 0.27, 'throughput': 975.76}
[INFO|2025-10-21 03:00:43] logging.py:143 >> {'loss': 0.5740, 'learning_rate': 4.8973e-05, 'epoch': 0.27, 'throughput': 975.75}
[INFO|2025-10-21 03:01:48] logging.py:143 >> {'loss': 0.6029, 'learning_rate': 4.8966e-05, 'epoch': 0.28, 'throughput': 975.75}
[INFO|2025-10-21 03:02:53] logging.py:143 >> {'loss': 0.5255, 'learning_rate': 4.8958e-05, 'epoch': 0.28, 'throughput': 975.76}
[INFO|2025-10-21 03:03:57] logging.py:143 >> {'loss': 0.5793, 'learning_rate': 4.8950e-05, 'epoch': 0.28, 'throughput': 975.76}
[INFO|2025-10-21 03:05:02] logging.py:143 >> {'loss': 0.6017, 'learning_rate': 4.8943e-05, 'epoch': 0.28, 'throughput': 975.77}
[INFO|2025-10-21 03:06:06] logging.py:143 >> {'loss': 0.4902, 'learning_rate': 4.8935e-05, 'epoch': 0.28, 'throughput': 975.77}
[INFO|2025-10-21 03:07:11] logging.py:143 >> {'loss': 0.5034, 'learning_rate': 4.8928e-05, 'epoch': 0.28, 'throughput': 975.77}
[INFO|2025-10-21 03:08:16] logging.py:143 >> {'loss': 0.4721, 'learning_rate': 4.8920e-05, 'epoch': 0.28, 'throughput': 975.79}
[INFO|2025-10-21 03:09:21] logging.py:143 >> {'loss': 0.4965, 'learning_rate': 4.8912e-05, 'epoch': 0.28, 'throughput': 975.81}
[INFO|2025-10-21 03:09:21] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1400
[INFO|2025-10-21 03:09:21] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 03:09:21] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 03:09:21] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1400/chat_template.jinja
[INFO|2025-10-21 03:09:21] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1400/tokenizer_config.json
[INFO|2025-10-21 03:09:21] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1400/special_tokens_map.json
[INFO|2025-10-21 03:10:26] logging.py:143 >> {'loss': 0.5293, 'learning_rate': 4.8904e-05, 'epoch': 0.28, 'throughput': 975.77}
[INFO|2025-10-21 03:11:31] logging.py:143 >> {'loss': 0.6887, 'learning_rate': 4.8897e-05, 'epoch': 0.28, 'throughput': 975.76}
[INFO|2025-10-21 03:12:35] logging.py:143 >> {'loss': 0.4625, 'learning_rate': 4.8889e-05, 'epoch': 0.29, 'throughput': 975.75}
[INFO|2025-10-21 03:13:39] logging.py:143 >> {'loss': 0.5949, 'learning_rate': 4.8881e-05, 'epoch': 0.29, 'throughput': 975.74}
[INFO|2025-10-21 03:14:43] logging.py:143 >> {'loss': 0.5493, 'learning_rate': 4.8873e-05, 'epoch': 0.29, 'throughput': 975.75}
[INFO|2025-10-21 03:15:48] logging.py:143 >> {'loss': 0.5258, 'learning_rate': 4.8865e-05, 'epoch': 0.29, 'throughput': 975.77}
[INFO|2025-10-21 03:16:54] logging.py:143 >> {'loss': 0.5650, 'learning_rate': 4.8857e-05, 'epoch': 0.29, 'throughput': 975.77}
[INFO|2025-10-21 03:17:58] logging.py:143 >> {'loss': 0.5128, 'learning_rate': 4.8850e-05, 'epoch': 0.29, 'throughput': 975.78}
[INFO|2025-10-21 03:19:02] logging.py:143 >> {'loss': 0.3803, 'learning_rate': 4.8842e-05, 'epoch': 0.29, 'throughput': 975.77}
[INFO|2025-10-21 03:20:06] logging.py:143 >> {'loss': 0.6036, 'learning_rate': 4.8834e-05, 'epoch': 0.29, 'throughput': 975.77}
[INFO|2025-10-21 03:21:10] logging.py:143 >> {'loss': 0.6562, 'learning_rate': 4.8826e-05, 'epoch': 0.29, 'throughput': 975.77}
[INFO|2025-10-21 03:22:14] logging.py:143 >> {'loss': 0.5224, 'learning_rate': 4.8818e-05, 'epoch': 0.30, 'throughput': 975.75}
[INFO|2025-10-21 03:23:18] logging.py:143 >> {'loss': 0.5673, 'learning_rate': 4.8810e-05, 'epoch': 0.30, 'throughput': 975.77}
[INFO|2025-10-21 03:24:23] logging.py:143 >> {'loss': 0.5455, 'learning_rate': 4.8801e-05, 'epoch': 0.30, 'throughput': 975.79}
[INFO|2025-10-21 03:25:28] logging.py:143 >> {'loss': 0.5459, 'learning_rate': 4.8793e-05, 'epoch': 0.30, 'throughput': 975.80}
[INFO|2025-10-21 03:26:32] logging.py:143 >> {'loss': 0.5789, 'learning_rate': 4.8785e-05, 'epoch': 0.30, 'throughput': 975.81}
[INFO|2025-10-21 03:27:36] logging.py:143 >> {'loss': 0.4625, 'learning_rate': 4.8777e-05, 'epoch': 0.30, 'throughput': 975.79}
[INFO|2025-10-21 03:28:40] logging.py:143 >> {'loss': 0.5315, 'learning_rate': 4.8769e-05, 'epoch': 0.30, 'throughput': 975.80}
[INFO|2025-10-21 03:29:44] logging.py:143 >> {'loss': 0.6840, 'learning_rate': 4.8761e-05, 'epoch': 0.30, 'throughput': 975.80}
[INFO|2025-10-21 03:30:49] logging.py:143 >> {'loss': 0.4631, 'learning_rate': 4.8752e-05, 'epoch': 0.30, 'throughput': 975.81}
[INFO|2025-10-21 03:30:49] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1500
[INFO|2025-10-21 03:30:49] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 03:30:49] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 03:30:49] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1500/chat_template.jinja
[INFO|2025-10-21 03:30:49] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1500/tokenizer_config.json
[INFO|2025-10-21 03:30:49] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1500/special_tokens_map.json
[INFO|2025-10-21 03:31:55] logging.py:143 >> {'loss': 0.5980, 'learning_rate': 4.8744e-05, 'epoch': 0.30, 'throughput': 975.76}
[INFO|2025-10-21 03:32:59] logging.py:143 >> {'loss': 0.5534, 'learning_rate': 4.8736e-05, 'epoch': 0.31, 'throughput': 975.76}
[INFO|2025-10-21 03:34:02] logging.py:143 >> {'loss': 0.6042, 'learning_rate': 4.8728e-05, 'epoch': 0.31, 'throughput': 975.76}
[INFO|2025-10-21 03:35:07] logging.py:143 >> {'loss': 0.5785, 'learning_rate': 4.8719e-05, 'epoch': 0.31, 'throughput': 975.77}
[INFO|2025-10-21 03:36:12] logging.py:143 >> {'loss': 0.3639, 'learning_rate': 4.8711e-05, 'epoch': 0.31, 'throughput': 975.78}
[INFO|2025-10-21 03:37:16] logging.py:143 >> {'loss': 0.7146, 'learning_rate': 4.8702e-05, 'epoch': 0.31, 'throughput': 975.76}
[INFO|2025-10-21 03:38:21] logging.py:143 >> {'loss': 0.5227, 'learning_rate': 4.8694e-05, 'epoch': 0.31, 'throughput': 975.76}
[INFO|2025-10-21 03:39:24] logging.py:143 >> {'loss': 0.5602, 'learning_rate': 4.8686e-05, 'epoch': 0.31, 'throughput': 975.76}
[INFO|2025-10-21 03:40:29] logging.py:143 >> {'loss': 0.4221, 'learning_rate': 4.8677e-05, 'epoch': 0.31, 'throughput': 975.75}
[INFO|2025-10-21 03:41:33] logging.py:143 >> {'loss': 0.4242, 'learning_rate': 4.8669e-05, 'epoch': 0.31, 'throughput': 975.76}
[INFO|2025-10-21 03:42:38] logging.py:143 >> {'loss': 0.5926, 'learning_rate': 4.8660e-05, 'epoch': 0.31, 'throughput': 975.75}
[INFO|2025-10-21 03:43:43] logging.py:143 >> {'loss': 0.5221, 'learning_rate': 4.8651e-05, 'epoch': 0.32, 'throughput': 975.77}
[INFO|2025-10-21 03:44:47] logging.py:143 >> {'loss': 0.6174, 'learning_rate': 4.8643e-05, 'epoch': 0.32, 'throughput': 975.77}
[INFO|2025-10-21 03:45:51] logging.py:143 >> {'loss': 0.4708, 'learning_rate': 4.8634e-05, 'epoch': 0.32, 'throughput': 975.77}
[INFO|2025-10-21 03:46:55] logging.py:143 >> {'loss': 0.7397, 'learning_rate': 4.8626e-05, 'epoch': 0.32, 'throughput': 975.76}
[INFO|2025-10-21 03:48:00] logging.py:143 >> {'loss': 0.6927, 'learning_rate': 4.8617e-05, 'epoch': 0.32, 'throughput': 975.75}
[INFO|2025-10-21 03:49:04] logging.py:143 >> {'loss': 0.6162, 'learning_rate': 4.8608e-05, 'epoch': 0.32, 'throughput': 975.75}
[INFO|2025-10-21 03:50:07] logging.py:143 >> {'loss': 0.8213, 'learning_rate': 4.8600e-05, 'epoch': 0.32, 'throughput': 975.75}
[INFO|2025-10-21 03:51:12] logging.py:143 >> {'loss': 0.6739, 'learning_rate': 4.8591e-05, 'epoch': 0.32, 'throughput': 975.76}
[INFO|2025-10-21 03:52:16] logging.py:143 >> {'loss': 0.5721, 'learning_rate': 4.8582e-05, 'epoch': 0.32, 'throughput': 975.76}
[INFO|2025-10-21 03:52:16] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1600
[INFO|2025-10-21 03:52:16] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 03:52:16] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 03:52:16] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1600/chat_template.jinja
[INFO|2025-10-21 03:52:16] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1600/tokenizer_config.json
[INFO|2025-10-21 03:52:16] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1600/special_tokens_map.json
[INFO|2025-10-21 03:53:21] logging.py:143 >> {'loss': 0.5834, 'learning_rate': 4.8573e-05, 'epoch': 0.32, 'throughput': 975.73}
[INFO|2025-10-21 03:54:26] logging.py:143 >> {'loss': 0.4889, 'learning_rate': 4.8564e-05, 'epoch': 0.33, 'throughput': 975.73}
[INFO|2025-10-21 03:55:30] logging.py:143 >> {'loss': 0.6433, 'learning_rate': 4.8556e-05, 'epoch': 0.33, 'throughput': 975.73}
[INFO|2025-10-21 03:56:33] logging.py:143 >> {'loss': 0.5764, 'learning_rate': 4.8547e-05, 'epoch': 0.33, 'throughput': 975.74}
[INFO|2025-10-21 03:57:38] logging.py:143 >> {'loss': 0.6391, 'learning_rate': 4.8538e-05, 'epoch': 0.33, 'throughput': 975.73}
[INFO|2025-10-21 03:58:42] logging.py:143 >> {'loss': 0.5746, 'learning_rate': 4.8529e-05, 'epoch': 0.33, 'throughput': 975.73}
[INFO|2025-10-21 03:59:46] logging.py:143 >> {'loss': 0.8260, 'learning_rate': 4.8520e-05, 'epoch': 0.33, 'throughput': 975.73}
[INFO|2025-10-21 04:00:50] logging.py:143 >> {'loss': 0.5185, 'learning_rate': 4.8511e-05, 'epoch': 0.33, 'throughput': 975.72}
[INFO|2025-10-21 04:01:54] logging.py:143 >> {'loss': 0.6423, 'learning_rate': 4.8502e-05, 'epoch': 0.33, 'throughput': 975.72}
[INFO|2025-10-21 04:03:00] logging.py:143 >> {'loss': 0.6119, 'learning_rate': 4.8493e-05, 'epoch': 0.33, 'throughput': 975.72}
[INFO|2025-10-21 04:04:04] logging.py:143 >> {'loss': 0.5492, 'learning_rate': 4.8484e-05, 'epoch': 0.33, 'throughput': 975.73}
[INFO|2025-10-21 04:05:08] logging.py:143 >> {'loss': 0.6220, 'learning_rate': 4.8475e-05, 'epoch': 0.34, 'throughput': 975.72}
[INFO|2025-10-21 04:06:12] logging.py:143 >> {'loss': 0.5081, 'learning_rate': 4.8466e-05, 'epoch': 0.34, 'throughput': 975.72}
[INFO|2025-10-21 04:07:16] logging.py:143 >> {'loss': 0.4786, 'learning_rate': 4.8457e-05, 'epoch': 0.34, 'throughput': 975.70}
[INFO|2025-10-21 04:08:21] logging.py:143 >> {'loss': 0.5061, 'learning_rate': 4.8447e-05, 'epoch': 0.34, 'throughput': 975.72}
[INFO|2025-10-21 04:09:26] logging.py:143 >> {'loss': 0.5069, 'learning_rate': 4.8438e-05, 'epoch': 0.34, 'throughput': 975.73}
[INFO|2025-10-21 04:10:30] logging.py:143 >> {'loss': 0.5286, 'learning_rate': 4.8429e-05, 'epoch': 0.34, 'throughput': 975.73}
[INFO|2025-10-21 04:11:34] logging.py:143 >> {'loss': 0.5434, 'learning_rate': 4.8420e-05, 'epoch': 0.34, 'throughput': 975.73}
[INFO|2025-10-21 04:12:38] logging.py:143 >> {'loss': 0.4168, 'learning_rate': 4.8410e-05, 'epoch': 0.34, 'throughput': 975.72}
[INFO|2025-10-21 04:13:43] logging.py:143 >> {'loss': 0.5813, 'learning_rate': 4.8401e-05, 'epoch': 0.34, 'throughput': 975.73}
[INFO|2025-10-21 04:13:43] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1700
[INFO|2025-10-21 04:13:43] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 04:13:43] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 04:13:43] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1700/chat_template.jinja
[INFO|2025-10-21 04:13:43] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1700/tokenizer_config.json
[INFO|2025-10-21 04:13:43] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1700/special_tokens_map.json
[INFO|2025-10-21 04:14:49] logging.py:143 >> {'loss': 0.7125, 'learning_rate': 4.8392e-05, 'epoch': 0.34, 'throughput': 975.71}
[INFO|2025-10-21 04:15:53] logging.py:143 >> {'loss': 0.4778, 'learning_rate': 4.8382e-05, 'epoch': 0.35, 'throughput': 975.71}
[INFO|2025-10-21 04:16:57] logging.py:143 >> {'loss': 0.7134, 'learning_rate': 4.8373e-05, 'epoch': 0.35, 'throughput': 975.70}
[INFO|2025-10-21 04:18:01] logging.py:143 >> {'loss': 0.4513, 'learning_rate': 4.8364e-05, 'epoch': 0.35, 'throughput': 975.70}
[INFO|2025-10-21 04:19:06] logging.py:143 >> {'loss': 0.5265, 'learning_rate': 4.8354e-05, 'epoch': 0.35, 'throughput': 975.72}
[INFO|2025-10-21 04:20:11] logging.py:143 >> {'loss': 0.5227, 'learning_rate': 4.8345e-05, 'epoch': 0.35, 'throughput': 975.72}
[INFO|2025-10-21 04:21:14] logging.py:143 >> {'loss': 0.4968, 'learning_rate': 4.8335e-05, 'epoch': 0.35, 'throughput': 975.70}
[INFO|2025-10-21 04:22:18] logging.py:143 >> {'loss': 0.4370, 'learning_rate': 4.8326e-05, 'epoch': 0.35, 'throughput': 975.70}
[INFO|2025-10-21 04:23:23] logging.py:143 >> {'loss': 0.5511, 'learning_rate': 4.8316e-05, 'epoch': 0.35, 'throughput': 975.70}
[INFO|2025-10-21 04:24:27] logging.py:143 >> {'loss': 0.5074, 'learning_rate': 4.8307e-05, 'epoch': 0.35, 'throughput': 975.70}
[INFO|2025-10-21 04:25:31] logging.py:143 >> {'loss': 0.5081, 'learning_rate': 4.8297e-05, 'epoch': 0.35, 'throughput': 975.70}
[INFO|2025-10-21 04:26:36] logging.py:143 >> {'loss': 0.4241, 'learning_rate': 4.8288e-05, 'epoch': 0.36, 'throughput': 975.70}
[INFO|2025-10-21 04:27:39] logging.py:143 >> {'loss': 0.5616, 'learning_rate': 4.8278e-05, 'epoch': 0.36, 'throughput': 975.69}
[INFO|2025-10-21 04:28:43] logging.py:143 >> {'loss': 0.5255, 'learning_rate': 4.8268e-05, 'epoch': 0.36, 'throughput': 975.69}
[INFO|2025-10-21 04:29:48] logging.py:143 >> {'loss': 0.4860, 'learning_rate': 4.8259e-05, 'epoch': 0.36, 'throughput': 975.70}
[INFO|2025-10-21 04:30:52] logging.py:143 >> {'loss': 0.5737, 'learning_rate': 4.8249e-05, 'epoch': 0.36, 'throughput': 975.70}
[INFO|2025-10-21 04:31:57] logging.py:143 >> {'loss': 0.5011, 'learning_rate': 4.8239e-05, 'epoch': 0.36, 'throughput': 975.69}
[INFO|2025-10-21 04:33:02] logging.py:143 >> {'loss': 0.4183, 'learning_rate': 4.8229e-05, 'epoch': 0.36, 'throughput': 975.71}
[INFO|2025-10-21 04:34:06] logging.py:143 >> {'loss': 0.4896, 'learning_rate': 4.8220e-05, 'epoch': 0.36, 'throughput': 975.72}
[INFO|2025-10-21 04:35:10] logging.py:143 >> {'loss': 0.4581, 'learning_rate': 4.8210e-05, 'epoch': 0.36, 'throughput': 975.72}
[INFO|2025-10-21 04:35:10] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1800
[INFO|2025-10-21 04:35:11] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 04:35:11] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 04:35:11] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1800/chat_template.jinja
[INFO|2025-10-21 04:35:11] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1800/tokenizer_config.json
[INFO|2025-10-21 04:35:11] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1800/special_tokens_map.json
[INFO|2025-10-21 04:36:16] logging.py:143 >> {'loss': 0.5575, 'learning_rate': 4.8200e-05, 'epoch': 0.36, 'throughput': 975.68}
[INFO|2025-10-21 04:37:20] logging.py:143 >> {'loss': 0.4409, 'learning_rate': 4.8190e-05, 'epoch': 0.37, 'throughput': 975.68}
[INFO|2025-10-21 04:38:25] logging.py:143 >> {'loss': 0.5409, 'learning_rate': 4.8180e-05, 'epoch': 0.37, 'throughput': 975.69}
[INFO|2025-10-21 04:39:29] logging.py:143 >> {'loss': 0.4438, 'learning_rate': 4.8170e-05, 'epoch': 0.37, 'throughput': 975.69}
[INFO|2025-10-21 04:40:34] logging.py:143 >> {'loss': 0.6467, 'learning_rate': 4.8160e-05, 'epoch': 0.37, 'throughput': 975.70}
[INFO|2025-10-21 04:41:38] logging.py:143 >> {'loss': 0.5589, 'learning_rate': 4.8150e-05, 'epoch': 0.37, 'throughput': 975.69}
[INFO|2025-10-21 04:42:42] logging.py:143 >> {'loss': 0.5667, 'learning_rate': 4.8140e-05, 'epoch': 0.37, 'throughput': 975.70}
[INFO|2025-10-21 04:43:47] logging.py:143 >> {'loss': 0.5066, 'learning_rate': 4.8130e-05, 'epoch': 0.37, 'throughput': 975.70}
[INFO|2025-10-21 04:44:51] logging.py:143 >> {'loss': 0.3901, 'learning_rate': 4.8120e-05, 'epoch': 0.37, 'throughput': 975.69}
[INFO|2025-10-21 04:45:56] logging.py:143 >> {'loss': 0.5027, 'learning_rate': 4.8110e-05, 'epoch': 0.37, 'throughput': 975.68}
[INFO|2025-10-21 04:46:59] logging.py:143 >> {'loss': 0.5955, 'learning_rate': 4.8100e-05, 'epoch': 0.37, 'throughput': 975.67}
[INFO|2025-10-21 04:48:03] logging.py:143 >> {'loss': 0.4503, 'learning_rate': 4.8090e-05, 'epoch': 0.38, 'throughput': 975.65}
[INFO|2025-10-21 04:49:07] logging.py:143 >> {'loss': 0.4637, 'learning_rate': 4.8080e-05, 'epoch': 0.38, 'throughput': 975.65}
[INFO|2025-10-21 04:50:11] logging.py:143 >> {'loss': 0.5740, 'learning_rate': 4.8070e-05, 'epoch': 0.38, 'throughput': 975.65}
[INFO|2025-10-21 04:51:17] logging.py:143 >> {'loss': 0.5196, 'learning_rate': 4.8059e-05, 'epoch': 0.38, 'throughput': 975.65}
[INFO|2025-10-21 04:52:20] logging.py:143 >> {'loss': 0.4862, 'learning_rate': 4.8049e-05, 'epoch': 0.38, 'throughput': 975.65}
[INFO|2025-10-21 04:53:24] logging.py:143 >> {'loss': 0.4978, 'learning_rate': 4.8039e-05, 'epoch': 0.38, 'throughput': 975.63}
[INFO|2025-10-21 04:54:27] logging.py:143 >> {'loss': 0.7475, 'learning_rate': 4.8029e-05, 'epoch': 0.38, 'throughput': 975.63}
[INFO|2025-10-21 04:55:32] logging.py:143 >> {'loss': 0.5009, 'learning_rate': 4.8018e-05, 'epoch': 0.38, 'throughput': 975.63}
[INFO|2025-10-21 04:56:35] logging.py:143 >> {'loss': 0.6105, 'learning_rate': 4.8008e-05, 'epoch': 0.38, 'throughput': 975.61}
[INFO|2025-10-21 04:56:35] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1900
[INFO|2025-10-21 04:56:35] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 04:56:35] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 04:56:35] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1900/chat_template.jinja
[INFO|2025-10-21 04:56:35] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1900/tokenizer_config.json
[INFO|2025-10-21 04:56:35] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-1900/special_tokens_map.json
[INFO|2025-10-21 04:57:41] logging.py:143 >> {'loss': 0.4993, 'learning_rate': 4.7998e-05, 'epoch': 0.39, 'throughput': 975.60}
[INFO|2025-10-21 04:58:46] logging.py:143 >> {'loss': 0.4953, 'learning_rate': 4.7987e-05, 'epoch': 0.39, 'throughput': 975.60}
[INFO|2025-10-21 04:59:50] logging.py:143 >> {'loss': 0.5682, 'learning_rate': 4.7977e-05, 'epoch': 0.39, 'throughput': 975.60}
[INFO|2025-10-21 05:00:54] logging.py:143 >> {'loss': 0.4616, 'learning_rate': 4.7966e-05, 'epoch': 0.39, 'throughput': 975.58}
[INFO|2025-10-21 05:01:57] logging.py:143 >> {'loss': 0.5134, 'learning_rate': 4.7956e-05, 'epoch': 0.39, 'throughput': 975.57}
[INFO|2025-10-21 05:03:03] logging.py:143 >> {'loss': 0.4406, 'learning_rate': 4.7945e-05, 'epoch': 0.39, 'throughput': 975.57}
[INFO|2025-10-21 05:04:08] logging.py:143 >> {'loss': 0.6329, 'learning_rate': 4.7935e-05, 'epoch': 0.39, 'throughput': 975.57}
[INFO|2025-10-21 05:05:12] logging.py:143 >> {'loss': 0.3978, 'learning_rate': 4.7924e-05, 'epoch': 0.39, 'throughput': 975.56}
[INFO|2025-10-21 05:06:17] logging.py:143 >> {'loss': 0.4754, 'learning_rate': 4.7914e-05, 'epoch': 0.39, 'throughput': 975.55}
[INFO|2025-10-21 05:07:21] logging.py:143 >> {'loss': 0.4910, 'learning_rate': 4.7903e-05, 'epoch': 0.39, 'throughput': 975.55}
[INFO|2025-10-21 05:08:25] logging.py:143 >> {'loss': 0.5409, 'learning_rate': 4.7893e-05, 'epoch': 0.40, 'throughput': 975.55}
[INFO|2025-10-21 05:09:29] logging.py:143 >> {'loss': 0.5543, 'learning_rate': 4.7882e-05, 'epoch': 0.40, 'throughput': 975.55}
[INFO|2025-10-21 05:10:34] logging.py:143 >> {'loss': 0.4477, 'learning_rate': 4.7871e-05, 'epoch': 0.40, 'throughput': 975.56}
[INFO|2025-10-21 05:11:38] logging.py:143 >> {'loss': 0.5160, 'learning_rate': 4.7861e-05, 'epoch': 0.40, 'throughput': 975.55}
[INFO|2025-10-21 05:12:42] logging.py:143 >> {'loss': 0.4801, 'learning_rate': 4.7850e-05, 'epoch': 0.40, 'throughput': 975.55}
[INFO|2025-10-21 05:13:46] logging.py:143 >> {'loss': 0.5473, 'learning_rate': 4.7839e-05, 'epoch': 0.40, 'throughput': 975.55}
[INFO|2025-10-21 05:14:50] logging.py:143 >> {'loss': 0.5177, 'learning_rate': 4.7828e-05, 'epoch': 0.40, 'throughput': 975.53}
[INFO|2025-10-21 05:15:54] logging.py:143 >> {'loss': 0.4074, 'learning_rate': 4.7817e-05, 'epoch': 0.40, 'throughput': 975.53}
[INFO|2025-10-21 05:16:59] logging.py:143 >> {'loss': 0.6098, 'learning_rate': 4.7807e-05, 'epoch': 0.40, 'throughput': 975.53}
[INFO|2025-10-21 05:18:03] logging.py:143 >> {'loss': 0.4273, 'learning_rate': 4.7796e-05, 'epoch': 0.40, 'throughput': 975.53}
[INFO|2025-10-21 05:18:03] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2000
[INFO|2025-10-21 05:18:03] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 05:18:03] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 05:18:03] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2000/chat_template.jinja
[INFO|2025-10-21 05:18:03] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2000/tokenizer_config.json
[INFO|2025-10-21 05:18:03] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2000/special_tokens_map.json
[INFO|2025-10-21 05:19:08] logging.py:143 >> {'loss': 0.4951, 'learning_rate': 4.7785e-05, 'epoch': 0.41, 'throughput': 975.49}
[INFO|2025-10-21 05:20:12] logging.py:143 >> {'loss': 0.4534, 'learning_rate': 4.7774e-05, 'epoch': 0.41, 'throughput': 975.48}
[INFO|2025-10-21 05:21:16] logging.py:143 >> {'loss': 0.4790, 'learning_rate': 4.7763e-05, 'epoch': 0.41, 'throughput': 975.48}
[INFO|2025-10-21 05:22:20] logging.py:143 >> {'loss': 0.5550, 'learning_rate': 4.7752e-05, 'epoch': 0.41, 'throughput': 975.48}
[INFO|2025-10-21 05:23:25] logging.py:143 >> {'loss': 0.4853, 'learning_rate': 4.7741e-05, 'epoch': 0.41, 'throughput': 975.49}
[INFO|2025-10-21 05:24:29] logging.py:143 >> {'loss': 0.3631, 'learning_rate': 4.7730e-05, 'epoch': 0.41, 'throughput': 975.49}
[INFO|2025-10-21 05:25:34] logging.py:143 >> {'loss': 0.6286, 'learning_rate': 4.7719e-05, 'epoch': 0.41, 'throughput': 975.49}
[INFO|2025-10-21 05:26:39] logging.py:143 >> {'loss': 0.4919, 'learning_rate': 4.7708e-05, 'epoch': 0.41, 'throughput': 975.50}
[INFO|2025-10-21 05:27:43] logging.py:143 >> {'loss': 0.4936, 'learning_rate': 4.7697e-05, 'epoch': 0.41, 'throughput': 975.49}
[INFO|2025-10-21 05:28:47] logging.py:143 >> {'loss': 0.5301, 'learning_rate': 4.7686e-05, 'epoch': 0.41, 'throughput': 975.50}
[INFO|2025-10-21 05:29:53] logging.py:143 >> {'loss': 0.5323, 'learning_rate': 4.7675e-05, 'epoch': 0.42, 'throughput': 975.49}
[INFO|2025-10-21 05:30:56] logging.py:143 >> {'loss': 0.4304, 'learning_rate': 4.7664e-05, 'epoch': 0.42, 'throughput': 975.49}
[INFO|2025-10-21 05:32:00] logging.py:143 >> {'loss': 0.4391, 'learning_rate': 4.7652e-05, 'epoch': 0.42, 'throughput': 975.50}
[INFO|2025-10-21 05:33:05] logging.py:143 >> {'loss': 0.5253, 'learning_rate': 4.7641e-05, 'epoch': 0.42, 'throughput': 975.50}
[INFO|2025-10-21 05:34:08] logging.py:143 >> {'loss': 0.5516, 'learning_rate': 4.7630e-05, 'epoch': 0.42, 'throughput': 975.50}
[INFO|2025-10-21 05:35:13] logging.py:143 >> {'loss': 0.6885, 'learning_rate': 4.7619e-05, 'epoch': 0.42, 'throughput': 975.50}
[INFO|2025-10-21 05:36:18] logging.py:143 >> {'loss': 0.5867, 'learning_rate': 4.7608e-05, 'epoch': 0.42, 'throughput': 975.51}
[INFO|2025-10-21 05:37:22] logging.py:143 >> {'loss': 0.4956, 'learning_rate': 4.7596e-05, 'epoch': 0.42, 'throughput': 975.51}
[INFO|2025-10-21 05:38:27] logging.py:143 >> {'loss': 0.4537, 'learning_rate': 4.7585e-05, 'epoch': 0.42, 'throughput': 975.52}
[INFO|2025-10-21 05:39:31] logging.py:143 >> {'loss': 0.5164, 'learning_rate': 4.7574e-05, 'epoch': 0.42, 'throughput': 975.52}
[INFO|2025-10-21 05:39:31] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2100
[INFO|2025-10-21 05:39:31] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 05:39:31] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 05:39:31] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2100/chat_template.jinja
[INFO|2025-10-21 05:39:31] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2100/tokenizer_config.json
[INFO|2025-10-21 05:39:31] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2100/special_tokens_map.json
[INFO|2025-10-21 05:40:36] logging.py:143 >> {'loss': 0.5086, 'learning_rate': 4.7562e-05, 'epoch': 0.43, 'throughput': 975.49}
[INFO|2025-10-21 05:41:40] logging.py:143 >> {'loss': 0.5500, 'learning_rate': 4.7551e-05, 'epoch': 0.43, 'throughput': 975.48}
[INFO|2025-10-21 05:42:45] logging.py:143 >> {'loss': 0.5103, 'learning_rate': 4.7539e-05, 'epoch': 0.43, 'throughput': 975.49}
[INFO|2025-10-21 05:43:49] logging.py:143 >> {'loss': 0.5923, 'learning_rate': 4.7528e-05, 'epoch': 0.43, 'throughput': 975.49}
[INFO|2025-10-21 05:44:55] logging.py:143 >> {'loss': 0.4133, 'learning_rate': 4.7516e-05, 'epoch': 0.43, 'throughput': 975.49}
[INFO|2025-10-21 05:45:59] logging.py:143 >> {'loss': 0.5133, 'learning_rate': 4.7505e-05, 'epoch': 0.43, 'throughput': 975.50}
[INFO|2025-10-21 05:47:03] logging.py:143 >> {'loss': 0.5442, 'learning_rate': 4.7493e-05, 'epoch': 0.43, 'throughput': 975.50}
[INFO|2025-10-21 05:48:07] logging.py:143 >> {'loss': 0.5210, 'learning_rate': 4.7482e-05, 'epoch': 0.43, 'throughput': 975.49}
[INFO|2025-10-21 05:49:11] logging.py:143 >> {'loss': 0.4092, 'learning_rate': 4.7470e-05, 'epoch': 0.43, 'throughput': 975.50}
[INFO|2025-10-21 05:50:17] logging.py:143 >> {'loss': 0.7534, 'learning_rate': 4.7459e-05, 'epoch': 0.43, 'throughput': 975.51}
[INFO|2025-10-21 05:51:21] logging.py:143 >> {'loss': 0.5005, 'learning_rate': 4.7447e-05, 'epoch': 0.44, 'throughput': 975.51}
[INFO|2025-10-21 05:52:26] logging.py:143 >> {'loss': 0.6064, 'learning_rate': 4.7435e-05, 'epoch': 0.44, 'throughput': 975.52}
[INFO|2025-10-21 05:53:31] logging.py:143 >> {'loss': 0.4338, 'learning_rate': 4.7424e-05, 'epoch': 0.44, 'throughput': 975.53}
[INFO|2025-10-21 05:54:34] logging.py:143 >> {'loss': 0.5409, 'learning_rate': 4.7412e-05, 'epoch': 0.44, 'throughput': 975.52}
[INFO|2025-10-21 05:55:39] logging.py:143 >> {'loss': 0.4957, 'learning_rate': 4.7400e-05, 'epoch': 0.44, 'throughput': 975.53}
[INFO|2025-10-21 05:56:43] logging.py:143 >> {'loss': 0.5990, 'learning_rate': 4.7388e-05, 'epoch': 0.44, 'throughput': 975.53}
[INFO|2025-10-21 05:57:46] logging.py:143 >> {'loss': 0.5450, 'learning_rate': 4.7377e-05, 'epoch': 0.44, 'throughput': 975.53}
[INFO|2025-10-21 05:58:51] logging.py:143 >> {'loss': 0.5160, 'learning_rate': 4.7365e-05, 'epoch': 0.44, 'throughput': 975.53}
[INFO|2025-10-21 05:59:55] logging.py:143 >> {'loss': 0.5472, 'learning_rate': 4.7353e-05, 'epoch': 0.44, 'throughput': 975.53}
[INFO|2025-10-21 06:00:59] logging.py:143 >> {'loss': 0.5100, 'learning_rate': 4.7341e-05, 'epoch': 0.44, 'throughput': 975.54}
[INFO|2025-10-21 06:00:59] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2200
[INFO|2025-10-21 06:00:59] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 06:00:59] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 06:00:59] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2200/chat_template.jinja
[INFO|2025-10-21 06:00:59] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2200/tokenizer_config.json
[INFO|2025-10-21 06:00:59] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2200/special_tokens_map.json
[INFO|2025-10-21 06:02:04] logging.py:143 >> {'loss': 0.4835, 'learning_rate': 4.7329e-05, 'epoch': 0.45, 'throughput': 975.51}
[INFO|2025-10-21 06:03:10] logging.py:143 >> {'loss': 0.4915, 'learning_rate': 4.7317e-05, 'epoch': 0.45, 'throughput': 975.52}
[INFO|2025-10-21 06:04:14] logging.py:143 >> {'loss': 0.5154, 'learning_rate': 4.7305e-05, 'epoch': 0.45, 'throughput': 975.53}
[INFO|2025-10-21 06:05:19] logging.py:143 >> {'loss': 0.7146, 'learning_rate': 4.7293e-05, 'epoch': 0.45, 'throughput': 975.52}
[INFO|2025-10-21 06:06:24] logging.py:143 >> {'loss': 0.5732, 'learning_rate': 4.7281e-05, 'epoch': 0.45, 'throughput': 975.51}
[INFO|2025-10-21 06:07:29] logging.py:143 >> {'loss': 0.4246, 'learning_rate': 4.7269e-05, 'epoch': 0.45, 'throughput': 975.52}
[INFO|2025-10-21 06:08:33] logging.py:143 >> {'loss': 0.4603, 'learning_rate': 4.7257e-05, 'epoch': 0.45, 'throughput': 975.53}
[INFO|2025-10-21 06:09:38] logging.py:143 >> {'loss': 0.5149, 'learning_rate': 4.7245e-05, 'epoch': 0.45, 'throughput': 975.53}
[INFO|2025-10-21 06:10:42] logging.py:143 >> {'loss': 0.5244, 'learning_rate': 4.7233e-05, 'epoch': 0.45, 'throughput': 975.52}
[INFO|2025-10-21 06:11:48] logging.py:143 >> {'loss': 0.4403, 'learning_rate': 4.7221e-05, 'epoch': 0.45, 'throughput': 975.53}
[INFO|2025-10-21 06:12:52] logging.py:143 >> {'loss': 0.4890, 'learning_rate': 4.7209e-05, 'epoch': 0.46, 'throughput': 975.54}
[INFO|2025-10-21 06:13:57] logging.py:143 >> {'loss': 0.5446, 'learning_rate': 4.7197e-05, 'epoch': 0.46, 'throughput': 975.54}
[INFO|2025-10-21 06:15:01] logging.py:143 >> {'loss': 0.5499, 'learning_rate': 4.7185e-05, 'epoch': 0.46, 'throughput': 975.54}
[INFO|2025-10-21 06:16:05] logging.py:143 >> {'loss': 0.6148, 'learning_rate': 4.7172e-05, 'epoch': 0.46, 'throughput': 975.54}
[INFO|2025-10-21 06:17:10] logging.py:143 >> {'loss': 0.3501, 'learning_rate': 4.7160e-05, 'epoch': 0.46, 'throughput': 975.55}
[INFO|2025-10-21 06:18:14] logging.py:143 >> {'loss': 0.4605, 'learning_rate': 4.7148e-05, 'epoch': 0.46, 'throughput': 975.55}
[INFO|2025-10-21 06:19:19] logging.py:143 >> {'loss': 0.5604, 'learning_rate': 4.7136e-05, 'epoch': 0.46, 'throughput': 975.55}
[INFO|2025-10-21 06:20:24] logging.py:143 >> {'loss': 0.4614, 'learning_rate': 4.7123e-05, 'epoch': 0.46, 'throughput': 975.56}
[INFO|2025-10-21 06:21:28] logging.py:143 >> {'loss': 0.5438, 'learning_rate': 4.7111e-05, 'epoch': 0.46, 'throughput': 975.55}
[INFO|2025-10-21 06:22:32] logging.py:143 >> {'loss': 0.5635, 'learning_rate': 4.7099e-05, 'epoch': 0.46, 'throughput': 975.54}
[INFO|2025-10-21 06:22:32] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2300
[INFO|2025-10-21 06:22:32] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 06:22:32] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 06:22:32] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2300/chat_template.jinja
[INFO|2025-10-21 06:22:32] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2300/tokenizer_config.json
[INFO|2025-10-21 06:22:32] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2300/special_tokens_map.json
[INFO|2025-10-21 06:23:37] logging.py:143 >> {'loss': 0.5136, 'learning_rate': 4.7086e-05, 'epoch': 0.47, 'throughput': 975.52}
[INFO|2025-10-21 06:24:40] logging.py:143 >> {'loss': 0.4814, 'learning_rate': 4.7074e-05, 'epoch': 0.47, 'throughput': 975.51}
[INFO|2025-10-21 06:25:45] logging.py:143 >> {'loss': 0.4239, 'learning_rate': 4.7061e-05, 'epoch': 0.47, 'throughput': 975.51}
[INFO|2025-10-21 06:26:49] logging.py:143 >> {'loss': 0.4966, 'learning_rate': 4.7049e-05, 'epoch': 0.47, 'throughput': 975.51}
[INFO|2025-10-21 06:27:53] logging.py:143 >> {'loss': 0.4631, 'learning_rate': 4.7036e-05, 'epoch': 0.47, 'throughput': 975.51}
[INFO|2025-10-21 06:28:58] logging.py:143 >> {'loss': 0.4906, 'learning_rate': 4.7024e-05, 'epoch': 0.47, 'throughput': 975.51}
[INFO|2025-10-21 06:30:02] logging.py:143 >> {'loss': 0.5924, 'learning_rate': 4.7011e-05, 'epoch': 0.47, 'throughput': 975.51}
[INFO|2025-10-21 06:31:07] logging.py:143 >> {'loss': 0.5455, 'learning_rate': 4.6999e-05, 'epoch': 0.47, 'throughput': 975.50}
[INFO|2025-10-21 06:32:12] logging.py:143 >> {'loss': 0.6040, 'learning_rate': 4.6986e-05, 'epoch': 0.47, 'throughput': 975.51}
[INFO|2025-10-21 06:33:16] logging.py:143 >> {'loss': 0.5471, 'learning_rate': 4.6974e-05, 'epoch': 0.47, 'throughput': 975.51}
[INFO|2025-10-21 06:34:20] logging.py:143 >> {'loss': 0.5097, 'learning_rate': 4.6961e-05, 'epoch': 0.48, 'throughput': 975.51}
[INFO|2025-10-21 06:35:25] logging.py:143 >> {'loss': 0.6165, 'learning_rate': 4.6948e-05, 'epoch': 0.48, 'throughput': 975.50}
[INFO|2025-10-21 06:36:30] logging.py:143 >> {'loss': 0.5125, 'learning_rate': 4.6936e-05, 'epoch': 0.48, 'throughput': 975.51}
[INFO|2025-10-21 06:37:35] logging.py:143 >> {'loss': 0.5101, 'learning_rate': 4.6923e-05, 'epoch': 0.48, 'throughput': 975.51}
[INFO|2025-10-21 06:38:38] logging.py:143 >> {'loss': 0.4812, 'learning_rate': 4.6910e-05, 'epoch': 0.48, 'throughput': 975.51}
[INFO|2025-10-21 06:39:43] logging.py:143 >> {'loss': 0.5097, 'learning_rate': 4.6898e-05, 'epoch': 0.48, 'throughput': 975.50}
[INFO|2025-10-21 06:40:47] logging.py:143 >> {'loss': 0.5585, 'learning_rate': 4.6885e-05, 'epoch': 0.48, 'throughput': 975.50}
[INFO|2025-10-21 06:41:52] logging.py:143 >> {'loss': 0.4781, 'learning_rate': 4.6872e-05, 'epoch': 0.48, 'throughput': 975.51}
[INFO|2025-10-21 06:42:56] logging.py:143 >> {'loss': 0.7114, 'learning_rate': 4.6859e-05, 'epoch': 0.48, 'throughput': 975.51}
[INFO|2025-10-21 06:44:01] logging.py:143 >> {'loss': 0.5849, 'learning_rate': 4.6846e-05, 'epoch': 0.49, 'throughput': 975.51}
[INFO|2025-10-21 06:44:01] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2400
[INFO|2025-10-21 06:44:01] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 06:44:01] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 06:44:01] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2400/chat_template.jinja
[INFO|2025-10-21 06:44:01] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2400/tokenizer_config.json
[INFO|2025-10-21 06:44:01] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2400/special_tokens_map.json
[INFO|2025-10-21 06:45:06] logging.py:143 >> {'loss': 0.4209, 'learning_rate': 4.6833e-05, 'epoch': 0.49, 'throughput': 975.47}
[INFO|2025-10-21 06:46:11] logging.py:143 >> {'loss': 0.5801, 'learning_rate': 4.6821e-05, 'epoch': 0.49, 'throughput': 975.47}
[INFO|2025-10-21 06:47:14] logging.py:143 >> {'loss': 0.5048, 'learning_rate': 4.6808e-05, 'epoch': 0.49, 'throughput': 975.47}
[INFO|2025-10-21 06:48:18] logging.py:143 >> {'loss': 0.4749, 'learning_rate': 4.6795e-05, 'epoch': 0.49, 'throughput': 975.46}
[INFO|2025-10-21 06:49:23] logging.py:143 >> {'loss': 0.4067, 'learning_rate': 4.6782e-05, 'epoch': 0.49, 'throughput': 975.46}
[INFO|2025-10-21 06:50:26] logging.py:143 >> {'loss': 0.5450, 'learning_rate': 4.6769e-05, 'epoch': 0.49, 'throughput': 975.45}
[INFO|2025-10-21 06:51:31] logging.py:143 >> {'loss': 0.5898, 'learning_rate': 4.6756e-05, 'epoch': 0.49, 'throughput': 975.45}
[INFO|2025-10-21 06:52:36] logging.py:143 >> {'loss': 0.5125, 'learning_rate': 4.6743e-05, 'epoch': 0.49, 'throughput': 975.46}
[INFO|2025-10-21 06:53:40] logging.py:143 >> {'loss': 0.3533, 'learning_rate': 4.6730e-05, 'epoch': 0.49, 'throughput': 975.45}
[INFO|2025-10-21 06:54:45] logging.py:143 >> {'loss': 0.3625, 'learning_rate': 4.6716e-05, 'epoch': 0.50, 'throughput': 975.46}
[INFO|2025-10-21 06:55:49] logging.py:143 >> {'loss': 0.4545, 'learning_rate': 4.6703e-05, 'epoch': 0.50, 'throughput': 975.45}
[INFO|2025-10-21 06:56:54] logging.py:143 >> {'loss': 0.5927, 'learning_rate': 4.6690e-05, 'epoch': 0.50, 'throughput': 975.45}
[INFO|2025-10-21 06:57:58] logging.py:143 >> {'loss': 0.4768, 'learning_rate': 4.6677e-05, 'epoch': 0.50, 'throughput': 975.45}
[INFO|2025-10-21 06:59:03] logging.py:143 >> {'loss': 0.5423, 'learning_rate': 4.6664e-05, 'epoch': 0.50, 'throughput': 975.45}
[INFO|2025-10-21 07:00:09] logging.py:143 >> {'loss': 0.5605, 'learning_rate': 4.6651e-05, 'epoch': 0.50, 'throughput': 975.46}
[INFO|2025-10-21 07:01:14] logging.py:143 >> {'loss': 0.4929, 'learning_rate': 4.6637e-05, 'epoch': 0.50, 'throughput': 975.46}
[INFO|2025-10-21 07:02:18] logging.py:143 >> {'loss': 0.4919, 'learning_rate': 4.6624e-05, 'epoch': 0.50, 'throughput': 975.47}
[INFO|2025-10-21 07:03:23] logging.py:143 >> {'loss': 0.5366, 'learning_rate': 4.6611e-05, 'epoch': 0.50, 'throughput': 975.45}
[INFO|2025-10-21 07:04:28] logging.py:143 >> {'loss': 0.4494, 'learning_rate': 4.6598e-05, 'epoch': 0.50, 'throughput': 975.46}
[INFO|2025-10-21 07:05:33] logging.py:143 >> {'loss': 0.6334, 'learning_rate': 4.6584e-05, 'epoch': 0.51, 'throughput': 975.47}
[INFO|2025-10-21 07:05:33] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2500
[INFO|2025-10-21 07:05:33] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 07:05:33] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 07:05:33] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2500/chat_template.jinja
[INFO|2025-10-21 07:05:33] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2500/tokenizer_config.json
[INFO|2025-10-21 07:05:33] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2500/special_tokens_map.json
[INFO|2025-10-21 07:06:38] logging.py:143 >> {'loss': 0.5018, 'learning_rate': 4.6571e-05, 'epoch': 0.51, 'throughput': 975.45}
[INFO|2025-10-21 07:07:43] logging.py:143 >> {'loss': 0.5086, 'learning_rate': 4.6557e-05, 'epoch': 0.51, 'throughput': 975.45}
[INFO|2025-10-21 07:08:48] logging.py:143 >> {'loss': 0.4650, 'learning_rate': 4.6544e-05, 'epoch': 0.51, 'throughput': 975.46}
[INFO|2025-10-21 07:09:52] logging.py:143 >> {'loss': 0.5698, 'learning_rate': 4.6531e-05, 'epoch': 0.51, 'throughput': 975.46}
[INFO|2025-10-21 07:10:57] logging.py:143 >> {'loss': 0.5736, 'learning_rate': 4.6517e-05, 'epoch': 0.51, 'throughput': 975.46}
[INFO|2025-10-21 07:12:01] logging.py:143 >> {'loss': 0.4447, 'learning_rate': 4.6504e-05, 'epoch': 0.51, 'throughput': 975.46}
[INFO|2025-10-21 07:13:06] logging.py:143 >> {'loss': 0.3838, 'learning_rate': 4.6490e-05, 'epoch': 0.51, 'throughput': 975.46}
[INFO|2025-10-21 07:14:09] logging.py:143 >> {'loss': 0.5738, 'learning_rate': 4.6477e-05, 'epoch': 0.51, 'throughput': 975.45}
[INFO|2025-10-21 07:15:13] logging.py:143 >> {'loss': 0.4432, 'learning_rate': 4.6463e-05, 'epoch': 0.51, 'throughput': 975.44}
[INFO|2025-10-21 07:16:18] logging.py:143 >> {'loss': 0.4359, 'learning_rate': 4.6450e-05, 'epoch': 0.52, 'throughput': 975.45}
[INFO|2025-10-21 07:17:22] logging.py:143 >> {'loss': 0.5300, 'learning_rate': 4.6436e-05, 'epoch': 0.52, 'throughput': 975.46}
[INFO|2025-10-21 07:18:26] logging.py:143 >> {'loss': 0.5070, 'learning_rate': 4.6422e-05, 'epoch': 0.52, 'throughput': 975.45}
[INFO|2025-10-21 07:19:29] logging.py:143 >> {'loss': 0.7150, 'learning_rate': 4.6409e-05, 'epoch': 0.52, 'throughput': 975.45}
[INFO|2025-10-21 07:20:33] logging.py:143 >> {'loss': 0.5495, 'learning_rate': 4.6395e-05, 'epoch': 0.52, 'throughput': 975.45}
[INFO|2025-10-21 07:21:37] logging.py:143 >> {'loss': 0.5454, 'learning_rate': 4.6381e-05, 'epoch': 0.52, 'throughput': 975.45}
[INFO|2025-10-21 07:22:40] logging.py:143 >> {'loss': 0.6341, 'learning_rate': 4.6368e-05, 'epoch': 0.52, 'throughput': 975.45}
[INFO|2025-10-21 07:23:45] logging.py:143 >> {'loss': 0.6041, 'learning_rate': 4.6354e-05, 'epoch': 0.52, 'throughput': 975.44}
[INFO|2025-10-21 07:24:49] logging.py:143 >> {'loss': 0.4931, 'learning_rate': 4.6340e-05, 'epoch': 0.52, 'throughput': 975.44}
[INFO|2025-10-21 07:25:54] logging.py:143 >> {'loss': 0.7110, 'learning_rate': 4.6326e-05, 'epoch': 0.52, 'throughput': 975.44}
[INFO|2025-10-21 07:26:58] logging.py:143 >> {'loss': 0.5865, 'learning_rate': 4.6312e-05, 'epoch': 0.53, 'throughput': 975.44}
[INFO|2025-10-21 07:26:58] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2600
[INFO|2025-10-21 07:26:58] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 07:26:58] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 07:26:58] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2600/chat_template.jinja
[INFO|2025-10-21 07:26:58] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2600/tokenizer_config.json
[INFO|2025-10-21 07:26:58] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2600/special_tokens_map.json
[INFO|2025-10-21 07:28:03] logging.py:143 >> {'loss': 0.4531, 'learning_rate': 4.6299e-05, 'epoch': 0.53, 'throughput': 975.41}
[INFO|2025-10-21 07:29:07] logging.py:143 >> {'loss': 0.5600, 'learning_rate': 4.6285e-05, 'epoch': 0.53, 'throughput': 975.41}
[INFO|2025-10-21 07:30:11] logging.py:143 >> {'loss': 0.4666, 'learning_rate': 4.6271e-05, 'epoch': 0.53, 'throughput': 975.41}
[INFO|2025-10-21 07:31:16] logging.py:143 >> {'loss': 0.6233, 'learning_rate': 4.6257e-05, 'epoch': 0.53, 'throughput': 975.41}
[INFO|2025-10-21 07:32:20] logging.py:143 >> {'loss': 0.6152, 'learning_rate': 4.6243e-05, 'epoch': 0.53, 'throughput': 975.42}
[INFO|2025-10-21 07:33:25] logging.py:143 >> {'loss': 0.6268, 'learning_rate': 4.6229e-05, 'epoch': 0.53, 'throughput': 975.40}
[INFO|2025-10-21 07:34:29] logging.py:143 >> {'loss': 0.6963, 'learning_rate': 4.6215e-05, 'epoch': 0.53, 'throughput': 975.40}
[INFO|2025-10-21 07:35:32] logging.py:143 >> {'loss': 0.5297, 'learning_rate': 4.6201e-05, 'epoch': 0.53, 'throughput': 975.40}
[INFO|2025-10-21 07:36:37] logging.py:143 >> {'loss': 0.4329, 'learning_rate': 4.6187e-05, 'epoch': 0.53, 'throughput': 975.41}
[INFO|2025-10-21 07:37:41] logging.py:143 >> {'loss': 0.4424, 'learning_rate': 4.6173e-05, 'epoch': 0.54, 'throughput': 975.40}
[INFO|2025-10-21 07:38:46] logging.py:143 >> {'loss': 0.6151, 'learning_rate': 4.6159e-05, 'epoch': 0.54, 'throughput': 975.40}
[INFO|2025-10-21 07:39:49] logging.py:143 >> {'loss': 0.4386, 'learning_rate': 4.6145e-05, 'epoch': 0.54, 'throughput': 975.40}
[INFO|2025-10-21 07:40:53] logging.py:143 >> {'loss': 0.5386, 'learning_rate': 4.6131e-05, 'epoch': 0.54, 'throughput': 975.40}
[INFO|2025-10-21 07:41:57] logging.py:143 >> {'loss': 0.5576, 'learning_rate': 4.6116e-05, 'epoch': 0.54, 'throughput': 975.40}
[INFO|2025-10-21 07:43:01] logging.py:143 >> {'loss': 0.4653, 'learning_rate': 4.6102e-05, 'epoch': 0.54, 'throughput': 975.39}
[INFO|2025-10-21 07:44:05] logging.py:143 >> {'loss': 0.4069, 'learning_rate': 4.6088e-05, 'epoch': 0.54, 'throughput': 975.39}
[INFO|2025-10-21 07:45:10] logging.py:143 >> {'loss': 0.6491, 'learning_rate': 4.6074e-05, 'epoch': 0.54, 'throughput': 975.39}
[INFO|2025-10-21 07:46:14] logging.py:143 >> {'loss': 0.4786, 'learning_rate': 4.6060e-05, 'epoch': 0.54, 'throughput': 975.39}
[INFO|2025-10-21 07:47:19] logging.py:143 >> {'loss': 0.5764, 'learning_rate': 4.6045e-05, 'epoch': 0.54, 'throughput': 975.39}
[INFO|2025-10-21 07:48:24] logging.py:143 >> {'loss': 0.5692, 'learning_rate': 4.6031e-05, 'epoch': 0.55, 'throughput': 975.39}
[INFO|2025-10-21 07:48:24] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2700
[INFO|2025-10-21 07:48:24] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 07:48:24] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 07:48:25] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2700/chat_template.jinja
[INFO|2025-10-21 07:48:25] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2700/tokenizer_config.json
[INFO|2025-10-21 07:48:25] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2700/special_tokens_map.json
[INFO|2025-10-21 07:49:29] logging.py:143 >> {'loss': 0.5673, 'learning_rate': 4.6017e-05, 'epoch': 0.55, 'throughput': 975.36}
[INFO|2025-10-21 07:50:33] logging.py:143 >> {'loss': 0.5105, 'learning_rate': 4.6002e-05, 'epoch': 0.55, 'throughput': 975.37}
[INFO|2025-10-21 07:51:37] logging.py:143 >> {'loss': 0.4981, 'learning_rate': 4.5988e-05, 'epoch': 0.55, 'throughput': 975.37}
[INFO|2025-10-21 07:52:42] logging.py:143 >> {'loss': 0.5109, 'learning_rate': 4.5974e-05, 'epoch': 0.55, 'throughput': 975.35}
[INFO|2025-10-21 07:53:46] logging.py:143 >> {'loss': 0.4194, 'learning_rate': 4.5959e-05, 'epoch': 0.55, 'throughput': 975.36}
[INFO|2025-10-21 07:54:49] logging.py:143 >> {'loss': 0.5475, 'learning_rate': 4.5945e-05, 'epoch': 0.55, 'throughput': 975.36}
[INFO|2025-10-21 07:55:54] logging.py:143 >> {'loss': 0.6850, 'learning_rate': 4.5930e-05, 'epoch': 0.55, 'throughput': 975.36}
[INFO|2025-10-21 07:56:57] logging.py:143 >> {'loss': 0.5645, 'learning_rate': 4.5916e-05, 'epoch': 0.55, 'throughput': 975.36}
[INFO|2025-10-21 07:58:02] logging.py:143 >> {'loss': 0.6381, 'learning_rate': 4.5901e-05, 'epoch': 0.55, 'throughput': 975.36}
[INFO|2025-10-21 07:59:06] logging.py:143 >> {'loss': 0.5177, 'learning_rate': 4.5887e-05, 'epoch': 0.56, 'throughput': 975.35}
[INFO|2025-10-21 08:00:12] logging.py:143 >> {'loss': 0.4138, 'learning_rate': 4.5872e-05, 'epoch': 0.56, 'throughput': 975.35}
[INFO|2025-10-21 08:01:16] logging.py:143 >> {'loss': 0.5336, 'learning_rate': 4.5858e-05, 'epoch': 0.56, 'throughput': 975.35}
[INFO|2025-10-21 08:02:20] logging.py:143 >> {'loss': 0.6298, 'learning_rate': 4.5843e-05, 'epoch': 0.56, 'throughput': 975.34}
[INFO|2025-10-21 08:03:24] logging.py:143 >> {'loss': 0.4170, 'learning_rate': 4.5829e-05, 'epoch': 0.56, 'throughput': 975.34}
[INFO|2025-10-21 08:04:28] logging.py:143 >> {'loss': 0.6250, 'learning_rate': 4.5814e-05, 'epoch': 0.56, 'throughput': 975.34}
[INFO|2025-10-21 08:05:32] logging.py:143 >> {'loss': 0.5238, 'learning_rate': 4.5799e-05, 'epoch': 0.56, 'throughput': 975.34}
[INFO|2025-10-21 08:06:36] logging.py:143 >> {'loss': 0.4085, 'learning_rate': 4.5785e-05, 'epoch': 0.56, 'throughput': 975.34}
[INFO|2025-10-21 08:07:42] logging.py:143 >> {'loss': 0.4940, 'learning_rate': 4.5770e-05, 'epoch': 0.56, 'throughput': 975.34}
[INFO|2025-10-21 08:08:45] logging.py:143 >> {'loss': 0.5971, 'learning_rate': 4.5755e-05, 'epoch': 0.56, 'throughput': 975.34}
[INFO|2025-10-21 08:09:49] logging.py:143 >> {'loss': 0.5092, 'learning_rate': 4.5740e-05, 'epoch': 0.57, 'throughput': 975.33}
[INFO|2025-10-21 08:09:49] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2800
[INFO|2025-10-21 08:09:49] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 08:09:49] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 08:09:49] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2800/chat_template.jinja
[INFO|2025-10-21 08:09:49] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2800/tokenizer_config.json
[INFO|2025-10-21 08:09:49] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2800/special_tokens_map.json
[INFO|2025-10-21 08:10:55] logging.py:143 >> {'loss': 0.6951, 'learning_rate': 4.5726e-05, 'epoch': 0.57, 'throughput': 975.31}
[INFO|2025-10-21 08:11:59] logging.py:143 >> {'loss': 0.6022, 'learning_rate': 4.5711e-05, 'epoch': 0.57, 'throughput': 975.31}
[INFO|2025-10-21 08:13:04] logging.py:143 >> {'loss': 0.5589, 'learning_rate': 4.5696e-05, 'epoch': 0.57, 'throughput': 975.30}
[INFO|2025-10-21 08:14:08] logging.py:143 >> {'loss': 0.5023, 'learning_rate': 4.5681e-05, 'epoch': 0.57, 'throughput': 975.30}
[INFO|2025-10-21 08:15:12] logging.py:143 >> {'loss': 0.7919, 'learning_rate': 4.5666e-05, 'epoch': 0.57, 'throughput': 975.29}
[INFO|2025-10-21 08:16:17] logging.py:143 >> {'loss': 0.6509, 'learning_rate': 4.5651e-05, 'epoch': 0.57, 'throughput': 975.29}
[INFO|2025-10-21 08:17:20] logging.py:143 >> {'loss': 0.5949, 'learning_rate': 4.5636e-05, 'epoch': 0.57, 'throughput': 975.28}
[INFO|2025-10-21 08:18:24] logging.py:143 >> {'loss': 0.3366, 'learning_rate': 4.5621e-05, 'epoch': 0.57, 'throughput': 975.28}
[INFO|2025-10-21 08:19:28] logging.py:143 >> {'loss': 0.5144, 'learning_rate': 4.5606e-05, 'epoch': 0.57, 'throughput': 975.28}
[INFO|2025-10-21 08:20:32] logging.py:143 >> {'loss': 0.4381, 'learning_rate': 4.5591e-05, 'epoch': 0.58, 'throughput': 975.28}
[INFO|2025-10-21 08:21:36] logging.py:143 >> {'loss': 0.5191, 'learning_rate': 4.5576e-05, 'epoch': 0.58, 'throughput': 975.28}
[INFO|2025-10-21 08:22:40] logging.py:143 >> {'loss': 0.4246, 'learning_rate': 4.5561e-05, 'epoch': 0.58, 'throughput': 975.28}
[INFO|2025-10-21 08:23:44] logging.py:143 >> {'loss': 0.5375, 'learning_rate': 4.5546e-05, 'epoch': 0.58, 'throughput': 975.27}
[INFO|2025-10-21 08:24:48] logging.py:143 >> {'loss': 0.5418, 'learning_rate': 4.5531e-05, 'epoch': 0.58, 'throughput': 975.27}
[INFO|2025-10-21 08:25:51] logging.py:143 >> {'loss': 0.4839, 'learning_rate': 4.5516e-05, 'epoch': 0.58, 'throughput': 975.27}
[INFO|2025-10-21 08:26:56] logging.py:143 >> {'loss': 0.5039, 'learning_rate': 4.5501e-05, 'epoch': 0.58, 'throughput': 975.27}
[INFO|2025-10-21 08:28:00] logging.py:143 >> {'loss': 0.3980, 'learning_rate': 4.5486e-05, 'epoch': 0.58, 'throughput': 975.28}
[INFO|2025-10-21 08:29:04] logging.py:143 >> {'loss': 0.4834, 'learning_rate': 4.5471e-05, 'epoch': 0.58, 'throughput': 975.27}
[INFO|2025-10-21 08:30:08] logging.py:143 >> {'loss': 0.3716, 'learning_rate': 4.5456e-05, 'epoch': 0.59, 'throughput': 975.26}
[INFO|2025-10-21 08:31:12] logging.py:143 >> {'loss': 0.5308, 'learning_rate': 4.5440e-05, 'epoch': 0.59, 'throughput': 975.27}
[INFO|2025-10-21 08:31:12] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2900
[INFO|2025-10-21 08:31:12] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 08:31:12] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 08:31:12] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2900/chat_template.jinja
[INFO|2025-10-21 08:31:12] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2900/tokenizer_config.json
[INFO|2025-10-21 08:31:12] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-2900/special_tokens_map.json
[INFO|2025-10-21 08:32:17] logging.py:143 >> {'loss': 0.5821, 'learning_rate': 4.5425e-05, 'epoch': 0.59, 'throughput': 975.25}
[INFO|2025-10-21 08:33:22] logging.py:143 >> {'loss': 0.4484, 'learning_rate': 4.5410e-05, 'epoch': 0.59, 'throughput': 975.25}
[INFO|2025-10-21 08:34:27] logging.py:143 >> {'loss': 0.6254, 'learning_rate': 4.5394e-05, 'epoch': 0.59, 'throughput': 975.26}
[INFO|2025-10-21 08:35:28] logging.py:143 >> {'loss': 0.5558, 'learning_rate': 4.5379e-05, 'epoch': 0.59, 'throughput': 975.33}
[INFO|2025-10-21 08:36:20] logging.py:143 >> {'loss': 0.3872, 'learning_rate': 4.5364e-05, 'epoch': 0.59, 'throughput': 975.59}
[INFO|2025-10-21 08:37:12] logging.py:143 >> {'loss': 0.5934, 'learning_rate': 4.5348e-05, 'epoch': 0.59, 'throughput': 975.87}
[INFO|2025-10-21 08:38:05] logging.py:143 >> {'loss': 0.6051, 'learning_rate': 4.5333e-05, 'epoch': 0.59, 'throughput': 976.14}
[INFO|2025-10-21 08:38:58] logging.py:143 >> {'loss': 0.4431, 'learning_rate': 4.5318e-05, 'epoch': 0.59, 'throughput': 976.43}
[INFO|2025-10-21 08:39:51] logging.py:143 >> {'loss': 0.5224, 'learning_rate': 4.5302e-05, 'epoch': 0.60, 'throughput': 976.71}
[INFO|2025-10-21 08:40:45] logging.py:143 >> {'loss': 0.4685, 'learning_rate': 4.5287e-05, 'epoch': 0.60, 'throughput': 976.99}
[INFO|2025-10-21 08:41:38] logging.py:143 >> {'loss': 0.5868, 'learning_rate': 4.5271e-05, 'epoch': 0.60, 'throughput': 977.27}
[INFO|2025-10-21 08:42:31] logging.py:143 >> {'loss': 0.6172, 'learning_rate': 4.5256e-05, 'epoch': 0.60, 'throughput': 977.55}
[INFO|2025-10-21 08:43:25] logging.py:143 >> {'loss': 0.5026, 'learning_rate': 4.5240e-05, 'epoch': 0.60, 'throughput': 977.83}
[INFO|2025-10-21 08:44:17] logging.py:143 >> {'loss': 0.4932, 'learning_rate': 4.5225e-05, 'epoch': 0.60, 'throughput': 978.09}
[INFO|2025-10-21 08:45:11] logging.py:143 >> {'loss': 0.5461, 'learning_rate': 4.5209e-05, 'epoch': 0.60, 'throughput': 978.37}
[INFO|2025-10-21 08:46:04] logging.py:143 >> {'loss': 0.4493, 'learning_rate': 4.5194e-05, 'epoch': 0.60, 'throughput': 978.65}
[INFO|2025-10-21 08:46:57] logging.py:143 >> {'loss': 0.5654, 'learning_rate': 4.5178e-05, 'epoch': 0.60, 'throughput': 978.92}
[INFO|2025-10-21 08:47:51] logging.py:143 >> {'loss': 0.5247, 'learning_rate': 4.5162e-05, 'epoch': 0.60, 'throughput': 979.20}
[INFO|2025-10-21 08:48:45] logging.py:143 >> {'loss': 0.4227, 'learning_rate': 4.5147e-05, 'epoch': 0.61, 'throughput': 979.48}
[INFO|2025-10-21 08:49:39] logging.py:143 >> {'loss': 0.4754, 'learning_rate': 4.5131e-05, 'epoch': 0.61, 'throughput': 979.75}
[INFO|2025-10-21 08:49:39] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3000
[INFO|2025-10-21 08:49:39] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 08:49:39] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 08:49:39] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3000/chat_template.jinja
[INFO|2025-10-21 08:49:39] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3000/tokenizer_config.json
[INFO|2025-10-21 08:49:39] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3000/special_tokens_map.json
[INFO|2025-10-21 08:50:32] logging.py:143 >> {'loss': 0.6961, 'learning_rate': 4.5115e-05, 'epoch': 0.61, 'throughput': 980.01}
[INFO|2025-10-21 08:51:26] logging.py:143 >> {'loss': 0.4995, 'learning_rate': 4.5100e-05, 'epoch': 0.61, 'throughput': 980.28}
[INFO|2025-10-21 08:52:19] logging.py:143 >> {'loss': 0.4651, 'learning_rate': 4.5084e-05, 'epoch': 0.61, 'throughput': 980.55}
[INFO|2025-10-21 08:53:12] logging.py:143 >> {'loss': 0.7191, 'learning_rate': 4.5068e-05, 'epoch': 0.61, 'throughput': 980.82}
[INFO|2025-10-21 08:54:05] logging.py:143 >> {'loss': 0.5428, 'learning_rate': 4.5052e-05, 'epoch': 0.61, 'throughput': 981.09}
[INFO|2025-10-21 08:54:58] logging.py:143 >> {'loss': 0.6536, 'learning_rate': 4.5037e-05, 'epoch': 0.61, 'throughput': 981.35}
[INFO|2025-10-21 08:55:52] logging.py:143 >> {'loss': 0.4659, 'learning_rate': 4.5021e-05, 'epoch': 0.61, 'throughput': 981.62}
[INFO|2025-10-21 08:56:45] logging.py:143 >> {'loss': 0.3888, 'learning_rate': 4.5005e-05, 'epoch': 0.61, 'throughput': 981.88}
[INFO|2025-10-21 08:57:38] logging.py:143 >> {'loss': 0.4761, 'learning_rate': 4.4989e-05, 'epoch': 0.62, 'throughput': 982.15}
[INFO|2025-10-21 08:58:32] logging.py:143 >> {'loss': 0.4892, 'learning_rate': 4.4973e-05, 'epoch': 0.62, 'throughput': 982.42}
[INFO|2025-10-21 08:59:25] logging.py:143 >> {'loss': 0.5311, 'learning_rate': 4.4957e-05, 'epoch': 0.62, 'throughput': 982.68}
[INFO|2025-10-21 09:00:18] logging.py:143 >> {'loss': 0.5044, 'learning_rate': 4.4941e-05, 'epoch': 0.62, 'throughput': 982.95}
[INFO|2025-10-21 09:01:12] logging.py:143 >> {'loss': 0.3682, 'learning_rate': 4.4925e-05, 'epoch': 0.62, 'throughput': 983.22}
[INFO|2025-10-21 09:02:05] logging.py:143 >> {'loss': 0.4379, 'learning_rate': 4.4909e-05, 'epoch': 0.62, 'throughput': 983.48}
[INFO|2025-10-21 09:02:59] logging.py:143 >> {'loss': 0.5184, 'learning_rate': 4.4893e-05, 'epoch': 0.62, 'throughput': 983.74}
[INFO|2025-10-21 09:03:52] logging.py:143 >> {'loss': 0.5484, 'learning_rate': 4.4877e-05, 'epoch': 0.62, 'throughput': 984.00}
[INFO|2025-10-21 09:04:46] logging.py:143 >> {'loss': 0.3659, 'learning_rate': 4.4861e-05, 'epoch': 0.62, 'throughput': 984.27}
[INFO|2025-10-21 09:05:38] logging.py:143 >> {'loss': 0.5038, 'learning_rate': 4.4845e-05, 'epoch': 0.62, 'throughput': 984.52}
[INFO|2025-10-21 09:06:32] logging.py:143 >> {'loss': 0.6551, 'learning_rate': 4.4829e-05, 'epoch': 0.63, 'throughput': 984.78}
[INFO|2025-10-21 09:07:26] logging.py:143 >> {'loss': 0.3600, 'learning_rate': 4.4813e-05, 'epoch': 0.63, 'throughput': 985.04}
[INFO|2025-10-21 09:07:26] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3100
[INFO|2025-10-21 09:07:26] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 09:07:26] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 09:07:26] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3100/chat_template.jinja
[INFO|2025-10-21 09:07:26] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3100/tokenizer_config.json
[INFO|2025-10-21 09:07:26] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3100/special_tokens_map.json
[INFO|2025-10-21 09:08:18] logging.py:143 >> {'loss': 0.5230, 'learning_rate': 4.4797e-05, 'epoch': 0.63, 'throughput': 985.27}
[INFO|2025-10-21 09:09:12] logging.py:143 >> {'loss': 0.4472, 'learning_rate': 4.4781e-05, 'epoch': 0.63, 'throughput': 985.53}
[INFO|2025-10-21 09:10:05] logging.py:143 >> {'loss': 0.4361, 'learning_rate': 4.4764e-05, 'epoch': 0.63, 'throughput': 985.78}
[INFO|2025-10-21 09:10:59] logging.py:143 >> {'loss': 0.5768, 'learning_rate': 4.4748e-05, 'epoch': 0.63, 'throughput': 986.05}
[INFO|2025-10-21 09:11:53] logging.py:143 >> {'loss': 0.4069, 'learning_rate': 4.4732e-05, 'epoch': 0.63, 'throughput': 986.30}
[INFO|2025-10-21 09:12:45] logging.py:143 >> {'loss': 0.5414, 'learning_rate': 4.4716e-05, 'epoch': 0.63, 'throughput': 986.55}
[INFO|2025-10-21 09:13:39] logging.py:143 >> {'loss': 0.4990, 'learning_rate': 4.4699e-05, 'epoch': 0.63, 'throughput': 986.81}
[INFO|2025-10-21 09:14:32] logging.py:143 >> {'loss': 0.5392, 'learning_rate': 4.4683e-05, 'epoch': 0.63, 'throughput': 987.06}
[INFO|2025-10-21 09:15:25] logging.py:143 >> {'loss': 0.5422, 'learning_rate': 4.4667e-05, 'epoch': 0.64, 'throughput': 987.32}
[INFO|2025-10-21 09:16:18] logging.py:143 >> {'loss': 0.5795, 'learning_rate': 4.4650e-05, 'epoch': 0.64, 'throughput': 987.57}
[INFO|2025-10-21 09:17:11] logging.py:143 >> {'loss': 0.6902, 'learning_rate': 4.4634e-05, 'epoch': 0.64, 'throughput': 987.81}
[INFO|2025-10-21 09:18:05] logging.py:143 >> {'loss': 0.6300, 'learning_rate': 4.4618e-05, 'epoch': 0.64, 'throughput': 988.07}
[INFO|2025-10-21 09:18:58] logging.py:143 >> {'loss': 0.4919, 'learning_rate': 4.4601e-05, 'epoch': 0.64, 'throughput': 988.31}
[INFO|2025-10-21 09:19:50] logging.py:143 >> {'loss': 0.4809, 'learning_rate': 4.4585e-05, 'epoch': 0.64, 'throughput': 988.55}
[INFO|2025-10-21 09:20:43] logging.py:143 >> {'loss': 0.4968, 'learning_rate': 4.4568e-05, 'epoch': 0.64, 'throughput': 988.80}
[INFO|2025-10-21 09:21:37] logging.py:143 >> {'loss': 0.5758, 'learning_rate': 4.4552e-05, 'epoch': 0.64, 'throughput': 989.05}
[INFO|2025-10-21 09:22:31] logging.py:143 >> {'loss': 0.5289, 'learning_rate': 4.4535e-05, 'epoch': 0.64, 'throughput': 989.30}
[INFO|2025-10-21 09:23:24] logging.py:143 >> {'loss': 0.4312, 'learning_rate': 4.4519e-05, 'epoch': 0.64, 'throughput': 989.55}
[INFO|2025-10-21 09:24:17] logging.py:143 >> {'loss': 0.5801, 'learning_rate': 4.4502e-05, 'epoch': 0.65, 'throughput': 989.79}
[INFO|2025-10-21 09:25:11] logging.py:143 >> {'loss': 0.5723, 'learning_rate': 4.4486e-05, 'epoch': 0.65, 'throughput': 990.05}
[INFO|2025-10-21 09:25:11] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3200
[INFO|2025-10-21 09:25:11] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 09:25:11] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 09:25:11] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3200/chat_template.jinja
[INFO|2025-10-21 09:25:11] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3200/tokenizer_config.json
[INFO|2025-10-21 09:25:11] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3200/special_tokens_map.json
[INFO|2025-10-21 09:26:04] logging.py:143 >> {'loss': 0.5676, 'learning_rate': 4.4469e-05, 'epoch': 0.65, 'throughput': 990.27}
[INFO|2025-10-21 09:26:58] logging.py:143 >> {'loss': 0.5694, 'learning_rate': 4.4453e-05, 'epoch': 0.65, 'throughput': 990.52}
[INFO|2025-10-21 09:27:51] logging.py:143 >> {'loss': 0.3461, 'learning_rate': 4.4436e-05, 'epoch': 0.65, 'throughput': 990.76}
[INFO|2025-10-21 09:28:45] logging.py:143 >> {'loss': 0.5518, 'learning_rate': 4.4419e-05, 'epoch': 0.65, 'throughput': 991.00}
[INFO|2025-10-21 09:29:38] logging.py:143 >> {'loss': 0.4984, 'learning_rate': 4.4403e-05, 'epoch': 0.65, 'throughput': 991.24}
[INFO|2025-10-21 09:30:31] logging.py:143 >> {'loss': 0.3877, 'learning_rate': 4.4386e-05, 'epoch': 0.65, 'throughput': 991.48}
[INFO|2025-10-21 09:31:26] logging.py:143 >> {'loss': 0.5974, 'learning_rate': 4.4369e-05, 'epoch': 0.65, 'throughput': 991.74}
[INFO|2025-10-21 09:32:20] logging.py:143 >> {'loss': 0.5399, 'learning_rate': 4.4353e-05, 'epoch': 0.65, 'throughput': 991.99}
[INFO|2025-10-21 09:33:14] logging.py:143 >> {'loss': 0.5915, 'learning_rate': 4.4336e-05, 'epoch': 0.66, 'throughput': 992.23}
[INFO|2025-10-21 09:34:06] logging.py:143 >> {'loss': 0.4062, 'learning_rate': 4.4319e-05, 'epoch': 0.66, 'throughput': 992.46}
[INFO|2025-10-21 09:34:59] logging.py:143 >> {'loss': 0.5776, 'learning_rate': 4.4302e-05, 'epoch': 0.66, 'throughput': 992.70}
[INFO|2025-10-21 09:35:52] logging.py:143 >> {'loss': 0.5311, 'learning_rate': 4.4285e-05, 'epoch': 0.66, 'throughput': 992.93}
[INFO|2025-10-21 09:36:46] logging.py:143 >> {'loss': 0.4897, 'learning_rate': 4.4268e-05, 'epoch': 0.66, 'throughput': 993.17}
[INFO|2025-10-21 09:37:40] logging.py:143 >> {'loss': 0.5326, 'learning_rate': 4.4252e-05, 'epoch': 0.66, 'throughput': 993.41}
[INFO|2025-10-21 09:38:33] logging.py:143 >> {'loss': 0.4872, 'learning_rate': 4.4235e-05, 'epoch': 0.66, 'throughput': 993.64}
[INFO|2025-10-21 09:39:26] logging.py:143 >> {'loss': 0.4673, 'learning_rate': 4.4218e-05, 'epoch': 0.66, 'throughput': 993.86}
[INFO|2025-10-21 09:40:20] logging.py:143 >> {'loss': 0.6207, 'learning_rate': 4.4201e-05, 'epoch': 0.66, 'throughput': 994.11}
[INFO|2025-10-21 09:41:13] logging.py:143 >> {'loss': 0.6759, 'learning_rate': 4.4184e-05, 'epoch': 0.66, 'throughput': 994.35}
[INFO|2025-10-21 09:42:05] logging.py:143 >> {'loss': 0.4657, 'learning_rate': 4.4167e-05, 'epoch': 0.67, 'throughput': 994.57}
[INFO|2025-10-21 09:42:59] logging.py:143 >> {'loss': 0.4073, 'learning_rate': 4.4150e-05, 'epoch': 0.67, 'throughput': 994.81}
[INFO|2025-10-21 09:42:59] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3300
[INFO|2025-10-21 09:42:59] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 09:42:59] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 09:43:00] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3300/chat_template.jinja
[INFO|2025-10-21 09:43:00] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3300/tokenizer_config.json
[INFO|2025-10-21 09:43:00] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3300/special_tokens_map.json
[INFO|2025-10-21 09:43:53] logging.py:143 >> {'loss': 0.6225, 'learning_rate': 4.4133e-05, 'epoch': 0.67, 'throughput': 995.03}
[INFO|2025-10-21 09:44:46] logging.py:143 >> {'loss': 0.5273, 'learning_rate': 4.4116e-05, 'epoch': 0.67, 'throughput': 995.26}
[INFO|2025-10-21 09:45:39] logging.py:143 >> {'loss': 0.4255, 'learning_rate': 4.4099e-05, 'epoch': 0.67, 'throughput': 995.48}
[INFO|2025-10-21 09:46:33] logging.py:143 >> {'loss': 0.4759, 'learning_rate': 4.4082e-05, 'epoch': 0.67, 'throughput': 995.71}
[INFO|2025-10-21 09:47:25] logging.py:143 >> {'loss': 0.6114, 'learning_rate': 4.4065e-05, 'epoch': 0.67, 'throughput': 995.93}
[INFO|2025-10-21 09:48:19] logging.py:143 >> {'loss': 0.4626, 'learning_rate': 4.4048e-05, 'epoch': 0.67, 'throughput': 996.17}
[INFO|2025-10-21 09:49:13] logging.py:143 >> {'loss': 0.5208, 'learning_rate': 4.4030e-05, 'epoch': 0.67, 'throughput': 996.41}
[INFO|2025-10-21 09:50:07] logging.py:143 >> {'loss': 0.4704, 'learning_rate': 4.4013e-05, 'epoch': 0.68, 'throughput': 996.64}
[INFO|2025-10-21 09:51:01] logging.py:143 >> {'loss': 0.5468, 'learning_rate': 4.3996e-05, 'epoch': 0.68, 'throughput': 996.88}
[INFO|2025-10-21 09:51:54] logging.py:143 >> {'loss': 0.4111, 'learning_rate': 4.3979e-05, 'epoch': 0.68, 'throughput': 997.10}
[INFO|2025-10-21 09:52:47] logging.py:143 >> {'loss': 0.5303, 'learning_rate': 4.3962e-05, 'epoch': 0.68, 'throughput': 997.32}
[INFO|2025-10-21 09:53:40] logging.py:143 >> {'loss': 0.5025, 'learning_rate': 4.3944e-05, 'epoch': 0.68, 'throughput': 997.55}
[INFO|2025-10-21 09:54:33] logging.py:143 >> {'loss': 0.6663, 'learning_rate': 4.3927e-05, 'epoch': 0.68, 'throughput': 997.77}
[INFO|2025-10-21 09:55:28] logging.py:143 >> {'loss': 0.3630, 'learning_rate': 4.3910e-05, 'epoch': 0.68, 'throughput': 998.00}
[INFO|2025-10-21 09:56:21] logging.py:143 >> {'loss': 0.4712, 'learning_rate': 4.3892e-05, 'epoch': 0.68, 'throughput': 998.22}
[INFO|2025-10-21 09:57:15] logging.py:143 >> {'loss': 0.6260, 'learning_rate': 4.3875e-05, 'epoch': 0.68, 'throughput': 998.45}
[INFO|2025-10-21 09:58:08] logging.py:143 >> {'loss': 0.4025, 'learning_rate': 4.3858e-05, 'epoch': 0.68, 'throughput': 998.67}
[INFO|2025-10-21 09:59:01] logging.py:143 >> {'loss': 0.5407, 'learning_rate': 4.3840e-05, 'epoch': 0.69, 'throughput': 998.89}
[INFO|2025-10-21 09:59:54] logging.py:143 >> {'loss': 0.5798, 'learning_rate': 4.3823e-05, 'epoch': 0.69, 'throughput': 999.11}
[INFO|2025-10-21 10:00:47] logging.py:143 >> {'loss': 0.5504, 'learning_rate': 4.3806e-05, 'epoch': 0.69, 'throughput': 999.34}
[INFO|2025-10-21 10:00:47] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3400
[INFO|2025-10-21 10:00:47] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 10:00:47] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 10:00:47] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3400/chat_template.jinja
[INFO|2025-10-21 10:00:47] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3400/tokenizer_config.json
[INFO|2025-10-21 10:00:47] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3400/special_tokens_map.json
[INFO|2025-10-21 10:01:40] logging.py:143 >> {'loss': 0.4125, 'learning_rate': 4.3788e-05, 'epoch': 0.69, 'throughput': 999.53}
[INFO|2025-10-21 10:02:33] logging.py:143 >> {'loss': 0.3908, 'learning_rate': 4.3771e-05, 'epoch': 0.69, 'throughput': 999.75}
[INFO|2025-10-21 10:03:27] logging.py:143 >> {'loss': 0.5168, 'learning_rate': 4.3753e-05, 'epoch': 0.69, 'throughput': 999.97}
[INFO|2025-10-21 10:04:20] logging.py:143 >> {'loss': 0.5516, 'learning_rate': 4.3736e-05, 'epoch': 0.69, 'throughput': 1000.19}
[INFO|2025-10-21 10:05:13] logging.py:143 >> {'loss': 0.3707, 'learning_rate': 4.3718e-05, 'epoch': 0.69, 'throughput': 1000.40}
[INFO|2025-10-21 10:06:05] logging.py:143 >> {'loss': 0.4163, 'learning_rate': 4.3701e-05, 'epoch': 0.69, 'throughput': 1000.62}
[INFO|2025-10-21 10:06:59] logging.py:143 >> {'loss': 0.5804, 'learning_rate': 4.3683e-05, 'epoch': 0.69, 'throughput': 1000.84}
[INFO|2025-10-21 10:07:53] logging.py:143 >> {'loss': 0.4184, 'learning_rate': 4.3665e-05, 'epoch': 0.70, 'throughput': 1001.06}
[INFO|2025-10-21 10:08:46] logging.py:143 >> {'loss': 0.4786, 'learning_rate': 4.3648e-05, 'epoch': 0.70, 'throughput': 1001.28}
[INFO|2025-10-21 10:09:39] logging.py:143 >> {'loss': 0.5572, 'learning_rate': 4.3630e-05, 'epoch': 0.70, 'throughput': 1001.50}
[INFO|2025-10-21 10:10:33] logging.py:143 >> {'loss': 0.5518, 'learning_rate': 4.3613e-05, 'epoch': 0.70, 'throughput': 1001.71}
[INFO|2025-10-21 10:11:25] logging.py:143 >> {'loss': 0.4945, 'learning_rate': 4.3595e-05, 'epoch': 0.70, 'throughput': 1001.93}
[INFO|2025-10-21 10:12:18] logging.py:143 >> {'loss': 0.5326, 'learning_rate': 4.3577e-05, 'epoch': 0.70, 'throughput': 1002.14}
[INFO|2025-10-21 10:13:12] logging.py:143 >> {'loss': 0.5759, 'learning_rate': 4.3559e-05, 'epoch': 0.70, 'throughput': 1002.36}
[INFO|2025-10-21 10:14:05] logging.py:143 >> {'loss': 0.7076, 'learning_rate': 4.3542e-05, 'epoch': 0.70, 'throughput': 1002.57}
[INFO|2025-10-21 10:14:59] logging.py:143 >> {'loss': 0.5985, 'learning_rate': 4.3524e-05, 'epoch': 0.70, 'throughput': 1002.79}
[INFO|2025-10-21 10:15:52] logging.py:143 >> {'loss': 0.5798, 'learning_rate': 4.3506e-05, 'epoch': 0.70, 'throughput': 1003.00}
[INFO|2025-10-21 10:16:46] logging.py:143 >> {'loss': 0.4816, 'learning_rate': 4.3488e-05, 'epoch': 0.71, 'throughput': 1003.22}
[INFO|2025-10-21 10:17:39] logging.py:143 >> {'loss': 0.3959, 'learning_rate': 4.3471e-05, 'epoch': 0.71, 'throughput': 1003.43}
[INFO|2025-10-21 10:18:33] logging.py:143 >> {'loss': 0.5913, 'learning_rate': 4.3453e-05, 'epoch': 0.71, 'throughput': 1003.65}
[INFO|2025-10-21 10:18:33] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3500
[INFO|2025-10-21 10:18:33] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 10:18:33] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 10:18:33] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3500/chat_template.jinja
[INFO|2025-10-21 10:18:33] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3500/tokenizer_config.json
[INFO|2025-10-21 10:18:33] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3500/special_tokens_map.json
[INFO|2025-10-21 10:19:27] logging.py:143 >> {'loss': 0.4783, 'learning_rate': 4.3435e-05, 'epoch': 0.71, 'throughput': 1003.84}
[INFO|2025-10-21 10:20:20] logging.py:143 >> {'loss': 0.6088, 'learning_rate': 4.3417e-05, 'epoch': 0.71, 'throughput': 1004.05}
[INFO|2025-10-21 10:21:14] logging.py:143 >> {'loss': 0.5611, 'learning_rate': 4.3399e-05, 'epoch': 0.71, 'throughput': 1004.26}
[INFO|2025-10-21 10:22:08] logging.py:143 >> {'loss': 0.6411, 'learning_rate': 4.3381e-05, 'epoch': 0.71, 'throughput': 1004.48}
[INFO|2025-10-21 10:23:01] logging.py:143 >> {'loss': 0.3991, 'learning_rate': 4.3363e-05, 'epoch': 0.71, 'throughput': 1004.69}
[INFO|2025-10-21 10:23:55] logging.py:143 >> {'loss': 0.4094, 'learning_rate': 4.3345e-05, 'epoch': 0.71, 'throughput': 1004.91}
[INFO|2025-10-21 10:24:48] logging.py:143 >> {'loss': 0.4571, 'learning_rate': 4.3327e-05, 'epoch': 0.71, 'throughput': 1005.11}
[INFO|2025-10-21 10:25:42] logging.py:143 >> {'loss': 0.6579, 'learning_rate': 4.3309e-05, 'epoch': 0.72, 'throughput': 1005.32}
[INFO|2025-10-21 10:26:35] logging.py:143 >> {'loss': 0.5520, 'learning_rate': 4.3291e-05, 'epoch': 0.72, 'throughput': 1005.53}
[INFO|2025-10-21 10:27:28] logging.py:143 >> {'loss': 0.6812, 'learning_rate': 4.3273e-05, 'epoch': 0.72, 'throughput': 1005.73}
[INFO|2025-10-21 10:28:22] logging.py:143 >> {'loss': 0.6096, 'learning_rate': 4.3255e-05, 'epoch': 0.72, 'throughput': 1005.94}
[INFO|2025-10-21 10:29:15] logging.py:143 >> {'loss': 0.5898, 'learning_rate': 4.3237e-05, 'epoch': 0.72, 'throughput': 1006.15}
[INFO|2025-10-21 10:30:08] logging.py:143 >> {'loss': 0.4730, 'learning_rate': 4.3219e-05, 'epoch': 0.72, 'throughput': 1006.35}
[INFO|2025-10-21 10:31:01] logging.py:143 >> {'loss': 0.4422, 'learning_rate': 4.3201e-05, 'epoch': 0.72, 'throughput': 1006.55}
[INFO|2025-10-21 10:31:54] logging.py:143 >> {'loss': 0.6451, 'learning_rate': 4.3183e-05, 'epoch': 0.72, 'throughput': 1006.75}
[INFO|2025-10-21 10:32:47] logging.py:143 >> {'loss': 0.4409, 'learning_rate': 4.3165e-05, 'epoch': 0.72, 'throughput': 1006.95}
[INFO|2025-10-21 10:33:40] logging.py:143 >> {'loss': 0.5422, 'learning_rate': 4.3146e-05, 'epoch': 0.72, 'throughput': 1007.15}
[INFO|2025-10-21 10:34:33] logging.py:143 >> {'loss': 0.4277, 'learning_rate': 4.3128e-05, 'epoch': 0.73, 'throughput': 1007.35}
[INFO|2025-10-21 10:35:27] logging.py:143 >> {'loss': 0.6317, 'learning_rate': 4.3110e-05, 'epoch': 0.73, 'throughput': 1007.55}
[INFO|2025-10-21 10:36:21] logging.py:143 >> {'loss': 0.4295, 'learning_rate': 4.3092e-05, 'epoch': 0.73, 'throughput': 1007.76}
[INFO|2025-10-21 10:36:21] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3600
[INFO|2025-10-21 10:36:21] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 10:36:21] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 10:36:21] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3600/chat_template.jinja
[INFO|2025-10-21 10:36:21] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3600/tokenizer_config.json
[INFO|2025-10-21 10:36:21] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3600/special_tokens_map.json
[INFO|2025-10-21 10:37:15] logging.py:143 >> {'loss': 0.6033, 'learning_rate': 4.3073e-05, 'epoch': 0.73, 'throughput': 1007.95}
[INFO|2025-10-21 10:38:08] logging.py:143 >> {'loss': 0.4966, 'learning_rate': 4.3055e-05, 'epoch': 0.73, 'throughput': 1008.14}
[INFO|2025-10-21 10:39:01] logging.py:143 >> {'loss': 0.6236, 'learning_rate': 4.3037e-05, 'epoch': 0.73, 'throughput': 1008.34}
[INFO|2025-10-21 10:39:53] logging.py:143 >> {'loss': 0.4249, 'learning_rate': 4.3018e-05, 'epoch': 0.73, 'throughput': 1008.53}
[INFO|2025-10-21 10:40:47] logging.py:143 >> {'loss': 0.5582, 'learning_rate': 4.3000e-05, 'epoch': 0.73, 'throughput': 1008.73}
[INFO|2025-10-21 10:41:40] logging.py:143 >> {'loss': 0.5401, 'learning_rate': 4.2982e-05, 'epoch': 0.73, 'throughput': 1008.93}
[INFO|2025-10-21 10:42:33] logging.py:143 >> {'loss': 0.5205, 'learning_rate': 4.2963e-05, 'epoch': 0.73, 'throughput': 1009.13}
[INFO|2025-10-21 10:43:26] logging.py:143 >> {'loss': 0.4193, 'learning_rate': 4.2945e-05, 'epoch': 0.74, 'throughput': 1009.33}
[INFO|2025-10-21 10:44:19] logging.py:143 >> {'loss': 0.5388, 'learning_rate': 4.2927e-05, 'epoch': 0.74, 'throughput': 1009.53}
[INFO|2025-10-21 10:45:13] logging.py:143 >> {'loss': 0.3786, 'learning_rate': 4.2908e-05, 'epoch': 0.74, 'throughput': 1009.73}
[INFO|2025-10-21 10:46:06] logging.py:143 >> {'loss': 0.4606, 'learning_rate': 4.2890e-05, 'epoch': 0.74, 'throughput': 1009.92}
[INFO|2025-10-21 10:46:59] logging.py:143 >> {'loss': 0.4636, 'learning_rate': 4.2871e-05, 'epoch': 0.74, 'throughput': 1010.12}
[INFO|2025-10-21 10:47:51] logging.py:143 >> {'loss': 0.4389, 'learning_rate': 4.2853e-05, 'epoch': 0.74, 'throughput': 1010.31}
[INFO|2025-10-21 10:48:44] logging.py:143 >> {'loss': 0.4070, 'learning_rate': 4.2834e-05, 'epoch': 0.74, 'throughput': 1010.51}
[INFO|2025-10-21 10:49:38] logging.py:143 >> {'loss': 0.4758, 'learning_rate': 4.2816e-05, 'epoch': 0.74, 'throughput': 1010.70}
[INFO|2025-10-21 10:50:31] logging.py:143 >> {'loss': 0.5177, 'learning_rate': 4.2797e-05, 'epoch': 0.74, 'throughput': 1010.89}
[INFO|2025-10-21 10:51:24] logging.py:143 >> {'loss': 0.5100, 'learning_rate': 4.2778e-05, 'epoch': 0.74, 'throughput': 1011.08}
[INFO|2025-10-21 10:52:17] logging.py:143 >> {'loss': 0.5504, 'learning_rate': 4.2760e-05, 'epoch': 0.75, 'throughput': 1011.28}
[INFO|2025-10-21 10:53:11] logging.py:143 >> {'loss': 0.6433, 'learning_rate': 4.2741e-05, 'epoch': 0.75, 'throughput': 1011.47}
[INFO|2025-10-21 10:54:05] logging.py:143 >> {'loss': 0.5225, 'learning_rate': 4.2723e-05, 'epoch': 0.75, 'throughput': 1011.67}
[INFO|2025-10-21 10:54:05] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3700
[INFO|2025-10-21 10:54:05] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 10:54:05] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 10:54:05] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3700/chat_template.jinja
[INFO|2025-10-21 10:54:05] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3700/tokenizer_config.json
[INFO|2025-10-21 10:54:05] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3700/special_tokens_map.json
[INFO|2025-10-21 10:54:59] logging.py:143 >> {'loss': 0.5568, 'learning_rate': 4.2704e-05, 'epoch': 0.75, 'throughput': 1011.85}
[INFO|2025-10-21 10:55:52] logging.py:143 >> {'loss': 0.3805, 'learning_rate': 4.2685e-05, 'epoch': 0.75, 'throughput': 1012.04}
[INFO|2025-10-21 10:56:45] logging.py:143 >> {'loss': 0.5043, 'learning_rate': 4.2666e-05, 'epoch': 0.75, 'throughput': 1012.23}
[INFO|2025-10-21 10:57:38] logging.py:143 >> {'loss': 0.8888, 'learning_rate': 4.2648e-05, 'epoch': 0.75, 'throughput': 1012.42}
[INFO|2025-10-21 10:58:30] logging.py:143 >> {'loss': 0.5251, 'learning_rate': 4.2629e-05, 'epoch': 0.75, 'throughput': 1012.60}
[INFO|2025-10-21 10:59:23] logging.py:143 >> {'loss': 0.5901, 'learning_rate': 4.2610e-05, 'epoch': 0.75, 'throughput': 1012.79}
[INFO|2025-10-21 11:00:16] logging.py:143 >> {'loss': 0.6426, 'learning_rate': 4.2591e-05, 'epoch': 0.75, 'throughput': 1012.98}
[INFO|2025-10-21 11:01:09] logging.py:143 >> {'loss': 0.4617, 'learning_rate': 4.2573e-05, 'epoch': 0.76, 'throughput': 1013.17}
[INFO|2025-10-21 11:02:02] logging.py:143 >> {'loss': 0.4512, 'learning_rate': 4.2554e-05, 'epoch': 0.76, 'throughput': 1013.36}
[INFO|2025-10-21 11:02:55] logging.py:143 >> {'loss': 0.4014, 'learning_rate': 4.2535e-05, 'epoch': 0.76, 'throughput': 1013.54}
[INFO|2025-10-21 11:03:48] logging.py:143 >> {'loss': 0.4966, 'learning_rate': 4.2516e-05, 'epoch': 0.76, 'throughput': 1013.73}
[INFO|2025-10-21 11:04:42] logging.py:143 >> {'loss': 0.5043, 'learning_rate': 4.2497e-05, 'epoch': 0.76, 'throughput': 1013.92}
[INFO|2025-10-21 11:05:35] logging.py:143 >> {'loss': 0.6536, 'learning_rate': 4.2478e-05, 'epoch': 0.76, 'throughput': 1014.10}
[INFO|2025-10-21 11:06:28] logging.py:143 >> {'loss': 0.6159, 'learning_rate': 4.2459e-05, 'epoch': 0.76, 'throughput': 1014.29}
[INFO|2025-10-21 11:07:21] logging.py:143 >> {'loss': 0.5486, 'learning_rate': 4.2440e-05, 'epoch': 0.76, 'throughput': 1014.47}
[INFO|2025-10-21 11:08:15] logging.py:143 >> {'loss': 0.4887, 'learning_rate': 4.2421e-05, 'epoch': 0.76, 'throughput': 1014.66}
[INFO|2025-10-21 11:09:08] logging.py:143 >> {'loss': 0.5059, 'learning_rate': 4.2402e-05, 'epoch': 0.76, 'throughput': 1014.84}
[INFO|2025-10-21 11:10:01] logging.py:143 >> {'loss': 0.4990, 'learning_rate': 4.2383e-05, 'epoch': 0.77, 'throughput': 1015.02}
[INFO|2025-10-21 11:10:54] logging.py:143 >> {'loss': 0.3771, 'learning_rate': 4.2364e-05, 'epoch': 0.77, 'throughput': 1015.21}
[INFO|2025-10-21 11:11:48] logging.py:143 >> {'loss': 0.4398, 'learning_rate': 4.2345e-05, 'epoch': 0.77, 'throughput': 1015.39}
[INFO|2025-10-21 11:11:48] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3800
[INFO|2025-10-21 11:11:48] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 11:11:48] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 11:11:48] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3800/chat_template.jinja
[INFO|2025-10-21 11:11:48] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3800/tokenizer_config.json
[INFO|2025-10-21 11:11:48] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3800/special_tokens_map.json
[INFO|2025-10-21 11:12:41] logging.py:143 >> {'loss': 0.4730, 'learning_rate': 4.2326e-05, 'epoch': 0.77, 'throughput': 1015.56}
[INFO|2025-10-21 11:13:34] logging.py:143 >> {'loss': 0.6298, 'learning_rate': 4.2307e-05, 'epoch': 0.77, 'throughput': 1015.73}
[INFO|2025-10-21 11:14:27] logging.py:143 >> {'loss': 0.5478, 'learning_rate': 4.2288e-05, 'epoch': 0.77, 'throughput': 1015.91}
[INFO|2025-10-21 11:15:21] logging.py:143 >> {'loss': 0.3771, 'learning_rate': 4.2269e-05, 'epoch': 0.77, 'throughput': 1016.10}
[INFO|2025-10-21 11:16:14] logging.py:143 >> {'loss': 0.6617, 'learning_rate': 4.2250e-05, 'epoch': 0.77, 'throughput': 1016.28}
[INFO|2025-10-21 11:17:06] logging.py:143 >> {'loss': 0.5416, 'learning_rate': 4.2231e-05, 'epoch': 0.77, 'throughput': 1016.45}
[INFO|2025-10-21 11:17:59] logging.py:143 >> {'loss': 0.5872, 'learning_rate': 4.2212e-05, 'epoch': 0.78, 'throughput': 1016.63}
[INFO|2025-10-21 11:18:52] logging.py:143 >> {'loss': 0.5713, 'learning_rate': 4.2192e-05, 'epoch': 0.78, 'throughput': 1016.81}
[INFO|2025-10-21 11:19:46] logging.py:143 >> {'loss': 0.5600, 'learning_rate': 4.2173e-05, 'epoch': 0.78, 'throughput': 1016.99}
[INFO|2025-10-21 11:20:39] logging.py:143 >> {'loss': 0.5064, 'learning_rate': 4.2154e-05, 'epoch': 0.78, 'throughput': 1017.17}
[INFO|2025-10-21 11:21:32] logging.py:143 >> {'loss': 0.5354, 'learning_rate': 4.2135e-05, 'epoch': 0.78, 'throughput': 1017.35}
[INFO|2025-10-21 11:22:26] logging.py:143 >> {'loss': 0.4877, 'learning_rate': 4.2115e-05, 'epoch': 0.78, 'throughput': 1017.54}
[INFO|2025-10-21 11:23:20] logging.py:143 >> {'loss': 0.5099, 'learning_rate': 4.2096e-05, 'epoch': 0.78, 'throughput': 1017.72}
[INFO|2025-10-21 11:24:13] logging.py:143 >> {'loss': 0.6220, 'learning_rate': 4.2077e-05, 'epoch': 0.78, 'throughput': 1017.89}
[INFO|2025-10-21 11:25:05] logging.py:143 >> {'loss': 0.4241, 'learning_rate': 4.2057e-05, 'epoch': 0.78, 'throughput': 1018.07}
[INFO|2025-10-21 11:25:58] logging.py:143 >> {'loss': 0.6003, 'learning_rate': 4.2038e-05, 'epoch': 0.78, 'throughput': 1018.25}
[INFO|2025-10-21 11:26:51] logging.py:143 >> {'loss': 0.5831, 'learning_rate': 4.2019e-05, 'epoch': 0.79, 'throughput': 1018.42}
[INFO|2025-10-21 11:27:43] logging.py:143 >> {'loss': 0.4293, 'learning_rate': 4.1999e-05, 'epoch': 0.79, 'throughput': 1018.60}
[INFO|2025-10-21 11:28:37] logging.py:143 >> {'loss': 0.4500, 'learning_rate': 4.1980e-05, 'epoch': 0.79, 'throughput': 1018.77}
[INFO|2025-10-21 11:29:30] logging.py:143 >> {'loss': 0.4905, 'learning_rate': 4.1960e-05, 'epoch': 0.79, 'throughput': 1018.95}
[INFO|2025-10-21 11:29:30] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3900
[INFO|2025-10-21 11:29:30] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 11:29:30] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 11:29:30] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3900/chat_template.jinja
[INFO|2025-10-21 11:29:30] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3900/tokenizer_config.json
[INFO|2025-10-21 11:29:30] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-3900/special_tokens_map.json
[INFO|2025-10-21 11:30:25] logging.py:143 >> {'loss': 0.4905, 'learning_rate': 4.1941e-05, 'epoch': 0.79, 'throughput': 1019.12}
[INFO|2025-10-21 11:31:19] logging.py:143 >> {'loss': 0.4971, 'learning_rate': 4.1922e-05, 'epoch': 0.79, 'throughput': 1019.30}
[INFO|2025-10-21 11:32:12] logging.py:143 >> {'loss': 0.4787, 'learning_rate': 4.1902e-05, 'epoch': 0.79, 'throughput': 1019.48}
[INFO|2025-10-21 11:33:06] logging.py:143 >> {'loss': 0.4368, 'learning_rate': 4.1883e-05, 'epoch': 0.79, 'throughput': 1019.65}
[INFO|2025-10-21 11:33:59] logging.py:143 >> {'loss': 0.5168, 'learning_rate': 4.1863e-05, 'epoch': 0.79, 'throughput': 1019.82}
[INFO|2025-10-21 11:34:52] logging.py:143 >> {'loss': 0.6332, 'learning_rate': 4.1844e-05, 'epoch': 0.79, 'throughput': 1020.00}
[INFO|2025-10-21 11:35:46] logging.py:143 >> {'loss': 0.4854, 'learning_rate': 4.1824e-05, 'epoch': 0.80, 'throughput': 1020.16}
[INFO|2025-10-21 11:36:39] logging.py:143 >> {'loss': 0.6378, 'learning_rate': 4.1804e-05, 'epoch': 0.80, 'throughput': 1020.35}
[INFO|2025-10-21 11:37:32] logging.py:143 >> {'loss': 0.5552, 'learning_rate': 4.1785e-05, 'epoch': 0.80, 'throughput': 1020.52}
[INFO|2025-10-21 11:38:25] logging.py:143 >> {'loss': 0.5488, 'learning_rate': 4.1765e-05, 'epoch': 0.80, 'throughput': 1020.69}
[INFO|2025-10-21 11:39:19] logging.py:143 >> {'loss': 0.3769, 'learning_rate': 4.1746e-05, 'epoch': 0.80, 'throughput': 1020.86}
[INFO|2025-10-21 11:40:11] logging.py:143 >> {'loss': 0.4216, 'learning_rate': 4.1726e-05, 'epoch': 0.80, 'throughput': 1021.03}
[INFO|2025-10-21 11:41:04] logging.py:143 >> {'loss': 0.6094, 'learning_rate': 4.1706e-05, 'epoch': 0.80, 'throughput': 1021.20}
[INFO|2025-10-21 11:41:56] logging.py:143 >> {'loss': 0.4877, 'learning_rate': 4.1687e-05, 'epoch': 0.80, 'throughput': 1021.36}
[INFO|2025-10-21 11:42:50] logging.py:143 >> {'loss': 0.5115, 'learning_rate': 4.1667e-05, 'epoch': 0.80, 'throughput': 1021.53}
[INFO|2025-10-21 11:43:43] logging.py:143 >> {'loss': 0.4921, 'learning_rate': 4.1647e-05, 'epoch': 0.80, 'throughput': 1021.70}
[INFO|2025-10-21 11:44:37] logging.py:143 >> {'loss': 0.5396, 'learning_rate': 4.1627e-05, 'epoch': 0.81, 'throughput': 1021.88}
[INFO|2025-10-21 11:45:30] logging.py:143 >> {'loss': 0.6498, 'learning_rate': 4.1608e-05, 'epoch': 0.81, 'throughput': 1022.05}
[INFO|2025-10-21 11:46:23] logging.py:143 >> {'loss': 0.4407, 'learning_rate': 4.1588e-05, 'epoch': 0.81, 'throughput': 1022.22}
[INFO|2025-10-21 11:47:17] logging.py:143 >> {'loss': 0.4595, 'learning_rate': 4.1568e-05, 'epoch': 0.81, 'throughput': 1022.39}
[INFO|2025-10-21 11:47:17] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4000
[INFO|2025-10-21 11:47:17] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 11:47:17] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 11:47:17] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4000/chat_template.jinja
[INFO|2025-10-21 11:47:17] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4000/tokenizer_config.json
[INFO|2025-10-21 11:47:17] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4000/special_tokens_map.json
[INFO|2025-10-21 11:48:10] logging.py:143 >> {'loss': 0.5878, 'learning_rate': 4.1548e-05, 'epoch': 0.81, 'throughput': 1022.54}
[INFO|2025-10-21 11:49:03] logging.py:143 >> {'loss': 0.5316, 'learning_rate': 4.1528e-05, 'epoch': 0.81, 'throughput': 1022.71}
[INFO|2025-10-21 11:49:57] logging.py:143 >> {'loss': 0.5974, 'learning_rate': 4.1508e-05, 'epoch': 0.81, 'throughput': 1022.88}
[INFO|2025-10-21 11:50:50] logging.py:143 >> {'loss': 0.5430, 'learning_rate': 4.1489e-05, 'epoch': 0.81, 'throughput': 1023.04}
[INFO|2025-10-21 11:51:42] logging.py:143 >> {'loss': 0.4786, 'learning_rate': 4.1469e-05, 'epoch': 0.81, 'throughput': 1023.20}
[INFO|2025-10-21 11:52:37] logging.py:143 >> {'loss': 0.6348, 'learning_rate': 4.1449e-05, 'epoch': 0.81, 'throughput': 1023.38}
[INFO|2025-10-21 11:53:30] logging.py:143 >> {'loss': 0.4308, 'learning_rate': 4.1429e-05, 'epoch': 0.82, 'throughput': 1023.54}
[INFO|2025-10-21 11:54:24] logging.py:143 >> {'loss': 0.4470, 'learning_rate': 4.1409e-05, 'epoch': 0.82, 'throughput': 1023.71}
[INFO|2025-10-21 11:55:17] logging.py:143 >> {'loss': 0.7395, 'learning_rate': 4.1389e-05, 'epoch': 0.82, 'throughput': 1023.88}
[INFO|2025-10-21 11:56:10] logging.py:143 >> {'loss': 0.4740, 'learning_rate': 4.1369e-05, 'epoch': 0.82, 'throughput': 1024.04}
[INFO|2025-10-21 11:57:04] logging.py:143 >> {'loss': 0.5308, 'learning_rate': 4.1349e-05, 'epoch': 0.82, 'throughput': 1024.21}
[INFO|2025-10-21 11:57:56] logging.py:143 >> {'loss': 0.3998, 'learning_rate': 4.1329e-05, 'epoch': 0.82, 'throughput': 1024.37}
[INFO|2025-10-21 11:58:49] logging.py:143 >> {'loss': 0.5473, 'learning_rate': 4.1309e-05, 'epoch': 0.82, 'throughput': 1024.53}
[INFO|2025-10-21 11:59:42] logging.py:143 >> {'loss': 0.5170, 'learning_rate': 4.1289e-05, 'epoch': 0.82, 'throughput': 1024.70}
[INFO|2025-10-21 12:00:35] logging.py:143 >> {'loss': 0.4780, 'learning_rate': 4.1269e-05, 'epoch': 0.82, 'throughput': 1024.86}
[INFO|2025-10-21 12:01:29] logging.py:143 >> {'loss': 0.5462, 'learning_rate': 4.1249e-05, 'epoch': 0.82, 'throughput': 1025.02}
[INFO|2025-10-21 12:02:21] logging.py:143 >> {'loss': 0.5436, 'learning_rate': 4.1229e-05, 'epoch': 0.83, 'throughput': 1025.18}
[INFO|2025-10-21 12:03:14] logging.py:143 >> {'loss': 0.5096, 'learning_rate': 4.1208e-05, 'epoch': 0.83, 'throughput': 1025.34}
[INFO|2025-10-21 12:04:08] logging.py:143 >> {'loss': 0.4902, 'learning_rate': 4.1188e-05, 'epoch': 0.83, 'throughput': 1025.50}
[INFO|2025-10-21 12:05:02] logging.py:143 >> {'loss': 0.6261, 'learning_rate': 4.1168e-05, 'epoch': 0.83, 'throughput': 1025.67}
[INFO|2025-10-21 12:05:02] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4100
[INFO|2025-10-21 12:05:02] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 12:05:02] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 12:05:02] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4100/chat_template.jinja
[INFO|2025-10-21 12:05:02] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4100/tokenizer_config.json
[INFO|2025-10-21 12:05:02] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4100/special_tokens_map.json
[INFO|2025-10-21 12:05:56] logging.py:143 >> {'loss': 0.6003, 'learning_rate': 4.1148e-05, 'epoch': 0.83, 'throughput': 1025.82}
[INFO|2025-10-21 12:06:49] logging.py:143 >> {'loss': 0.3258, 'learning_rate': 4.1128e-05, 'epoch': 0.83, 'throughput': 1025.97}
[INFO|2025-10-21 12:07:42] logging.py:143 >> {'loss': 0.4689, 'learning_rate': 4.1107e-05, 'epoch': 0.83, 'throughput': 1026.13}
[INFO|2025-10-21 12:08:35] logging.py:143 >> {'loss': 0.6500, 'learning_rate': 4.1087e-05, 'epoch': 0.83, 'throughput': 1026.29}
[INFO|2025-10-21 12:09:28] logging.py:143 >> {'loss': 0.5363, 'learning_rate': 4.1067e-05, 'epoch': 0.83, 'throughput': 1026.45}
[INFO|2025-10-21 12:10:22] logging.py:143 >> {'loss': 0.5236, 'learning_rate': 4.1047e-05, 'epoch': 0.83, 'throughput': 1026.62}
[INFO|2025-10-21 12:11:16] logging.py:143 >> {'loss': 0.4351, 'learning_rate': 4.1026e-05, 'epoch': 0.84, 'throughput': 1026.78}
[INFO|2025-10-21 12:12:09] logging.py:143 >> {'loss': 0.5584, 'learning_rate': 4.1006e-05, 'epoch': 0.84, 'throughput': 1026.94}
[INFO|2025-10-21 12:13:02] logging.py:143 >> {'loss': 0.5123, 'learning_rate': 4.0986e-05, 'epoch': 0.84, 'throughput': 1027.10}
[INFO|2025-10-21 12:13:55] logging.py:143 >> {'loss': 0.5888, 'learning_rate': 4.0965e-05, 'epoch': 0.84, 'throughput': 1027.26}
[INFO|2025-10-21 12:14:48] logging.py:143 >> {'loss': 0.5416, 'learning_rate': 4.0945e-05, 'epoch': 0.84, 'throughput': 1027.41}
[INFO|2025-10-21 12:15:40] logging.py:143 >> {'loss': 0.5021, 'learning_rate': 4.0925e-05, 'epoch': 0.84, 'throughput': 1027.57}
[INFO|2025-10-21 12:16:34] logging.py:143 >> {'loss': 0.5692, 'learning_rate': 4.0904e-05, 'epoch': 0.84, 'throughput': 1027.72}
[INFO|2025-10-21 12:17:26] logging.py:143 >> {'loss': 0.4817, 'learning_rate': 4.0884e-05, 'epoch': 0.84, 'throughput': 1027.87}
[INFO|2025-10-21 12:18:20] logging.py:143 >> {'loss': 0.5357, 'learning_rate': 4.0863e-05, 'epoch': 0.84, 'throughput': 1028.03}
[INFO|2025-10-21 12:19:13] logging.py:143 >> {'loss': 0.4601, 'learning_rate': 4.0843e-05, 'epoch': 0.84, 'throughput': 1028.19}
[INFO|2025-10-21 12:20:06] logging.py:143 >> {'loss': 0.4855, 'learning_rate': 4.0822e-05, 'epoch': 0.85, 'throughput': 1028.34}
[INFO|2025-10-21 12:21:00] logging.py:143 >> {'loss': 0.4067, 'learning_rate': 4.0802e-05, 'epoch': 0.85, 'throughput': 1028.51}
[INFO|2025-10-21 12:21:54] logging.py:143 >> {'loss': 0.5307, 'learning_rate': 4.0781e-05, 'epoch': 0.85, 'throughput': 1028.66}
[INFO|2025-10-21 12:22:47] logging.py:143 >> {'loss': 0.4758, 'learning_rate': 4.0761e-05, 'epoch': 0.85, 'throughput': 1028.82}
[INFO|2025-10-21 12:22:47] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4200
[INFO|2025-10-21 12:22:47] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 12:22:47] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 12:22:47] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4200/chat_template.jinja
[INFO|2025-10-21 12:22:47] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4200/tokenizer_config.json
[INFO|2025-10-21 12:22:47] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4200/special_tokens_map.json
[INFO|2025-10-21 12:23:41] logging.py:143 >> {'loss': 0.4621, 'learning_rate': 4.0740e-05, 'epoch': 0.85, 'throughput': 1028.96}
[INFO|2025-10-21 12:24:34] logging.py:143 >> {'loss': 0.4628, 'learning_rate': 4.0720e-05, 'epoch': 0.85, 'throughput': 1029.11}
[INFO|2025-10-21 12:25:27] logging.py:143 >> {'loss': 0.5246, 'learning_rate': 4.0699e-05, 'epoch': 0.85, 'throughput': 1029.27}
[INFO|2025-10-21 12:26:20] logging.py:143 >> {'loss': 0.4736, 'learning_rate': 4.0679e-05, 'epoch': 0.85, 'throughput': 1029.42}
[INFO|2025-10-21 12:27:13] logging.py:143 >> {'loss': 0.6847, 'learning_rate': 4.0658e-05, 'epoch': 0.85, 'throughput': 1029.58}
[INFO|2025-10-21 12:28:06] logging.py:143 >> {'loss': 0.4952, 'learning_rate': 4.0637e-05, 'epoch': 0.85, 'throughput': 1029.73}
[INFO|2025-10-21 12:28:59] logging.py:143 >> {'loss': 0.6600, 'learning_rate': 4.0617e-05, 'epoch': 0.86, 'throughput': 1029.88}
[INFO|2025-10-21 12:29:53] logging.py:143 >> {'loss': 0.4808, 'learning_rate': 4.0596e-05, 'epoch': 0.86, 'throughput': 1030.04}
[INFO|2025-10-21 12:30:46] logging.py:143 >> {'loss': 0.4625, 'learning_rate': 4.0575e-05, 'epoch': 0.86, 'throughput': 1030.19}
[INFO|2025-10-21 12:31:39] logging.py:143 >> {'loss': 0.7242, 'learning_rate': 4.0555e-05, 'epoch': 0.86, 'throughput': 1030.34}
[INFO|2025-10-21 12:32:32] logging.py:143 >> {'loss': 0.4742, 'learning_rate': 4.0534e-05, 'epoch': 0.86, 'throughput': 1030.49}
[INFO|2025-10-21 12:33:25] logging.py:143 >> {'loss': 0.6671, 'learning_rate': 4.0513e-05, 'epoch': 0.86, 'throughput': 1030.63}
[INFO|2025-10-21 12:34:18] logging.py:143 >> {'loss': 0.6215, 'learning_rate': 4.0493e-05, 'epoch': 0.86, 'throughput': 1030.78}
[INFO|2025-10-21 12:35:12] logging.py:143 >> {'loss': 0.5935, 'learning_rate': 4.0472e-05, 'epoch': 0.86, 'throughput': 1030.94}
[INFO|2025-10-21 12:36:05] logging.py:143 >> {'loss': 0.4426, 'learning_rate': 4.0451e-05, 'epoch': 0.86, 'throughput': 1031.08}
[INFO|2025-10-21 12:36:58] logging.py:143 >> {'loss': 0.5552, 'learning_rate': 4.0430e-05, 'epoch': 0.86, 'throughput': 1031.24}
[INFO|2025-10-21 12:37:52] logging.py:143 >> {'loss': 0.5460, 'learning_rate': 4.0409e-05, 'epoch': 0.87, 'throughput': 1031.39}
[INFO|2025-10-21 12:38:44] logging.py:143 >> {'loss': 0.5631, 'learning_rate': 4.0388e-05, 'epoch': 0.87, 'throughput': 1031.54}
[INFO|2025-10-21 12:39:38] logging.py:143 >> {'loss': 0.6394, 'learning_rate': 4.0368e-05, 'epoch': 0.87, 'throughput': 1031.69}
[INFO|2025-10-21 12:40:31] logging.py:143 >> {'loss': 0.4586, 'learning_rate': 4.0347e-05, 'epoch': 0.87, 'throughput': 1031.83}
[INFO|2025-10-21 12:40:31] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4300
[INFO|2025-10-21 12:40:31] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 12:40:31] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 12:40:31] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4300/chat_template.jinja
[INFO|2025-10-21 12:40:31] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4300/tokenizer_config.json
[INFO|2025-10-21 12:40:31] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4300/special_tokens_map.json
[INFO|2025-10-21 12:41:25] logging.py:143 >> {'loss': 0.5432, 'learning_rate': 4.0326e-05, 'epoch': 0.87, 'throughput': 1031.97}
[INFO|2025-10-21 12:42:18] logging.py:143 >> {'loss': 0.5597, 'learning_rate': 4.0305e-05, 'epoch': 0.87, 'throughput': 1032.12}
[INFO|2025-10-21 12:43:12] logging.py:143 >> {'loss': 0.5635, 'learning_rate': 4.0284e-05, 'epoch': 0.87, 'throughput': 1032.27}
[INFO|2025-10-21 12:44:06] logging.py:143 >> {'loss': 0.4578, 'learning_rate': 4.0263e-05, 'epoch': 0.87, 'throughput': 1032.42}
[INFO|2025-10-21 12:44:59] logging.py:143 >> {'loss': 0.5340, 'learning_rate': 4.0242e-05, 'epoch': 0.87, 'throughput': 1032.57}
[INFO|2025-10-21 12:45:52] logging.py:143 >> {'loss': 0.4880, 'learning_rate': 4.0221e-05, 'epoch': 0.88, 'throughput': 1032.71}
[INFO|2025-10-21 12:46:46] logging.py:143 >> {'loss': 0.5762, 'learning_rate': 4.0200e-05, 'epoch': 0.88, 'throughput': 1032.87}
[INFO|2025-10-21 12:47:40] logging.py:143 >> {'loss': 0.4879, 'learning_rate': 4.0179e-05, 'epoch': 0.88, 'throughput': 1033.01}
[INFO|2025-10-21 12:48:33] logging.py:143 >> {'loss': 0.4608, 'learning_rate': 4.0158e-05, 'epoch': 0.88, 'throughput': 1033.16}
[INFO|2025-10-21 12:49:26] logging.py:143 >> {'loss': 0.5812, 'learning_rate': 4.0137e-05, 'epoch': 0.88, 'throughput': 1033.31}
[INFO|2025-10-21 12:50:20] logging.py:143 >> {'loss': 0.4389, 'learning_rate': 4.0116e-05, 'epoch': 0.88, 'throughput': 1033.45}
[INFO|2025-10-21 12:51:13] logging.py:143 >> {'loss': 0.4783, 'learning_rate': 4.0095e-05, 'epoch': 0.88, 'throughput': 1033.59}
[INFO|2025-10-21 12:52:07] logging.py:143 >> {'loss': 0.4512, 'learning_rate': 4.0074e-05, 'epoch': 0.88, 'throughput': 1033.75}
[INFO|2025-10-21 12:53:00] logging.py:143 >> {'loss': 0.4716, 'learning_rate': 4.0053e-05, 'epoch': 0.88, 'throughput': 1033.89}
[INFO|2025-10-21 12:53:53] logging.py:143 >> {'loss': 0.5299, 'learning_rate': 4.0032e-05, 'epoch': 0.88, 'throughput': 1034.04}
[INFO|2025-10-21 12:54:46] logging.py:143 >> {'loss': 0.6183, 'learning_rate': 4.0010e-05, 'epoch': 0.89, 'throughput': 1034.18}
[INFO|2025-10-21 12:55:40] logging.py:143 >> {'loss': 0.4609, 'learning_rate': 3.9989e-05, 'epoch': 0.89, 'throughput': 1034.33}
[INFO|2025-10-21 12:56:34] logging.py:143 >> {'loss': 0.4403, 'learning_rate': 3.9968e-05, 'epoch': 0.89, 'throughput': 1034.47}
[INFO|2025-10-21 12:57:27] logging.py:143 >> {'loss': 0.4968, 'learning_rate': 3.9947e-05, 'epoch': 0.89, 'throughput': 1034.62}
[INFO|2025-10-21 12:58:20] logging.py:143 >> {'loss': 0.4843, 'learning_rate': 3.9926e-05, 'epoch': 0.89, 'throughput': 1034.76}
[INFO|2025-10-21 12:58:20] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4400
[INFO|2025-10-21 12:58:20] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 12:58:20] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 12:58:21] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4400/chat_template.jinja
[INFO|2025-10-21 12:58:21] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4400/tokenizer_config.json
[INFO|2025-10-21 12:58:21] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4400/special_tokens_map.json
[INFO|2025-10-21 12:59:14] logging.py:143 >> {'loss': 0.5641, 'learning_rate': 3.9904e-05, 'epoch': 0.89, 'throughput': 1034.89}
[INFO|2025-10-21 13:00:08] logging.py:143 >> {'loss': 0.4885, 'learning_rate': 3.9883e-05, 'epoch': 0.89, 'throughput': 1035.04}
[INFO|2025-10-21 13:01:02] logging.py:143 >> {'loss': 0.5356, 'learning_rate': 3.9862e-05, 'epoch': 0.89, 'throughput': 1035.18}
[INFO|2025-10-21 13:01:54] logging.py:143 >> {'loss': 0.4647, 'learning_rate': 3.9841e-05, 'epoch': 0.89, 'throughput': 1035.32}
[INFO|2025-10-21 13:02:48] logging.py:143 >> {'loss': 0.4625, 'learning_rate': 3.9819e-05, 'epoch': 0.89, 'throughput': 1035.47}
[INFO|2025-10-21 13:03:42] logging.py:143 >> {'loss': 0.4414, 'learning_rate': 3.9798e-05, 'epoch': 0.90, 'throughput': 1035.61}
[INFO|2025-10-21 13:04:35] logging.py:143 >> {'loss': 0.5721, 'learning_rate': 3.9777e-05, 'epoch': 0.90, 'throughput': 1035.76}
[INFO|2025-10-21 13:05:28] logging.py:143 >> {'loss': 0.6013, 'learning_rate': 3.9755e-05, 'epoch': 0.90, 'throughput': 1035.90}
[INFO|2025-10-21 13:06:21] logging.py:143 >> {'loss': 0.4300, 'learning_rate': 3.9734e-05, 'epoch': 0.90, 'throughput': 1036.04}
[INFO|2025-10-21 13:07:15] logging.py:143 >> {'loss': 0.6016, 'learning_rate': 3.9713e-05, 'epoch': 0.90, 'throughput': 1036.18}
[INFO|2025-10-21 13:08:08] logging.py:143 >> {'loss': 0.6479, 'learning_rate': 3.9691e-05, 'epoch': 0.90, 'throughput': 1036.32}
[INFO|2025-10-21 13:09:01] logging.py:143 >> {'loss': 0.6088, 'learning_rate': 3.9670e-05, 'epoch': 0.90, 'throughput': 1036.46}
[INFO|2025-10-21 13:09:54] logging.py:143 >> {'loss': 0.5345, 'learning_rate': 3.9648e-05, 'epoch': 0.90, 'throughput': 1036.59}
[INFO|2025-10-21 13:10:47] logging.py:143 >> {'loss': 0.4541, 'learning_rate': 3.9627e-05, 'epoch': 0.90, 'throughput': 1036.73}
[INFO|2025-10-21 13:11:40] logging.py:143 >> {'loss': 0.4144, 'learning_rate': 3.9605e-05, 'epoch': 0.90, 'throughput': 1036.87}
[INFO|2025-10-21 13:12:34] logging.py:143 >> {'loss': 0.4356, 'learning_rate': 3.9584e-05, 'epoch': 0.91, 'throughput': 1037.02}
[INFO|2025-10-21 13:13:28] logging.py:143 >> {'loss': 0.5287, 'learning_rate': 3.9562e-05, 'epoch': 0.91, 'throughput': 1037.15}
[INFO|2025-10-21 13:14:21] logging.py:143 >> {'loss': 0.5098, 'learning_rate': 3.9541e-05, 'epoch': 0.91, 'throughput': 1037.29}
[INFO|2025-10-21 13:15:13] logging.py:143 >> {'loss': 0.5002, 'learning_rate': 3.9519e-05, 'epoch': 0.91, 'throughput': 1037.42}
[INFO|2025-10-21 13:16:07] logging.py:143 >> {'loss': 0.5726, 'learning_rate': 3.9498e-05, 'epoch': 0.91, 'throughput': 1037.56}
[INFO|2025-10-21 13:16:07] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4500
[INFO|2025-10-21 13:16:07] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 13:16:07] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 13:16:07] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4500/chat_template.jinja
[INFO|2025-10-21 13:16:07] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4500/tokenizer_config.json
[INFO|2025-10-21 13:16:07] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4500/special_tokens_map.json
[INFO|2025-10-21 13:17:02] logging.py:143 >> {'loss': 0.5611, 'learning_rate': 3.9476e-05, 'epoch': 0.91, 'throughput': 1037.69}
[INFO|2025-10-21 13:17:56] logging.py:143 >> {'loss': 0.5681, 'learning_rate': 3.9455e-05, 'epoch': 0.91, 'throughput': 1037.83}
[INFO|2025-10-21 13:18:50] logging.py:143 >> {'loss': 0.6045, 'learning_rate': 3.9433e-05, 'epoch': 0.91, 'throughput': 1037.98}
[INFO|2025-10-21 13:19:44] logging.py:143 >> {'loss': 0.4985, 'learning_rate': 3.9412e-05, 'epoch': 0.91, 'throughput': 1038.12}
[INFO|2025-10-21 13:20:37] logging.py:143 >> {'loss': 0.3921, 'learning_rate': 3.9390e-05, 'epoch': 0.91, 'throughput': 1038.25}
[INFO|2025-10-21 13:21:29] logging.py:143 >> {'loss': 0.5642, 'learning_rate': 3.9368e-05, 'epoch': 0.92, 'throughput': 1038.38}
[INFO|2025-10-21 13:22:23] logging.py:143 >> {'loss': 0.5829, 'learning_rate': 3.9347e-05, 'epoch': 0.92, 'throughput': 1038.52}
[INFO|2025-10-21 13:23:15] logging.py:143 >> {'loss': 0.5644, 'learning_rate': 3.9325e-05, 'epoch': 0.92, 'throughput': 1038.65}
[INFO|2025-10-21 13:24:09] logging.py:143 >> {'loss': 0.4868, 'learning_rate': 3.9303e-05, 'epoch': 0.92, 'throughput': 1038.79}
[INFO|2025-10-21 13:25:02] logging.py:143 >> {'loss': 0.4939, 'learning_rate': 3.9282e-05, 'epoch': 0.92, 'throughput': 1038.92}
[INFO|2025-10-21 13:25:55] logging.py:143 >> {'loss': 0.5992, 'learning_rate': 3.9260e-05, 'epoch': 0.92, 'throughput': 1039.06}
[INFO|2025-10-21 13:26:49] logging.py:143 >> {'loss': 0.5607, 'learning_rate': 3.9238e-05, 'epoch': 0.92, 'throughput': 1039.20}
[INFO|2025-10-21 13:27:42] logging.py:143 >> {'loss': 0.5038, 'learning_rate': 3.9216e-05, 'epoch': 0.92, 'throughput': 1039.33}
[INFO|2025-10-21 13:28:35] logging.py:143 >> {'loss': 0.5139, 'learning_rate': 3.9195e-05, 'epoch': 0.92, 'throughput': 1039.46}
[INFO|2025-10-21 13:29:28] logging.py:143 >> {'loss': 0.6485, 'learning_rate': 3.9173e-05, 'epoch': 0.92, 'throughput': 1039.60}
[INFO|2025-10-21 13:30:22] logging.py:143 >> {'loss': 0.4648, 'learning_rate': 3.9151e-05, 'epoch': 0.93, 'throughput': 1039.73}
[INFO|2025-10-21 13:31:14] logging.py:143 >> {'loss': 0.4679, 'learning_rate': 3.9129e-05, 'epoch': 0.93, 'throughput': 1039.86}
[INFO|2025-10-21 13:32:07] logging.py:143 >> {'loss': 0.5926, 'learning_rate': 3.9107e-05, 'epoch': 0.93, 'throughput': 1040.00}
[INFO|2025-10-21 13:33:00] logging.py:143 >> {'loss': 0.5385, 'learning_rate': 3.9085e-05, 'epoch': 0.93, 'throughput': 1040.13}
[INFO|2025-10-21 13:33:52] logging.py:143 >> {'loss': 0.6416, 'learning_rate': 3.9064e-05, 'epoch': 0.93, 'throughput': 1040.26}
[INFO|2025-10-21 13:33:52] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4600
[INFO|2025-10-21 13:33:52] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 13:33:52] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 13:33:53] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4600/chat_template.jinja
[INFO|2025-10-21 13:33:53] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4600/tokenizer_config.json
[INFO|2025-10-21 13:33:53] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4600/special_tokens_map.json
[INFO|2025-10-21 13:34:46] logging.py:143 >> {'loss': 0.6690, 'learning_rate': 3.9042e-05, 'epoch': 0.93, 'throughput': 1040.37}
[INFO|2025-10-21 13:35:38] logging.py:143 >> {'loss': 0.4299, 'learning_rate': 3.9020e-05, 'epoch': 0.93, 'throughput': 1040.50}
[INFO|2025-10-21 13:36:31] logging.py:143 >> {'loss': 0.5213, 'learning_rate': 3.8998e-05, 'epoch': 0.93, 'throughput': 1040.63}
[INFO|2025-10-21 13:37:24] logging.py:143 >> {'loss': 0.4476, 'learning_rate': 3.8976e-05, 'epoch': 0.93, 'throughput': 1040.76}
[INFO|2025-10-21 13:38:18] logging.py:143 >> {'loss': 0.5109, 'learning_rate': 3.8954e-05, 'epoch': 0.93, 'throughput': 1040.89}
[INFO|2025-10-21 13:39:11] logging.py:143 >> {'loss': 0.5957, 'learning_rate': 3.8932e-05, 'epoch': 0.94, 'throughput': 1041.03}
[INFO|2025-10-21 13:40:04] logging.py:143 >> {'loss': 0.4821, 'learning_rate': 3.8910e-05, 'epoch': 0.94, 'throughput': 1041.16}
[INFO|2025-10-21 13:40:56] logging.py:143 >> {'loss': 0.6078, 'learning_rate': 3.8888e-05, 'epoch': 0.94, 'throughput': 1041.28}
[INFO|2025-10-21 13:41:49] logging.py:143 >> {'loss': 0.4315, 'learning_rate': 3.8866e-05, 'epoch': 0.94, 'throughput': 1041.42}
[INFO|2025-10-21 13:42:43] logging.py:143 >> {'loss': 0.5606, 'learning_rate': 3.8844e-05, 'epoch': 0.94, 'throughput': 1041.55}
[INFO|2025-10-21 13:43:36] logging.py:143 >> {'loss': 0.4991, 'learning_rate': 3.8822e-05, 'epoch': 0.94, 'throughput': 1041.68}
[INFO|2025-10-21 13:44:29] logging.py:143 >> {'loss': 0.4818, 'learning_rate': 3.8800e-05, 'epoch': 0.94, 'throughput': 1041.81}
[INFO|2025-10-21 13:45:21] logging.py:143 >> {'loss': 0.4524, 'learning_rate': 3.8778e-05, 'epoch': 0.94, 'throughput': 1041.94}
[INFO|2025-10-21 13:46:15] logging.py:143 >> {'loss': 0.5234, 'learning_rate': 3.8756e-05, 'epoch': 0.94, 'throughput': 1042.07}
[INFO|2025-10-21 13:47:09] logging.py:143 >> {'loss': 0.5906, 'learning_rate': 3.8734e-05, 'epoch': 0.94, 'throughput': 1042.20}
[INFO|2025-10-21 13:48:02] logging.py:143 >> {'loss': 0.5419, 'learning_rate': 3.8712e-05, 'epoch': 0.95, 'throughput': 1042.33}
[INFO|2025-10-21 13:48:55] logging.py:143 >> {'loss': 0.5040, 'learning_rate': 3.8690e-05, 'epoch': 0.95, 'throughput': 1042.46}
[INFO|2025-10-21 13:49:48] logging.py:143 >> {'loss': 0.7064, 'learning_rate': 3.8667e-05, 'epoch': 0.95, 'throughput': 1042.59}
[INFO|2025-10-21 13:50:42] logging.py:143 >> {'loss': 0.4451, 'learning_rate': 3.8645e-05, 'epoch': 0.95, 'throughput': 1042.72}
[INFO|2025-10-21 13:51:35] logging.py:143 >> {'loss': 0.6174, 'learning_rate': 3.8623e-05, 'epoch': 0.95, 'throughput': 1042.85}
[INFO|2025-10-21 13:51:35] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4700
[INFO|2025-10-21 13:51:35] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 13:51:35] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 13:51:35] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4700/chat_template.jinja
[INFO|2025-10-21 13:51:35] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4700/tokenizer_config.json
[INFO|2025-10-21 13:51:35] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4700/special_tokens_map.json
[INFO|2025-10-21 13:52:28] logging.py:143 >> {'loss': 0.5833, 'learning_rate': 3.8601e-05, 'epoch': 0.95, 'throughput': 1042.96}
[INFO|2025-10-21 13:53:22] logging.py:143 >> {'loss': 0.5692, 'learning_rate': 3.8579e-05, 'epoch': 0.95, 'throughput': 1043.10}
[INFO|2025-10-21 13:54:16] logging.py:143 >> {'loss': 0.5811, 'learning_rate': 3.8556e-05, 'epoch': 0.95, 'throughput': 1043.22}
[INFO|2025-10-21 13:55:08] logging.py:143 >> {'loss': 0.6330, 'learning_rate': 3.8534e-05, 'epoch': 0.95, 'throughput': 1043.35}
[INFO|2025-10-21 13:56:02] logging.py:143 >> {'loss': 0.4274, 'learning_rate': 3.8512e-05, 'epoch': 0.95, 'throughput': 1043.48}
[INFO|2025-10-21 13:56:55] logging.py:143 >> {'loss': 0.4848, 'learning_rate': 3.8490e-05, 'epoch': 0.96, 'throughput': 1043.61}
[INFO|2025-10-21 13:57:49] logging.py:143 >> {'loss': 0.5108, 'learning_rate': 3.8467e-05, 'epoch': 0.96, 'throughput': 1043.74}
[INFO|2025-10-21 13:58:41] logging.py:143 >> {'loss': 0.4017, 'learning_rate': 3.8445e-05, 'epoch': 0.96, 'throughput': 1043.86}
[INFO|2025-10-21 13:59:35] logging.py:143 >> {'loss': 0.5471, 'learning_rate': 3.8423e-05, 'epoch': 0.96, 'throughput': 1044.00}
[INFO|2025-10-21 14:00:28] logging.py:143 >> {'loss': 0.5035, 'learning_rate': 3.8400e-05, 'epoch': 0.96, 'throughput': 1044.12}
[INFO|2025-10-21 14:01:22] logging.py:143 >> {'loss': 0.3585, 'learning_rate': 3.8378e-05, 'epoch': 0.96, 'throughput': 1044.25}
[INFO|2025-10-21 14:02:16] logging.py:143 >> {'loss': 0.6593, 'learning_rate': 3.8356e-05, 'epoch': 0.96, 'throughput': 1044.38}
[INFO|2025-10-21 14:03:09] logging.py:143 >> {'loss': 0.5946, 'learning_rate': 3.8333e-05, 'epoch': 0.96, 'throughput': 1044.50}
[INFO|2025-10-21 14:04:02] logging.py:143 >> {'loss': 0.5391, 'learning_rate': 3.8311e-05, 'epoch': 0.96, 'throughput': 1044.63}
[INFO|2025-10-21 14:04:55] logging.py:143 >> {'loss': 0.4878, 'learning_rate': 3.8289e-05, 'epoch': 0.97, 'throughput': 1044.75}
[INFO|2025-10-21 14:05:48] logging.py:143 >> {'loss': 0.5272, 'learning_rate': 3.8266e-05, 'epoch': 0.97, 'throughput': 1044.87}
[INFO|2025-10-21 14:06:42] logging.py:143 >> {'loss': 0.6182, 'learning_rate': 3.8244e-05, 'epoch': 0.97, 'throughput': 1044.99}
[INFO|2025-10-21 14:07:35] logging.py:143 >> {'loss': 0.6267, 'learning_rate': 3.8221e-05, 'epoch': 0.97, 'throughput': 1045.12}
[INFO|2025-10-21 14:08:27] logging.py:143 >> {'loss': 0.4823, 'learning_rate': 3.8199e-05, 'epoch': 0.97, 'throughput': 1045.23}
[INFO|2025-10-21 14:09:20] logging.py:143 >> {'loss': 0.5923, 'learning_rate': 3.8176e-05, 'epoch': 0.97, 'throughput': 1045.36}
[INFO|2025-10-21 14:09:20] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4800
[INFO|2025-10-21 14:09:20] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 14:09:20] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 14:09:20] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4800/chat_template.jinja
[INFO|2025-10-21 14:09:20] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4800/tokenizer_config.json
[INFO|2025-10-21 14:09:20] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4800/special_tokens_map.json
[INFO|2025-10-21 14:10:15] logging.py:143 >> {'loss': 0.5535, 'learning_rate': 3.8154e-05, 'epoch': 0.97, 'throughput': 1045.48}
[INFO|2025-10-21 14:11:08] logging.py:143 >> {'loss': 0.5913, 'learning_rate': 3.8131e-05, 'epoch': 0.97, 'throughput': 1045.60}
[INFO|2025-10-21 14:12:01] logging.py:143 >> {'loss': 0.5799, 'learning_rate': 3.8109e-05, 'epoch': 0.97, 'throughput': 1045.72}
[INFO|2025-10-21 14:12:53] logging.py:143 >> {'loss': 0.5382, 'learning_rate': 3.8086e-05, 'epoch': 0.97, 'throughput': 1045.84}
[INFO|2025-10-21 14:13:47] logging.py:143 >> {'loss': 0.4933, 'learning_rate': 3.8064e-05, 'epoch': 0.98, 'throughput': 1045.96}
[INFO|2025-10-21 14:14:40] logging.py:143 >> {'loss': 0.5837, 'learning_rate': 3.8041e-05, 'epoch': 0.98, 'throughput': 1046.08}
[INFO|2025-10-21 14:15:33] logging.py:143 >> {'loss': 0.4188, 'learning_rate': 3.8019e-05, 'epoch': 0.98, 'throughput': 1046.20}
[INFO|2025-10-21 14:16:26] logging.py:143 >> {'loss': 0.4633, 'learning_rate': 3.7996e-05, 'epoch': 0.98, 'throughput': 1046.32}
[INFO|2025-10-21 14:17:20] logging.py:143 >> {'loss': 0.4085, 'learning_rate': 3.7973e-05, 'epoch': 0.98, 'throughput': 1046.44}
[INFO|2025-10-21 14:18:14] logging.py:143 >> {'loss': 0.3644, 'learning_rate': 3.7951e-05, 'epoch': 0.98, 'throughput': 1046.56}
[INFO|2025-10-21 14:19:06] logging.py:143 >> {'loss': 0.5068, 'learning_rate': 3.7928e-05, 'epoch': 0.98, 'throughput': 1046.68}
[INFO|2025-10-21 14:19:59] logging.py:143 >> {'loss': 0.5130, 'learning_rate': 3.7906e-05, 'epoch': 0.98, 'throughput': 1046.80}
[INFO|2025-10-21 14:20:52] logging.py:143 >> {'loss': 0.6945, 'learning_rate': 3.7883e-05, 'epoch': 0.98, 'throughput': 1046.92}
[INFO|2025-10-21 14:21:46] logging.py:143 >> {'loss': 0.4850, 'learning_rate': 3.7860e-05, 'epoch': 0.98, 'throughput': 1047.05}
[INFO|2025-10-21 14:22:39] logging.py:143 >> {'loss': 0.4950, 'learning_rate': 3.7838e-05, 'epoch': 0.99, 'throughput': 1047.17}
[INFO|2025-10-21 14:23:32] logging.py:143 >> {'loss': 0.7462, 'learning_rate': 3.7815e-05, 'epoch': 0.99, 'throughput': 1047.29}
[INFO|2025-10-21 14:24:25] logging.py:143 >> {'loss': 0.5005, 'learning_rate': 3.7792e-05, 'epoch': 0.99, 'throughput': 1047.40}
[INFO|2025-10-21 14:25:17] logging.py:143 >> {'loss': 0.6437, 'learning_rate': 3.7769e-05, 'epoch': 0.99, 'throughput': 1047.52}
[INFO|2025-10-21 14:26:10] logging.py:143 >> {'loss': 0.3949, 'learning_rate': 3.7747e-05, 'epoch': 0.99, 'throughput': 1047.64}
[INFO|2025-10-21 14:27:03] logging.py:143 >> {'loss': 0.6835, 'learning_rate': 3.7724e-05, 'epoch': 0.99, 'throughput': 1047.76}
[INFO|2025-10-21 14:27:03] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4900
[INFO|2025-10-21 14:27:03] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 14:27:03] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 14:27:04] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4900/chat_template.jinja
[INFO|2025-10-21 14:27:04] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4900/tokenizer_config.json
[INFO|2025-10-21 14:27:04] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-4900/special_tokens_map.json
[INFO|2025-10-21 14:27:58] logging.py:143 >> {'loss': 0.4061, 'learning_rate': 3.7701e-05, 'epoch': 0.99, 'throughput': 1047.87}
[INFO|2025-10-21 14:28:50] logging.py:143 >> {'loss': 0.7935, 'learning_rate': 3.7678e-05, 'epoch': 0.99, 'throughput': 1047.98}
[INFO|2025-10-21 14:29:44] logging.py:143 >> {'loss': 0.4764, 'learning_rate': 3.7655e-05, 'epoch': 0.99, 'throughput': 1048.10}
[INFO|2025-10-21 14:30:37] logging.py:143 >> {'loss': 0.6318, 'learning_rate': 3.7633e-05, 'epoch': 0.99, 'throughput': 1048.22}
[INFO|2025-10-21 14:31:30] logging.py:143 >> {'loss': 0.6433, 'learning_rate': 3.7610e-05, 'epoch': 1.00, 'throughput': 1048.34}
[INFO|2025-10-21 14:32:24] logging.py:143 >> {'loss': 0.7554, 'learning_rate': 3.7587e-05, 'epoch': 1.00, 'throughput': 1048.46}
[INFO|2025-10-21 14:33:16] logging.py:143 >> {'loss': 0.5585, 'learning_rate': 3.7564e-05, 'epoch': 1.00, 'throughput': 1048.57}
[INFO|2025-10-21 14:34:08] logging.py:143 >> {'loss': 0.5027, 'learning_rate': 3.7541e-05, 'epoch': 1.00, 'throughput': 1048.68}
[INFO|2025-10-21 14:35:02] logging.py:143 >> {'loss': 0.5626, 'learning_rate': 3.7518e-05, 'epoch': 1.00, 'throughput': 1048.80}
[INFO|2025-10-21 14:35:55] logging.py:143 >> {'loss': 0.4789, 'learning_rate': 3.7495e-05, 'epoch': 1.00, 'throughput': 1048.92}
[INFO|2025-10-21 14:36:48] logging.py:143 >> {'loss': 0.5321, 'learning_rate': 3.7472e-05, 'epoch': 1.00, 'throughput': 1049.03}
[INFO|2025-10-21 14:37:42] logging.py:143 >> {'loss': 0.4101, 'learning_rate': 3.7450e-05, 'epoch': 1.00, 'throughput': 1049.15}
[INFO|2025-10-21 14:38:35] logging.py:143 >> {'loss': 0.4525, 'learning_rate': 3.7427e-05, 'epoch': 1.00, 'throughput': 1049.27}
[INFO|2025-10-21 14:39:28] logging.py:143 >> {'loss': 0.3625, 'learning_rate': 3.7404e-05, 'epoch': 1.00, 'throughput': 1049.38}
[INFO|2025-10-21 14:40:21] logging.py:143 >> {'loss': 0.3294, 'learning_rate': 3.7381e-05, 'epoch': 1.01, 'throughput': 1049.50}
[INFO|2025-10-21 14:41:13] logging.py:143 >> {'loss': 0.6111, 'learning_rate': 3.7358e-05, 'epoch': 1.01, 'throughput': 1049.61}
[INFO|2025-10-21 14:42:07] logging.py:143 >> {'loss': 0.3448, 'learning_rate': 3.7335e-05, 'epoch': 1.01, 'throughput': 1049.73}
[INFO|2025-10-21 14:43:00] logging.py:143 >> {'loss': 0.4309, 'learning_rate': 3.7312e-05, 'epoch': 1.01, 'throughput': 1049.84}
[INFO|2025-10-21 14:43:54] logging.py:143 >> {'loss': 0.5213, 'learning_rate': 3.7289e-05, 'epoch': 1.01, 'throughput': 1049.96}
[INFO|2025-10-21 14:44:47] logging.py:143 >> {'loss': 0.5163, 'learning_rate': 3.7266e-05, 'epoch': 1.01, 'throughput': 1050.08}
[INFO|2025-10-21 14:44:47] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-5000
[INFO|2025-10-21 14:44:47] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 14:44:47] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 14:44:47] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-5000/chat_template.jinja
[INFO|2025-10-21 14:44:47] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-5000/tokenizer_config.json
[INFO|2025-10-21 14:44:47] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-5000/special_tokens_map.json
[INFO|2025-10-21 14:45:40] logging.py:143 >> {'loss': 0.3950, 'learning_rate': 3.7243e-05, 'epoch': 1.01, 'throughput': 1050.18}
[INFO|2025-10-21 14:46:33] logging.py:143 >> {'loss': 0.5294, 'learning_rate': 3.7219e-05, 'epoch': 1.01, 'throughput': 1050.29}
[INFO|2025-10-21 14:47:27] logging.py:143 >> {'loss': 0.4993, 'learning_rate': 3.7196e-05, 'epoch': 1.01, 'throughput': 1050.40}
[INFO|2025-10-21 14:48:20] logging.py:143 >> {'loss': 0.4356, 'learning_rate': 3.7173e-05, 'epoch': 1.01, 'throughput': 1050.52}
[INFO|2025-10-21 14:49:12] logging.py:143 >> {'loss': 0.3739, 'learning_rate': 3.7150e-05, 'epoch': 1.02, 'throughput': 1050.63}
[INFO|2025-10-21 14:50:06] logging.py:143 >> {'loss': 0.4355, 'learning_rate': 3.7127e-05, 'epoch': 1.02, 'throughput': 1050.75}
[INFO|2025-10-21 14:50:59] logging.py:143 >> {'loss': 0.4493, 'learning_rate': 3.7104e-05, 'epoch': 1.02, 'throughput': 1050.86}
[INFO|2025-10-21 14:51:53] logging.py:143 >> {'loss': 0.4387, 'learning_rate': 3.7081e-05, 'epoch': 1.02, 'throughput': 1050.98}
[INFO|2025-10-21 14:52:46] logging.py:143 >> {'loss': 0.4627, 'learning_rate': 3.7058e-05, 'epoch': 1.02, 'throughput': 1051.10}
[INFO|2025-10-21 14:53:38] logging.py:143 >> {'loss': 0.5299, 'learning_rate': 3.7034e-05, 'epoch': 1.02, 'throughput': 1051.21}
[INFO|2025-10-21 14:54:32] logging.py:143 >> {'loss': 0.4974, 'learning_rate': 3.7011e-05, 'epoch': 1.02, 'throughput': 1051.32}
[INFO|2025-10-21 14:55:25] logging.py:143 >> {'loss': 0.3598, 'learning_rate': 3.6988e-05, 'epoch': 1.02, 'throughput': 1051.43}
[INFO|2025-10-21 14:56:18] logging.py:143 >> {'loss': 0.3500, 'learning_rate': 3.6965e-05, 'epoch': 1.02, 'throughput': 1051.54}
[INFO|2025-10-21 14:57:11] logging.py:143 >> {'loss': 0.3281, 'learning_rate': 3.6942e-05, 'epoch': 1.02, 'throughput': 1051.65}
[INFO|2025-10-21 14:58:05] logging.py:143 >> {'loss': 0.4797, 'learning_rate': 3.6918e-05, 'epoch': 1.03, 'throughput': 1051.77}
[INFO|2025-10-21 14:58:58] logging.py:143 >> {'loss': 0.2865, 'learning_rate': 3.6895e-05, 'epoch': 1.03, 'throughput': 1051.89}
[INFO|2025-10-21 14:59:51] logging.py:143 >> {'loss': 0.4418, 'learning_rate': 3.6872e-05, 'epoch': 1.03, 'throughput': 1052.00}
[INFO|2025-10-21 15:00:44] logging.py:143 >> {'loss': 0.3953, 'learning_rate': 3.6848e-05, 'epoch': 1.03, 'throughput': 1052.11}
[INFO|2025-10-21 15:01:37] logging.py:143 >> {'loss': 0.4531, 'learning_rate': 3.6825e-05, 'epoch': 1.03, 'throughput': 1052.22}
[INFO|2025-10-21 15:02:30] logging.py:143 >> {'loss': 0.3589, 'learning_rate': 3.6802e-05, 'epoch': 1.03, 'throughput': 1052.33}
[INFO|2025-10-21 15:02:30] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-5100
[INFO|2025-10-21 15:02:30] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 15:02:30] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 15:02:31] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-5100/chat_template.jinja
[INFO|2025-10-21 15:02:31] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-5100/tokenizer_config.json
[INFO|2025-10-21 15:02:31] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-5100/special_tokens_map.json
[INFO|2025-10-21 15:03:25] logging.py:143 >> {'loss': 0.3016, 'learning_rate': 3.6779e-05, 'epoch': 1.03, 'throughput': 1052.43}
[INFO|2025-10-21 15:04:18] logging.py:143 >> {'loss': 0.3599, 'learning_rate': 3.6755e-05, 'epoch': 1.03, 'throughput': 1052.54}
[INFO|2025-10-21 15:05:11] logging.py:143 >> {'loss': 0.4286, 'learning_rate': 3.6732e-05, 'epoch': 1.03, 'throughput': 1052.65}
[INFO|2025-10-21 15:06:04] logging.py:143 >> {'loss': 0.4638, 'learning_rate': 3.6708e-05, 'epoch': 1.03, 'throughput': 1052.75}
[INFO|2025-10-21 15:06:57] logging.py:143 >> {'loss': 0.4395, 'learning_rate': 3.6685e-05, 'epoch': 1.04, 'throughput': 1052.86}
[INFO|2025-10-21 15:07:50] logging.py:143 >> {'loss': 0.4128, 'learning_rate': 3.6662e-05, 'epoch': 1.04, 'throughput': 1052.97}
[INFO|2025-10-21 15:08:43] logging.py:143 >> {'loss': 0.4899, 'learning_rate': 3.6638e-05, 'epoch': 1.04, 'throughput': 1053.08}
[INFO|2025-10-21 15:09:36] logging.py:143 >> {'loss': 0.4270, 'learning_rate': 3.6615e-05, 'epoch': 1.04, 'throughput': 1053.19}
[INFO|2025-10-21 15:10:28] logging.py:143 >> {'loss': 0.3719, 'learning_rate': 3.6591e-05, 'epoch': 1.04, 'throughput': 1053.29}
[INFO|2025-10-21 15:11:22] logging.py:143 >> {'loss': 0.4679, 'learning_rate': 3.6568e-05, 'epoch': 1.04, 'throughput': 1053.40}
[INFO|2025-10-21 15:12:14] logging.py:143 >> {'loss': 0.4128, 'learning_rate': 3.6544e-05, 'epoch': 1.04, 'throughput': 1053.51}
[INFO|2025-10-21 15:13:07] logging.py:143 >> {'loss': 0.3903, 'learning_rate': 3.6521e-05, 'epoch': 1.04, 'throughput': 1053.62}
[INFO|2025-10-21 15:14:01] logging.py:143 >> {'loss': 0.6297, 'learning_rate': 3.6498e-05, 'epoch': 1.04, 'throughput': 1053.73}
[INFO|2025-10-21 15:14:54] logging.py:143 >> {'loss': 0.4863, 'learning_rate': 3.6474e-05, 'epoch': 1.04, 'throughput': 1053.83}
[INFO|2025-10-21 15:15:47] logging.py:143 >> {'loss': 0.3879, 'learning_rate': 3.6451e-05, 'epoch': 1.05, 'throughput': 1053.94}
[INFO|2025-10-21 15:16:40] logging.py:143 >> {'loss': 0.4867, 'learning_rate': 3.6427e-05, 'epoch': 1.05, 'throughput': 1054.05}
[INFO|2025-10-21 15:17:33] logging.py:143 >> {'loss': 0.6123, 'learning_rate': 3.6403e-05, 'epoch': 1.05, 'throughput': 1054.16}
[INFO|2025-10-21 15:18:26] logging.py:143 >> {'loss': 0.3906, 'learning_rate': 3.6380e-05, 'epoch': 1.05, 'throughput': 1054.26}
[INFO|2025-10-21 15:19:19] logging.py:143 >> {'loss': 0.5193, 'learning_rate': 3.6356e-05, 'epoch': 1.05, 'throughput': 1054.37}
[INFO|2025-10-21 15:20:12] logging.py:143 >> {'loss': 0.4235, 'learning_rate': 3.6333e-05, 'epoch': 1.05, 'throughput': 1054.48}
[INFO|2025-10-21 15:20:12] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-5200
[INFO|2025-10-21 15:20:12] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 15:20:12] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 15:20:13] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-5200/chat_template.jinja
[INFO|2025-10-21 15:20:13] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-5200/tokenizer_config.json
[INFO|2025-10-21 15:20:13] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-5200/special_tokens_map.json
[INFO|2025-10-21 15:21:06] logging.py:143 >> {'loss': 0.6852, 'learning_rate': 3.6309e-05, 'epoch': 1.05, 'throughput': 1054.57}
[INFO|2025-10-21 15:21:58] logging.py:143 >> {'loss': 0.4083, 'learning_rate': 3.6286e-05, 'epoch': 1.05, 'throughput': 1054.67}
[INFO|2025-10-21 15:22:52] logging.py:143 >> {'loss': 0.4840, 'learning_rate': 3.6262e-05, 'epoch': 1.05, 'throughput': 1054.78}
[INFO|2025-10-21 15:23:45] logging.py:143 >> {'loss': 0.4801, 'learning_rate': 3.6238e-05, 'epoch': 1.05, 'throughput': 1054.89}
[INFO|2025-10-21 15:24:38] logging.py:143 >> {'loss': 0.4788, 'learning_rate': 3.6215e-05, 'epoch': 1.06, 'throughput': 1055.00}
[INFO|2025-10-21 15:25:31] logging.py:143 >> {'loss': 0.4841, 'learning_rate': 3.6191e-05, 'epoch': 1.06, 'throughput': 1055.11}
[INFO|2025-10-21 15:26:23] logging.py:143 >> {'loss': 0.4183, 'learning_rate': 3.6167e-05, 'epoch': 1.06, 'throughput': 1055.20}
[INFO|2025-10-21 15:27:16] logging.py:143 >> {'loss': 0.4251, 'learning_rate': 3.6144e-05, 'epoch': 1.06, 'throughput': 1055.31}
[INFO|2025-10-21 15:28:10] logging.py:143 >> {'loss': 0.3860, 'learning_rate': 3.6120e-05, 'epoch': 1.06, 'throughput': 1055.42}
[INFO|2025-10-21 15:29:03] logging.py:143 >> {'loss': 0.5240, 'learning_rate': 3.6096e-05, 'epoch': 1.06, 'throughput': 1055.53}
[INFO|2025-10-21 15:29:56] logging.py:143 >> {'loss': 0.5609, 'learning_rate': 3.6073e-05, 'epoch': 1.06, 'throughput': 1055.63}
[INFO|2025-10-21 15:30:49] logging.py:143 >> {'loss': 0.3606, 'learning_rate': 3.6049e-05, 'epoch': 1.06, 'throughput': 1055.74}
[INFO|2025-10-21 15:31:42] logging.py:143 >> {'loss': 0.4887, 'learning_rate': 3.6025e-05, 'epoch': 1.06, 'throughput': 1055.84}
[INFO|2025-10-21 15:32:35] logging.py:143 >> {'loss': 0.2950, 'learning_rate': 3.6001e-05, 'epoch': 1.07, 'throughput': 1055.94}
[INFO|2025-10-21 15:33:28] logging.py:143 >> {'loss': 0.4900, 'learning_rate': 3.5978e-05, 'epoch': 1.07, 'throughput': 1056.05}
[INFO|2025-10-21 15:34:21] logging.py:143 >> {'loss': 0.4402, 'learning_rate': 3.5954e-05, 'epoch': 1.07, 'throughput': 1056.15}
[INFO|2025-10-21 15:35:14] logging.py:143 >> {'loss': 0.3620, 'learning_rate': 3.5930e-05, 'epoch': 1.07, 'throughput': 1056.26}
[INFO|2025-10-21 15:36:07] logging.py:143 >> {'loss': 0.3546, 'learning_rate': 3.5906e-05, 'epoch': 1.07, 'throughput': 1056.36}
[INFO|2025-10-21 15:37:00] logging.py:143 >> {'loss': 0.4137, 'learning_rate': 3.5882e-05, 'epoch': 1.07, 'throughput': 1056.47}
[INFO|2025-10-21 15:37:54] logging.py:143 >> {'loss': 0.3813, 'learning_rate': 3.5859e-05, 'epoch': 1.07, 'throughput': 1056.58}
[INFO|2025-10-21 15:37:54] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-5300
[INFO|2025-10-21 15:37:54] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 15:37:54] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 15:37:54] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-5300/chat_template.jinja
[INFO|2025-10-21 15:37:54] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-5300/tokenizer_config.json
[INFO|2025-10-21 15:37:54] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/checkpoint-5300/special_tokens_map.json
[INFO|2025-10-21 15:38:49] logging.py:143 >> {'loss': 0.4713, 'learning_rate': 3.5835e-05, 'epoch': 1.07, 'throughput': 1056.66}
[INFO|2025-10-21 15:39:42] logging.py:143 >> {'loss': 0.4634, 'learning_rate': 3.5811e-05, 'epoch': 1.07, 'throughput': 1056.77}
[INFO|2025-10-21 15:40:36] logging.py:143 >> {'loss': 0.3682, 'learning_rate': 3.5787e-05, 'epoch': 1.07, 'throughput': 1056.87}
[INFO|2025-10-21 15:41:28] logging.py:143 >> {'loss': 0.4622, 'learning_rate': 3.5763e-05, 'epoch': 1.08, 'throughput': 1056.97}
[INFO|2025-10-21 15:41:28] trainer.py:2810 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|2025-10-21 15:41:28] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47
[INFO|2025-10-21 15:41:29] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 15:41:29] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 15:41:29] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/chat_template.jinja
[INFO|2025-10-21 15:41:29] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/tokenizer_config.json
[INFO|2025-10-21 15:41:29] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-20-21-57-47/special_tokens_map.json
[WARNING|2025-10-21 15:41:29] logging.py:148 >> No metric eval_loss to plot.
[WARNING|2025-10-21 15:41:29] logging.py:148 >> No metric eval_accuracy to plot.
[INFO|2025-10-21 15:41:29] modelcard.py:456 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
