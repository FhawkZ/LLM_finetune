{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 4948,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0010105092966855296,
      "grad_norm": 37.429664611816406,
      "learning_rate": 3.999993549994088e-06,
      "loss": 8.0922,
      "num_input_tokens_seen": 62192,
      "step": 5,
      "train_runtime": 53.4865,
      "train_tokens_per_second": 1162.76
    },
    {
      "epoch": 0.002021018593371059,
      "grad_norm": 25.085432052612305,
      "learning_rate": 3.9999673469163725e-06,
      "loss": 7.0781,
      "num_input_tokens_seen": 124880,
      "step": 10,
      "train_runtime": 106.7025,
      "train_tokens_per_second": 1170.357
    },
    {
      "epoch": 0.0030315278900565883,
      "grad_norm": 26.949338912963867,
      "learning_rate": 3.999920987905358e-06,
      "loss": 7.1881,
      "num_input_tokens_seen": 188112,
      "step": 15,
      "train_runtime": 160.3302,
      "train_tokens_per_second": 1173.279
    },
    {
      "epoch": 0.004042037186742118,
      "grad_norm": 28.23147964477539,
      "learning_rate": 3.999854473428258e-06,
      "loss": 6.4948,
      "num_input_tokens_seen": 250560,
      "step": 20,
      "train_runtime": 213.4226,
      "train_tokens_per_second": 1174.009
    },
    {
      "epoch": 0.0050525464834276475,
      "grad_norm": 25.193567276000977,
      "learning_rate": 3.999767804155413e-06,
      "loss": 6.1312,
      "num_input_tokens_seen": 313680,
      "step": 25,
      "train_runtime": 266.9305,
      "train_tokens_per_second": 1175.137
    },
    {
      "epoch": 0.006063055780113177,
      "grad_norm": 24.865787506103516,
      "learning_rate": 3.9996609809602875e-06,
      "loss": 5.4979,
      "num_input_tokens_seen": 376048,
      "step": 30,
      "train_runtime": 319.9242,
      "train_tokens_per_second": 1175.428
    },
    {
      "epoch": 0.007073565076798707,
      "grad_norm": 29.468177795410156,
      "learning_rate": 3.99953400491946e-06,
      "loss": 5.2443,
      "num_input_tokens_seen": 438016,
      "step": 35,
      "train_runtime": 372.5887,
      "train_tokens_per_second": 1175.602
    },
    {
      "epoch": 0.008084074373484237,
      "grad_norm": 23.354333877563477,
      "learning_rate": 3.999386877312612e-06,
      "loss": 4.3395,
      "num_input_tokens_seen": 500992,
      "step": 40,
      "train_runtime": 426.0641,
      "train_tokens_per_second": 1175.861
    },
    {
      "epoch": 0.009094583670169765,
      "grad_norm": 28.282201766967773,
      "learning_rate": 3.9992195996225135e-06,
      "loss": 4.0471,
      "num_input_tokens_seen": 563120,
      "step": 45,
      "train_runtime": 478.871,
      "train_tokens_per_second": 1175.933
    },
    {
      "epoch": 0.010105092966855295,
      "grad_norm": 18.143774032592773,
      "learning_rate": 3.999032173535012e-06,
      "loss": 3.4529,
      "num_input_tokens_seen": 626336,
      "step": 50,
      "train_runtime": 532.5301,
      "train_tokens_per_second": 1176.151
    },
    {
      "epoch": 0.011115602263540825,
      "grad_norm": 14.894266128540039,
      "learning_rate": 3.998824600939013e-06,
      "loss": 3.3163,
      "num_input_tokens_seen": 689616,
      "step": 55,
      "train_runtime": 586.1945,
      "train_tokens_per_second": 1176.429
    },
    {
      "epoch": 0.012126111560226353,
      "grad_norm": 12.097404479980469,
      "learning_rate": 3.998596883926459e-06,
      "loss": 2.9471,
      "num_input_tokens_seen": 752400,
      "step": 60,
      "train_runtime": 639.5771,
      "train_tokens_per_second": 1176.402
    },
    {
      "epoch": 0.013136620856911883,
      "grad_norm": 10.424253463745117,
      "learning_rate": 3.998349024792312e-06,
      "loss": 2.4595,
      "num_input_tokens_seen": 815760,
      "step": 65,
      "train_runtime": 693.2525,
      "train_tokens_per_second": 1176.714
    },
    {
      "epoch": 0.014147130153597413,
      "grad_norm": 10.263360977172852,
      "learning_rate": 3.9980810260345305e-06,
      "loss": 2.3361,
      "num_input_tokens_seen": 877968,
      "step": 70,
      "train_runtime": 746.1862,
      "train_tokens_per_second": 1176.607
    },
    {
      "epoch": 0.015157639450282943,
      "grad_norm": 11.694735527038574,
      "learning_rate": 3.997792890354039e-06,
      "loss": 2.0035,
      "num_input_tokens_seen": 941408,
      "step": 75,
      "train_runtime": 799.9046,
      "train_tokens_per_second": 1176.9
    },
    {
      "epoch": 0.016168148746968473,
      "grad_norm": 8.173532485961914,
      "learning_rate": 3.997484620654707e-06,
      "loss": 2.1344,
      "num_input_tokens_seen": 1003664,
      "step": 80,
      "train_runtime": 852.8194,
      "train_tokens_per_second": 1176.878
    },
    {
      "epoch": 0.017178658043654,
      "grad_norm": 6.884355545043945,
      "learning_rate": 3.9971562200433186e-06,
      "loss": 1.7256,
      "num_input_tokens_seen": 1066112,
      "step": 85,
      "train_runtime": 905.912,
      "train_tokens_per_second": 1176.838
    },
    {
      "epoch": 0.01818916734033953,
      "grad_norm": 6.904550552368164,
      "learning_rate": 3.996807691829537e-06,
      "loss": 1.5091,
      "num_input_tokens_seen": 1129520,
      "step": 90,
      "train_runtime": 959.7742,
      "train_tokens_per_second": 1176.86
    },
    {
      "epoch": 0.01919967663702506,
      "grad_norm": 5.4079365730285645,
      "learning_rate": 3.9964390395258756e-06,
      "loss": 1.2946,
      "num_input_tokens_seen": 1192496,
      "step": 95,
      "train_runtime": 1013.305,
      "train_tokens_per_second": 1176.838
    },
    {
      "epoch": 0.02021018593371059,
      "grad_norm": 8.377837181091309,
      "learning_rate": 3.996050266847659e-06,
      "loss": 1.2065,
      "num_input_tokens_seen": 1255568,
      "step": 100,
      "train_runtime": 1066.8876,
      "train_tokens_per_second": 1176.851
    },
    {
      "epoch": 0.02122069523039612,
      "grad_norm": 5.166053771972656,
      "learning_rate": 3.9956413777129925e-06,
      "loss": 1.206,
      "num_input_tokens_seen": 1318256,
      "step": 105,
      "train_runtime": 1120.6722,
      "train_tokens_per_second": 1176.308
    },
    {
      "epoch": 0.02223120452708165,
      "grad_norm": 4.854104518890381,
      "learning_rate": 3.995212376242712e-06,
      "loss": 1.127,
      "num_input_tokens_seen": 1380864,
      "step": 110,
      "train_runtime": 1173.8549,
      "train_tokens_per_second": 1176.35
    },
    {
      "epoch": 0.02324171382376718,
      "grad_norm": 5.08274507522583,
      "learning_rate": 3.994763266760352e-06,
      "loss": 1.03,
      "num_input_tokens_seen": 1443840,
      "step": 115,
      "train_runtime": 1227.2256,
      "train_tokens_per_second": 1176.507
    },
    {
      "epoch": 0.024252223120452707,
      "grad_norm": 4.332393646240234,
      "learning_rate": 3.994294053792095e-06,
      "loss": 1.069,
      "num_input_tokens_seen": 1506384,
      "step": 120,
      "train_runtime": 1280.4372,
      "train_tokens_per_second": 1176.461
    },
    {
      "epoch": 0.025262732417138237,
      "grad_norm": 7.843504428863525,
      "learning_rate": 3.993804742066733e-06,
      "loss": 0.8232,
      "num_input_tokens_seen": 1568640,
      "step": 125,
      "train_runtime": 1333.5627,
      "train_tokens_per_second": 1176.278
    },
    {
      "epoch": 0.026273241713823767,
      "grad_norm": 4.397436618804932,
      "learning_rate": 3.993295336515612e-06,
      "loss": 1.0445,
      "num_input_tokens_seen": 1629984,
      "step": 130,
      "train_runtime": 1385.7976,
      "train_tokens_per_second": 1176.206
    },
    {
      "epoch": 0.027283751010509297,
      "grad_norm": 5.203807353973389,
      "learning_rate": 3.992765842272589e-06,
      "loss": 0.9583,
      "num_input_tokens_seen": 1692880,
      "step": 135,
      "train_runtime": 1439.1726,
      "train_tokens_per_second": 1176.287
    },
    {
      "epoch": 0.028294260307194827,
      "grad_norm": 5.966144561767578,
      "learning_rate": 3.992216264673977e-06,
      "loss": 0.7914,
      "num_input_tokens_seen": 1754960,
      "step": 140,
      "train_runtime": 1492.0815,
      "train_tokens_per_second": 1176.182
    },
    {
      "epoch": 0.029304769603880357,
      "grad_norm": 5.567983627319336,
      "learning_rate": 3.991646609258489e-06,
      "loss": 0.7615,
      "num_input_tokens_seen": 1818576,
      "step": 145,
      "train_runtime": 1546.0952,
      "train_tokens_per_second": 1176.238
    },
    {
      "epoch": 0.030315278900565887,
      "grad_norm": 4.188170909881592,
      "learning_rate": 3.991056881767189e-06,
      "loss": 0.8504,
      "num_input_tokens_seen": 1880640,
      "step": 150,
      "train_runtime": 1598.8863,
      "train_tokens_per_second": 1176.219
    },
    {
      "epoch": 0.03132578819725142,
      "grad_norm": 4.86121129989624,
      "learning_rate": 3.990447088143427e-06,
      "loss": 0.923,
      "num_input_tokens_seen": 1942960,
      "step": 155,
      "train_runtime": 1651.9553,
      "train_tokens_per_second": 1176.158
    },
    {
      "epoch": 0.03233629749393695,
      "grad_norm": 2.985060453414917,
      "learning_rate": 3.989817234532783e-06,
      "loss": 0.6843,
      "num_input_tokens_seen": 2005200,
      "step": 160,
      "train_runtime": 1704.8097,
      "train_tokens_per_second": 1176.202
    },
    {
      "epoch": 0.03334680679062248,
      "grad_norm": 3.5929648876190186,
      "learning_rate": 3.989167327283005e-06,
      "loss": 0.6831,
      "num_input_tokens_seen": 2068416,
      "step": 165,
      "train_runtime": 1758.5082,
      "train_tokens_per_second": 1176.233
    },
    {
      "epoch": 0.034357316087308,
      "grad_norm": 3.8017780780792236,
      "learning_rate": 3.988497372943944e-06,
      "loss": 1.0845,
      "num_input_tokens_seen": 2131328,
      "step": 170,
      "train_runtime": 1811.9291,
      "train_tokens_per_second": 1176.276
    },
    {
      "epoch": 0.03536782538399353,
      "grad_norm": 4.390603542327881,
      "learning_rate": 3.987807378267487e-06,
      "loss": 0.705,
      "num_input_tokens_seen": 2193616,
      "step": 175,
      "train_runtime": 1864.9351,
      "train_tokens_per_second": 1176.243
    },
    {
      "epoch": 0.03637833468067906,
      "grad_norm": 4.697729110717773,
      "learning_rate": 3.9870973502074894e-06,
      "loss": 0.7109,
      "num_input_tokens_seen": 2257216,
      "step": 180,
      "train_runtime": 1918.8174,
      "train_tokens_per_second": 1176.358
    },
    {
      "epoch": 0.03738884397736459,
      "grad_norm": 4.934900283813477,
      "learning_rate": 3.98636729591971e-06,
      "loss": 0.6746,
      "num_input_tokens_seen": 2320544,
      "step": 185,
      "train_runtime": 1972.6787,
      "train_tokens_per_second": 1176.342
    },
    {
      "epoch": 0.03839935327405012,
      "grad_norm": 4.316061019897461,
      "learning_rate": 3.98561722276173e-06,
      "loss": 0.7602,
      "num_input_tokens_seen": 2382608,
      "step": 190,
      "train_runtime": 2025.5121,
      "train_tokens_per_second": 1176.299
    },
    {
      "epoch": 0.03940986257073565,
      "grad_norm": 4.053694725036621,
      "learning_rate": 3.984847138292885e-06,
      "loss": 0.7632,
      "num_input_tokens_seen": 2445056,
      "step": 195,
      "train_runtime": 2078.6341,
      "train_tokens_per_second": 1176.28
    },
    {
      "epoch": 0.04042037186742118,
      "grad_norm": 3.211214780807495,
      "learning_rate": 3.984057050274189e-06,
      "loss": 0.6204,
      "num_input_tokens_seen": 2507808,
      "step": 200,
      "train_runtime": 2131.9163,
      "train_tokens_per_second": 1176.316
    },
    {
      "epoch": 0.04143088116410671,
      "grad_norm": 3.677788496017456,
      "learning_rate": 3.98324696666825e-06,
      "loss": 0.7369,
      "num_input_tokens_seen": 2571520,
      "step": 205,
      "train_runtime": 2186.6715,
      "train_tokens_per_second": 1175.997
    },
    {
      "epoch": 0.04244139046079224,
      "grad_norm": 6.4659905433654785,
      "learning_rate": 3.9824168956392e-06,
      "loss": 0.8126,
      "num_input_tokens_seen": 2633552,
      "step": 210,
      "train_runtime": 2239.5392,
      "train_tokens_per_second": 1175.935
    },
    {
      "epoch": 0.04345189975747777,
      "grad_norm": 3.6705574989318848,
      "learning_rate": 3.9815668455526e-06,
      "loss": 0.7262,
      "num_input_tokens_seen": 2696704,
      "step": 215,
      "train_runtime": 2293.2407,
      "train_tokens_per_second": 1175.936
    },
    {
      "epoch": 0.0444624090541633,
      "grad_norm": 4.199230194091797,
      "learning_rate": 3.98069682497537e-06,
      "loss": 0.6968,
      "num_input_tokens_seen": 2759712,
      "step": 220,
      "train_runtime": 2346.8537,
      "train_tokens_per_second": 1175.92
    },
    {
      "epoch": 0.04547291835084883,
      "grad_norm": 4.482109069824219,
      "learning_rate": 3.979806842675688e-06,
      "loss": 0.7374,
      "num_input_tokens_seen": 2822080,
      "step": 225,
      "train_runtime": 2399.9292,
      "train_tokens_per_second": 1175.901
    },
    {
      "epoch": 0.04648342764753436,
      "grad_norm": 2.742821455001831,
      "learning_rate": 3.978896907622915e-06,
      "loss": 0.7831,
      "num_input_tokens_seen": 2884624,
      "step": 230,
      "train_runtime": 2453.0425,
      "train_tokens_per_second": 1175.937
    },
    {
      "epoch": 0.04749393694421989,
      "grad_norm": 4.0307087898254395,
      "learning_rate": 3.977967028987496e-06,
      "loss": 0.5801,
      "num_input_tokens_seen": 2948224,
      "step": 235,
      "train_runtime": 2507.0132,
      "train_tokens_per_second": 1175.991
    },
    {
      "epoch": 0.04850444624090541,
      "grad_norm": 7.223431587219238,
      "learning_rate": 3.9770172161408675e-06,
      "loss": 0.5858,
      "num_input_tokens_seen": 3010896,
      "step": 240,
      "train_runtime": 2560.4311,
      "train_tokens_per_second": 1175.933
    },
    {
      "epoch": 0.04951495553759094,
      "grad_norm": 9.795647621154785,
      "learning_rate": 3.97604747865537e-06,
      "loss": 0.6123,
      "num_input_tokens_seen": 3073568,
      "step": 245,
      "train_runtime": 2613.6527,
      "train_tokens_per_second": 1175.966
    },
    {
      "epoch": 0.05052546483427647,
      "grad_norm": 3.4166038036346436,
      "learning_rate": 3.9750578263041445e-06,
      "loss": 0.6494,
      "num_input_tokens_seen": 3135856,
      "step": 250,
      "train_runtime": 2666.5639,
      "train_tokens_per_second": 1175.991
    },
    {
      "epoch": 0.051535974130962,
      "grad_norm": 4.826728343963623,
      "learning_rate": 3.974048269061036e-06,
      "loss": 0.8759,
      "num_input_tokens_seen": 3198352,
      "step": 255,
      "train_runtime": 2719.7686,
      "train_tokens_per_second": 1175.965
    },
    {
      "epoch": 0.05254648342764753,
      "grad_norm": 3.141911268234253,
      "learning_rate": 3.973018817100498e-06,
      "loss": 0.6397,
      "num_input_tokens_seen": 3260944,
      "step": 260,
      "train_runtime": 2773.0029,
      "train_tokens_per_second": 1175.961
    },
    {
      "epoch": 0.05355699272433306,
      "grad_norm": 3.9760117530822754,
      "learning_rate": 3.9719694807974795e-06,
      "loss": 0.531,
      "num_input_tokens_seen": 3323472,
      "step": 265,
      "train_runtime": 2826.1259,
      "train_tokens_per_second": 1175.982
    },
    {
      "epoch": 0.054567502021018593,
      "grad_norm": 4.025146484375,
      "learning_rate": 3.970900270727329e-06,
      "loss": 0.788,
      "num_input_tokens_seen": 3385536,
      "step": 270,
      "train_runtime": 2878.9242,
      "train_tokens_per_second": 1175.973
    },
    {
      "epoch": 0.055578011317704124,
      "grad_norm": 6.369166851043701,
      "learning_rate": 3.9698111976656895e-06,
      "loss": 0.6924,
      "num_input_tokens_seen": 3448544,
      "step": 275,
      "train_runtime": 2932.3712,
      "train_tokens_per_second": 1176.026
    },
    {
      "epoch": 0.056588520614389654,
      "grad_norm": 5.210278511047363,
      "learning_rate": 3.968702272588379e-06,
      "loss": 0.758,
      "num_input_tokens_seen": 3510736,
      "step": 280,
      "train_runtime": 2985.1752,
      "train_tokens_per_second": 1176.057
    },
    {
      "epoch": 0.057599029911075184,
      "grad_norm": 4.0517497062683105,
      "learning_rate": 3.967573506671291e-06,
      "loss": 0.7342,
      "num_input_tokens_seen": 3573408,
      "step": 285,
      "train_runtime": 3038.3638,
      "train_tokens_per_second": 1176.096
    },
    {
      "epoch": 0.058609539207760714,
      "grad_norm": 3.0932469367980957,
      "learning_rate": 3.966424911290276e-06,
      "loss": 0.7686,
      "num_input_tokens_seen": 3636704,
      "step": 290,
      "train_runtime": 3091.906,
      "train_tokens_per_second": 1176.201
    },
    {
      "epoch": 0.059620048504446244,
      "grad_norm": 3.0981619358062744,
      "learning_rate": 3.96525649802103e-06,
      "loss": 0.696,
      "num_input_tokens_seen": 3698928,
      "step": 295,
      "train_runtime": 3144.8979,
      "train_tokens_per_second": 1176.168
    },
    {
      "epoch": 0.060630557801131774,
      "grad_norm": 7.08219051361084,
      "learning_rate": 3.964068278638973e-06,
      "loss": 0.762,
      "num_input_tokens_seen": 3761792,
      "step": 300,
      "train_runtime": 3198.4598,
      "train_tokens_per_second": 1176.126
    },
    {
      "epoch": 0.0616410670978173,
      "grad_norm": 4.29225492477417,
      "learning_rate": 3.962860265119138e-06,
      "loss": 0.6355,
      "num_input_tokens_seen": 3823280,
      "step": 305,
      "train_runtime": 3251.71,
      "train_tokens_per_second": 1175.775
    },
    {
      "epoch": 0.06265157639450283,
      "grad_norm": 3.84035325050354,
      "learning_rate": 3.961632469636043e-06,
      "loss": 0.6717,
      "num_input_tokens_seen": 3885344,
      "step": 310,
      "train_runtime": 3304.5508,
      "train_tokens_per_second": 1175.756
    },
    {
      "epoch": 0.06366208569118836,
      "grad_norm": 2.8374664783477783,
      "learning_rate": 3.960384904563573e-06,
      "loss": 0.75,
      "num_input_tokens_seen": 3948368,
      "step": 315,
      "train_runtime": 3358.0803,
      "train_tokens_per_second": 1175.781
    },
    {
      "epoch": 0.0646725949878739,
      "grad_norm": 2.5798633098602295,
      "learning_rate": 3.9591175824748504e-06,
      "loss": 0.6298,
      "num_input_tokens_seen": 4010480,
      "step": 320,
      "train_runtime": 3410.8636,
      "train_tokens_per_second": 1175.796
    },
    {
      "epoch": 0.06568310428455942,
      "grad_norm": 4.394871711730957,
      "learning_rate": 3.957830516142115e-06,
      "loss": 0.6946,
      "num_input_tokens_seen": 4072464,
      "step": 325,
      "train_runtime": 3463.7359,
      "train_tokens_per_second": 1175.743
    },
    {
      "epoch": 0.06669361358124495,
      "grad_norm": 3.625187635421753,
      "learning_rate": 3.956523718536589e-06,
      "loss": 0.8503,
      "num_input_tokens_seen": 4135472,
      "step": 330,
      "train_runtime": 3517.4144,
      "train_tokens_per_second": 1175.714
    },
    {
      "epoch": 0.06770412287793048,
      "grad_norm": 3.4414892196655273,
      "learning_rate": 3.955197202828351e-06,
      "loss": 0.7564,
      "num_input_tokens_seen": 4198016,
      "step": 335,
      "train_runtime": 3570.4987,
      "train_tokens_per_second": 1175.751
    },
    {
      "epoch": 0.068714632174616,
      "grad_norm": 3.5158703327178955,
      "learning_rate": 3.9538509823862e-06,
      "loss": 0.6947,
      "num_input_tokens_seen": 4261136,
      "step": 340,
      "train_runtime": 3624.1806,
      "train_tokens_per_second": 1175.752
    },
    {
      "epoch": 0.06972514147130153,
      "grad_norm": 4.094060897827148,
      "learning_rate": 3.952485070777523e-06,
      "loss": 0.7288,
      "num_input_tokens_seen": 4323616,
      "step": 345,
      "train_runtime": 3677.4925,
      "train_tokens_per_second": 1175.697
    },
    {
      "epoch": 0.07073565076798706,
      "grad_norm": 2.998650312423706,
      "learning_rate": 3.951099481768155e-06,
      "loss": 0.5889,
      "num_input_tokens_seen": 4386112,
      "step": 350,
      "train_runtime": 3730.623,
      "train_tokens_per_second": 1175.705
    },
    {
      "epoch": 0.07174616006467259,
      "grad_norm": 6.841073036193848,
      "learning_rate": 3.949694229322245e-06,
      "loss": 0.6522,
      "num_input_tokens_seen": 4449232,
      "step": 355,
      "train_runtime": 3784.3607,
      "train_tokens_per_second": 1175.689
    },
    {
      "epoch": 0.07275666936135812,
      "grad_norm": 4.250971794128418,
      "learning_rate": 3.94826932760211e-06,
      "loss": 0.7199,
      "num_input_tokens_seen": 4511968,
      "step": 360,
      "train_runtime": 3837.7511,
      "train_tokens_per_second": 1175.68
    },
    {
      "epoch": 0.07376717865804365,
      "grad_norm": 3.7069289684295654,
      "learning_rate": 3.946824790968099e-06,
      "loss": 0.4661,
      "num_input_tokens_seen": 4574736,
      "step": 365,
      "train_runtime": 3891.1484,
      "train_tokens_per_second": 1175.678
    },
    {
      "epoch": 0.07477768795472918,
      "grad_norm": 2.9372382164001465,
      "learning_rate": 3.945360633978439e-06,
      "loss": 0.7628,
      "num_input_tokens_seen": 4637088,
      "step": 370,
      "train_runtime": 3944.3946,
      "train_tokens_per_second": 1175.615
    },
    {
      "epoch": 0.07578819725141471,
      "grad_norm": 4.45995569229126,
      "learning_rate": 3.943876871389098e-06,
      "loss": 0.6188,
      "num_input_tokens_seen": 4700208,
      "step": 375,
      "train_runtime": 3997.932,
      "train_tokens_per_second": 1175.66
    },
    {
      "epoch": 0.07679870654810024,
      "grad_norm": 4.243090629577637,
      "learning_rate": 3.94237351815363e-06,
      "loss": 0.6788,
      "num_input_tokens_seen": 4763072,
      "step": 380,
      "train_runtime": 4051.321,
      "train_tokens_per_second": 1175.684
    },
    {
      "epoch": 0.07780921584478577,
      "grad_norm": 4.3541789054870605,
      "learning_rate": 3.940850589423025e-06,
      "loss": 0.7056,
      "num_input_tokens_seen": 4824752,
      "step": 385,
      "train_runtime": 4103.856,
      "train_tokens_per_second": 1175.663
    },
    {
      "epoch": 0.0788197251414713,
      "grad_norm": 2.91888165473938,
      "learning_rate": 3.9393081005455585e-06,
      "loss": 0.7405,
      "num_input_tokens_seen": 4887616,
      "step": 390,
      "train_runtime": 4157.2011,
      "train_tokens_per_second": 1175.699
    },
    {
      "epoch": 0.07983023443815683,
      "grad_norm": 4.1584343910217285,
      "learning_rate": 3.937746067066638e-06,
      "loss": 0.6928,
      "num_input_tokens_seen": 4949904,
      "step": 395,
      "train_runtime": 4210.2934,
      "train_tokens_per_second": 1175.667
    },
    {
      "epoch": 0.08084074373484236,
      "grad_norm": 5.124073505401611,
      "learning_rate": 3.936164504728639e-06,
      "loss": 0.6887,
      "num_input_tokens_seen": 5013344,
      "step": 400,
      "train_runtime": 4264.1053,
      "train_tokens_per_second": 1175.708
    },
    {
      "epoch": 0.08185125303152789,
      "grad_norm": 3.6062448024749756,
      "learning_rate": 3.934563429470755e-06,
      "loss": 0.5829,
      "num_input_tokens_seen": 5076016,
      "step": 405,
      "train_runtime": 4318.1047,
      "train_tokens_per_second": 1175.519
    },
    {
      "epoch": 0.08286176232821342,
      "grad_norm": 7.221596717834473,
      "learning_rate": 3.932942857428834e-06,
      "loss": 0.5491,
      "num_input_tokens_seen": 5138864,
      "step": 410,
      "train_runtime": 4371.6704,
      "train_tokens_per_second": 1175.492
    },
    {
      "epoch": 0.08387227162489895,
      "grad_norm": 4.008368492126465,
      "learning_rate": 3.931302804935213e-06,
      "loss": 0.8809,
      "num_input_tokens_seen": 5201792,
      "step": 415,
      "train_runtime": 4425.178,
      "train_tokens_per_second": 1175.499
    },
    {
      "epoch": 0.08488278092158448,
      "grad_norm": 4.68516731262207,
      "learning_rate": 3.929643288518554e-06,
      "loss": 0.886,
      "num_input_tokens_seen": 5264048,
      "step": 420,
      "train_runtime": 4478.1256,
      "train_tokens_per_second": 1175.503
    },
    {
      "epoch": 0.08589329021827001,
      "grad_norm": 3.966327428817749,
      "learning_rate": 3.927964324903684e-06,
      "loss": 0.6576,
      "num_input_tokens_seen": 5326160,
      "step": 425,
      "train_runtime": 4531.0882,
      "train_tokens_per_second": 1175.47
    },
    {
      "epoch": 0.08690379951495554,
      "grad_norm": 3.4438529014587402,
      "learning_rate": 3.926265931011418e-06,
      "loss": 0.6753,
      "num_input_tokens_seen": 5388544,
      "step": 430,
      "train_runtime": 4584.2145,
      "train_tokens_per_second": 1175.456
    },
    {
      "epoch": 0.08791430881164107,
      "grad_norm": 4.1160407066345215,
      "learning_rate": 3.924548123958391e-06,
      "loss": 0.6905,
      "num_input_tokens_seen": 5451072,
      "step": 435,
      "train_runtime": 4637.4997,
      "train_tokens_per_second": 1175.433
    },
    {
      "epoch": 0.0889248181083266,
      "grad_norm": 3.261730432510376,
      "learning_rate": 3.92281092105689e-06,
      "loss": 0.6303,
      "num_input_tokens_seen": 5513872,
      "step": 440,
      "train_runtime": 4690.8826,
      "train_tokens_per_second": 1175.444
    },
    {
      "epoch": 0.08993532740501213,
      "grad_norm": 3.067660331726074,
      "learning_rate": 3.921054339814672e-06,
      "loss": 0.6763,
      "num_input_tokens_seen": 5578208,
      "step": 445,
      "train_runtime": 4745.3533,
      "train_tokens_per_second": 1175.51
    },
    {
      "epoch": 0.09094583670169766,
      "grad_norm": 3.219339609146118,
      "learning_rate": 3.919278397934793e-06,
      "loss": 0.6665,
      "num_input_tokens_seen": 5640096,
      "step": 450,
      "train_runtime": 4798.1144,
      "train_tokens_per_second": 1175.482
    },
    {
      "epoch": 0.09195634599838319,
      "grad_norm": 3.8605051040649414,
      "learning_rate": 3.91748311331543e-06,
      "loss": 0.556,
      "num_input_tokens_seen": 5702000,
      "step": 455,
      "train_runtime": 4850.9234,
      "train_tokens_per_second": 1175.446
    },
    {
      "epoch": 0.09296685529506872,
      "grad_norm": 4.062769889831543,
      "learning_rate": 3.915668504049695e-06,
      "loss": 0.6183,
      "num_input_tokens_seen": 5765456,
      "step": 460,
      "train_runtime": 4904.7265,
      "train_tokens_per_second": 1175.49
    },
    {
      "epoch": 0.09397736459175425,
      "grad_norm": 3.631878137588501,
      "learning_rate": 3.913834588425461e-06,
      "loss": 0.7662,
      "num_input_tokens_seen": 5827632,
      "step": 465,
      "train_runtime": 4957.7662,
      "train_tokens_per_second": 1175.455
    },
    {
      "epoch": 0.09498787388843978,
      "grad_norm": 3.3980908393859863,
      "learning_rate": 3.911981384925167e-06,
      "loss": 0.7666,
      "num_input_tokens_seen": 5890768,
      "step": 470,
      "train_runtime": 5011.3643,
      "train_tokens_per_second": 1175.482
    },
    {
      "epoch": 0.0959983831851253,
      "grad_norm": 4.615910053253174,
      "learning_rate": 3.910108912225642e-06,
      "loss": 0.6372,
      "num_input_tokens_seen": 5953152,
      "step": 475,
      "train_runtime": 5064.4029,
      "train_tokens_per_second": 1175.489
    },
    {
      "epoch": 0.09700889248181083,
      "grad_norm": 4.117048263549805,
      "learning_rate": 3.908217189197913e-06,
      "loss": 0.6836,
      "num_input_tokens_seen": 6015952,
      "step": 480,
      "train_runtime": 5117.7263,
      "train_tokens_per_second": 1175.513
    },
    {
      "epoch": 0.09801940177849636,
      "grad_norm": 2.587526798248291,
      "learning_rate": 3.906306234907011e-06,
      "loss": 0.5558,
      "num_input_tokens_seen": 6079312,
      "step": 485,
      "train_runtime": 5171.6248,
      "train_tokens_per_second": 1175.513
    },
    {
      "epoch": 0.09902991107518189,
      "grad_norm": 2.878868341445923,
      "learning_rate": 3.904376068611785e-06,
      "loss": 0.6917,
      "num_input_tokens_seen": 6142048,
      "step": 490,
      "train_runtime": 5224.822,
      "train_tokens_per_second": 1175.552
    },
    {
      "epoch": 0.10004042037186742,
      "grad_norm": 3.7121775150299072,
      "learning_rate": 3.902426709764704e-06,
      "loss": 0.7244,
      "num_input_tokens_seen": 6205056,
      "step": 495,
      "train_runtime": 5278.4447,
      "train_tokens_per_second": 1175.546
    },
    {
      "epoch": 0.10105092966855295,
      "grad_norm": 3.2476165294647217,
      "learning_rate": 3.900458178011663e-06,
      "loss": 0.692,
      "num_input_tokens_seen": 6267344,
      "step": 500,
      "train_runtime": 5331.4099,
      "train_tokens_per_second": 1175.551
    },
    {
      "epoch": 0.10206143896523848,
      "grad_norm": 3.7049877643585205,
      "learning_rate": 3.8984704931917814e-06,
      "loss": 0.6409,
      "num_input_tokens_seen": 6330496,
      "step": 505,
      "train_runtime": 5385.8304,
      "train_tokens_per_second": 1175.398
    },
    {
      "epoch": 0.103071948261924,
      "grad_norm": 2.9748308658599854,
      "learning_rate": 3.896463675337209e-06,
      "loss": 0.5669,
      "num_input_tokens_seen": 6392816,
      "step": 510,
      "train_runtime": 5439.0532,
      "train_tokens_per_second": 1175.355
    },
    {
      "epoch": 0.10408245755860954,
      "grad_norm": 8.324978828430176,
      "learning_rate": 3.894437744672919e-06,
      "loss": 0.901,
      "num_input_tokens_seen": 6454528,
      "step": 515,
      "train_runtime": 5491.6597,
      "train_tokens_per_second": 1175.333
    },
    {
      "epoch": 0.10509296685529507,
      "grad_norm": 4.74837589263916,
      "learning_rate": 3.892392721616506e-06,
      "loss": 0.5989,
      "num_input_tokens_seen": 6517904,
      "step": 520,
      "train_runtime": 5545.6185,
      "train_tokens_per_second": 1175.325
    },
    {
      "epoch": 0.1061034761519806,
      "grad_norm": 4.0597453117370605,
      "learning_rate": 3.890328626777981e-06,
      "loss": 0.7055,
      "num_input_tokens_seen": 6579648,
      "step": 525,
      "train_runtime": 5598.4901,
      "train_tokens_per_second": 1175.254
    },
    {
      "epoch": 0.10711398544866613,
      "grad_norm": 3.591029644012451,
      "learning_rate": 3.888245480959564e-06,
      "loss": 0.6084,
      "num_input_tokens_seen": 6643296,
      "step": 530,
      "train_runtime": 5652.4411,
      "train_tokens_per_second": 1175.297
    },
    {
      "epoch": 0.10812449474535166,
      "grad_norm": 4.3003621101379395,
      "learning_rate": 3.886143305155469e-06,
      "loss": 0.7044,
      "num_input_tokens_seen": 6705600,
      "step": 535,
      "train_runtime": 5705.3372,
      "train_tokens_per_second": 1175.321
    },
    {
      "epoch": 0.10913500404203719,
      "grad_norm": 5.13422966003418,
      "learning_rate": 3.884022120551702e-06,
      "loss": 0.6145,
      "num_input_tokens_seen": 6768736,
      "step": 540,
      "train_runtime": 5758.9229,
      "train_tokens_per_second": 1175.348
    },
    {
      "epoch": 0.11014551333872272,
      "grad_norm": 3.058093547821045,
      "learning_rate": 3.881881948525839e-06,
      "loss": 0.5508,
      "num_input_tokens_seen": 6832432,
      "step": 545,
      "train_runtime": 5812.9076,
      "train_tokens_per_second": 1175.39
    },
    {
      "epoch": 0.11115602263540825,
      "grad_norm": 35.32107925415039,
      "learning_rate": 3.879722810646814e-06,
      "loss": 0.5848,
      "num_input_tokens_seen": 6896928,
      "step": 550,
      "train_runtime": 5867.4079,
      "train_tokens_per_second": 1175.464
    },
    {
      "epoch": 0.11216653193209378,
      "grad_norm": 2.985271453857422,
      "learning_rate": 3.877544728674701e-06,
      "loss": 0.6638,
      "num_input_tokens_seen": 6958976,
      "step": 555,
      "train_runtime": 5920.2454,
      "train_tokens_per_second": 1175.454
    },
    {
      "epoch": 0.11317704122877931,
      "grad_norm": 5.026381969451904,
      "learning_rate": 3.875347724560498e-06,
      "loss": 0.628,
      "num_input_tokens_seen": 7022448,
      "step": 560,
      "train_runtime": 5974.0867,
      "train_tokens_per_second": 1175.485
    },
    {
      "epoch": 0.11418755052546484,
      "grad_norm": 3.8012847900390625,
      "learning_rate": 3.8731318204458985e-06,
      "loss": 0.6623,
      "num_input_tokens_seen": 7084960,
      "step": 565,
      "train_runtime": 6027.3918,
      "train_tokens_per_second": 1175.46
    },
    {
      "epoch": 0.11519805982215037,
      "grad_norm": 4.237667560577393,
      "learning_rate": 3.870897038663077e-06,
      "loss": 0.6995,
      "num_input_tokens_seen": 7148032,
      "step": 570,
      "train_runtime": 6081.0794,
      "train_tokens_per_second": 1175.454
    },
    {
      "epoch": 0.1162085691188359,
      "grad_norm": 5.4145402908325195,
      "learning_rate": 3.868643401734455e-06,
      "loss": 0.6548,
      "num_input_tokens_seen": 7211216,
      "step": 575,
      "train_runtime": 6134.7501,
      "train_tokens_per_second": 1175.47
    },
    {
      "epoch": 0.11721907841552143,
      "grad_norm": 3.7704644203186035,
      "learning_rate": 3.866370932372484e-06,
      "loss": 0.5756,
      "num_input_tokens_seen": 7274432,
      "step": 580,
      "train_runtime": 6188.4039,
      "train_tokens_per_second": 1175.494
    },
    {
      "epoch": 0.11822958771220696,
      "grad_norm": 4.202622413635254,
      "learning_rate": 3.864079653479407e-06,
      "loss": 0.6073,
      "num_input_tokens_seen": 7337472,
      "step": 585,
      "train_runtime": 6241.955,
      "train_tokens_per_second": 1175.509
    },
    {
      "epoch": 0.11924009700889249,
      "grad_norm": 4.806297302246094,
      "learning_rate": 3.861769588147032e-06,
      "loss": 0.6611,
      "num_input_tokens_seen": 7401232,
      "step": 590,
      "train_runtime": 6296.0624,
      "train_tokens_per_second": 1175.533
    },
    {
      "epoch": 0.12025060630557802,
      "grad_norm": 3.2422499656677246,
      "learning_rate": 3.859440759656502e-06,
      "loss": 0.6208,
      "num_input_tokens_seen": 7463856,
      "step": 595,
      "train_runtime": 6349.2317,
      "train_tokens_per_second": 1175.553
    },
    {
      "epoch": 0.12126111560226355,
      "grad_norm": 3.7372372150421143,
      "learning_rate": 3.857093191478056e-06,
      "loss": 0.5971,
      "num_input_tokens_seen": 7525728,
      "step": 600,
      "train_runtime": 6402.1793,
      "train_tokens_per_second": 1175.495
    },
    {
      "epoch": 0.12227162489894908,
      "grad_norm": 4.844786643981934,
      "learning_rate": 3.854726907270794e-06,
      "loss": 0.7342,
      "num_input_tokens_seen": 7588480,
      "step": 605,
      "train_runtime": 6456.2431,
      "train_tokens_per_second": 1175.371
    },
    {
      "epoch": 0.1232821341956346,
      "grad_norm": 2.865542411804199,
      "learning_rate": 3.852341930882438e-06,
      "loss": 0.5571,
      "num_input_tokens_seen": 7651072,
      "step": 610,
      "train_runtime": 6509.4123,
      "train_tokens_per_second": 1175.386
    },
    {
      "epoch": 0.12429264349232012,
      "grad_norm": 4.772171497344971,
      "learning_rate": 3.849938286349093e-06,
      "loss": 0.7672,
      "num_input_tokens_seen": 7712992,
      "step": 615,
      "train_runtime": 6562.2116,
      "train_tokens_per_second": 1175.365
    },
    {
      "epoch": 0.12530315278900567,
      "grad_norm": 3.2261109352111816,
      "learning_rate": 3.847515997895006e-06,
      "loss": 0.6542,
      "num_input_tokens_seen": 7775632,
      "step": 620,
      "train_runtime": 6615.7051,
      "train_tokens_per_second": 1175.329
    },
    {
      "epoch": 0.1263136620856912,
      "grad_norm": 3.8147804737091064,
      "learning_rate": 3.845075089932314e-06,
      "loss": 0.5159,
      "num_input_tokens_seen": 7838400,
      "step": 625,
      "train_runtime": 6669.0896,
      "train_tokens_per_second": 1175.333
    },
    {
      "epoch": 0.12732417138237673,
      "grad_norm": 3.1391756534576416,
      "learning_rate": 3.842615587060812e-06,
      "loss": 0.5643,
      "num_input_tokens_seen": 7902096,
      "step": 630,
      "train_runtime": 6723.3141,
      "train_tokens_per_second": 1175.328
    },
    {
      "epoch": 0.12833468067906226,
      "grad_norm": 3.7617239952087402,
      "learning_rate": 3.840137514067688e-06,
      "loss": 0.614,
      "num_input_tokens_seen": 7965392,
      "step": 635,
      "train_runtime": 6777.0235,
      "train_tokens_per_second": 1175.353
    },
    {
      "epoch": 0.1293451899757478,
      "grad_norm": 4.292144775390625,
      "learning_rate": 3.837640895927291e-06,
      "loss": 0.7284,
      "num_input_tokens_seen": 8028672,
      "step": 640,
      "train_runtime": 6830.7819,
      "train_tokens_per_second": 1175.366
    },
    {
      "epoch": 0.13035569927243332,
      "grad_norm": 8.08123779296875,
      "learning_rate": 3.835125757800863e-06,
      "loss": 0.6209,
      "num_input_tokens_seen": 8090864,
      "step": 645,
      "train_runtime": 6883.6433,
      "train_tokens_per_second": 1175.375
    },
    {
      "epoch": 0.13136620856911885,
      "grad_norm": 4.01224422454834,
      "learning_rate": 3.832592125036298e-06,
      "loss": 0.7537,
      "num_input_tokens_seen": 8154432,
      "step": 650,
      "train_runtime": 6937.4831,
      "train_tokens_per_second": 1175.416
    },
    {
      "epoch": 0.13237671786580438,
      "grad_norm": 4.9284186363220215,
      "learning_rate": 3.830040023167879e-06,
      "loss": 0.664,
      "num_input_tokens_seen": 8217360,
      "step": 655,
      "train_runtime": 6991.0151,
      "train_tokens_per_second": 1175.417
    },
    {
      "epoch": 0.1333872271624899,
      "grad_norm": 3.9275057315826416,
      "learning_rate": 3.827469477916022e-06,
      "loss": 0.6216,
      "num_input_tokens_seen": 8279328,
      "step": 660,
      "train_runtime": 7043.735,
      "train_tokens_per_second": 1175.417
    },
    {
      "epoch": 0.13439773645917544,
      "grad_norm": 10.51347827911377,
      "learning_rate": 3.824880515187022e-06,
      "loss": 0.7498,
      "num_input_tokens_seen": 8341984,
      "step": 665,
      "train_runtime": 7097.0739,
      "train_tokens_per_second": 1175.412
    },
    {
      "epoch": 0.13540824575586097,
      "grad_norm": 3.3341922760009766,
      "learning_rate": 3.822273161072782e-06,
      "loss": 0.6286,
      "num_input_tokens_seen": 8404224,
      "step": 670,
      "train_runtime": 7149.849,
      "train_tokens_per_second": 1175.441
    },
    {
      "epoch": 0.13641875505254647,
      "grad_norm": 3.1467573642730713,
      "learning_rate": 3.819647441850561e-06,
      "loss": 0.511,
      "num_input_tokens_seen": 8467744,
      "step": 675,
      "train_runtime": 7203.6965,
      "train_tokens_per_second": 1175.472
    },
    {
      "epoch": 0.137429264349232,
      "grad_norm": 7.691067695617676,
      "learning_rate": 3.8170033839827015e-06,
      "loss": 0.4942,
      "num_input_tokens_seen": 8529216,
      "step": 680,
      "train_runtime": 7256.0867,
      "train_tokens_per_second": 1175.457
    },
    {
      "epoch": 0.13843977364591753,
      "grad_norm": 4.036873817443848,
      "learning_rate": 3.8143410141163645e-06,
      "loss": 0.7301,
      "num_input_tokens_seen": 8590928,
      "step": 685,
      "train_runtime": 7308.7452,
      "train_tokens_per_second": 1175.431
    },
    {
      "epoch": 0.13945028294260306,
      "grad_norm": 3.5859339237213135,
      "learning_rate": 3.811660359083263e-06,
      "loss": 0.6492,
      "num_input_tokens_seen": 8652432,
      "step": 690,
      "train_runtime": 7361.2642,
      "train_tokens_per_second": 1175.4
    },
    {
      "epoch": 0.1404607922392886,
      "grad_norm": 3.209474802017212,
      "learning_rate": 3.808961445899391e-06,
      "loss": 0.5925,
      "num_input_tokens_seen": 8715696,
      "step": 695,
      "train_runtime": 7414.9517,
      "train_tokens_per_second": 1175.422
    },
    {
      "epoch": 0.14147130153597412,
      "grad_norm": 3.4691197872161865,
      "learning_rate": 3.8062443017647494e-06,
      "loss": 0.4582,
      "num_input_tokens_seen": 8778944,
      "step": 700,
      "train_runtime": 7468.6767,
      "train_tokens_per_second": 1175.435
    },
    {
      "epoch": 0.14248181083265965,
      "grad_norm": 4.440786838531494,
      "learning_rate": 3.8035089540630724e-06,
      "loss": 0.6605,
      "num_input_tokens_seen": 8842416,
      "step": 705,
      "train_runtime": 7523.1148,
      "train_tokens_per_second": 1175.366
    },
    {
      "epoch": 0.14349232012934518,
      "grad_norm": 2.776029348373413,
      "learning_rate": 3.800755430361554e-06,
      "loss": 0.8152,
      "num_input_tokens_seen": 8906528,
      "step": 710,
      "train_runtime": 7577.3573,
      "train_tokens_per_second": 1175.413
    },
    {
      "epoch": 0.1445028294260307,
      "grad_norm": 3.4379076957702637,
      "learning_rate": 3.7979837584105667e-06,
      "loss": 0.5439,
      "num_input_tokens_seen": 8969088,
      "step": 715,
      "train_runtime": 7630.5191,
      "train_tokens_per_second": 1175.423
    },
    {
      "epoch": 0.14551333872271624,
      "grad_norm": 3.3745479583740234,
      "learning_rate": 3.795193966143384e-06,
      "loss": 0.5481,
      "num_input_tokens_seen": 9032320,
      "step": 720,
      "train_runtime": 7684.1626,
      "train_tokens_per_second": 1175.446
    },
    {
      "epoch": 0.14652384801940177,
      "grad_norm": 3.8883936405181885,
      "learning_rate": 3.7923860816758985e-06,
      "loss": 0.6939,
      "num_input_tokens_seen": 9095456,
      "step": 725,
      "train_runtime": 7737.859,
      "train_tokens_per_second": 1175.449
    },
    {
      "epoch": 0.1475343573160873,
      "grad_norm": 3.570350408554077,
      "learning_rate": 3.789560133306339e-06,
      "loss": 0.5407,
      "num_input_tokens_seen": 9157712,
      "step": 730,
      "train_runtime": 7790.7234,
      "train_tokens_per_second": 1175.464
    },
    {
      "epoch": 0.14854486661277283,
      "grad_norm": 3.236635684967041,
      "learning_rate": 3.7867161495149826e-06,
      "loss": 0.5728,
      "num_input_tokens_seen": 9219904,
      "step": 735,
      "train_runtime": 7843.5895,
      "train_tokens_per_second": 1175.47
    },
    {
      "epoch": 0.14955537590945836,
      "grad_norm": 2.7600464820861816,
      "learning_rate": 3.783854158963872e-06,
      "loss": 0.4969,
      "num_input_tokens_seen": 9282592,
      "step": 740,
      "train_runtime": 7896.7382,
      "train_tokens_per_second": 1175.497
    },
    {
      "epoch": 0.1505658852061439,
      "grad_norm": 3.2668344974517822,
      "learning_rate": 3.7809741904965227e-06,
      "loss": 0.6154,
      "num_input_tokens_seen": 9344784,
      "step": 745,
      "train_runtime": 7949.7729,
      "train_tokens_per_second": 1175.478
    },
    {
      "epoch": 0.15157639450282942,
      "grad_norm": 2.528810501098633,
      "learning_rate": 3.7780762731376353e-06,
      "loss": 0.5897,
      "num_input_tokens_seen": 9408208,
      "step": 750,
      "train_runtime": 8003.5005,
      "train_tokens_per_second": 1175.512
    },
    {
      "epoch": 0.15258690379951495,
      "grad_norm": 3.496053457260132,
      "learning_rate": 3.7751604360928013e-06,
      "loss": 0.6698,
      "num_input_tokens_seen": 9469552,
      "step": 755,
      "train_runtime": 8055.8775,
      "train_tokens_per_second": 1175.484
    },
    {
      "epoch": 0.15359741309620048,
      "grad_norm": 3.210442304611206,
      "learning_rate": 3.7722267087482083e-06,
      "loss": 0.4517,
      "num_input_tokens_seen": 9531744,
      "step": 760,
      "train_runtime": 8108.7135,
      "train_tokens_per_second": 1175.494
    },
    {
      "epoch": 0.154607922392886,
      "grad_norm": 2.834822177886963,
      "learning_rate": 3.7692751206703453e-06,
      "loss": 0.6037,
      "num_input_tokens_seen": 9594448,
      "step": 765,
      "train_runtime": 8162.0216,
      "train_tokens_per_second": 1175.499
    },
    {
      "epoch": 0.15561843168957154,
      "grad_norm": 4.160487651824951,
      "learning_rate": 3.766305701605705e-06,
      "loss": 0.5804,
      "num_input_tokens_seen": 9656384,
      "step": 770,
      "train_runtime": 8214.7872,
      "train_tokens_per_second": 1175.488
    },
    {
      "epoch": 0.15662894098625707,
      "grad_norm": 3.5465452671051025,
      "learning_rate": 3.763318481480482e-06,
      "loss": 0.5664,
      "num_input_tokens_seen": 9719136,
      "step": 775,
      "train_runtime": 8268.0857,
      "train_tokens_per_second": 1175.5
    },
    {
      "epoch": 0.1576394502829426,
      "grad_norm": 3.319720983505249,
      "learning_rate": 3.7603134904002724e-06,
      "loss": 0.6785,
      "num_input_tokens_seen": 9783280,
      "step": 780,
      "train_runtime": 8322.2677,
      "train_tokens_per_second": 1175.555
    },
    {
      "epoch": 0.15864995957962813,
      "grad_norm": 2.4884696006774902,
      "learning_rate": 3.7572907586497706e-06,
      "loss": 0.7208,
      "num_input_tokens_seen": 9846448,
      "step": 785,
      "train_runtime": 8375.9924,
      "train_tokens_per_second": 1175.556
    },
    {
      "epoch": 0.15966046887631366,
      "grad_norm": 3.03959059715271,
      "learning_rate": 3.7542503166924634e-06,
      "loss": 0.6304,
      "num_input_tokens_seen": 9908928,
      "step": 790,
      "train_runtime": 8428.9795,
      "train_tokens_per_second": 1175.579
    },
    {
      "epoch": 0.1606709781729992,
      "grad_norm": 3.878382682800293,
      "learning_rate": 3.7511921951703247e-06,
      "loss": 0.722,
      "num_input_tokens_seen": 9972016,
      "step": 795,
      "train_runtime": 8482.6064,
      "train_tokens_per_second": 1175.584
    },
    {
      "epoch": 0.16168148746968472,
      "grad_norm": 2.792785167694092,
      "learning_rate": 3.748116424903503e-06,
      "loss": 0.6119,
      "num_input_tokens_seen": 10033296,
      "step": 800,
      "train_runtime": 8534.9799,
      "train_tokens_per_second": 1175.55
    },
    {
      "epoch": 0.16269199676637025,
      "grad_norm": 3.632317543029785,
      "learning_rate": 3.745023036890016e-06,
      "loss": 0.725,
      "num_input_tokens_seen": 10095872,
      "step": 805,
      "train_runtime": 8588.8665,
      "train_tokens_per_second": 1175.46
    },
    {
      "epoch": 0.16370250606305578,
      "grad_norm": 4.489703178405762,
      "learning_rate": 3.741912062305433e-06,
      "loss": 0.5536,
      "num_input_tokens_seen": 10159184,
      "step": 810,
      "train_runtime": 8642.6408,
      "train_tokens_per_second": 1175.472
    },
    {
      "epoch": 0.1647130153597413,
      "grad_norm": 3.8489625453948975,
      "learning_rate": 3.7387835325025634e-06,
      "loss": 0.6998,
      "num_input_tokens_seen": 10221888,
      "step": 815,
      "train_runtime": 8695.9768,
      "train_tokens_per_second": 1175.473
    },
    {
      "epoch": 0.16572352465642684,
      "grad_norm": 2.4502532482147217,
      "learning_rate": 3.7356374790111427e-06,
      "loss": 0.881,
      "num_input_tokens_seen": 10284512,
      "step": 820,
      "train_runtime": 8749.0334,
      "train_tokens_per_second": 1175.503
    },
    {
      "epoch": 0.16673403395311237,
      "grad_norm": 3.332329273223877,
      "learning_rate": 3.7324739335375094e-06,
      "loss": 0.7103,
      "num_input_tokens_seen": 10347360,
      "step": 825,
      "train_runtime": 8802.2588,
      "train_tokens_per_second": 1175.535
    },
    {
      "epoch": 0.1677445432497979,
      "grad_norm": 4.031126499176025,
      "learning_rate": 3.7292929279642906e-06,
      "loss": 0.6177,
      "num_input_tokens_seen": 10409680,
      "step": 830,
      "train_runtime": 8855.3204,
      "train_tokens_per_second": 1175.528
    },
    {
      "epoch": 0.16875505254648343,
      "grad_norm": 6.556054592132568,
      "learning_rate": 3.726094494350077e-06,
      "loss": 0.7118,
      "num_input_tokens_seen": 10472640,
      "step": 835,
      "train_runtime": 8908.6362,
      "train_tokens_per_second": 1175.56
    },
    {
      "epoch": 0.16976556184316896,
      "grad_norm": 3.4012272357940674,
      "learning_rate": 3.7228786649291033e-06,
      "loss": 0.4893,
      "num_input_tokens_seen": 10534624,
      "step": 840,
      "train_runtime": 8961.3919,
      "train_tokens_per_second": 1175.557
    },
    {
      "epoch": 0.1707760711398545,
      "grad_norm": 3.267211437225342,
      "learning_rate": 3.719645472110919e-06,
      "loss": 0.6717,
      "num_input_tokens_seen": 10597248,
      "step": 845,
      "train_runtime": 9014.6568,
      "train_tokens_per_second": 1175.558
    },
    {
      "epoch": 0.17178658043654002,
      "grad_norm": 2.536677122116089,
      "learning_rate": 3.716394948480066e-06,
      "loss": 0.669,
      "num_input_tokens_seen": 10659152,
      "step": 850,
      "train_runtime": 9067.1521,
      "train_tokens_per_second": 1175.579
    },
    {
      "epoch": 0.17279708973322555,
      "grad_norm": 2.4148473739624023,
      "learning_rate": 3.713127126795748e-06,
      "loss": 0.6337,
      "num_input_tokens_seen": 10721904,
      "step": 855,
      "train_runtime": 9120.7249,
      "train_tokens_per_second": 1175.554
    },
    {
      "epoch": 0.17380759902991108,
      "grad_norm": 2.883779525756836,
      "learning_rate": 3.7098420399915005e-06,
      "loss": 0.5397,
      "num_input_tokens_seen": 10785440,
      "step": 860,
      "train_runtime": 9174.5407,
      "train_tokens_per_second": 1175.584
    },
    {
      "epoch": 0.1748181083265966,
      "grad_norm": 4.7265305519104,
      "learning_rate": 3.7065397211748598e-06,
      "loss": 0.7211,
      "num_input_tokens_seen": 10848384,
      "step": 865,
      "train_runtime": 9228.026,
      "train_tokens_per_second": 1175.591
    },
    {
      "epoch": 0.17582861762328214,
      "grad_norm": 3.5837161540985107,
      "learning_rate": 3.703220203627028e-06,
      "loss": 0.6669,
      "num_input_tokens_seen": 10910592,
      "step": 870,
      "train_runtime": 9280.9985,
      "train_tokens_per_second": 1175.584
    },
    {
      "epoch": 0.17683912691996767,
      "grad_norm": 2.7159736156463623,
      "learning_rate": 3.6998835208025377e-06,
      "loss": 0.6623,
      "num_input_tokens_seen": 10973536,
      "step": 875,
      "train_runtime": 9334.4287,
      "train_tokens_per_second": 1175.598
    },
    {
      "epoch": 0.1778496362166532,
      "grad_norm": 7.89189338684082,
      "learning_rate": 3.6965297063289165e-06,
      "loss": 0.5993,
      "num_input_tokens_seen": 11036576,
      "step": 880,
      "train_runtime": 9387.788,
      "train_tokens_per_second": 1175.631
    },
    {
      "epoch": 0.17886014551333873,
      "grad_norm": 4.693795680999756,
      "learning_rate": 3.693158794006347e-06,
      "loss": 0.5558,
      "num_input_tokens_seen": 11098848,
      "step": 885,
      "train_runtime": 9440.7278,
      "train_tokens_per_second": 1175.635
    },
    {
      "epoch": 0.17987065481002426,
      "grad_norm": 2.7113873958587646,
      "learning_rate": 3.689770817807325e-06,
      "loss": 0.467,
      "num_input_tokens_seen": 11161456,
      "step": 890,
      "train_runtime": 9493.864,
      "train_tokens_per_second": 1175.649
    },
    {
      "epoch": 0.1808811641067098,
      "grad_norm": 3.3604936599731445,
      "learning_rate": 3.6863658118763205e-06,
      "loss": 0.5255,
      "num_input_tokens_seen": 11224848,
      "step": 895,
      "train_runtime": 9547.4086,
      "train_tokens_per_second": 1175.696
    },
    {
      "epoch": 0.18189167340339532,
      "grad_norm": 3.4218239784240723,
      "learning_rate": 3.682943810529429e-06,
      "loss": 0.6028,
      "num_input_tokens_seen": 11287264,
      "step": 900,
      "train_runtime": 9600.4034,
      "train_tokens_per_second": 1175.707
    },
    {
      "epoch": 0.18290218270008085,
      "grad_norm": 2.316255569458008,
      "learning_rate": 3.6795048482540293e-06,
      "loss": 0.8297,
      "num_input_tokens_seen": 11348992,
      "step": 905,
      "train_runtime": 9653.5741,
      "train_tokens_per_second": 1175.626
    },
    {
      "epoch": 0.18391269199676638,
      "grad_norm": 2.9446609020233154,
      "learning_rate": 3.6760489597084337e-06,
      "loss": 0.5778,
      "num_input_tokens_seen": 11412208,
      "step": 910,
      "train_runtime": 9707.2006,
      "train_tokens_per_second": 1175.644
    },
    {
      "epoch": 0.1849232012934519,
      "grad_norm": 3.060992479324341,
      "learning_rate": 3.672576179721541e-06,
      "loss": 0.715,
      "num_input_tokens_seen": 11474752,
      "step": 915,
      "train_runtime": 9760.3459,
      "train_tokens_per_second": 1175.65
    },
    {
      "epoch": 0.18593371059013744,
      "grad_norm": 4.147984504699707,
      "learning_rate": 3.6690865432924836e-06,
      "loss": 0.6041,
      "num_input_tokens_seen": 11537552,
      "step": 920,
      "train_runtime": 9813.5933,
      "train_tokens_per_second": 1175.67
    },
    {
      "epoch": 0.18694421988682297,
      "grad_norm": 2.423511028289795,
      "learning_rate": 3.665580085590275e-06,
      "loss": 0.5344,
      "num_input_tokens_seen": 11601120,
      "step": 925,
      "train_runtime": 9867.4107,
      "train_tokens_per_second": 1175.701
    },
    {
      "epoch": 0.1879547291835085,
      "grad_norm": 3.281507968902588,
      "learning_rate": 3.662056841953456e-06,
      "loss": 0.5915,
      "num_input_tokens_seen": 11663216,
      "step": 930,
      "train_runtime": 9920.3592,
      "train_tokens_per_second": 1175.685
    },
    {
      "epoch": 0.18896523848019403,
      "grad_norm": 2.8211944103240967,
      "learning_rate": 3.6585168478897382e-06,
      "loss": 0.7089,
      "num_input_tokens_seen": 11725168,
      "step": 935,
      "train_runtime": 9973.2116,
      "train_tokens_per_second": 1175.666
    },
    {
      "epoch": 0.18997574777687956,
      "grad_norm": 3.6956236362457275,
      "learning_rate": 3.654960139075646e-06,
      "loss": 0.5789,
      "num_input_tokens_seen": 11788336,
      "step": 940,
      "train_runtime": 10026.8498,
      "train_tokens_per_second": 1175.677
    },
    {
      "epoch": 0.19098625707356506,
      "grad_norm": 2.933264970779419,
      "learning_rate": 3.651386751356158e-06,
      "loss": 0.7485,
      "num_input_tokens_seen": 11851504,
      "step": 945,
      "train_runtime": 10080.5519,
      "train_tokens_per_second": 1175.68
    },
    {
      "epoch": 0.1919967663702506,
      "grad_norm": 2.6265976428985596,
      "learning_rate": 3.6477967207443438e-06,
      "loss": 0.4652,
      "num_input_tokens_seen": 11914544,
      "step": 950,
      "train_runtime": 10134.0182,
      "train_tokens_per_second": 1175.698
    },
    {
      "epoch": 0.19300727566693612,
      "grad_norm": 3.192777156829834,
      "learning_rate": 3.6441900834210024e-06,
      "loss": 0.677,
      "num_input_tokens_seen": 11976768,
      "step": 955,
      "train_runtime": 10187.0086,
      "train_tokens_per_second": 1175.69
    },
    {
      "epoch": 0.19401778496362165,
      "grad_norm": 2.874342441558838,
      "learning_rate": 3.6405668757342994e-06,
      "loss": 0.5958,
      "num_input_tokens_seen": 12039344,
      "step": 960,
      "train_runtime": 10240.2459,
      "train_tokens_per_second": 1175.689
    },
    {
      "epoch": 0.19502829426030718,
      "grad_norm": 3.3972768783569336,
      "learning_rate": 3.636927134199396e-06,
      "loss": 0.4645,
      "num_input_tokens_seen": 12102544,
      "step": 965,
      "train_runtime": 10293.8713,
      "train_tokens_per_second": 1175.704
    },
    {
      "epoch": 0.1960388035569927,
      "grad_norm": 3.401434898376465,
      "learning_rate": 3.6332708954980854e-06,
      "loss": 0.6209,
      "num_input_tokens_seen": 12164928,
      "step": 970,
      "train_runtime": 10347.1266,
      "train_tokens_per_second": 1175.682
    },
    {
      "epoch": 0.19704931285367824,
      "grad_norm": 2.834341526031494,
      "learning_rate": 3.6295981964784205e-06,
      "loss": 0.5899,
      "num_input_tokens_seen": 12229056,
      "step": 975,
      "train_runtime": 10401.5073,
      "train_tokens_per_second": 1175.7
    },
    {
      "epoch": 0.19805982215036377,
      "grad_norm": 2.825404644012451,
      "learning_rate": 3.6259090741543446e-06,
      "loss": 0.698,
      "num_input_tokens_seen": 12292160,
      "step": 980,
      "train_runtime": 10455.0535,
      "train_tokens_per_second": 1175.715
    },
    {
      "epoch": 0.1990703314470493,
      "grad_norm": 3.0747578144073486,
      "learning_rate": 3.622203565705316e-06,
      "loss": 0.4878,
      "num_input_tokens_seen": 12354544,
      "step": 985,
      "train_runtime": 10508.1027,
      "train_tokens_per_second": 1175.716
    },
    {
      "epoch": 0.20008084074373483,
      "grad_norm": 3.192239761352539,
      "learning_rate": 3.618481708475934e-06,
      "loss": 0.5052,
      "num_input_tokens_seen": 12417408,
      "step": 990,
      "train_runtime": 10561.4965,
      "train_tokens_per_second": 1175.724
    },
    {
      "epoch": 0.20109135004042036,
      "grad_norm": 2.8916831016540527,
      "learning_rate": 3.6147435399755663e-06,
      "loss": 0.6432,
      "num_input_tokens_seen": 12480320,
      "step": 995,
      "train_runtime": 10614.9819,
      "train_tokens_per_second": 1175.727
    },
    {
      "epoch": 0.2021018593371059,
      "grad_norm": 3.8152198791503906,
      "learning_rate": 3.610989097877963e-06,
      "loss": 0.7236,
      "num_input_tokens_seen": 12543584,
      "step": 1000,
      "train_runtime": 10668.5354,
      "train_tokens_per_second": 1175.755
    },
    {
      "epoch": 0.20311236863379142,
      "grad_norm": 5.640397548675537,
      "learning_rate": 3.607218420020886e-06,
      "loss": 0.7032,
      "num_input_tokens_seen": 12605776,
      "step": 1005,
      "train_runtime": 10722.1586,
      "train_tokens_per_second": 1175.675
    },
    {
      "epoch": 0.20412287793047695,
      "grad_norm": 3.0265555381774902,
      "learning_rate": 3.60343154440572e-06,
      "loss": 0.6757,
      "num_input_tokens_seen": 12668784,
      "step": 1010,
      "train_runtime": 10775.6583,
      "train_tokens_per_second": 1175.685
    },
    {
      "epoch": 0.20513338722716248,
      "grad_norm": 3.357038736343384,
      "learning_rate": 3.5996285091970955e-06,
      "loss": 0.6012,
      "num_input_tokens_seen": 12731792,
      "step": 1015,
      "train_runtime": 10829.1353,
      "train_tokens_per_second": 1175.698
    },
    {
      "epoch": 0.206143896523848,
      "grad_norm": 4.051517963409424,
      "learning_rate": 3.595809352722499e-06,
      "loss": 0.7505,
      "num_input_tokens_seen": 12794896,
      "step": 1020,
      "train_runtime": 10882.6817,
      "train_tokens_per_second": 1175.712
    },
    {
      "epoch": 0.20715440582053354,
      "grad_norm": 3.3321402072906494,
      "learning_rate": 3.591974113471892e-06,
      "loss": 0.5411,
      "num_input_tokens_seen": 12857840,
      "step": 1025,
      "train_runtime": 10936.2861,
      "train_tokens_per_second": 1175.704
    },
    {
      "epoch": 0.20816491511721907,
      "grad_norm": 3.9247918128967285,
      "learning_rate": 3.5881228300973173e-06,
      "loss": 0.5092,
      "num_input_tokens_seen": 12920528,
      "step": 1030,
      "train_runtime": 10989.4806,
      "train_tokens_per_second": 1175.718
    },
    {
      "epoch": 0.2091754244139046,
      "grad_norm": 3.577371120452881,
      "learning_rate": 3.5842555414125153e-06,
      "loss": 0.6226,
      "num_input_tokens_seen": 12982720,
      "step": 1035,
      "train_runtime": 11042.3939,
      "train_tokens_per_second": 1175.716
    },
    {
      "epoch": 0.21018593371059013,
      "grad_norm": 3.741654396057129,
      "learning_rate": 3.580372286392528e-06,
      "loss": 0.7058,
      "num_input_tokens_seen": 13045264,
      "step": 1040,
      "train_runtime": 11095.4763,
      "train_tokens_per_second": 1175.728
    },
    {
      "epoch": 0.21119644300727566,
      "grad_norm": 4.182802200317383,
      "learning_rate": 3.576473104173308e-06,
      "loss": 0.5577,
      "num_input_tokens_seen": 13108352,
      "step": 1045,
      "train_runtime": 11149.0271,
      "train_tokens_per_second": 1175.74
    },
    {
      "epoch": 0.2122069523039612,
      "grad_norm": 3.5830905437469482,
      "learning_rate": 3.572558034051327e-06,
      "loss": 0.6708,
      "num_input_tokens_seen": 13171952,
      "step": 1050,
      "train_runtime": 11202.9267,
      "train_tokens_per_second": 1175.76
    },
    {
      "epoch": 0.21321746160064672,
      "grad_norm": 2.489022970199585,
      "learning_rate": 3.5686271154831745e-06,
      "loss": 0.5781,
      "num_input_tokens_seen": 13234576,
      "step": 1055,
      "train_runtime": 11256.0087,
      "train_tokens_per_second": 1175.779
    },
    {
      "epoch": 0.21422797089733225,
      "grad_norm": 2.8714654445648193,
      "learning_rate": 3.564680388085164e-06,
      "loss": 0.6473,
      "num_input_tokens_seen": 13298592,
      "step": 1060,
      "train_runtime": 11310.3153,
      "train_tokens_per_second": 1175.793
    },
    {
      "epoch": 0.21523848019401778,
      "grad_norm": 4.018681049346924,
      "learning_rate": 3.5607178916329312e-06,
      "loss": 0.7162,
      "num_input_tokens_seen": 13361088,
      "step": 1065,
      "train_runtime": 11363.2902,
      "train_tokens_per_second": 1175.812
    },
    {
      "epoch": 0.21624898949070331,
      "grad_norm": 2.992124319076538,
      "learning_rate": 3.556739666061035e-06,
      "loss": 0.6906,
      "num_input_tokens_seen": 13424304,
      "step": 1070,
      "train_runtime": 11416.9832,
      "train_tokens_per_second": 1175.819
    },
    {
      "epoch": 0.21725949878738884,
      "grad_norm": 3.898655652999878,
      "learning_rate": 3.5527457514625557e-06,
      "loss": 0.6976,
      "num_input_tokens_seen": 13486864,
      "step": 1075,
      "train_runtime": 11470.073,
      "train_tokens_per_second": 1175.831
    },
    {
      "epoch": 0.21827000808407437,
      "grad_norm": 5.196657657623291,
      "learning_rate": 3.548736188088688e-06,
      "loss": 0.7294,
      "num_input_tokens_seen": 13550032,
      "step": 1080,
      "train_runtime": 11523.6412,
      "train_tokens_per_second": 1175.846
    },
    {
      "epoch": 0.2192805173807599,
      "grad_norm": 2.9031929969787598,
      "learning_rate": 3.544711016348338e-06,
      "loss": 0.6109,
      "num_input_tokens_seen": 13613824,
      "step": 1085,
      "train_runtime": 11577.531,
      "train_tokens_per_second": 1175.883
    },
    {
      "epoch": 0.22029102667744543,
      "grad_norm": 4.772411823272705,
      "learning_rate": 3.540670276807714e-06,
      "loss": 0.6002,
      "num_input_tokens_seen": 13675296,
      "step": 1090,
      "train_runtime": 11629.939,
      "train_tokens_per_second": 1175.87
    },
    {
      "epoch": 0.22130153597413096,
      "grad_norm": 2.9537243843078613,
      "learning_rate": 3.5366140101899196e-06,
      "loss": 0.562,
      "num_input_tokens_seen": 13738080,
      "step": 1095,
      "train_runtime": 11683.1602,
      "train_tokens_per_second": 1175.887
    },
    {
      "epoch": 0.2223120452708165,
      "grad_norm": 3.263195753097534,
      "learning_rate": 3.5325422573745414e-06,
      "loss": 0.6024,
      "num_input_tokens_seen": 13800480,
      "step": 1100,
      "train_runtime": 11736.1673,
      "train_tokens_per_second": 1175.893
    },
    {
      "epoch": 0.22332255456750202,
      "grad_norm": 3.7250351905822754,
      "learning_rate": 3.5284550593972393e-06,
      "loss": 0.6128,
      "num_input_tokens_seen": 13862400,
      "step": 1105,
      "train_runtime": 11789.4604,
      "train_tokens_per_second": 1175.83
    },
    {
      "epoch": 0.22433306386418755,
      "grad_norm": 4.057899475097656,
      "learning_rate": 3.5243524574493293e-06,
      "loss": 0.6269,
      "num_input_tokens_seen": 13926432,
      "step": 1110,
      "train_runtime": 11843.5481,
      "train_tokens_per_second": 1175.867
    },
    {
      "epoch": 0.22534357316087308,
      "grad_norm": 3.4912497997283936,
      "learning_rate": 3.5202344928773736e-06,
      "loss": 0.5975,
      "num_input_tokens_seen": 13989376,
      "step": 1115,
      "train_runtime": 11897.1076,
      "train_tokens_per_second": 1175.864
    },
    {
      "epoch": 0.22635408245755861,
      "grad_norm": 3.279301166534424,
      "learning_rate": 3.5161012071827587e-06,
      "loss": 0.6421,
      "num_input_tokens_seen": 14051056,
      "step": 1120,
      "train_runtime": 11949.7241,
      "train_tokens_per_second": 1175.848
    },
    {
      "epoch": 0.22736459175424414,
      "grad_norm": 3.8593833446502686,
      "learning_rate": 3.5119526420212804e-06,
      "loss": 0.5632,
      "num_input_tokens_seen": 14113488,
      "step": 1125,
      "train_runtime": 12002.854,
      "train_tokens_per_second": 1175.844
    },
    {
      "epoch": 0.22837510105092967,
      "grad_norm": 3.911926031112671,
      "learning_rate": 3.507788839202722e-06,
      "loss": 0.6126,
      "num_input_tokens_seen": 14176320,
      "step": 1130,
      "train_runtime": 12056.1334,
      "train_tokens_per_second": 1175.86
    },
    {
      "epoch": 0.2293856103476152,
      "grad_norm": 3.1790895462036133,
      "learning_rate": 3.5036098406904347e-06,
      "loss": 0.5801,
      "num_input_tokens_seen": 14238976,
      "step": 1135,
      "train_runtime": 12109.2555,
      "train_tokens_per_second": 1175.875
    },
    {
      "epoch": 0.23039611964430073,
      "grad_norm": 3.5457937717437744,
      "learning_rate": 3.499415688600915e-06,
      "loss": 0.7558,
      "num_input_tokens_seen": 14301296,
      "step": 1140,
      "train_runtime": 12162.1938,
      "train_tokens_per_second": 1175.881
    },
    {
      "epoch": 0.23140662894098626,
      "grad_norm": 3.34647798538208,
      "learning_rate": 3.4952064252033755e-06,
      "loss": 0.5368,
      "num_input_tokens_seen": 14363872,
      "step": 1145,
      "train_runtime": 12215.3716,
      "train_tokens_per_second": 1175.885
    },
    {
      "epoch": 0.2324171382376718,
      "grad_norm": 2.229672908782959,
      "learning_rate": 3.490982092919327e-06,
      "loss": 0.5478,
      "num_input_tokens_seen": 14426816,
      "step": 1150,
      "train_runtime": 12268.7504,
      "train_tokens_per_second": 1175.899
    },
    {
      "epoch": 0.23342764753435732,
      "grad_norm": 3.071789026260376,
      "learning_rate": 3.486742734322144e-06,
      "loss": 0.7167,
      "num_input_tokens_seen": 14488336,
      "step": 1155,
      "train_runtime": 12321.1941,
      "train_tokens_per_second": 1175.887
    },
    {
      "epoch": 0.23443815683104285,
      "grad_norm": 3.550621509552002,
      "learning_rate": 3.4824883921366378e-06,
      "loss": 0.6356,
      "num_input_tokens_seen": 14550880,
      "step": 1160,
      "train_runtime": 12374.199,
      "train_tokens_per_second": 1175.905
    },
    {
      "epoch": 0.23544866612772838,
      "grad_norm": 2.445793867111206,
      "learning_rate": 3.478219109238628e-06,
      "loss": 0.6336,
      "num_input_tokens_seen": 14613248,
      "step": 1165,
      "train_runtime": 12427.1084,
      "train_tokens_per_second": 1175.917
    },
    {
      "epoch": 0.23645917542441391,
      "grad_norm": 2.403684377670288,
      "learning_rate": 3.4739349286545074e-06,
      "loss": 0.713,
      "num_input_tokens_seen": 14677152,
      "step": 1170,
      "train_runtime": 12481.2955,
      "train_tokens_per_second": 1175.932
    },
    {
      "epoch": 0.23746968472109944,
      "grad_norm": 3.2226784229278564,
      "learning_rate": 3.4696358935608094e-06,
      "loss": 0.6477,
      "num_input_tokens_seen": 14740528,
      "step": 1175,
      "train_runtime": 12535.1148,
      "train_tokens_per_second": 1175.939
    },
    {
      "epoch": 0.23848019401778497,
      "grad_norm": 3.2362639904022217,
      "learning_rate": 3.4653220472837744e-06,
      "loss": 0.7539,
      "num_input_tokens_seen": 14803200,
      "step": 1180,
      "train_runtime": 12588.319,
      "train_tokens_per_second": 1175.947
    },
    {
      "epoch": 0.2394907033144705,
      "grad_norm": 2.7904305458068848,
      "learning_rate": 3.4609934332989113e-06,
      "loss": 0.5938,
      "num_input_tokens_seen": 14866480,
      "step": 1185,
      "train_runtime": 12642.0102,
      "train_tokens_per_second": 1175.959
    },
    {
      "epoch": 0.24050121261115603,
      "grad_norm": 3.561581611633301,
      "learning_rate": 3.456650095230559e-06,
      "loss": 0.6628,
      "num_input_tokens_seen": 14929440,
      "step": 1190,
      "train_runtime": 12695.4001,
      "train_tokens_per_second": 1175.972
    },
    {
      "epoch": 0.24151172190784156,
      "grad_norm": 2.9090638160705566,
      "learning_rate": 3.4522920768514494e-06,
      "loss": 0.6825,
      "num_input_tokens_seen": 14990816,
      "step": 1195,
      "train_runtime": 12747.6755,
      "train_tokens_per_second": 1175.965
    },
    {
      "epoch": 0.2425222312045271,
      "grad_norm": 7.100903511047363,
      "learning_rate": 3.447919422082262e-06,
      "loss": 0.5584,
      "num_input_tokens_seen": 15053216,
      "step": 1200,
      "train_runtime": 12800.7069,
      "train_tokens_per_second": 1175.968
    },
    {
      "epoch": 0.24353274050121262,
      "grad_norm": 3.0111007690429688,
      "learning_rate": 3.4435321749911863e-06,
      "loss": 0.6087,
      "num_input_tokens_seen": 15116208,
      "step": 1205,
      "train_runtime": 12855.1854,
      "train_tokens_per_second": 1175.884
    },
    {
      "epoch": 0.24454324979789815,
      "grad_norm": 3.1876559257507324,
      "learning_rate": 3.4391303797934734e-06,
      "loss": 0.7613,
      "num_input_tokens_seen": 15179488,
      "step": 1210,
      "train_runtime": 12908.8121,
      "train_tokens_per_second": 1175.901
    },
    {
      "epoch": 0.24555375909458366,
      "grad_norm": 10.577621459960938,
      "learning_rate": 3.4347140808509934e-06,
      "loss": 0.809,
      "num_input_tokens_seen": 15244112,
      "step": 1215,
      "train_runtime": 12963.2969,
      "train_tokens_per_second": 1175.944
    },
    {
      "epoch": 0.2465642683912692,
      "grad_norm": 3.007319450378418,
      "learning_rate": 3.4302833226717863e-06,
      "loss": 0.4833,
      "num_input_tokens_seen": 15305680,
      "step": 1220,
      "train_runtime": 13015.8283,
      "train_tokens_per_second": 1175.928
    },
    {
      "epoch": 0.24757477768795472,
      "grad_norm": 3.259730577468872,
      "learning_rate": 3.425838149909613e-06,
      "loss": 0.7113,
      "num_input_tokens_seen": 15369376,
      "step": 1225,
      "train_runtime": 13069.5541,
      "train_tokens_per_second": 1175.968
    },
    {
      "epoch": 0.24858528698464025,
      "grad_norm": 3.703536033630371,
      "learning_rate": 3.42137860736351e-06,
      "loss": 0.7355,
      "num_input_tokens_seen": 15432720,
      "step": 1230,
      "train_runtime": 13123.2951,
      "train_tokens_per_second": 1175.979
    },
    {
      "epoch": 0.24959579628132578,
      "grad_norm": 3.165496587753296,
      "learning_rate": 3.4169047399773305e-06,
      "loss": 0.7578,
      "num_input_tokens_seen": 15495808,
      "step": 1235,
      "train_runtime": 13176.7078,
      "train_tokens_per_second": 1176.0
    },
    {
      "epoch": 0.25060630557801133,
      "grad_norm": 3.476715087890625,
      "learning_rate": 3.412416592839298e-06,
      "loss": 0.5447,
      "num_input_tokens_seen": 15557760,
      "step": 1240,
      "train_runtime": 13229.429,
      "train_tokens_per_second": 1175.996
    },
    {
      "epoch": 0.25161681487469684,
      "grad_norm": 3.8323416709899902,
      "learning_rate": 3.4079142111815467e-06,
      "loss": 0.5981,
      "num_input_tokens_seen": 15621056,
      "step": 1245,
      "train_runtime": 13283.0524,
      "train_tokens_per_second": 1176.014
    },
    {
      "epoch": 0.2526273241713824,
      "grad_norm": 3.1489956378936768,
      "learning_rate": 3.403397640379671e-06,
      "loss": 0.6588,
      "num_input_tokens_seen": 15683456,
      "step": 1250,
      "train_runtime": 13336.0807,
      "train_tokens_per_second": 1176.017
    },
    {
      "epoch": 0.2536378334680679,
      "grad_norm": 4.806820869445801,
      "learning_rate": 3.398866925952263e-06,
      "loss": 0.6006,
      "num_input_tokens_seen": 15746528,
      "step": 1255,
      "train_runtime": 13389.6169,
      "train_tokens_per_second": 1176.025
    },
    {
      "epoch": 0.25464834276475345,
      "grad_norm": 3.9655725955963135,
      "learning_rate": 3.3943221135604583e-06,
      "loss": 0.6275,
      "num_input_tokens_seen": 15809872,
      "step": 1260,
      "train_runtime": 13443.266,
      "train_tokens_per_second": 1176.044
    },
    {
      "epoch": 0.25565885206143896,
      "grad_norm": 3.6766741275787354,
      "learning_rate": 3.389763249007472e-06,
      "loss": 0.4685,
      "num_input_tokens_seen": 15873696,
      "step": 1265,
      "train_runtime": 13497.0925,
      "train_tokens_per_second": 1176.083
    },
    {
      "epoch": 0.2566693613581245,
      "grad_norm": 2.3900187015533447,
      "learning_rate": 3.3851903782381372e-06,
      "loss": 0.6212,
      "num_input_tokens_seen": 15936224,
      "step": 1270,
      "train_runtime": 13550.169,
      "train_tokens_per_second": 1176.09
    },
    {
      "epoch": 0.25767987065481,
      "grad_norm": 2.8502635955810547,
      "learning_rate": 3.3806035473384482e-06,
      "loss": 0.5108,
      "num_input_tokens_seen": 15999632,
      "step": 1275,
      "train_runtime": 13603.7815,
      "train_tokens_per_second": 1176.117
    },
    {
      "epoch": 0.2586903799514956,
      "grad_norm": 3.9885141849517822,
      "learning_rate": 3.3760028025350874e-06,
      "loss": 0.5347,
      "num_input_tokens_seen": 16061296,
      "step": 1280,
      "train_runtime": 13656.2858,
      "train_tokens_per_second": 1176.11
    },
    {
      "epoch": 0.2597008892481811,
      "grad_norm": 3.030423164367676,
      "learning_rate": 3.3713881901949643e-06,
      "loss": 0.5688,
      "num_input_tokens_seen": 16124368,
      "step": 1285,
      "train_runtime": 13709.6759,
      "train_tokens_per_second": 1176.131
    },
    {
      "epoch": 0.26071139854486663,
      "grad_norm": 4.418292999267578,
      "learning_rate": 3.3667597568247477e-06,
      "loss": 0.7579,
      "num_input_tokens_seen": 16187328,
      "step": 1290,
      "train_runtime": 13763.0068,
      "train_tokens_per_second": 1176.148
    },
    {
      "epoch": 0.26172190784155214,
      "grad_norm": 11.091780662536621,
      "learning_rate": 3.3621175490703965e-06,
      "loss": 0.5622,
      "num_input_tokens_seen": 16251072,
      "step": 1295,
      "train_runtime": 13817.0225,
      "train_tokens_per_second": 1176.163
    },
    {
      "epoch": 0.2627324171382377,
      "grad_norm": 4.871023178100586,
      "learning_rate": 3.3574616137166886e-06,
      "loss": 0.6312,
      "num_input_tokens_seen": 16312992,
      "step": 1300,
      "train_runtime": 13869.736,
      "train_tokens_per_second": 1176.157
    },
    {
      "epoch": 0.2637429264349232,
      "grad_norm": 3.1373586654663086,
      "learning_rate": 3.3527919976867523e-06,
      "loss": 0.565,
      "num_input_tokens_seen": 16375136,
      "step": 1305,
      "train_runtime": 13923.1711,
      "train_tokens_per_second": 1176.107
    },
    {
      "epoch": 0.26475343573160875,
      "grad_norm": 2.8557615280151367,
      "learning_rate": 3.3481087480415907e-06,
      "loss": 0.535,
      "num_input_tokens_seen": 16438512,
      "step": 1310,
      "train_runtime": 13976.9154,
      "train_tokens_per_second": 1176.119
    },
    {
      "epoch": 0.26576394502829426,
      "grad_norm": 3.038393259048462,
      "learning_rate": 3.343411911979607e-06,
      "loss": 0.5921,
      "num_input_tokens_seen": 16500640,
      "step": 1315,
      "train_runtime": 14029.8591,
      "train_tokens_per_second": 1176.109
    },
    {
      "epoch": 0.2667744543249798,
      "grad_norm": 4.269800662994385,
      "learning_rate": 3.338701536836132e-06,
      "loss": 0.6115,
      "num_input_tokens_seen": 16563056,
      "step": 1320,
      "train_runtime": 14082.8773,
      "train_tokens_per_second": 1176.113
    },
    {
      "epoch": 0.2677849636216653,
      "grad_norm": 3.7320048809051514,
      "learning_rate": 3.3339776700829447e-06,
      "loss": 0.5833,
      "num_input_tokens_seen": 16624624,
      "step": 1325,
      "train_runtime": 14135.3053,
      "train_tokens_per_second": 1176.106
    },
    {
      "epoch": 0.2687954729183509,
      "grad_norm": 3.948378801345825,
      "learning_rate": 3.3292403593277926e-06,
      "loss": 0.6707,
      "num_input_tokens_seen": 16687520,
      "step": 1330,
      "train_runtime": 14188.6177,
      "train_tokens_per_second": 1176.12
    },
    {
      "epoch": 0.2698059822150364,
      "grad_norm": 4.018688678741455,
      "learning_rate": 3.324489652313916e-06,
      "loss": 0.6397,
      "num_input_tokens_seen": 16749984,
      "step": 1335,
      "train_runtime": 14241.5693,
      "train_tokens_per_second": 1176.133
    },
    {
      "epoch": 0.27081649151172194,
      "grad_norm": 2.8655548095703125,
      "learning_rate": 3.3197255969195634e-06,
      "loss": 0.6215,
      "num_input_tokens_seen": 16812896,
      "step": 1340,
      "train_runtime": 14294.9117,
      "train_tokens_per_second": 1176.145
    },
    {
      "epoch": 0.27182700080840744,
      "grad_norm": 6.769258499145508,
      "learning_rate": 3.3149482411575102e-06,
      "loss": 0.5628,
      "num_input_tokens_seen": 16875040,
      "step": 1345,
      "train_runtime": 14347.6585,
      "train_tokens_per_second": 1176.153
    },
    {
      "epoch": 0.27283751010509294,
      "grad_norm": 4.8859710693359375,
      "learning_rate": 3.3101576331745744e-06,
      "loss": 0.5276,
      "num_input_tokens_seen": 16937008,
      "step": 1350,
      "train_runtime": 14400.2112,
      "train_tokens_per_second": 1176.164
    },
    {
      "epoch": 0.2738480194017785,
      "grad_norm": 6.371273517608643,
      "learning_rate": 3.305353821251131e-06,
      "loss": 0.6151,
      "num_input_tokens_seen": 16999536,
      "step": 1355,
      "train_runtime": 14453.2894,
      "train_tokens_per_second": 1176.171
    },
    {
      "epoch": 0.274858528698464,
      "grad_norm": 3.7256035804748535,
      "learning_rate": 3.3005368538006276e-06,
      "loss": 0.6502,
      "num_input_tokens_seen": 17062064,
      "step": 1360,
      "train_runtime": 14506.4095,
      "train_tokens_per_second": 1176.174
    },
    {
      "epoch": 0.27586903799514956,
      "grad_norm": 3.665360689163208,
      "learning_rate": 3.2957067793690935e-06,
      "loss": 0.6473,
      "num_input_tokens_seen": 17125728,
      "step": 1365,
      "train_runtime": 14560.2313,
      "train_tokens_per_second": 1176.199
    },
    {
      "epoch": 0.27687954729183506,
      "grad_norm": 3.610779047012329,
      "learning_rate": 3.290863646634653e-06,
      "loss": 0.6201,
      "num_input_tokens_seen": 17188912,
      "step": 1370,
      "train_runtime": 14613.7362,
      "train_tokens_per_second": 1176.216
    },
    {
      "epoch": 0.2778900565885206,
      "grad_norm": 3.2647628784179688,
      "learning_rate": 3.2860075044070333e-06,
      "loss": 0.6332,
      "num_input_tokens_seen": 17251504,
      "step": 1375,
      "train_runtime": 14666.9516,
      "train_tokens_per_second": 1176.216
    },
    {
      "epoch": 0.2789005658852061,
      "grad_norm": 5.023301601409912,
      "learning_rate": 3.281138401627072e-06,
      "loss": 0.7268,
      "num_input_tokens_seen": 17314528,
      "step": 1380,
      "train_runtime": 14720.3909,
      "train_tokens_per_second": 1176.227
    },
    {
      "epoch": 0.2799110751818917,
      "grad_norm": 7.296985149383545,
      "learning_rate": 3.276256387366227e-06,
      "loss": 0.5492,
      "num_input_tokens_seen": 17377744,
      "step": 1385,
      "train_runtime": 14774.1225,
      "train_tokens_per_second": 1176.228
    },
    {
      "epoch": 0.2809215844785772,
      "grad_norm": 3.9270238876342773,
      "learning_rate": 3.271361510826077e-06,
      "loss": 0.5729,
      "num_input_tokens_seen": 17440928,
      "step": 1390,
      "train_runtime": 14827.634,
      "train_tokens_per_second": 1176.245
    },
    {
      "epoch": 0.28193209377526274,
      "grad_norm": 2.933479070663452,
      "learning_rate": 3.2664538213378315e-06,
      "loss": 0.5473,
      "num_input_tokens_seen": 17504224,
      "step": 1395,
      "train_runtime": 14881.0911,
      "train_tokens_per_second": 1176.273
    },
    {
      "epoch": 0.28294260307194824,
      "grad_norm": 3.4271483421325684,
      "learning_rate": 3.261533368361828e-06,
      "loss": 0.5622,
      "num_input_tokens_seen": 17568016,
      "step": 1400,
      "train_runtime": 14935.0243,
      "train_tokens_per_second": 1176.296
    },
    {
      "epoch": 0.2839531123686338,
      "grad_norm": 3.9711341857910156,
      "learning_rate": 3.2566002014870364e-06,
      "loss": 0.5999,
      "num_input_tokens_seen": 17631152,
      "step": 1405,
      "train_runtime": 14989.4123,
      "train_tokens_per_second": 1176.24
    },
    {
      "epoch": 0.2849636216653193,
      "grad_norm": 2.528247356414795,
      "learning_rate": 3.2516543704305612e-06,
      "loss": 0.7426,
      "num_input_tokens_seen": 17693888,
      "step": 1410,
      "train_runtime": 15042.5914,
      "train_tokens_per_second": 1176.253
    },
    {
      "epoch": 0.28597413096200486,
      "grad_norm": 2.85998272895813,
      "learning_rate": 3.246695925037136e-06,
      "loss": 0.553,
      "num_input_tokens_seen": 17756224,
      "step": 1415,
      "train_runtime": 15095.584,
      "train_tokens_per_second": 1176.253
    },
    {
      "epoch": 0.28698464025869036,
      "grad_norm": 3.8059957027435303,
      "learning_rate": 3.241724915278624e-06,
      "loss": 0.6502,
      "num_input_tokens_seen": 17818864,
      "step": 1420,
      "train_runtime": 15148.6407,
      "train_tokens_per_second": 1176.268
    },
    {
      "epoch": 0.2879951495553759,
      "grad_norm": 2.8514206409454346,
      "learning_rate": 3.2367413912535133e-06,
      "loss": 0.6262,
      "num_input_tokens_seen": 17881648,
      "step": 1425,
      "train_runtime": 15201.9627,
      "train_tokens_per_second": 1176.272
    },
    {
      "epoch": 0.2890056588520614,
      "grad_norm": 14.146597862243652,
      "learning_rate": 3.2317454031864133e-06,
      "loss": 0.5802,
      "num_input_tokens_seen": 17945440,
      "step": 1430,
      "train_runtime": 15255.9317,
      "train_tokens_per_second": 1176.293
    },
    {
      "epoch": 0.290016168148747,
      "grad_norm": 3.4294049739837646,
      "learning_rate": 3.226737001427547e-06,
      "loss": 0.6093,
      "num_input_tokens_seen": 18009408,
      "step": 1435,
      "train_runtime": 15310.0191,
      "train_tokens_per_second": 1176.315
    },
    {
      "epoch": 0.2910266774454325,
      "grad_norm": 4.383077144622803,
      "learning_rate": 3.221716236452244e-06,
      "loss": 0.5493,
      "num_input_tokens_seen": 18072160,
      "step": 1440,
      "train_runtime": 15363.1185,
      "train_tokens_per_second": 1176.334
    },
    {
      "epoch": 0.29203718674211804,
      "grad_norm": 3.0703701972961426,
      "learning_rate": 3.2166831588604336e-06,
      "loss": 0.4534,
      "num_input_tokens_seen": 18134208,
      "step": 1445,
      "train_runtime": 15415.9453,
      "train_tokens_per_second": 1176.328
    },
    {
      "epoch": 0.29304769603880354,
      "grad_norm": 3.891867160797119,
      "learning_rate": 3.211637819376131e-06,
      "loss": 0.6931,
      "num_input_tokens_seen": 18196800,
      "step": 1450,
      "train_runtime": 15469.187,
      "train_tokens_per_second": 1176.326
    },
    {
      "epoch": 0.2940582053354891,
      "grad_norm": 7.801604270935059,
      "learning_rate": 3.206580268846929e-06,
      "loss": 0.7404,
      "num_input_tokens_seen": 18259360,
      "step": 1455,
      "train_runtime": 15522.4776,
      "train_tokens_per_second": 1176.317
    },
    {
      "epoch": 0.2950687146321746,
      "grad_norm": 2.6910641193389893,
      "learning_rate": 3.2015105582434865e-06,
      "loss": 0.5767,
      "num_input_tokens_seen": 18321200,
      "step": 1460,
      "train_runtime": 15575.023,
      "train_tokens_per_second": 1176.319
    },
    {
      "epoch": 0.29607922392886016,
      "grad_norm": 3.0482568740844727,
      "learning_rate": 3.1964287386590122e-06,
      "loss": 0.6108,
      "num_input_tokens_seen": 18384864,
      "step": 1465,
      "train_runtime": 15628.9672,
      "train_tokens_per_second": 1176.333
    },
    {
      "epoch": 0.29708973322554566,
      "grad_norm": 3.0077011585235596,
      "learning_rate": 3.191334861308749e-06,
      "loss": 0.5887,
      "num_input_tokens_seen": 18447824,
      "step": 1470,
      "train_runtime": 15682.3178,
      "train_tokens_per_second": 1176.346
    },
    {
      "epoch": 0.2981002425222312,
      "grad_norm": 2.712881565093994,
      "learning_rate": 3.1862289775294626e-06,
      "loss": 0.6058,
      "num_input_tokens_seen": 18511504,
      "step": 1475,
      "train_runtime": 15736.3002,
      "train_tokens_per_second": 1176.357
    },
    {
      "epoch": 0.2991107518189167,
      "grad_norm": 3.1182010173797607,
      "learning_rate": 3.1811111387789187e-06,
      "loss": 0.6563,
      "num_input_tokens_seen": 18574320,
      "step": 1480,
      "train_runtime": 15789.526,
      "train_tokens_per_second": 1176.37
    },
    {
      "epoch": 0.3001212611156023,
      "grad_norm": 2.8583602905273438,
      "learning_rate": 3.175981396635367e-06,
      "loss": 0.5268,
      "num_input_tokens_seen": 18636944,
      "step": 1485,
      "train_runtime": 15842.729,
      "train_tokens_per_second": 1176.372
    },
    {
      "epoch": 0.3011317704122878,
      "grad_norm": 3.007528781890869,
      "learning_rate": 3.170839802797024e-06,
      "loss": 0.5945,
      "num_input_tokens_seen": 18699312,
      "step": 1490,
      "train_runtime": 15895.6923,
      "train_tokens_per_second": 1176.376
    },
    {
      "epoch": 0.30214227970897334,
      "grad_norm": 3.390963315963745,
      "learning_rate": 3.1656864090815453e-06,
      "loss": 0.7351,
      "num_input_tokens_seen": 18761632,
      "step": 1495,
      "train_runtime": 15948.6081,
      "train_tokens_per_second": 1176.381
    },
    {
      "epoch": 0.30315278900565884,
      "grad_norm": 3.0329396724700928,
      "learning_rate": 3.1605212674255094e-06,
      "loss": 0.5939,
      "num_input_tokens_seen": 18825360,
      "step": 1500,
      "train_runtime": 16002.6073,
      "train_tokens_per_second": 1176.393
    },
    {
      "epoch": 0.3041632983023444,
      "grad_norm": 2.7592573165893555,
      "learning_rate": 3.155344429883891e-06,
      "loss": 0.7064,
      "num_input_tokens_seen": 18888192,
      "step": 1505,
      "train_runtime": 16056.7514,
      "train_tokens_per_second": 1176.34
    },
    {
      "epoch": 0.3051738075990299,
      "grad_norm": 4.2670979499816895,
      "learning_rate": 3.150155948629538e-06,
      "loss": 0.6287,
      "num_input_tokens_seen": 18950656,
      "step": 1510,
      "train_runtime": 16109.754,
      "train_tokens_per_second": 1176.347
    },
    {
      "epoch": 0.30618431689571546,
      "grad_norm": 2.90279483795166,
      "learning_rate": 3.144955875952646e-06,
      "loss": 0.7107,
      "num_input_tokens_seen": 19013008,
      "step": 1515,
      "train_runtime": 16162.6654,
      "train_tokens_per_second": 1176.353
    },
    {
      "epoch": 0.30719482619240096,
      "grad_norm": 3.137493133544922,
      "learning_rate": 3.139744264260229e-06,
      "loss": 0.6388,
      "num_input_tokens_seen": 19075776,
      "step": 1520,
      "train_runtime": 16215.8567,
      "train_tokens_per_second": 1176.366
    },
    {
      "epoch": 0.3082053354890865,
      "grad_norm": 2.954411268234253,
      "learning_rate": 3.1345211660755936e-06,
      "loss": 0.41,
      "num_input_tokens_seen": 19139456,
      "step": 1525,
      "train_runtime": 16269.7851,
      "train_tokens_per_second": 1176.38
    },
    {
      "epoch": 0.309215844785772,
      "grad_norm": 3.9726264476776123,
      "learning_rate": 3.1292866340378084e-06,
      "loss": 0.7457,
      "num_input_tokens_seen": 19201776,
      "step": 1530,
      "train_runtime": 16322.8052,
      "train_tokens_per_second": 1176.377
    },
    {
      "epoch": 0.3102263540824576,
      "grad_norm": 3.5691118240356445,
      "learning_rate": 3.124040720901174e-06,
      "loss": 0.6141,
      "num_input_tokens_seen": 19264944,
      "step": 1535,
      "train_runtime": 16376.3792,
      "train_tokens_per_second": 1176.386
    },
    {
      "epoch": 0.3112368633791431,
      "grad_norm": 4.747558116912842,
      "learning_rate": 3.118783479534691e-06,
      "loss": 0.6515,
      "num_input_tokens_seen": 19327136,
      "step": 1540,
      "train_runtime": 16429.1756,
      "train_tokens_per_second": 1176.391
    },
    {
      "epoch": 0.31224737267582864,
      "grad_norm": 2.84836483001709,
      "learning_rate": 3.1135149629215276e-06,
      "loss": 0.4877,
      "num_input_tokens_seen": 19389792,
      "step": 1545,
      "train_runtime": 16482.4575,
      "train_tokens_per_second": 1176.39
    },
    {
      "epoch": 0.31325788197251414,
      "grad_norm": 3.176996946334839,
      "learning_rate": 3.108235224158486e-06,
      "loss": 0.4903,
      "num_input_tokens_seen": 19452288,
      "step": 1550,
      "train_runtime": 16535.691,
      "train_tokens_per_second": 1176.382
    },
    {
      "epoch": 0.3142683912691997,
      "grad_norm": 2.0486483573913574,
      "learning_rate": 3.102944316455465e-06,
      "loss": 0.6653,
      "num_input_tokens_seen": 19515392,
      "step": 1555,
      "train_runtime": 16589.1656,
      "train_tokens_per_second": 1176.394
    },
    {
      "epoch": 0.3152789005658852,
      "grad_norm": 4.0579705238342285,
      "learning_rate": 3.0976422931349283e-06,
      "loss": 0.5501,
      "num_input_tokens_seen": 19579552,
      "step": 1560,
      "train_runtime": 16643.4571,
      "train_tokens_per_second": 1176.411
    },
    {
      "epoch": 0.31628940986257076,
      "grad_norm": 4.04211950302124,
      "learning_rate": 3.0923292076313603e-06,
      "loss": 0.6828,
      "num_input_tokens_seen": 19641792,
      "step": 1565,
      "train_runtime": 16696.2169,
      "train_tokens_per_second": 1176.422
    },
    {
      "epoch": 0.31729991915925626,
      "grad_norm": 7.144432067871094,
      "learning_rate": 3.0870051134907354e-06,
      "loss": 0.5309,
      "num_input_tokens_seen": 19704656,
      "step": 1570,
      "train_runtime": 16749.5581,
      "train_tokens_per_second": 1176.428
    },
    {
      "epoch": 0.3183104284559418,
      "grad_norm": 3.529740333557129,
      "learning_rate": 3.081670064369972e-06,
      "loss": 0.7835,
      "num_input_tokens_seen": 19767312,
      "step": 1575,
      "train_runtime": 16802.8555,
      "train_tokens_per_second": 1176.426
    },
    {
      "epoch": 0.3193209377526273,
      "grad_norm": 3.0834341049194336,
      "learning_rate": 3.076324114036396e-06,
      "loss": 0.7354,
      "num_input_tokens_seen": 19829968,
      "step": 1580,
      "train_runtime": 16856.141,
      "train_tokens_per_second": 1176.424
    },
    {
      "epoch": 0.3203314470493129,
      "grad_norm": 4.357460021972656,
      "learning_rate": 3.070967316367195e-06,
      "loss": 0.65,
      "num_input_tokens_seen": 19892048,
      "step": 1585,
      "train_runtime": 16908.8467,
      "train_tokens_per_second": 1176.428
    },
    {
      "epoch": 0.3213419563459984,
      "grad_norm": 3.9819517135620117,
      "learning_rate": 3.0655997253488794e-06,
      "loss": 0.8998,
      "num_input_tokens_seen": 19954256,
      "step": 1590,
      "train_runtime": 16961.6045,
      "train_tokens_per_second": 1176.437
    },
    {
      "epoch": 0.32235246564268394,
      "grad_norm": 3.0440618991851807,
      "learning_rate": 3.060221395076735e-06,
      "loss": 0.7421,
      "num_input_tokens_seen": 20017520,
      "step": 1595,
      "train_runtime": 17015.1526,
      "train_tokens_per_second": 1176.453
    },
    {
      "epoch": 0.32336297493936944,
      "grad_norm": 2.736577033996582,
      "learning_rate": 3.0548323797542805e-06,
      "loss": 0.5968,
      "num_input_tokens_seen": 20079872,
      "step": 1600,
      "train_runtime": 17068.012,
      "train_tokens_per_second": 1176.462
    },
    {
      "epoch": 0.324373484236055,
      "grad_norm": 2.962750196456909,
      "learning_rate": 3.0494327336927192e-06,
      "loss": 0.6283,
      "num_input_tokens_seen": 20142784,
      "step": 1605,
      "train_runtime": 17122.0809,
      "train_tokens_per_second": 1176.421
    },
    {
      "epoch": 0.3253839935327405,
      "grad_norm": 4.26544189453125,
      "learning_rate": 3.044022511310393e-06,
      "loss": 0.5486,
      "num_input_tokens_seen": 20206000,
      "step": 1610,
      "train_runtime": 17175.7081,
      "train_tokens_per_second": 1176.429
    },
    {
      "epoch": 0.326394502829426,
      "grad_norm": 4.116427898406982,
      "learning_rate": 3.0386017671322313e-06,
      "loss": 0.6774,
      "num_input_tokens_seen": 20268464,
      "step": 1615,
      "train_runtime": 17228.7663,
      "train_tokens_per_second": 1176.432
    },
    {
      "epoch": 0.32740501212611156,
      "grad_norm": 3.6985037326812744,
      "learning_rate": 3.0331705557892065e-06,
      "loss": 0.602,
      "num_input_tokens_seen": 20330720,
      "step": 1620,
      "train_runtime": 17281.5436,
      "train_tokens_per_second": 1176.441
    },
    {
      "epoch": 0.32841552142279706,
      "grad_norm": 3.1198649406433105,
      "learning_rate": 3.0277289320177785e-06,
      "loss": 0.694,
      "num_input_tokens_seen": 20393488,
      "step": 1625,
      "train_runtime": 17334.8128,
      "train_tokens_per_second": 1176.447
    },
    {
      "epoch": 0.3294260307194826,
      "grad_norm": 3.9708313941955566,
      "learning_rate": 3.0222769506593456e-06,
      "loss": 0.6293,
      "num_input_tokens_seen": 20455728,
      "step": 1630,
      "train_runtime": 17387.7446,
      "train_tokens_per_second": 1176.445
    },
    {
      "epoch": 0.3304365400161681,
      "grad_norm": 5.078725814819336,
      "learning_rate": 3.0168146666596914e-06,
      "loss": 0.9056,
      "num_input_tokens_seen": 20518352,
      "step": 1635,
      "train_runtime": 17440.9172,
      "train_tokens_per_second": 1176.449
    },
    {
      "epoch": 0.3314470493128537,
      "grad_norm": 6.572999954223633,
      "learning_rate": 3.01134213506843e-06,
      "loss": 0.6013,
      "num_input_tokens_seen": 20580496,
      "step": 1640,
      "train_runtime": 17493.7861,
      "train_tokens_per_second": 1176.446
    },
    {
      "epoch": 0.3324575586095392,
      "grad_norm": 3.237039089202881,
      "learning_rate": 3.0058594110384537e-06,
      "loss": 0.7281,
      "num_input_tokens_seen": 20642912,
      "step": 1645,
      "train_runtime": 17546.7292,
      "train_tokens_per_second": 1176.454
    },
    {
      "epoch": 0.33346806790622474,
      "grad_norm": 3.6064066886901855,
      "learning_rate": 3.0003665498253737e-06,
      "loss": 0.7027,
      "num_input_tokens_seen": 20707248,
      "step": 1650,
      "train_runtime": 17601.1451,
      "train_tokens_per_second": 1176.472
    },
    {
      "epoch": 0.33447857720291024,
      "grad_norm": 2.408318519592285,
      "learning_rate": 2.994863606786966e-06,
      "loss": 0.6082,
      "num_input_tokens_seen": 20770736,
      "step": 1655,
      "train_runtime": 17654.903,
      "train_tokens_per_second": 1176.485
    },
    {
      "epoch": 0.3354890864995958,
      "grad_norm": 3.739574670791626,
      "learning_rate": 2.9893506373826138e-06,
      "loss": 0.6896,
      "num_input_tokens_seen": 20832288,
      "step": 1660,
      "train_runtime": 17707.3136,
      "train_tokens_per_second": 1176.479
    },
    {
      "epoch": 0.3364995957962813,
      "grad_norm": 3.5523746013641357,
      "learning_rate": 2.983827697172743e-06,
      "loss": 0.5855,
      "num_input_tokens_seen": 20895040,
      "step": 1665,
      "train_runtime": 17760.5236,
      "train_tokens_per_second": 1176.488
    },
    {
      "epoch": 0.33751010509296686,
      "grad_norm": 3.5267200469970703,
      "learning_rate": 2.9782948418182734e-06,
      "loss": 0.5613,
      "num_input_tokens_seen": 20957136,
      "step": 1670,
      "train_runtime": 17813.2901,
      "train_tokens_per_second": 1176.489
    },
    {
      "epoch": 0.33852061438965236,
      "grad_norm": 2.6889121532440186,
      "learning_rate": 2.972752127080044e-06,
      "loss": 0.5806,
      "num_input_tokens_seen": 21020880,
      "step": 1675,
      "train_runtime": 17867.1741,
      "train_tokens_per_second": 1176.508
    },
    {
      "epoch": 0.3395311236863379,
      "grad_norm": 3.3664584159851074,
      "learning_rate": 2.967199608818263e-06,
      "loss": 0.5607,
      "num_input_tokens_seen": 21084080,
      "step": 1680,
      "train_runtime": 17920.8043,
      "train_tokens_per_second": 1176.514
    },
    {
      "epoch": 0.3405416329830234,
      "grad_norm": 5.35931396484375,
      "learning_rate": 2.9616373429919364e-06,
      "loss": 0.6016,
      "num_input_tokens_seen": 21146640,
      "step": 1685,
      "train_runtime": 17974.0125,
      "train_tokens_per_second": 1176.512
    },
    {
      "epoch": 0.341552142279709,
      "grad_norm": 3.6088929176330566,
      "learning_rate": 2.9560653856583107e-06,
      "loss": 0.5951,
      "num_input_tokens_seen": 21209136,
      "step": 1690,
      "train_runtime": 18027.0731,
      "train_tokens_per_second": 1176.516
    },
    {
      "epoch": 0.3425626515763945,
      "grad_norm": 2.7327282428741455,
      "learning_rate": 2.950483792972301e-06,
      "loss": 0.4669,
      "num_input_tokens_seen": 21272160,
      "step": 1695,
      "train_runtime": 18080.4422,
      "train_tokens_per_second": 1176.529
    },
    {
      "epoch": 0.34357316087308004,
      "grad_norm": 2.951916217803955,
      "learning_rate": 2.944892621185932e-06,
      "loss": 0.6566,
      "num_input_tokens_seen": 21335488,
      "step": 1700,
      "train_runtime": 18134.0098,
      "train_tokens_per_second": 1176.546
    },
    {
      "epoch": 0.34458367016976554,
      "grad_norm": 3.364086389541626,
      "learning_rate": 2.939291926647766e-06,
      "loss": 0.7644,
      "num_input_tokens_seen": 21398784,
      "step": 1705,
      "train_runtime": 18188.3861,
      "train_tokens_per_second": 1176.508
    },
    {
      "epoch": 0.3455941794664511,
      "grad_norm": 3.000518560409546,
      "learning_rate": 2.933681765802337e-06,
      "loss": 0.5527,
      "num_input_tokens_seen": 21461360,
      "step": 1710,
      "train_runtime": 18241.5094,
      "train_tokens_per_second": 1176.512
    },
    {
      "epoch": 0.3466046887631366,
      "grad_norm": 5.457815647125244,
      "learning_rate": 2.9280621951895822e-06,
      "loss": 0.8091,
      "num_input_tokens_seen": 21524304,
      "step": 1715,
      "train_runtime": 18295.0886,
      "train_tokens_per_second": 1176.507
    },
    {
      "epoch": 0.34761519805982216,
      "grad_norm": 3.6932711601257324,
      "learning_rate": 2.922433271444272e-06,
      "loss": 0.5432,
      "num_input_tokens_seen": 21586608,
      "step": 1720,
      "train_runtime": 18347.9295,
      "train_tokens_per_second": 1176.515
    },
    {
      "epoch": 0.34862570735650766,
      "grad_norm": 4.944134712219238,
      "learning_rate": 2.916795051295438e-06,
      "loss": 0.6431,
      "num_input_tokens_seen": 21650352,
      "step": 1725,
      "train_runtime": 18401.891,
      "train_tokens_per_second": 1176.529
    },
    {
      "epoch": 0.3496362166531932,
      "grad_norm": 3.928687572479248,
      "learning_rate": 2.9111475915658022e-06,
      "loss": 0.5834,
      "num_input_tokens_seen": 21713968,
      "step": 1730,
      "train_runtime": 18455.7774,
      "train_tokens_per_second": 1176.54
    },
    {
      "epoch": 0.3506467259498787,
      "grad_norm": 2.45914888381958,
      "learning_rate": 2.905490949171205e-06,
      "loss": 0.5779,
      "num_input_tokens_seen": 21775088,
      "step": 1735,
      "train_runtime": 18507.8748,
      "train_tokens_per_second": 1176.531
    },
    {
      "epoch": 0.3516572352465643,
      "grad_norm": 5.723708152770996,
      "learning_rate": 2.899825181120031e-06,
      "loss": 0.5441,
      "num_input_tokens_seen": 21837392,
      "step": 1740,
      "train_runtime": 18560.7602,
      "train_tokens_per_second": 1176.535
    },
    {
      "epoch": 0.3526677445432498,
      "grad_norm": 5.416782855987549,
      "learning_rate": 2.8941503445126324e-06,
      "loss": 0.6264,
      "num_input_tokens_seen": 21900176,
      "step": 1745,
      "train_runtime": 18614.0117,
      "train_tokens_per_second": 1176.543
    },
    {
      "epoch": 0.35367825383993534,
      "grad_norm": 2.723778009414673,
      "learning_rate": 2.8884664965407585e-06,
      "loss": 0.5676,
      "num_input_tokens_seen": 21962624,
      "step": 1750,
      "train_runtime": 18667.037,
      "train_tokens_per_second": 1176.546
    },
    {
      "epoch": 0.35468876313662084,
      "grad_norm": 5.285996913909912,
      "learning_rate": 2.8827736944869747e-06,
      "loss": 0.5586,
      "num_input_tokens_seen": 22025360,
      "step": 1755,
      "train_runtime": 18720.3299,
      "train_tokens_per_second": 1176.548
    },
    {
      "epoch": 0.3556992724333064,
      "grad_norm": 4.2422027587890625,
      "learning_rate": 2.877071995724086e-06,
      "loss": 0.4766,
      "num_input_tokens_seen": 22088544,
      "step": 1760,
      "train_runtime": 18773.8036,
      "train_tokens_per_second": 1176.562
    },
    {
      "epoch": 0.3567097817299919,
      "grad_norm": 3.4025697708129883,
      "learning_rate": 2.8713614577145616e-06,
      "loss": 0.6146,
      "num_input_tokens_seen": 22150416,
      "step": 1765,
      "train_runtime": 18826.391,
      "train_tokens_per_second": 1176.562
    },
    {
      "epoch": 0.35772029102667746,
      "grad_norm": 3.6553449630737305,
      "learning_rate": 2.8656421380099513e-06,
      "loss": 0.5679,
      "num_input_tokens_seen": 22212592,
      "step": 1770,
      "train_runtime": 18879.2397,
      "train_tokens_per_second": 1176.562
    },
    {
      "epoch": 0.35873080032336296,
      "grad_norm": 3.248884916305542,
      "learning_rate": 2.8599140942503114e-06,
      "loss": 0.5745,
      "num_input_tokens_seen": 22276256,
      "step": 1775,
      "train_runtime": 18933.1094,
      "train_tokens_per_second": 1176.577
    },
    {
      "epoch": 0.3597413096200485,
      "grad_norm": 4.54911994934082,
      "learning_rate": 2.854177384163616e-06,
      "loss": 0.6332,
      "num_input_tokens_seen": 22339104,
      "step": 1780,
      "train_runtime": 18986.402,
      "train_tokens_per_second": 1176.584
    },
    {
      "epoch": 0.360751818916734,
      "grad_norm": 4.699230194091797,
      "learning_rate": 2.848432065565185e-06,
      "loss": 0.5841,
      "num_input_tokens_seen": 22401520,
      "step": 1785,
      "train_runtime": 19039.3695,
      "train_tokens_per_second": 1176.589
    },
    {
      "epoch": 0.3617623282134196,
      "grad_norm": 4.126104831695557,
      "learning_rate": 2.84267819635709e-06,
      "loss": 0.4888,
      "num_input_tokens_seen": 22465600,
      "step": 1790,
      "train_runtime": 19093.5577,
      "train_tokens_per_second": 1176.606
    },
    {
      "epoch": 0.3627728375101051,
      "grad_norm": 6.944115161895752,
      "learning_rate": 2.8369158345275816e-06,
      "loss": 0.5632,
      "num_input_tokens_seen": 22528416,
      "step": 1795,
      "train_runtime": 19146.7755,
      "train_tokens_per_second": 1176.617
    },
    {
      "epoch": 0.36378334680679064,
      "grad_norm": 3.6419284343719482,
      "learning_rate": 2.8311450381504966e-06,
      "loss": 0.532,
      "num_input_tokens_seen": 22591152,
      "step": 1800,
      "train_runtime": 19200.0715,
      "train_tokens_per_second": 1176.618
    },
    {
      "epoch": 0.36479385610347614,
      "grad_norm": 3.737381935119629,
      "learning_rate": 2.82536586538468e-06,
      "loss": 0.6468,
      "num_input_tokens_seen": 22653872,
      "step": 1805,
      "train_runtime": 19253.9909,
      "train_tokens_per_second": 1176.581
    },
    {
      "epoch": 0.3658043654001617,
      "grad_norm": 3.461130142211914,
      "learning_rate": 2.819578374473392e-06,
      "loss": 0.512,
      "num_input_tokens_seen": 22716592,
      "step": 1810,
      "train_runtime": 19307.1865,
      "train_tokens_per_second": 1176.587
    },
    {
      "epoch": 0.3668148746968472,
      "grad_norm": 9.620305061340332,
      "learning_rate": 2.8137826237437252e-06,
      "loss": 0.6492,
      "num_input_tokens_seen": 22779936,
      "step": 1815,
      "train_runtime": 19360.9978,
      "train_tokens_per_second": 1176.589
    },
    {
      "epoch": 0.36782538399353276,
      "grad_norm": 2.9488351345062256,
      "learning_rate": 2.807978671606016e-06,
      "loss": 0.4879,
      "num_input_tokens_seen": 22843184,
      "step": 1820,
      "train_runtime": 19414.713,
      "train_tokens_per_second": 1176.591
    },
    {
      "epoch": 0.36883589329021826,
      "grad_norm": 3.515941858291626,
      "learning_rate": 2.8021665765532556e-06,
      "loss": 0.6996,
      "num_input_tokens_seen": 22906096,
      "step": 1825,
      "train_runtime": 19468.0046,
      "train_tokens_per_second": 1176.602
    },
    {
      "epoch": 0.3698464025869038,
      "grad_norm": 2.8218352794647217,
      "learning_rate": 2.7963463971604995e-06,
      "loss": 0.6387,
      "num_input_tokens_seen": 22968544,
      "step": 1830,
      "train_runtime": 19521.1081,
      "train_tokens_per_second": 1176.6
    },
    {
      "epoch": 0.3708569118835893,
      "grad_norm": 3.574018955230713,
      "learning_rate": 2.7905181920842808e-06,
      "loss": 0.6173,
      "num_input_tokens_seen": 23031520,
      "step": 1835,
      "train_runtime": 19574.4975,
      "train_tokens_per_second": 1176.608
    },
    {
      "epoch": 0.3718674211802749,
      "grad_norm": 4.970846176147461,
      "learning_rate": 2.7846820200620136e-06,
      "loss": 0.5874,
      "num_input_tokens_seen": 23094448,
      "step": 1840,
      "train_runtime": 19627.9193,
      "train_tokens_per_second": 1176.612
    },
    {
      "epoch": 0.3728779304769604,
      "grad_norm": 3.886343479156494,
      "learning_rate": 2.778837939911406e-06,
      "loss": 0.459,
      "num_input_tokens_seen": 23156880,
      "step": 1845,
      "train_runtime": 19680.9357,
      "train_tokens_per_second": 1176.615
    },
    {
      "epoch": 0.37388843977364594,
      "grad_norm": 3.744309902191162,
      "learning_rate": 2.7729860105298634e-06,
      "loss": 0.574,
      "num_input_tokens_seen": 23219760,
      "step": 1850,
      "train_runtime": 19734.3322,
      "train_tokens_per_second": 1176.617
    },
    {
      "epoch": 0.37489894907033144,
      "grad_norm": 3.28615665435791,
      "learning_rate": 2.7671262908938993e-06,
      "loss": 0.5933,
      "num_input_tokens_seen": 23281008,
      "step": 1855,
      "train_runtime": 19786.5259,
      "train_tokens_per_second": 1176.609
    },
    {
      "epoch": 0.375909458367017,
      "grad_norm": 3.1548731327056885,
      "learning_rate": 2.7612588400585354e-06,
      "loss": 0.5003,
      "num_input_tokens_seen": 23342992,
      "step": 1860,
      "train_runtime": 19839.4337,
      "train_tokens_per_second": 1176.596
    },
    {
      "epoch": 0.3769199676637025,
      "grad_norm": 3.3828413486480713,
      "learning_rate": 2.7553837171567117e-06,
      "loss": 0.5391,
      "num_input_tokens_seen": 23405728,
      "step": 1865,
      "train_runtime": 19892.6745,
      "train_tokens_per_second": 1176.6
    },
    {
      "epoch": 0.37793047696038806,
      "grad_norm": 3.304410219192505,
      "learning_rate": 2.749500981398688e-06,
      "loss": 0.626,
      "num_input_tokens_seen": 23468496,
      "step": 1870,
      "train_runtime": 19946.0928,
      "train_tokens_per_second": 1176.596
    },
    {
      "epoch": 0.37894098625707356,
      "grad_norm": 4.338794231414795,
      "learning_rate": 2.7436106920714457e-06,
      "loss": 0.5976,
      "num_input_tokens_seen": 23532240,
      "step": 1875,
      "train_runtime": 19999.9242,
      "train_tokens_per_second": 1176.616
    },
    {
      "epoch": 0.3799514955537591,
      "grad_norm": 2.867887496948242,
      "learning_rate": 2.737712908538095e-06,
      "loss": 0.512,
      "num_input_tokens_seen": 23594320,
      "step": 1880,
      "train_runtime": 20052.6969,
      "train_tokens_per_second": 1176.616
    },
    {
      "epoch": 0.3809620048504446,
      "grad_norm": 3.222823143005371,
      "learning_rate": 2.7318076902372713e-06,
      "loss": 0.5453,
      "num_input_tokens_seen": 23656032,
      "step": 1885,
      "train_runtime": 20105.2123,
      "train_tokens_per_second": 1176.612
    },
    {
      "epoch": 0.3819725141471301,
      "grad_norm": 4.912517070770264,
      "learning_rate": 2.7258950966825387e-06,
      "loss": 0.8227,
      "num_input_tokens_seen": 23717888,
      "step": 1890,
      "train_runtime": 20157.8616,
      "train_tokens_per_second": 1176.607
    },
    {
      "epoch": 0.3829830234438157,
      "grad_norm": 3.3007333278656006,
      "learning_rate": 2.719975187461792e-06,
      "loss": 0.5607,
      "num_input_tokens_seen": 23780464,
      "step": 1895,
      "train_runtime": 20211.1801,
      "train_tokens_per_second": 1176.599
    },
    {
      "epoch": 0.3839935327405012,
      "grad_norm": 4.263962268829346,
      "learning_rate": 2.714048022236652e-06,
      "loss": 0.6824,
      "num_input_tokens_seen": 23841968,
      "step": 1900,
      "train_runtime": 20263.4517,
      "train_tokens_per_second": 1176.6
    },
    {
      "epoch": 0.38500404203718674,
      "grad_norm": 4.154531002044678,
      "learning_rate": 2.7081136607418683e-06,
      "loss": 0.5432,
      "num_input_tokens_seen": 23905728,
      "step": 1905,
      "train_runtime": 20318.1856,
      "train_tokens_per_second": 1176.568
    },
    {
      "epoch": 0.38601455133387225,
      "grad_norm": 3.6693150997161865,
      "learning_rate": 2.7021721627847135e-06,
      "loss": 0.547,
      "num_input_tokens_seen": 23969360,
      "step": 1910,
      "train_runtime": 20372.0048,
      "train_tokens_per_second": 1176.583
    },
    {
      "epoch": 0.3870250606305578,
      "grad_norm": 10.746560096740723,
      "learning_rate": 2.6962235882443844e-06,
      "loss": 0.651,
      "num_input_tokens_seen": 24032208,
      "step": 1915,
      "train_runtime": 20425.2944,
      "train_tokens_per_second": 1176.591
    },
    {
      "epoch": 0.3880355699272433,
      "grad_norm": 2.670816421508789,
      "learning_rate": 2.6902679970713945e-06,
      "loss": 0.522,
      "num_input_tokens_seen": 24093776,
      "step": 1920,
      "train_runtime": 20477.7289,
      "train_tokens_per_second": 1176.584
    },
    {
      "epoch": 0.38904607922392886,
      "grad_norm": 3.2105329036712646,
      "learning_rate": 2.684305449286975e-06,
      "loss": 0.6065,
      "num_input_tokens_seen": 24155520,
      "step": 1925,
      "train_runtime": 20530.3509,
      "train_tokens_per_second": 1176.576
    },
    {
      "epoch": 0.39005658852061437,
      "grad_norm": 3.582928419113159,
      "learning_rate": 2.678336004982462e-06,
      "loss": 0.4948,
      "num_input_tokens_seen": 24219264,
      "step": 1930,
      "train_runtime": 20584.4365,
      "train_tokens_per_second": 1176.581
    },
    {
      "epoch": 0.3910670978172999,
      "grad_norm": 3.8100945949554443,
      "learning_rate": 2.6723597243187004e-06,
      "loss": 0.7279,
      "num_input_tokens_seen": 24282544,
      "step": 1935,
      "train_runtime": 20638.1736,
      "train_tokens_per_second": 1176.584
    },
    {
      "epoch": 0.3920776071139854,
      "grad_norm": 2.5632197856903076,
      "learning_rate": 2.6663766675254304e-06,
      "loss": 0.4619,
      "num_input_tokens_seen": 24345472,
      "step": 1940,
      "train_runtime": 20691.6157,
      "train_tokens_per_second": 1176.586
    },
    {
      "epoch": 0.393088116410671,
      "grad_norm": 3.167564868927002,
      "learning_rate": 2.6603868949006824e-06,
      "loss": 0.5386,
      "num_input_tokens_seen": 24407712,
      "step": 1945,
      "train_runtime": 20744.6853,
      "train_tokens_per_second": 1176.577
    },
    {
      "epoch": 0.3940986257073565,
      "grad_norm": 3.0450758934020996,
      "learning_rate": 2.6543904668101716e-06,
      "loss": 0.5364,
      "num_input_tokens_seen": 24470144,
      "step": 1950,
      "train_runtime": 20797.7459,
      "train_tokens_per_second": 1176.577
    },
    {
      "epoch": 0.39510913500404204,
      "grad_norm": 2.644662857055664,
      "learning_rate": 2.6483874436866864e-06,
      "loss": 0.6136,
      "num_input_tokens_seen": 24533664,
      "step": 1955,
      "train_runtime": 20851.5188,
      "train_tokens_per_second": 1176.589
    },
    {
      "epoch": 0.39611964430072755,
      "grad_norm": 2.910806894302368,
      "learning_rate": 2.6423778860294813e-06,
      "loss": 0.6195,
      "num_input_tokens_seen": 24595952,
      "step": 1960,
      "train_runtime": 20904.4031,
      "train_tokens_per_second": 1176.592
    },
    {
      "epoch": 0.3971301535974131,
      "grad_norm": 3.1841044425964355,
      "learning_rate": 2.6363618544036677e-06,
      "loss": 0.5013,
      "num_input_tokens_seen": 24658928,
      "step": 1965,
      "train_runtime": 20957.6999,
      "train_tokens_per_second": 1176.605
    },
    {
      "epoch": 0.3981406628940986,
      "grad_norm": 4.007697105407715,
      "learning_rate": 2.630339409439601e-06,
      "loss": 0.5886,
      "num_input_tokens_seen": 24721088,
      "step": 1970,
      "train_runtime": 21010.5348,
      "train_tokens_per_second": 1176.604
    },
    {
      "epoch": 0.39915117219078416,
      "grad_norm": 4.3501200675964355,
      "learning_rate": 2.6243106118322714e-06,
      "loss": 0.571,
      "num_input_tokens_seen": 24783648,
      "step": 1975,
      "train_runtime": 21063.5842,
      "train_tokens_per_second": 1176.611
    },
    {
      "epoch": 0.40016168148746967,
      "grad_norm": 3.5786592960357666,
      "learning_rate": 2.6182755223406922e-06,
      "loss": 0.5797,
      "num_input_tokens_seen": 24846272,
      "step": 1980,
      "train_runtime": 21116.6716,
      "train_tokens_per_second": 1176.619
    },
    {
      "epoch": 0.4011721907841552,
      "grad_norm": 5.644720077514648,
      "learning_rate": 2.612234201787287e-06,
      "loss": 0.5936,
      "num_input_tokens_seen": 24908112,
      "step": 1985,
      "train_runtime": 21169.1576,
      "train_tokens_per_second": 1176.623
    },
    {
      "epoch": 0.4021827000808407,
      "grad_norm": 3.142220973968506,
      "learning_rate": 2.606186711057277e-06,
      "loss": 0.4577,
      "num_input_tokens_seen": 24971024,
      "step": 1990,
      "train_runtime": 21222.6038,
      "train_tokens_per_second": 1176.624
    },
    {
      "epoch": 0.4031932093775263,
      "grad_norm": 2.8748204708099365,
      "learning_rate": 2.6001331110980676e-06,
      "loss": 0.7144,
      "num_input_tokens_seen": 25033904,
      "step": 1995,
      "train_runtime": 21275.8951,
      "train_tokens_per_second": 1176.632
    },
    {
      "epoch": 0.4042037186742118,
      "grad_norm": 2.583357810974121,
      "learning_rate": 2.594073462918633e-06,
      "loss": 0.5053,
      "num_input_tokens_seen": 25096080,
      "step": 2000,
      "train_runtime": 21328.7386,
      "train_tokens_per_second": 1176.632
    },
    {
      "epoch": 0.40521422797089734,
      "grad_norm": 3.375333547592163,
      "learning_rate": 2.588007827588902e-06,
      "loss": 0.5239,
      "num_input_tokens_seen": 25158448,
      "step": 2005,
      "train_runtime": 21382.4032,
      "train_tokens_per_second": 1176.596
    },
    {
      "epoch": 0.40622473726758285,
      "grad_norm": 4.408908367156982,
      "learning_rate": 2.5819362662391444e-06,
      "loss": 0.5039,
      "num_input_tokens_seen": 25220688,
      "step": 2010,
      "train_runtime": 21435.2332,
      "train_tokens_per_second": 1176.6
    },
    {
      "epoch": 0.4072352465642684,
      "grad_norm": 2.8501296043395996,
      "learning_rate": 2.5758588400593503e-06,
      "loss": 0.5787,
      "num_input_tokens_seen": 25283280,
      "step": 2015,
      "train_runtime": 21488.4147,
      "train_tokens_per_second": 1176.601
    },
    {
      "epoch": 0.4082457558609539,
      "grad_norm": 3.691087245941162,
      "learning_rate": 2.5697756102986195e-06,
      "loss": 0.6472,
      "num_input_tokens_seen": 25345712,
      "step": 2020,
      "train_runtime": 21541.2683,
      "train_tokens_per_second": 1176.612
    },
    {
      "epoch": 0.40925626515763947,
      "grad_norm": 10.191792488098145,
      "learning_rate": 2.5636866382645387e-06,
      "loss": 0.5386,
      "num_input_tokens_seen": 25408976,
      "step": 2025,
      "train_runtime": 21594.9341,
      "train_tokens_per_second": 1176.617
    },
    {
      "epoch": 0.41026677445432497,
      "grad_norm": 2.2505037784576416,
      "learning_rate": 2.557591985322568e-06,
      "loss": 0.4028,
      "num_input_tokens_seen": 25472208,
      "step": 2030,
      "train_runtime": 21648.4277,
      "train_tokens_per_second": 1176.631
    },
    {
      "epoch": 0.4112772837510105,
      "grad_norm": 2.7366943359375,
      "learning_rate": 2.5514917128954176e-06,
      "loss": 0.6926,
      "num_input_tokens_seen": 25535008,
      "step": 2035,
      "train_runtime": 21701.6759,
      "train_tokens_per_second": 1176.638
    },
    {
      "epoch": 0.412287793047696,
      "grad_norm": 3.423129081726074,
      "learning_rate": 2.5453858824624356e-06,
      "loss": 0.5582,
      "num_input_tokens_seen": 25598464,
      "step": 2040,
      "train_runtime": 21755.4473,
      "train_tokens_per_second": 1176.646
    },
    {
      "epoch": 0.4132983023443816,
      "grad_norm": 2.881652355194092,
      "learning_rate": 2.53927455555898e-06,
      "loss": 0.5421,
      "num_input_tokens_seen": 25660880,
      "step": 2045,
      "train_runtime": 21808.4376,
      "train_tokens_per_second": 1176.649
    },
    {
      "epoch": 0.4143088116410671,
      "grad_norm": 3.675844192504883,
      "learning_rate": 2.533157793775806e-06,
      "loss": 0.598,
      "num_input_tokens_seen": 25723648,
      "step": 2050,
      "train_runtime": 21861.5841,
      "train_tokens_per_second": 1176.66
    },
    {
      "epoch": 0.41531932093775265,
      "grad_norm": 4.681539058685303,
      "learning_rate": 2.5270356587584414e-06,
      "loss": 0.611,
      "num_input_tokens_seen": 25787472,
      "step": 2055,
      "train_runtime": 21915.5845,
      "train_tokens_per_second": 1176.673
    },
    {
      "epoch": 0.41632983023443815,
      "grad_norm": 5.8855485916137695,
      "learning_rate": 2.520908212206566e-06,
      "loss": 0.5292,
      "num_input_tokens_seen": 25849968,
      "step": 2060,
      "train_runtime": 21968.5641,
      "train_tokens_per_second": 1176.68
    },
    {
      "epoch": 0.4173403395311237,
      "grad_norm": 3.3078877925872803,
      "learning_rate": 2.51477551587339e-06,
      "loss": 0.5152,
      "num_input_tokens_seen": 25912432,
      "step": 2065,
      "train_runtime": 22021.7273,
      "train_tokens_per_second": 1176.676
    },
    {
      "epoch": 0.4183508488278092,
      "grad_norm": 2.341024160385132,
      "learning_rate": 2.508637631565031e-06,
      "loss": 0.5862,
      "num_input_tokens_seen": 25975040,
      "step": 2070,
      "train_runtime": 22074.9583,
      "train_tokens_per_second": 1176.674
    },
    {
      "epoch": 0.41936135812449477,
      "grad_norm": 2.5771820545196533,
      "learning_rate": 2.5024946211398926e-06,
      "loss": 0.6319,
      "num_input_tokens_seen": 26037344,
      "step": 2075,
      "train_runtime": 22127.9166,
      "train_tokens_per_second": 1176.674
    },
    {
      "epoch": 0.42037186742118027,
      "grad_norm": 3.589078903198242,
      "learning_rate": 2.496346546508041e-06,
      "loss": 0.72,
      "num_input_tokens_seen": 26100272,
      "step": 2080,
      "train_runtime": 22181.1481,
      "train_tokens_per_second": 1176.687
    },
    {
      "epoch": 0.4213823767178658,
      "grad_norm": 4.1583733558654785,
      "learning_rate": 2.490193469630577e-06,
      "loss": 0.6548,
      "num_input_tokens_seen": 26163584,
      "step": 2085,
      "train_runtime": 22234.595,
      "train_tokens_per_second": 1176.706
    },
    {
      "epoch": 0.4223928860145513,
      "grad_norm": 2.1948020458221436,
      "learning_rate": 2.484035452519018e-06,
      "loss": 0.5558,
      "num_input_tokens_seen": 26226848,
      "step": 2090,
      "train_runtime": 22288.1177,
      "train_tokens_per_second": 1176.719
    },
    {
      "epoch": 0.4234033953112369,
      "grad_norm": 2.471421957015991,
      "learning_rate": 2.4778725572346674e-06,
      "loss": 0.4996,
      "num_input_tokens_seen": 26289744,
      "step": 2095,
      "train_runtime": 22341.5111,
      "train_tokens_per_second": 1176.722
    },
    {
      "epoch": 0.4244139046079224,
      "grad_norm": 6.726010322570801,
      "learning_rate": 2.4717048458879926e-06,
      "loss": 0.5691,
      "num_input_tokens_seen": 26352512,
      "step": 2100,
      "train_runtime": 22394.7375,
      "train_tokens_per_second": 1176.728
    },
    {
      "epoch": 0.42542441390460795,
      "grad_norm": 2.7426021099090576,
      "learning_rate": 2.4655323806379984e-06,
      "loss": 0.5768,
      "num_input_tokens_seen": 26415536,
      "step": 2105,
      "train_runtime": 22448.9563,
      "train_tokens_per_second": 1176.693
    },
    {
      "epoch": 0.42643492320129345,
      "grad_norm": 4.212181091308594,
      "learning_rate": 2.4593552236915986e-06,
      "loss": 0.6315,
      "num_input_tokens_seen": 26477424,
      "step": 2110,
      "train_runtime": 22501.6543,
      "train_tokens_per_second": 1176.688
    },
    {
      "epoch": 0.427445432497979,
      "grad_norm": 4.0044684410095215,
      "learning_rate": 2.4531734373029915e-06,
      "loss": 0.5714,
      "num_input_tokens_seen": 26540784,
      "step": 2115,
      "train_runtime": 22555.3445,
      "train_tokens_per_second": 1176.696
    },
    {
      "epoch": 0.4284559417946645,
      "grad_norm": 4.020651817321777,
      "learning_rate": 2.446987083773031e-06,
      "loss": 0.686,
      "num_input_tokens_seen": 26604016,
      "step": 2120,
      "train_runtime": 22608.9386,
      "train_tokens_per_second": 1176.703
    },
    {
      "epoch": 0.42946645109135007,
      "grad_norm": 4.15017032623291,
      "learning_rate": 2.4407962254486003e-06,
      "loss": 0.4842,
      "num_input_tokens_seen": 26668000,
      "step": 2125,
      "train_runtime": 22663.0335,
      "train_tokens_per_second": 1176.718
    },
    {
      "epoch": 0.43047696038803557,
      "grad_norm": 4.734328269958496,
      "learning_rate": 2.434600924721981e-06,
      "loss": 0.5434,
      "num_input_tokens_seen": 26730352,
      "step": 2130,
      "train_runtime": 22715.9778,
      "train_tokens_per_second": 1176.72
    },
    {
      "epoch": 0.4314874696847211,
      "grad_norm": 3.40374755859375,
      "learning_rate": 2.4284012440302274e-06,
      "loss": 0.575,
      "num_input_tokens_seen": 26793088,
      "step": 2135,
      "train_runtime": 22769.2336,
      "train_tokens_per_second": 1176.723
    },
    {
      "epoch": 0.43249797898140663,
      "grad_norm": 3.257279872894287,
      "learning_rate": 2.4221972458545346e-06,
      "loss": 0.6142,
      "num_input_tokens_seen": 26855168,
      "step": 2140,
      "train_runtime": 22822.1545,
      "train_tokens_per_second": 1176.715
    },
    {
      "epoch": 0.4335084882780922,
      "grad_norm": 2.7015247344970703,
      "learning_rate": 2.4159889927196092e-06,
      "loss": 0.4698,
      "num_input_tokens_seen": 26918112,
      "step": 2145,
      "train_runtime": 22875.3743,
      "train_tokens_per_second": 1176.729
    },
    {
      "epoch": 0.4345189975747777,
      "grad_norm": 3.3192410469055176,
      "learning_rate": 2.409776547193042e-06,
      "loss": 0.8394,
      "num_input_tokens_seen": 26982048,
      "step": 2150,
      "train_runtime": 22929.406,
      "train_tokens_per_second": 1176.744
    },
    {
      "epoch": 0.4355295068714632,
      "grad_norm": 2.8496975898742676,
      "learning_rate": 2.403559971884673e-06,
      "loss": 0.5828,
      "num_input_tokens_seen": 27045520,
      "step": 2155,
      "train_runtime": 22983.2542,
      "train_tokens_per_second": 1176.749
    },
    {
      "epoch": 0.43654001616814875,
      "grad_norm": 2.9721732139587402,
      "learning_rate": 2.397339329445965e-06,
      "loss": 0.6166,
      "num_input_tokens_seen": 27108960,
      "step": 2160,
      "train_runtime": 23036.9889,
      "train_tokens_per_second": 1176.758
    },
    {
      "epoch": 0.43755052546483425,
      "grad_norm": 5.522364616394043,
      "learning_rate": 2.391114682569367e-06,
      "loss": 0.5197,
      "num_input_tokens_seen": 27171984,
      "step": 2165,
      "train_runtime": 23090.4085,
      "train_tokens_per_second": 1176.765
    },
    {
      "epoch": 0.4385610347615198,
      "grad_norm": 4.193235397338867,
      "learning_rate": 2.3848860939876884e-06,
      "loss": 0.6045,
      "num_input_tokens_seen": 27234096,
      "step": 2170,
      "train_runtime": 23143.217,
      "train_tokens_per_second": 1176.764
    },
    {
      "epoch": 0.4395715440582053,
      "grad_norm": 4.53651237487793,
      "learning_rate": 2.3786536264734604e-06,
      "loss": 0.5972,
      "num_input_tokens_seen": 27296944,
      "step": 2175,
      "train_runtime": 23196.5326,
      "train_tokens_per_second": 1176.768
    },
    {
      "epoch": 0.44058205335489087,
      "grad_norm": 3.1025307178497314,
      "learning_rate": 2.37241734283831e-06,
      "loss": 0.6531,
      "num_input_tokens_seen": 27359776,
      "step": 2180,
      "train_runtime": 23249.7824,
      "train_tokens_per_second": 1176.776
    },
    {
      "epoch": 0.44159256265157637,
      "grad_norm": 3.4561688899993896,
      "learning_rate": 2.3661773059323203e-06,
      "loss": 0.6343,
      "num_input_tokens_seen": 27421520,
      "step": 2185,
      "train_runtime": 23302.2399,
      "train_tokens_per_second": 1176.776
    },
    {
      "epoch": 0.44260307194826193,
      "grad_norm": 3.765409231185913,
      "learning_rate": 2.359933578643403e-06,
      "loss": 0.5811,
      "num_input_tokens_seen": 27484064,
      "step": 2190,
      "train_runtime": 23355.3717,
      "train_tokens_per_second": 1176.777
    },
    {
      "epoch": 0.44361358124494743,
      "grad_norm": 3.980699300765991,
      "learning_rate": 2.35368622389666e-06,
      "loss": 0.6091,
      "num_input_tokens_seen": 27546832,
      "step": 2195,
      "train_runtime": 23408.4753,
      "train_tokens_per_second": 1176.789
    },
    {
      "epoch": 0.444624090541633,
      "grad_norm": 2.6866743564605713,
      "learning_rate": 2.3474353046537526e-06,
      "loss": 0.5612,
      "num_input_tokens_seen": 27609328,
      "step": 2200,
      "train_runtime": 23461.4047,
      "train_tokens_per_second": 1176.798
    },
    {
      "epoch": 0.4456345998383185,
      "grad_norm": 2.526900053024292,
      "learning_rate": 2.3411808839122646e-06,
      "loss": 0.5389,
      "num_input_tokens_seen": 27672480,
      "step": 2205,
      "train_runtime": 23515.6944,
      "train_tokens_per_second": 1176.766
    },
    {
      "epoch": 0.44664510913500405,
      "grad_norm": 3.709812879562378,
      "learning_rate": 2.3349230247050693e-06,
      "loss": 0.5689,
      "num_input_tokens_seen": 27736704,
      "step": 2210,
      "train_runtime": 23569.9375,
      "train_tokens_per_second": 1176.783
    },
    {
      "epoch": 0.44765561843168955,
      "grad_norm": 4.040143966674805,
      "learning_rate": 2.3286617900996923e-06,
      "loss": 0.5939,
      "num_input_tokens_seen": 27799872,
      "step": 2215,
      "train_runtime": 23623.4811,
      "train_tokens_per_second": 1176.79
    },
    {
      "epoch": 0.4486661277283751,
      "grad_norm": 2.6815268993377686,
      "learning_rate": 2.322397243197677e-06,
      "loss": 0.7729,
      "num_input_tokens_seen": 27862352,
      "step": 2220,
      "train_runtime": 23676.5481,
      "train_tokens_per_second": 1176.791
    },
    {
      "epoch": 0.4496766370250606,
      "grad_norm": 4.338621616363525,
      "learning_rate": 2.3161294471339497e-06,
      "loss": 0.6197,
      "num_input_tokens_seen": 27925872,
      "step": 2225,
      "train_runtime": 23730.3629,
      "train_tokens_per_second": 1176.799
    },
    {
      "epoch": 0.45068714632174617,
      "grad_norm": 4.31621789932251,
      "learning_rate": 2.3098584650761814e-06,
      "loss": 0.4996,
      "num_input_tokens_seen": 27989648,
      "step": 2230,
      "train_runtime": 23784.1859,
      "train_tokens_per_second": 1176.818
    },
    {
      "epoch": 0.45169765561843167,
      "grad_norm": 2.9194281101226807,
      "learning_rate": 2.303584360224151e-06,
      "loss": 0.5526,
      "num_input_tokens_seen": 28052560,
      "step": 2235,
      "train_runtime": 23837.7485,
      "train_tokens_per_second": 1176.812
    },
    {
      "epoch": 0.45270816491511723,
      "grad_norm": 3.0267200469970703,
      "learning_rate": 2.2973071958091117e-06,
      "loss": 0.5462,
      "num_input_tokens_seen": 28115376,
      "step": 2240,
      "train_runtime": 23890.9043,
      "train_tokens_per_second": 1176.823
    },
    {
      "epoch": 0.45371867421180273,
      "grad_norm": 3.1791086196899414,
      "learning_rate": 2.2910270350931487e-06,
      "loss": 0.6201,
      "num_input_tokens_seen": 28178176,
      "step": 2245,
      "train_runtime": 23944.2961,
      "train_tokens_per_second": 1176.822
    },
    {
      "epoch": 0.4547291835084883,
      "grad_norm": 2.9762167930603027,
      "learning_rate": 2.2847439413685457e-06,
      "loss": 0.5204,
      "num_input_tokens_seen": 28242496,
      "step": 2250,
      "train_runtime": 23998.5604,
      "train_tokens_per_second": 1176.841
    },
    {
      "epoch": 0.4557396928051738,
      "grad_norm": 8.53632926940918,
      "learning_rate": 2.278457977957147e-06,
      "loss": 0.5236,
      "num_input_tokens_seen": 28304848,
      "step": 2255,
      "train_runtime": 24051.5272,
      "train_tokens_per_second": 1176.842
    },
    {
      "epoch": 0.45675020210185935,
      "grad_norm": 3.5953760147094727,
      "learning_rate": 2.2721692082097145e-06,
      "loss": 0.6035,
      "num_input_tokens_seen": 28368144,
      "step": 2260,
      "train_runtime": 24105.2817,
      "train_tokens_per_second": 1176.843
    },
    {
      "epoch": 0.45776071139854485,
      "grad_norm": 4.875913619995117,
      "learning_rate": 2.265877695505297e-06,
      "loss": 0.614,
      "num_input_tokens_seen": 28430864,
      "step": 2265,
      "train_runtime": 24158.4608,
      "train_tokens_per_second": 1176.849
    },
    {
      "epoch": 0.4587712206952304,
      "grad_norm": 3.841163396835327,
      "learning_rate": 2.259583503250582e-06,
      "loss": 0.692,
      "num_input_tokens_seen": 28493856,
      "step": 2270,
      "train_runtime": 24211.832,
      "train_tokens_per_second": 1176.857
    },
    {
      "epoch": 0.4597817299919159,
      "grad_norm": 3.399881362915039,
      "learning_rate": 2.253286694879268e-06,
      "loss": 0.4207,
      "num_input_tokens_seen": 28557504,
      "step": 2275,
      "train_runtime": 24265.7194,
      "train_tokens_per_second": 1176.866
    },
    {
      "epoch": 0.46079223928860147,
      "grad_norm": 2.3026201725006104,
      "learning_rate": 2.246987333851413e-06,
      "loss": 0.5122,
      "num_input_tokens_seen": 28619808,
      "step": 2280,
      "train_runtime": 24318.7491,
      "train_tokens_per_second": 1176.862
    },
    {
      "epoch": 0.46180274858528697,
      "grad_norm": 3.9023959636688232,
      "learning_rate": 2.2406854836528057e-06,
      "loss": 0.6128,
      "num_input_tokens_seen": 28683120,
      "step": 2285,
      "train_runtime": 24372.4432,
      "train_tokens_per_second": 1176.867
    },
    {
      "epoch": 0.46281325788197253,
      "grad_norm": 3.192598581314087,
      "learning_rate": 2.2343812077943186e-06,
      "loss": 0.5483,
      "num_input_tokens_seen": 28746608,
      "step": 2290,
      "train_runtime": 24426.2153,
      "train_tokens_per_second": 1176.875
    },
    {
      "epoch": 0.46382376717865803,
      "grad_norm": 3.297786235809326,
      "learning_rate": 2.2280745698112707e-06,
      "loss": 0.5788,
      "num_input_tokens_seen": 28809296,
      "step": 2295,
      "train_runtime": 24479.4304,
      "train_tokens_per_second": 1176.878
    },
    {
      "epoch": 0.4648342764753436,
      "grad_norm": 3.088548183441162,
      "learning_rate": 2.2217656332627867e-06,
      "loss": 0.6322,
      "num_input_tokens_seen": 28871168,
      "step": 2300,
      "train_runtime": 24532.0192,
      "train_tokens_per_second": 1176.877
    },
    {
      "epoch": 0.4658447857720291,
      "grad_norm": 3.522118091583252,
      "learning_rate": 2.215454461731158e-06,
      "loss": 0.5923,
      "num_input_tokens_seen": 28933280,
      "step": 2305,
      "train_runtime": 24585.4405,
      "train_tokens_per_second": 1176.846
    },
    {
      "epoch": 0.46685529506871465,
      "grad_norm": 3.0257456302642822,
      "learning_rate": 2.209141118821198e-06,
      "loss": 0.5235,
      "num_input_tokens_seen": 28995472,
      "step": 2310,
      "train_runtime": 24638.2796,
      "train_tokens_per_second": 1176.846
    },
    {
      "epoch": 0.46786580436540015,
      "grad_norm": 3.284646511077881,
      "learning_rate": 2.202825668159605e-06,
      "loss": 0.4929,
      "num_input_tokens_seen": 29058032,
      "step": 2315,
      "train_runtime": 24691.4402,
      "train_tokens_per_second": 1176.846
    },
    {
      "epoch": 0.4688763136620857,
      "grad_norm": 2.997713327407837,
      "learning_rate": 2.19650817339432e-06,
      "loss": 0.5742,
      "num_input_tokens_seen": 29120944,
      "step": 2320,
      "train_runtime": 24744.82,
      "train_tokens_per_second": 1176.85
    },
    {
      "epoch": 0.4698868229587712,
      "grad_norm": 2.97430419921875,
      "learning_rate": 2.1901886981938834e-06,
      "loss": 0.5224,
      "num_input_tokens_seen": 29183424,
      "step": 2325,
      "train_runtime": 24797.6982,
      "train_tokens_per_second": 1176.86
    },
    {
      "epoch": 0.47089733225545677,
      "grad_norm": 2.6886940002441406,
      "learning_rate": 2.1838673062467955e-06,
      "loss": 0.5408,
      "num_input_tokens_seen": 29246880,
      "step": 2330,
      "train_runtime": 24851.6872,
      "train_tokens_per_second": 1176.857
    },
    {
      "epoch": 0.47190784155214227,
      "grad_norm": 3.1506166458129883,
      "learning_rate": 2.177544061260874e-06,
      "loss": 0.634,
      "num_input_tokens_seen": 29309408,
      "step": 2335,
      "train_runtime": 24904.7849,
      "train_tokens_per_second": 1176.859
    },
    {
      "epoch": 0.47291835084882783,
      "grad_norm": 3.1047439575195312,
      "learning_rate": 2.1712190269626104e-06,
      "loss": 0.6083,
      "num_input_tokens_seen": 29371920,
      "step": 2340,
      "train_runtime": 24957.8435,
      "train_tokens_per_second": 1176.861
    },
    {
      "epoch": 0.47392886014551333,
      "grad_norm": 4.408260345458984,
      "learning_rate": 2.164892267096531e-06,
      "loss": 0.6487,
      "num_input_tokens_seen": 29435344,
      "step": 2345,
      "train_runtime": 25011.5126,
      "train_tokens_per_second": 1176.872
    },
    {
      "epoch": 0.4749393694421989,
      "grad_norm": 4.246489524841309,
      "learning_rate": 2.1585638454245515e-06,
      "loss": 0.5801,
      "num_input_tokens_seen": 29498288,
      "step": 2350,
      "train_runtime": 25064.8796,
      "train_tokens_per_second": 1176.877
    },
    {
      "epoch": 0.4759498787388844,
      "grad_norm": 3.386871576309204,
      "learning_rate": 2.1522338257253364e-06,
      "loss": 0.5921,
      "num_input_tokens_seen": 29561136,
      "step": 2355,
      "train_runtime": 25118.0867,
      "train_tokens_per_second": 1176.886
    },
    {
      "epoch": 0.47696038803556995,
      "grad_norm": 2.8586254119873047,
      "learning_rate": 2.1459022717936533e-06,
      "loss": 0.6469,
      "num_input_tokens_seen": 29623936,
      "step": 2360,
      "train_runtime": 25171.2778,
      "train_tokens_per_second": 1176.894
    },
    {
      "epoch": 0.47797089733225545,
      "grad_norm": 5.2786078453063965,
      "learning_rate": 2.1395692474397344e-06,
      "loss": 0.5856,
      "num_input_tokens_seen": 29687088,
      "step": 2365,
      "train_runtime": 25224.7302,
      "train_tokens_per_second": 1176.904
    },
    {
      "epoch": 0.478981406628941,
      "grad_norm": 2.149900197982788,
      "learning_rate": 2.13323481648863e-06,
      "loss": 0.5538,
      "num_input_tokens_seen": 29750672,
      "step": 2370,
      "train_runtime": 25278.496,
      "train_tokens_per_second": 1176.916
    },
    {
      "epoch": 0.4799919159256265,
      "grad_norm": 3.0774247646331787,
      "learning_rate": 2.1268990427795655e-06,
      "loss": 0.5475,
      "num_input_tokens_seen": 29812096,
      "step": 2375,
      "train_runtime": 25330.6417,
      "train_tokens_per_second": 1176.918
    },
    {
      "epoch": 0.48100242522231207,
      "grad_norm": 3.8270184993743896,
      "learning_rate": 2.1205619901653e-06,
      "loss": 0.5856,
      "num_input_tokens_seen": 29875408,
      "step": 2380,
      "train_runtime": 25384.2308,
      "train_tokens_per_second": 1176.928
    },
    {
      "epoch": 0.48201293451899757,
      "grad_norm": 3.974811553955078,
      "learning_rate": 2.1142237225114807e-06,
      "loss": 0.6462,
      "num_input_tokens_seen": 29938336,
      "step": 2385,
      "train_runtime": 25437.489,
      "train_tokens_per_second": 1176.938
    },
    {
      "epoch": 0.48302344381568313,
      "grad_norm": 3.6479172706604004,
      "learning_rate": 2.1078843036960016e-06,
      "loss": 0.5204,
      "num_input_tokens_seen": 30001616,
      "step": 2390,
      "train_runtime": 25491.078,
      "train_tokens_per_second": 1176.946
    },
    {
      "epoch": 0.48403395311236863,
      "grad_norm": 4.979466438293457,
      "learning_rate": 2.101543797608356e-06,
      "loss": 0.7895,
      "num_input_tokens_seen": 30064208,
      "step": 2395,
      "train_runtime": 25544.0125,
      "train_tokens_per_second": 1176.957
    },
    {
      "epoch": 0.4850444624090542,
      "grad_norm": 3.3176639080047607,
      "learning_rate": 2.0952022681489965e-06,
      "loss": 0.6463,
      "num_input_tokens_seen": 30127456,
      "step": 2400,
      "train_runtime": 25597.6742,
      "train_tokens_per_second": 1176.961
    },
    {
      "epoch": 0.4860549717057397,
      "grad_norm": 4.10718297958374,
      "learning_rate": 2.08885977922869e-06,
      "loss": 0.4849,
      "num_input_tokens_seen": 30190160,
      "step": 2405,
      "train_runtime": 25651.9998,
      "train_tokens_per_second": 1176.913
    },
    {
      "epoch": 0.48706548100242525,
      "grad_norm": 4.854747772216797,
      "learning_rate": 2.0825163947678696e-06,
      "loss": 0.6434,
      "num_input_tokens_seen": 30252688,
      "step": 2410,
      "train_runtime": 25705.0133,
      "train_tokens_per_second": 1176.918
    },
    {
      "epoch": 0.48807599029911075,
      "grad_norm": 4.122872829437256,
      "learning_rate": 2.076172178695998e-06,
      "loss": 0.5656,
      "num_input_tokens_seen": 30314832,
      "step": 2415,
      "train_runtime": 25757.7896,
      "train_tokens_per_second": 1176.919
    },
    {
      "epoch": 0.4890864995957963,
      "grad_norm": 3.3896431922912598,
      "learning_rate": 2.0698271949509163e-06,
      "loss": 0.5289,
      "num_input_tokens_seen": 30377216,
      "step": 2420,
      "train_runtime": 25810.6839,
      "train_tokens_per_second": 1176.924
    },
    {
      "epoch": 0.4900970088924818,
      "grad_norm": 3.020862579345703,
      "learning_rate": 2.0634815074782036e-06,
      "loss": 0.4339,
      "num_input_tokens_seen": 30440160,
      "step": 2425,
      "train_runtime": 25864.0635,
      "train_tokens_per_second": 1176.929
    },
    {
      "epoch": 0.4911075181891673,
      "grad_norm": 3.2073497772216797,
      "learning_rate": 2.0571351802305296e-06,
      "loss": 0.6227,
      "num_input_tokens_seen": 30501344,
      "step": 2430,
      "train_runtime": 25916.0651,
      "train_tokens_per_second": 1176.928
    },
    {
      "epoch": 0.49211802748585287,
      "grad_norm": 3.301938533782959,
      "learning_rate": 2.050788277167014e-06,
      "loss": 0.6725,
      "num_input_tokens_seen": 30564768,
      "step": 2435,
      "train_runtime": 25969.8583,
      "train_tokens_per_second": 1176.932
    },
    {
      "epoch": 0.4931285367825384,
      "grad_norm": 4.361098289489746,
      "learning_rate": 2.0444408622525785e-06,
      "loss": 0.5814,
      "num_input_tokens_seen": 30628256,
      "step": 2440,
      "train_runtime": 26023.7132,
      "train_tokens_per_second": 1176.936
    },
    {
      "epoch": 0.49413904607922393,
      "grad_norm": 4.206846714019775,
      "learning_rate": 2.0380929994573035e-06,
      "loss": 0.4539,
      "num_input_tokens_seen": 30690640,
      "step": 2445,
      "train_runtime": 26076.5677,
      "train_tokens_per_second": 1176.943
    },
    {
      "epoch": 0.49514955537590943,
      "grad_norm": 2.6016950607299805,
      "learning_rate": 2.031744752755781e-06,
      "loss": 0.429,
      "num_input_tokens_seen": 30753760,
      "step": 2450,
      "train_runtime": 26130.0478,
      "train_tokens_per_second": 1176.95
    },
    {
      "epoch": 0.496160064672595,
      "grad_norm": 2.6760683059692383,
      "learning_rate": 2.025396186126477e-06,
      "loss": 0.524,
      "num_input_tokens_seen": 30816416,
      "step": 2455,
      "train_runtime": 26183.1948,
      "train_tokens_per_second": 1176.954
    },
    {
      "epoch": 0.4971705739692805,
      "grad_norm": 4.106766223907471,
      "learning_rate": 2.0190473635510767e-06,
      "loss": 0.6713,
      "num_input_tokens_seen": 30879488,
      "step": 2460,
      "train_runtime": 26236.7324,
      "train_tokens_per_second": 1176.956
    },
    {
      "epoch": 0.49818108326596605,
      "grad_norm": 3.045276641845703,
      "learning_rate": 2.012698349013848e-06,
      "loss": 0.524,
      "num_input_tokens_seen": 30942304,
      "step": 2465,
      "train_runtime": 26289.9539,
      "train_tokens_per_second": 1176.963
    },
    {
      "epoch": 0.49919159256265155,
      "grad_norm": 1.9487931728363037,
      "learning_rate": 2.0063492065009918e-06,
      "loss": 0.6109,
      "num_input_tokens_seen": 31005424,
      "step": 2470,
      "train_runtime": 26343.4104,
      "train_tokens_per_second": 1176.971
    },
    {
      "epoch": 0.5002021018593371,
      "grad_norm": 2.66090989112854,
      "learning_rate": 2e-06,
      "loss": 0.6026,
      "num_input_tokens_seen": 31070096,
      "step": 2475,
      "train_runtime": 26398.0017,
      "train_tokens_per_second": 1176.987
    },
    {
      "epoch": 0.5012126111560227,
      "grad_norm": 2.331244707107544,
      "learning_rate": 1.993650793499008e-06,
      "loss": 0.5379,
      "num_input_tokens_seen": 31133616,
      "step": 2480,
      "train_runtime": 26451.7907,
      "train_tokens_per_second": 1176.995
    },
    {
      "epoch": 0.5022231204527081,
      "grad_norm": 4.133775234222412,
      "learning_rate": 1.9873016509861525e-06,
      "loss": 0.5568,
      "num_input_tokens_seen": 31196320,
      "step": 2485,
      "train_runtime": 26505.0403,
      "train_tokens_per_second": 1176.996
    },
    {
      "epoch": 0.5032336297493937,
      "grad_norm": 3.273393154144287,
      "learning_rate": 1.980952636448923e-06,
      "loss": 0.5845,
      "num_input_tokens_seen": 31259376,
      "step": 2490,
      "train_runtime": 26558.5591,
      "train_tokens_per_second": 1176.998
    },
    {
      "epoch": 0.5042441390460792,
      "grad_norm": 3.290846586227417,
      "learning_rate": 1.9746038138735233e-06,
      "loss": 0.4871,
      "num_input_tokens_seen": 31322656,
      "step": 2495,
      "train_runtime": 26612.102,
      "train_tokens_per_second": 1177.008
    },
    {
      "epoch": 0.5052546483427648,
      "grad_norm": 3.3169267177581787,
      "learning_rate": 1.9682552472442185e-06,
      "loss": 0.6666,
      "num_input_tokens_seen": 31386752,
      "step": 2500,
      "train_runtime": 26666.2595,
      "train_tokens_per_second": 1177.021
    },
    {
      "epoch": 0.5062651576394502,
      "grad_norm": 2.6601037979125977,
      "learning_rate": 1.9619070005426976e-06,
      "loss": 0.523,
      "num_input_tokens_seen": 31449328,
      "step": 2505,
      "train_runtime": 26720.1592,
      "train_tokens_per_second": 1176.989
    },
    {
      "epoch": 0.5072756669361358,
      "grad_norm": 4.021296977996826,
      "learning_rate": 1.9555591377474217e-06,
      "loss": 0.5564,
      "num_input_tokens_seen": 31512544,
      "step": 2510,
      "train_runtime": 26773.6814,
      "train_tokens_per_second": 1176.997
    },
    {
      "epoch": 0.5082861762328214,
      "grad_norm": 3.4886465072631836,
      "learning_rate": 1.9492117228329856e-06,
      "loss": 0.53,
      "num_input_tokens_seen": 31576336,
      "step": 2515,
      "train_runtime": 26827.6494,
      "train_tokens_per_second": 1177.007
    },
    {
      "epoch": 0.5092966855295069,
      "grad_norm": 2.5411412715911865,
      "learning_rate": 1.94286481976947e-06,
      "loss": 0.6632,
      "num_input_tokens_seen": 31639104,
      "step": 2520,
      "train_runtime": 26880.8152,
      "train_tokens_per_second": 1177.014
    },
    {
      "epoch": 0.5103071948261924,
      "grad_norm": 4.133396148681641,
      "learning_rate": 1.936518492521797e-06,
      "loss": 0.6558,
      "num_input_tokens_seen": 31702048,
      "step": 2525,
      "train_runtime": 26934.2586,
      "train_tokens_per_second": 1177.016
    },
    {
      "epoch": 0.5113177041228779,
      "grad_norm": 3.858910322189331,
      "learning_rate": 1.930172805049084e-06,
      "loss": 0.5018,
      "num_input_tokens_seen": 31765088,
      "step": 2530,
      "train_runtime": 26987.615,
      "train_tokens_per_second": 1177.025
    },
    {
      "epoch": 0.5123282134195635,
      "grad_norm": 2.6547317504882812,
      "learning_rate": 1.923827821304002e-06,
      "loss": 0.4454,
      "num_input_tokens_seen": 31827840,
      "step": 2535,
      "train_runtime": 27040.8397,
      "train_tokens_per_second": 1177.029
    },
    {
      "epoch": 0.513338722716249,
      "grad_norm": 3.5943284034729004,
      "learning_rate": 1.9174836052321302e-06,
      "loss": 0.5974,
      "num_input_tokens_seen": 31889200,
      "step": 2540,
      "train_runtime": 27093.0741,
      "train_tokens_per_second": 1177.024
    },
    {
      "epoch": 0.5143492320129345,
      "grad_norm": 2.893228769302368,
      "learning_rate": 1.9111402207713107e-06,
      "loss": 0.4928,
      "num_input_tokens_seen": 31951328,
      "step": 2545,
      "train_runtime": 27145.8973,
      "train_tokens_per_second": 1177.022
    },
    {
      "epoch": 0.51535974130962,
      "grad_norm": 3.0926644802093506,
      "learning_rate": 1.9047977318510033e-06,
      "loss": 0.4805,
      "num_input_tokens_seen": 32015216,
      "step": 2550,
      "train_runtime": 27199.8688,
      "train_tokens_per_second": 1177.036
    },
    {
      "epoch": 0.5163702506063056,
      "grad_norm": 3.0089261531829834,
      "learning_rate": 1.8984562023916442e-06,
      "loss": 0.599,
      "num_input_tokens_seen": 32078272,
      "step": 2555,
      "train_runtime": 27253.2584,
      "train_tokens_per_second": 1177.044
    },
    {
      "epoch": 0.5173807599029911,
      "grad_norm": 3.5797200202941895,
      "learning_rate": 1.892115696303998e-06,
      "loss": 0.5942,
      "num_input_tokens_seen": 32139904,
      "step": 2560,
      "train_runtime": 27305.7387,
      "train_tokens_per_second": 1177.038
    },
    {
      "epoch": 0.5183912691996766,
      "grad_norm": 3.3238866329193115,
      "learning_rate": 1.8857762774885193e-06,
      "loss": 0.7289,
      "num_input_tokens_seen": 32201328,
      "step": 2565,
      "train_runtime": 27358.002,
      "train_tokens_per_second": 1177.035
    },
    {
      "epoch": 0.5194017784963622,
      "grad_norm": 4.136465549468994,
      "learning_rate": 1.8794380098347002e-06,
      "loss": 0.6213,
      "num_input_tokens_seen": 32263728,
      "step": 2570,
      "train_runtime": 27410.9988,
      "train_tokens_per_second": 1177.036
    },
    {
      "epoch": 0.5204122877930477,
      "grad_norm": 6.523774147033691,
      "learning_rate": 1.873100957220435e-06,
      "loss": 0.6097,
      "num_input_tokens_seen": 32326592,
      "step": 2575,
      "train_runtime": 27464.192,
      "train_tokens_per_second": 1177.045
    },
    {
      "epoch": 0.5214227970897333,
      "grad_norm": 3.110567331314087,
      "learning_rate": 1.86676518351137e-06,
      "loss": 0.6956,
      "num_input_tokens_seen": 32388048,
      "step": 2580,
      "train_runtime": 27516.6515,
      "train_tokens_per_second": 1177.034
    },
    {
      "epoch": 0.5224333063864187,
      "grad_norm": 4.714151382446289,
      "learning_rate": 1.8604307525602657e-06,
      "loss": 0.6323,
      "num_input_tokens_seen": 32450368,
      "step": 2585,
      "train_runtime": 27569.5968,
      "train_tokens_per_second": 1177.035
    },
    {
      "epoch": 0.5234438156831043,
      "grad_norm": 2.751249313354492,
      "learning_rate": 1.8540977282063467e-06,
      "loss": 0.5404,
      "num_input_tokens_seen": 32513456,
      "step": 2590,
      "train_runtime": 27623.104,
      "train_tokens_per_second": 1177.038
    },
    {
      "epoch": 0.5244543249797898,
      "grad_norm": 4.55974817276001,
      "learning_rate": 1.847766174274664e-06,
      "loss": 0.7468,
      "num_input_tokens_seen": 32576752,
      "step": 2595,
      "train_runtime": 27676.748,
      "train_tokens_per_second": 1177.044
    },
    {
      "epoch": 0.5254648342764754,
      "grad_norm": 5.043099403381348,
      "learning_rate": 1.841436154575448e-06,
      "loss": 0.6323,
      "num_input_tokens_seen": 32638704,
      "step": 2600,
      "train_runtime": 27729.3997,
      "train_tokens_per_second": 1177.043
    },
    {
      "epoch": 0.5264753435731608,
      "grad_norm": 2.9323129653930664,
      "learning_rate": 1.8351077329034694e-06,
      "loss": 0.4728,
      "num_input_tokens_seen": 32701600,
      "step": 2605,
      "train_runtime": 27783.6035,
      "train_tokens_per_second": 1177.011
    },
    {
      "epoch": 0.5274858528698464,
      "grad_norm": 5.034780502319336,
      "learning_rate": 1.8287809730373898e-06,
      "loss": 0.6283,
      "num_input_tokens_seen": 32764096,
      "step": 2610,
      "train_runtime": 27836.6263,
      "train_tokens_per_second": 1177.014
    },
    {
      "epoch": 0.528496362166532,
      "grad_norm": 2.3806684017181396,
      "learning_rate": 1.8224559387391262e-06,
      "loss": 0.4812,
      "num_input_tokens_seen": 32826736,
      "step": 2615,
      "train_runtime": 27889.7655,
      "train_tokens_per_second": 1177.017
    },
    {
      "epoch": 0.5295068714632175,
      "grad_norm": 3.487856149673462,
      "learning_rate": 1.8161326937532045e-06,
      "loss": 0.6574,
      "num_input_tokens_seen": 32889680,
      "step": 2620,
      "train_runtime": 27943.0395,
      "train_tokens_per_second": 1177.026
    },
    {
      "epoch": 0.530517380759903,
      "grad_norm": 2.5578949451446533,
      "learning_rate": 1.809811301806117e-06,
      "loss": 0.6527,
      "num_input_tokens_seen": 32952704,
      "step": 2625,
      "train_runtime": 27996.3896,
      "train_tokens_per_second": 1177.034
    },
    {
      "epoch": 0.5315278900565885,
      "grad_norm": 4.2749247550964355,
      "learning_rate": 1.80349182660568e-06,
      "loss": 0.7267,
      "num_input_tokens_seen": 33014928,
      "step": 2630,
      "train_runtime": 28049.2626,
      "train_tokens_per_second": 1177.034
    },
    {
      "epoch": 0.5325383993532741,
      "grad_norm": 3.668194532394409,
      "learning_rate": 1.7971743318403946e-06,
      "loss": 0.7937,
      "num_input_tokens_seen": 33077232,
      "step": 2635,
      "train_runtime": 28102.2932,
      "train_tokens_per_second": 1177.03
    },
    {
      "epoch": 0.5335489086499596,
      "grad_norm": 3.4752485752105713,
      "learning_rate": 1.790858881178802e-06,
      "loss": 0.5602,
      "num_input_tokens_seen": 33139200,
      "step": 2640,
      "train_runtime": 28154.9476,
      "train_tokens_per_second": 1177.029
    },
    {
      "epoch": 0.5345594179466451,
      "grad_norm": 3.2372093200683594,
      "learning_rate": 1.7845455382688424e-06,
      "loss": 0.4656,
      "num_input_tokens_seen": 33202752,
      "step": 2645,
      "train_runtime": 28208.7382,
      "train_tokens_per_second": 1177.038
    },
    {
      "epoch": 0.5355699272433306,
      "grad_norm": 3.158914089202881,
      "learning_rate": 1.7782343667372133e-06,
      "loss": 0.495,
      "num_input_tokens_seen": 33265168,
      "step": 2650,
      "train_runtime": 28261.831,
      "train_tokens_per_second": 1177.035
    },
    {
      "epoch": 0.5365804365400162,
      "grad_norm": 3.9026753902435303,
      "learning_rate": 1.7719254301887292e-06,
      "loss": 0.6704,
      "num_input_tokens_seen": 33328032,
      "step": 2655,
      "train_runtime": 28315.0289,
      "train_tokens_per_second": 1177.044
    },
    {
      "epoch": 0.5375909458367018,
      "grad_norm": 3.3987326622009277,
      "learning_rate": 1.7656187922056813e-06,
      "loss": 0.488,
      "num_input_tokens_seen": 33389840,
      "step": 2660,
      "train_runtime": 28367.5925,
      "train_tokens_per_second": 1177.042
    },
    {
      "epoch": 0.5386014551333872,
      "grad_norm": 4.036410808563232,
      "learning_rate": 1.7593145163471945e-06,
      "loss": 0.6028,
      "num_input_tokens_seen": 33452544,
      "step": 2665,
      "train_runtime": 28420.7084,
      "train_tokens_per_second": 1177.048
    },
    {
      "epoch": 0.5396119644300728,
      "grad_norm": 3.9410598278045654,
      "learning_rate": 1.7530126661485871e-06,
      "loss": 0.6258,
      "num_input_tokens_seen": 33515056,
      "step": 2670,
      "train_runtime": 28473.7908,
      "train_tokens_per_second": 1177.049
    },
    {
      "epoch": 0.5406224737267583,
      "grad_norm": 2.654327869415283,
      "learning_rate": 1.7467133051207323e-06,
      "loss": 0.5702,
      "num_input_tokens_seen": 33576896,
      "step": 2675,
      "train_runtime": 28526.3368,
      "train_tokens_per_second": 1177.049
    },
    {
      "epoch": 0.5416329830234439,
      "grad_norm": 3.6612696647644043,
      "learning_rate": 1.7404164967494175e-06,
      "loss": 0.4773,
      "num_input_tokens_seen": 33639392,
      "step": 2680,
      "train_runtime": 28579.3196,
      "train_tokens_per_second": 1177.054
    },
    {
      "epoch": 0.5426434923201293,
      "grad_norm": 3.2474873065948486,
      "learning_rate": 1.7341223044947037e-06,
      "loss": 0.7007,
      "num_input_tokens_seen": 33702848,
      "step": 2685,
      "train_runtime": 28633.0926,
      "train_tokens_per_second": 1177.059
    },
    {
      "epoch": 0.5436540016168149,
      "grad_norm": 3.1331093311309814,
      "learning_rate": 1.7278307917902855e-06,
      "loss": 0.5401,
      "num_input_tokens_seen": 33765488,
      "step": 2690,
      "train_runtime": 28686.2371,
      "train_tokens_per_second": 1177.062
    },
    {
      "epoch": 0.5446645109135004,
      "grad_norm": 4.301627159118652,
      "learning_rate": 1.721542022042853e-06,
      "loss": 0.6514,
      "num_input_tokens_seen": 33828128,
      "step": 2695,
      "train_runtime": 28739.4218,
      "train_tokens_per_second": 1177.064
    },
    {
      "epoch": 0.5456750202101859,
      "grad_norm": 2.708400011062622,
      "learning_rate": 1.7152560586314537e-06,
      "loss": 0.6403,
      "num_input_tokens_seen": 33892064,
      "step": 2700,
      "train_runtime": 28793.48,
      "train_tokens_per_second": 1177.074
    },
    {
      "epoch": 0.5466855295068714,
      "grad_norm": 3.843168020248413,
      "learning_rate": 1.7089729649068515e-06,
      "loss": 0.6461,
      "num_input_tokens_seen": 33954048,
      "step": 2705,
      "train_runtime": 28846.9758,
      "train_tokens_per_second": 1177.04
    },
    {
      "epoch": 0.547696038803557,
      "grad_norm": 3.0779361724853516,
      "learning_rate": 1.7026928041908888e-06,
      "loss": 0.5807,
      "num_input_tokens_seen": 34016976,
      "step": 2710,
      "train_runtime": 28900.3627,
      "train_tokens_per_second": 1177.043
    },
    {
      "epoch": 0.5487065481002426,
      "grad_norm": 3.8177998065948486,
      "learning_rate": 1.696415639775849e-06,
      "loss": 0.5659,
      "num_input_tokens_seen": 34079504,
      "step": 2715,
      "train_runtime": 28953.3981,
      "train_tokens_per_second": 1177.047
    },
    {
      "epoch": 0.549717057396928,
      "grad_norm": 2.2769408226013184,
      "learning_rate": 1.6901415349238184e-06,
      "loss": 0.5318,
      "num_input_tokens_seen": 34141744,
      "step": 2720,
      "train_runtime": 29006.1745,
      "train_tokens_per_second": 1177.051
    },
    {
      "epoch": 0.5507275666936136,
      "grad_norm": 3.684278726577759,
      "learning_rate": 1.68387055286605e-06,
      "loss": 0.4676,
      "num_input_tokens_seen": 34204624,
      "step": 2725,
      "train_runtime": 29059.4409,
      "train_tokens_per_second": 1177.057
    },
    {
      "epoch": 0.5517380759902991,
      "grad_norm": 3.358304262161255,
      "learning_rate": 1.6776027568023228e-06,
      "loss": 0.5864,
      "num_input_tokens_seen": 34266416,
      "step": 2730,
      "train_runtime": 29111.9828,
      "train_tokens_per_second": 1177.055
    },
    {
      "epoch": 0.5527485852869847,
      "grad_norm": 3.7341976165771484,
      "learning_rate": 1.671338209900308e-06,
      "loss": 0.7664,
      "num_input_tokens_seen": 34329520,
      "step": 2735,
      "train_runtime": 29165.5923,
      "train_tokens_per_second": 1177.055
    },
    {
      "epoch": 0.5537590945836701,
      "grad_norm": 3.217775583267212,
      "learning_rate": 1.6650769752949303e-06,
      "loss": 0.6361,
      "num_input_tokens_seen": 34391344,
      "step": 2740,
      "train_runtime": 29218.1803,
      "train_tokens_per_second": 1177.053
    },
    {
      "epoch": 0.5547696038803557,
      "grad_norm": 6.479480266571045,
      "learning_rate": 1.6588191160877357e-06,
      "loss": 0.7617,
      "num_input_tokens_seen": 34454784,
      "step": 2745,
      "train_runtime": 29272.0483,
      "train_tokens_per_second": 1177.054
    },
    {
      "epoch": 0.5557801131770412,
      "grad_norm": 2.763436794281006,
      "learning_rate": 1.6525646953462472e-06,
      "loss": 0.5832,
      "num_input_tokens_seen": 34517008,
      "step": 2750,
      "train_runtime": 29324.9062,
      "train_tokens_per_second": 1177.054
    },
    {
      "epoch": 0.5567906224737268,
      "grad_norm": 3.1597933769226074,
      "learning_rate": 1.64631377610334e-06,
      "loss": 0.4843,
      "num_input_tokens_seen": 34580624,
      "step": 2755,
      "train_runtime": 29378.7379,
      "train_tokens_per_second": 1177.063
    },
    {
      "epoch": 0.5578011317704122,
      "grad_norm": 3.1697349548339844,
      "learning_rate": 1.6400664213565973e-06,
      "loss": 0.5711,
      "num_input_tokens_seen": 34643136,
      "step": 2760,
      "train_runtime": 29431.7261,
      "train_tokens_per_second": 1177.068
    },
    {
      "epoch": 0.5588116410670978,
      "grad_norm": 3.5070974826812744,
      "learning_rate": 1.6338226940676797e-06,
      "loss": 0.662,
      "num_input_tokens_seen": 34705136,
      "step": 2765,
      "train_runtime": 29484.4351,
      "train_tokens_per_second": 1177.066
    },
    {
      "epoch": 0.5598221503637834,
      "grad_norm": 3.2680013179779053,
      "learning_rate": 1.6275826571616899e-06,
      "loss": 0.4641,
      "num_input_tokens_seen": 34768032,
      "step": 2770,
      "train_runtime": 29537.8034,
      "train_tokens_per_second": 1177.069
    },
    {
      "epoch": 0.5608326596604689,
      "grad_norm": 2.9452290534973145,
      "learning_rate": 1.6213463735265392e-06,
      "loss": 0.6824,
      "num_input_tokens_seen": 34830432,
      "step": 2775,
      "train_runtime": 29590.8353,
      "train_tokens_per_second": 1177.068
    },
    {
      "epoch": 0.5618431689571544,
      "grad_norm": 4.4741902351379395,
      "learning_rate": 1.615113906012312e-06,
      "loss": 0.5922,
      "num_input_tokens_seen": 34892944,
      "step": 2780,
      "train_runtime": 29643.9128,
      "train_tokens_per_second": 1177.069
    },
    {
      "epoch": 0.5628536782538399,
      "grad_norm": 3.7714080810546875,
      "learning_rate": 1.608885317430633e-06,
      "loss": 0.4696,
      "num_input_tokens_seen": 34954960,
      "step": 2785,
      "train_runtime": 29696.6018,
      "train_tokens_per_second": 1177.069
    },
    {
      "epoch": 0.5638641875505255,
      "grad_norm": 3.9216787815093994,
      "learning_rate": 1.602660670554035e-06,
      "loss": 0.5547,
      "num_input_tokens_seen": 35019216,
      "step": 2790,
      "train_runtime": 29750.9638,
      "train_tokens_per_second": 1177.078
    },
    {
      "epoch": 0.564874696847211,
      "grad_norm": 4.057190418243408,
      "learning_rate": 1.5964400281153263e-06,
      "loss": 0.6393,
      "num_input_tokens_seen": 35081168,
      "step": 2795,
      "train_runtime": 29803.8014,
      "train_tokens_per_second": 1177.07
    },
    {
      "epoch": 0.5658852061438965,
      "grad_norm": 4.745249271392822,
      "learning_rate": 1.5902234528069583e-06,
      "loss": 0.6,
      "num_input_tokens_seen": 35143328,
      "step": 2800,
      "train_runtime": 29856.6779,
      "train_tokens_per_second": 1177.068
    },
    {
      "epoch": 0.566895715440582,
      "grad_norm": 2.7646138668060303,
      "learning_rate": 1.5840110072803908e-06,
      "loss": 0.7674,
      "num_input_tokens_seen": 35206720,
      "step": 2805,
      "train_runtime": 29911.2382,
      "train_tokens_per_second": 1177.04
    },
    {
      "epoch": 0.5679062247372676,
      "grad_norm": 3.73464298248291,
      "learning_rate": 1.5778027541454656e-06,
      "loss": 0.6415,
      "num_input_tokens_seen": 35269072,
      "step": 2810,
      "train_runtime": 29964.2128,
      "train_tokens_per_second": 1177.04
    },
    {
      "epoch": 0.5689167340339532,
      "grad_norm": 4.649129390716553,
      "learning_rate": 1.571598755969772e-06,
      "loss": 0.6078,
      "num_input_tokens_seen": 35331792,
      "step": 2815,
      "train_runtime": 30017.4256,
      "train_tokens_per_second": 1177.043
    },
    {
      "epoch": 0.5699272433306386,
      "grad_norm": 3.0686893463134766,
      "learning_rate": 1.5653990752780187e-06,
      "loss": 0.555,
      "num_input_tokens_seen": 35394368,
      "step": 2820,
      "train_runtime": 30070.5756,
      "train_tokens_per_second": 1177.043
    },
    {
      "epoch": 0.5709377526273242,
      "grad_norm": 3.10135817527771,
      "learning_rate": 1.5592037745513998e-06,
      "loss": 0.8557,
      "num_input_tokens_seen": 35456352,
      "step": 2825,
      "train_runtime": 30123.2634,
      "train_tokens_per_second": 1177.042
    },
    {
      "epoch": 0.5719482619240097,
      "grad_norm": 4.063092231750488,
      "learning_rate": 1.5530129162269689e-06,
      "loss": 0.705,
      "num_input_tokens_seen": 35519600,
      "step": 2830,
      "train_runtime": 30176.9107,
      "train_tokens_per_second": 1177.046
    },
    {
      "epoch": 0.5729587712206953,
      "grad_norm": 3.9732825756073,
      "learning_rate": 1.5468265626970083e-06,
      "loss": 0.6634,
      "num_input_tokens_seen": 35581472,
      "step": 2835,
      "train_runtime": 30229.5048,
      "train_tokens_per_second": 1177.044
    },
    {
      "epoch": 0.5739692805173807,
      "grad_norm": 3.480329990386963,
      "learning_rate": 1.540644776308402e-06,
      "loss": 0.4211,
      "num_input_tokens_seen": 35643376,
      "step": 2840,
      "train_runtime": 30282.0547,
      "train_tokens_per_second": 1177.046
    },
    {
      "epoch": 0.5749797898140663,
      "grad_norm": 3.160053253173828,
      "learning_rate": 1.5344676193620018e-06,
      "loss": 0.5592,
      "num_input_tokens_seen": 35705520,
      "step": 2845,
      "train_runtime": 30334.7951,
      "train_tokens_per_second": 1177.048
    },
    {
      "epoch": 0.5759902991107518,
      "grad_norm": 3.4342527389526367,
      "learning_rate": 1.5282951541120072e-06,
      "loss": 0.5074,
      "num_input_tokens_seen": 35768304,
      "step": 2850,
      "train_runtime": 30387.9918,
      "train_tokens_per_second": 1177.054
    },
    {
      "epoch": 0.5770008084074374,
      "grad_norm": 3.730576992034912,
      "learning_rate": 1.5221274427653324e-06,
      "loss": 0.5422,
      "num_input_tokens_seen": 35830992,
      "step": 2855,
      "train_runtime": 30441.2366,
      "train_tokens_per_second": 1177.054
    },
    {
      "epoch": 0.5780113177041228,
      "grad_norm": 3.450794219970703,
      "learning_rate": 1.5159645474809828e-06,
      "loss": 0.482,
      "num_input_tokens_seen": 35893008,
      "step": 2860,
      "train_runtime": 30493.8463,
      "train_tokens_per_second": 1177.057
    },
    {
      "epoch": 0.5790218270008084,
      "grad_norm": 3.011150598526001,
      "learning_rate": 1.509806530369423e-06,
      "loss": 0.5526,
      "num_input_tokens_seen": 35955136,
      "step": 2865,
      "train_runtime": 30546.4618,
      "train_tokens_per_second": 1177.064
    },
    {
      "epoch": 0.580032336297494,
      "grad_norm": 3.649747371673584,
      "learning_rate": 1.5036534534919594e-06,
      "loss": 0.5699,
      "num_input_tokens_seen": 36017328,
      "step": 2870,
      "train_runtime": 30599.2875,
      "train_tokens_per_second": 1177.064
    },
    {
      "epoch": 0.5810428455941795,
      "grad_norm": 3.952714443206787,
      "learning_rate": 1.497505378860107e-06,
      "loss": 0.5045,
      "num_input_tokens_seen": 36079136,
      "step": 2875,
      "train_runtime": 30651.7085,
      "train_tokens_per_second": 1177.068
    },
    {
      "epoch": 0.582053354890865,
      "grad_norm": 3.5933213233947754,
      "learning_rate": 1.4913623684349697e-06,
      "loss": 0.5496,
      "num_input_tokens_seen": 36142544,
      "step": 2880,
      "train_runtime": 30705.2658,
      "train_tokens_per_second": 1177.08
    },
    {
      "epoch": 0.5830638641875505,
      "grad_norm": 3.6288535594940186,
      "learning_rate": 1.4852244841266103e-06,
      "loss": 0.428,
      "num_input_tokens_seen": 36205104,
      "step": 2885,
      "train_runtime": 30758.2355,
      "train_tokens_per_second": 1177.087
    },
    {
      "epoch": 0.5840743734842361,
      "grad_norm": 3.1761391162872314,
      "learning_rate": 1.4790917877934339e-06,
      "loss": 0.5565,
      "num_input_tokens_seen": 36267552,
      "step": 2890,
      "train_runtime": 30811.2572,
      "train_tokens_per_second": 1177.088
    },
    {
      "epoch": 0.5850848827809216,
      "grad_norm": 3.45098876953125,
      "learning_rate": 1.4729643412415584e-06,
      "loss": 0.4282,
      "num_input_tokens_seen": 36329904,
      "step": 2895,
      "train_runtime": 30864.1692,
      "train_tokens_per_second": 1177.09
    },
    {
      "epoch": 0.5860953920776071,
      "grad_norm": 3.616436719894409,
      "learning_rate": 1.4668422062241942e-06,
      "loss": 0.6043,
      "num_input_tokens_seen": 36392096,
      "step": 2900,
      "train_runtime": 30916.9695,
      "train_tokens_per_second": 1177.091
    },
    {
      "epoch": 0.5871059013742926,
      "grad_norm": 3.2372870445251465,
      "learning_rate": 1.4607254444410204e-06,
      "loss": 0.6567,
      "num_input_tokens_seen": 36454720,
      "step": 2905,
      "train_runtime": 30970.889,
      "train_tokens_per_second": 1177.064
    },
    {
      "epoch": 0.5881164106709782,
      "grad_norm": 2.9603569507598877,
      "learning_rate": 1.4546141175375643e-06,
      "loss": 0.5063,
      "num_input_tokens_seen": 36518400,
      "step": 2910,
      "train_runtime": 31024.8412,
      "train_tokens_per_second": 1177.07
    },
    {
      "epoch": 0.5891269199676638,
      "grad_norm": 3.7121551036834717,
      "learning_rate": 1.4485082871045818e-06,
      "loss": 0.6809,
      "num_input_tokens_seen": 36581632,
      "step": 2915,
      "train_runtime": 31078.3858,
      "train_tokens_per_second": 1177.076
    },
    {
      "epoch": 0.5901374292643492,
      "grad_norm": 5.524441242218018,
      "learning_rate": 1.4424080146774324e-06,
      "loss": 0.6173,
      "num_input_tokens_seen": 36643952,
      "step": 2920,
      "train_runtime": 31131.4643,
      "train_tokens_per_second": 1177.071
    },
    {
      "epoch": 0.5911479385610348,
      "grad_norm": 3.3063929080963135,
      "learning_rate": 1.4363133617354615e-06,
      "loss": 0.4493,
      "num_input_tokens_seen": 36704592,
      "step": 2925,
      "train_runtime": 31183.149,
      "train_tokens_per_second": 1177.065
    },
    {
      "epoch": 0.5921584478577203,
      "grad_norm": 5.018991470336914,
      "learning_rate": 1.4302243897013807e-06,
      "loss": 0.6481,
      "num_input_tokens_seen": 36766304,
      "step": 2930,
      "train_runtime": 31235.4924,
      "train_tokens_per_second": 1177.068
    },
    {
      "epoch": 0.5931689571544059,
      "grad_norm": 3.906702995300293,
      "learning_rate": 1.4241411599406497e-06,
      "loss": 0.6757,
      "num_input_tokens_seen": 36828128,
      "step": 2935,
      "train_runtime": 31288.1276,
      "train_tokens_per_second": 1177.064
    },
    {
      "epoch": 0.5941794664510913,
      "grad_norm": 2.6368072032928467,
      "learning_rate": 1.4180637337608565e-06,
      "loss": 0.4957,
      "num_input_tokens_seen": 36890752,
      "step": 2940,
      "train_runtime": 31341.2664,
      "train_tokens_per_second": 1177.066
    },
    {
      "epoch": 0.5951899757477769,
      "grad_norm": 3.4744346141815186,
      "learning_rate": 1.4119921724110983e-06,
      "loss": 0.6078,
      "num_input_tokens_seen": 36953232,
      "step": 2945,
      "train_runtime": 31394.232,
      "train_tokens_per_second": 1177.071
    },
    {
      "epoch": 0.5962004850444624,
      "grad_norm": 2.900623321533203,
      "learning_rate": 1.405926537081367e-06,
      "loss": 0.5363,
      "num_input_tokens_seen": 37015664,
      "step": 2950,
      "train_runtime": 31447.1644,
      "train_tokens_per_second": 1177.075
    },
    {
      "epoch": 0.597210994341148,
      "grad_norm": 3.193075656890869,
      "learning_rate": 1.3998668889019322e-06,
      "loss": 0.6405,
      "num_input_tokens_seen": 37078272,
      "step": 2955,
      "train_runtime": 31500.3514,
      "train_tokens_per_second": 1177.075
    },
    {
      "epoch": 0.5982215036378334,
      "grad_norm": 6.411989212036133,
      "learning_rate": 1.393813288942723e-06,
      "loss": 0.6563,
      "num_input_tokens_seen": 37141344,
      "step": 2960,
      "train_runtime": 31553.758,
      "train_tokens_per_second": 1177.081
    },
    {
      "epoch": 0.599232012934519,
      "grad_norm": 3.4912474155426025,
      "learning_rate": 1.3877657982127132e-06,
      "loss": 0.557,
      "num_input_tokens_seen": 37203936,
      "step": 2965,
      "train_runtime": 31606.8812,
      "train_tokens_per_second": 1177.083
    },
    {
      "epoch": 0.6002425222312046,
      "grad_norm": 3.5439951419830322,
      "learning_rate": 1.3817244776593082e-06,
      "loss": 0.5426,
      "num_input_tokens_seen": 37265456,
      "step": 2970,
      "train_runtime": 31659.2946,
      "train_tokens_per_second": 1177.078
    },
    {
      "epoch": 0.60125303152789,
      "grad_norm": 3.329246759414673,
      "learning_rate": 1.3756893881677285e-06,
      "loss": 0.6128,
      "num_input_tokens_seen": 37328304,
      "step": 2975,
      "train_runtime": 31712.5383,
      "train_tokens_per_second": 1177.083
    },
    {
      "epoch": 0.6022635408245756,
      "grad_norm": 3.0922765731811523,
      "learning_rate": 1.3696605905603993e-06,
      "loss": 0.4926,
      "num_input_tokens_seen": 37390976,
      "step": 2980,
      "train_runtime": 31765.6918,
      "train_tokens_per_second": 1177.087
    },
    {
      "epoch": 0.6032740501212611,
      "grad_norm": 3.0178754329681396,
      "learning_rate": 1.363638145596332e-06,
      "loss": 0.6319,
      "num_input_tokens_seen": 37453280,
      "step": 2985,
      "train_runtime": 31818.5965,
      "train_tokens_per_second": 1177.088
    },
    {
      "epoch": 0.6042845594179467,
      "grad_norm": 4.00108528137207,
      "learning_rate": 1.3576221139705185e-06,
      "loss": 0.5711,
      "num_input_tokens_seen": 37516864,
      "step": 2990,
      "train_runtime": 31872.4287,
      "train_tokens_per_second": 1177.095
    },
    {
      "epoch": 0.6052950687146321,
      "grad_norm": 3.4854142665863037,
      "learning_rate": 1.3516125563133134e-06,
      "loss": 0.4774,
      "num_input_tokens_seen": 37580432,
      "step": 2995,
      "train_runtime": 31926.1961,
      "train_tokens_per_second": 1177.103
    },
    {
      "epoch": 0.6063055780113177,
      "grad_norm": 3.0226337909698486,
      "learning_rate": 1.345609533189829e-06,
      "loss": 0.4931,
      "num_input_tokens_seen": 37643664,
      "step": 3000,
      "train_runtime": 31979.8554,
      "train_tokens_per_second": 1177.106
    },
    {
      "epoch": 0.6073160873080032,
      "grad_norm": 4.055277347564697,
      "learning_rate": 1.3396131050993176e-06,
      "loss": 0.77,
      "num_input_tokens_seen": 37705952,
      "step": 3005,
      "train_runtime": 32033.4775,
      "train_tokens_per_second": 1177.08
    },
    {
      "epoch": 0.6083265966046888,
      "grad_norm": 2.5480053424835205,
      "learning_rate": 1.3336233324745699e-06,
      "loss": 0.5696,
      "num_input_tokens_seen": 37769024,
      "step": 3010,
      "train_runtime": 32086.9294,
      "train_tokens_per_second": 1177.084
    },
    {
      "epoch": 0.6093371059013742,
      "grad_norm": 4.330738067626953,
      "learning_rate": 1.3276402756812996e-06,
      "loss": 0.5238,
      "num_input_tokens_seen": 37831232,
      "step": 3015,
      "train_runtime": 32139.7262,
      "train_tokens_per_second": 1177.086
    },
    {
      "epoch": 0.6103476151980598,
      "grad_norm": 3.1076090335845947,
      "learning_rate": 1.3216639950175382e-06,
      "loss": 0.7487,
      "num_input_tokens_seen": 37893824,
      "step": 3020,
      "train_runtime": 32192.7642,
      "train_tokens_per_second": 1177.091
    },
    {
      "epoch": 0.6113581244947454,
      "grad_norm": 2.774488925933838,
      "learning_rate": 1.3156945507130252e-06,
      "loss": 0.6147,
      "num_input_tokens_seen": 37956816,
      "step": 3025,
      "train_runtime": 32246.0973,
      "train_tokens_per_second": 1177.098
    },
    {
      "epoch": 0.6123686337914309,
      "grad_norm": 3.6871187686920166,
      "learning_rate": 1.309732002928605e-06,
      "loss": 0.6934,
      "num_input_tokens_seen": 38018384,
      "step": 3030,
      "train_runtime": 32298.4026,
      "train_tokens_per_second": 1177.098
    },
    {
      "epoch": 0.6133791430881164,
      "grad_norm": 3.3353817462921143,
      "learning_rate": 1.3037764117556159e-06,
      "loss": 0.5265,
      "num_input_tokens_seen": 38081264,
      "step": 3035,
      "train_runtime": 32351.7025,
      "train_tokens_per_second": 1177.102
    },
    {
      "epoch": 0.6143896523848019,
      "grad_norm": 2.8142507076263428,
      "learning_rate": 1.2978278372152867e-06,
      "loss": 0.4684,
      "num_input_tokens_seen": 38143744,
      "step": 3040,
      "train_runtime": 32404.6606,
      "train_tokens_per_second": 1177.107
    },
    {
      "epoch": 0.6154001616814875,
      "grad_norm": 4.695192813873291,
      "learning_rate": 1.2918863392581316e-06,
      "loss": 0.5557,
      "num_input_tokens_seen": 38206352,
      "step": 3045,
      "train_runtime": 32457.5907,
      "train_tokens_per_second": 1177.116
    },
    {
      "epoch": 0.616410670978173,
      "grad_norm": 3.7524161338806152,
      "learning_rate": 1.2859519777633474e-06,
      "loss": 0.5556,
      "num_input_tokens_seen": 38269760,
      "step": 3050,
      "train_runtime": 32511.2835,
      "train_tokens_per_second": 1177.122
    },
    {
      "epoch": 0.6174211802748585,
      "grad_norm": 3.483600378036499,
      "learning_rate": 1.280024812538208e-06,
      "loss": 0.5822,
      "num_input_tokens_seen": 38331856,
      "step": 3055,
      "train_runtime": 32564.1067,
      "train_tokens_per_second": 1177.12
    },
    {
      "epoch": 0.618431689571544,
      "grad_norm": 3.6474876403808594,
      "learning_rate": 1.274104903317461e-06,
      "loss": 0.5602,
      "num_input_tokens_seen": 38394672,
      "step": 3060,
      "train_runtime": 32617.3027,
      "train_tokens_per_second": 1177.126
    },
    {
      "epoch": 0.6194421988682296,
      "grad_norm": 2.6882874965667725,
      "learning_rate": 1.2681923097627292e-06,
      "loss": 0.4053,
      "num_input_tokens_seen": 38458400,
      "step": 3065,
      "train_runtime": 32671.2907,
      "train_tokens_per_second": 1177.131
    },
    {
      "epoch": 0.6204527081649152,
      "grad_norm": 2.999479055404663,
      "learning_rate": 1.262287091461905e-06,
      "loss": 0.5106,
      "num_input_tokens_seen": 38520832,
      "step": 3070,
      "train_runtime": 32724.2563,
      "train_tokens_per_second": 1177.134
    },
    {
      "epoch": 0.6214632174616006,
      "grad_norm": 3.4575798511505127,
      "learning_rate": 1.2563893079285542e-06,
      "loss": 0.5898,
      "num_input_tokens_seen": 38584080,
      "step": 3075,
      "train_runtime": 32777.8065,
      "train_tokens_per_second": 1177.14
    },
    {
      "epoch": 0.6224737267582862,
      "grad_norm": 6.7781476974487305,
      "learning_rate": 1.2504990186013125e-06,
      "loss": 0.6197,
      "num_input_tokens_seen": 38646112,
      "step": 3080,
      "train_runtime": 32830.5096,
      "train_tokens_per_second": 1177.14
    },
    {
      "epoch": 0.6234842360549717,
      "grad_norm": 3.5706896781921387,
      "learning_rate": 1.2446162828432885e-06,
      "loss": 0.4366,
      "num_input_tokens_seen": 38709920,
      "step": 3085,
      "train_runtime": 32884.4605,
      "train_tokens_per_second": 1177.149
    },
    {
      "epoch": 0.6244947453516573,
      "grad_norm": 3.3581526279449463,
      "learning_rate": 1.2387411599414644e-06,
      "loss": 0.5344,
      "num_input_tokens_seen": 38771632,
      "step": 3090,
      "train_runtime": 32936.9953,
      "train_tokens_per_second": 1177.145
    },
    {
      "epoch": 0.6255052546483427,
      "grad_norm": 5.660045623779297,
      "learning_rate": 1.2328737091061014e-06,
      "loss": 0.74,
      "num_input_tokens_seen": 38834912,
      "step": 3095,
      "train_runtime": 32990.7529,
      "train_tokens_per_second": 1177.145
    },
    {
      "epoch": 0.6265157639450283,
      "grad_norm": 4.6893439292907715,
      "learning_rate": 1.2270139894701364e-06,
      "loss": 0.4148,
      "num_input_tokens_seen": 38897584,
      "step": 3100,
      "train_runtime": 33043.9952,
      "train_tokens_per_second": 1177.145
    },
    {
      "epoch": 0.6275262732417138,
      "grad_norm": 2.3373544216156006,
      "learning_rate": 1.2211620600885942e-06,
      "loss": 0.5945,
      "num_input_tokens_seen": 38958928,
      "step": 3105,
      "train_runtime": 33096.8315,
      "train_tokens_per_second": 1177.12
    },
    {
      "epoch": 0.6285367825383994,
      "grad_norm": 2.783747911453247,
      "learning_rate": 1.215317979937986e-06,
      "loss": 0.4828,
      "num_input_tokens_seen": 39021392,
      "step": 3110,
      "train_runtime": 33149.7639,
      "train_tokens_per_second": 1177.124
    },
    {
      "epoch": 0.6295472918350848,
      "grad_norm": 4.3362016677856445,
      "learning_rate": 1.2094818079157197e-06,
      "loss": 0.4927,
      "num_input_tokens_seen": 39084464,
      "step": 3115,
      "train_runtime": 33203.3158,
      "train_tokens_per_second": 1177.125
    },
    {
      "epoch": 0.6305578011317704,
      "grad_norm": 3.9497506618499756,
      "learning_rate": 1.2036536028395001e-06,
      "loss": 0.6355,
      "num_input_tokens_seen": 39147728,
      "step": 3120,
      "train_runtime": 33256.8302,
      "train_tokens_per_second": 1177.133
    },
    {
      "epoch": 0.631568310428456,
      "grad_norm": 3.2210099697113037,
      "learning_rate": 1.1978334234467447e-06,
      "loss": 0.4743,
      "num_input_tokens_seen": 39210912,
      "step": 3125,
      "train_runtime": 33310.4233,
      "train_tokens_per_second": 1177.136
    },
    {
      "epoch": 0.6325788197251415,
      "grad_norm": 29.284860610961914,
      "learning_rate": 1.1920213283939839e-06,
      "loss": 0.5942,
      "num_input_tokens_seen": 39272944,
      "step": 3130,
      "train_runtime": 33363.0549,
      "train_tokens_per_second": 1177.139
    },
    {
      "epoch": 0.633589329021827,
      "grad_norm": 2.90531063079834,
      "learning_rate": 1.1862173762562752e-06,
      "loss": 0.5569,
      "num_input_tokens_seen": 39335536,
      "step": 3135,
      "train_runtime": 33416.1727,
      "train_tokens_per_second": 1177.141
    },
    {
      "epoch": 0.6345998383185125,
      "grad_norm": 4.1407551765441895,
      "learning_rate": 1.1804216255266082e-06,
      "loss": 0.6141,
      "num_input_tokens_seen": 39398304,
      "step": 3140,
      "train_runtime": 33469.3635,
      "train_tokens_per_second": 1177.145
    },
    {
      "epoch": 0.6356103476151981,
      "grad_norm": 2.8686373233795166,
      "learning_rate": 1.17463413461532e-06,
      "loss": 0.5607,
      "num_input_tokens_seen": 39461456,
      "step": 3145,
      "train_runtime": 33522.87,
      "train_tokens_per_second": 1177.15
    },
    {
      "epoch": 0.6366208569118836,
      "grad_norm": 3.5863723754882812,
      "learning_rate": 1.168854961849503e-06,
      "loss": 0.6258,
      "num_input_tokens_seen": 39523360,
      "step": 3150,
      "train_runtime": 33575.4129,
      "train_tokens_per_second": 1177.152
    },
    {
      "epoch": 0.6376313662085691,
      "grad_norm": 6.260061264038086,
      "learning_rate": 1.163084165472419e-06,
      "loss": 0.7759,
      "num_input_tokens_seen": 39585648,
      "step": 3155,
      "train_runtime": 33628.2745,
      "train_tokens_per_second": 1177.154
    },
    {
      "epoch": 0.6386418755052546,
      "grad_norm": 6.395378112792969,
      "learning_rate": 1.15732180364291e-06,
      "loss": 0.7127,
      "num_input_tokens_seen": 39648720,
      "step": 3160,
      "train_runtime": 33681.6846,
      "train_tokens_per_second": 1177.16
    },
    {
      "epoch": 0.6396523848019402,
      "grad_norm": 2.520770788192749,
      "learning_rate": 1.1515679344348157e-06,
      "loss": 0.5112,
      "num_input_tokens_seen": 39711008,
      "step": 3165,
      "train_runtime": 33734.5947,
      "train_tokens_per_second": 1177.16
    },
    {
      "epoch": 0.6406628940986258,
      "grad_norm": 3.1341233253479004,
      "learning_rate": 1.1458226158363835e-06,
      "loss": 0.5623,
      "num_input_tokens_seen": 39772224,
      "step": 3170,
      "train_runtime": 33786.6314,
      "train_tokens_per_second": 1177.159
    },
    {
      "epoch": 0.6416734033953112,
      "grad_norm": 2.467548131942749,
      "learning_rate": 1.1400859057496893e-06,
      "loss": 0.566,
      "num_input_tokens_seen": 39835200,
      "step": 3175,
      "train_runtime": 33839.9166,
      "train_tokens_per_second": 1177.166
    },
    {
      "epoch": 0.6426839126919968,
      "grad_norm": 2.8305630683898926,
      "learning_rate": 1.1343578619900486e-06,
      "loss": 0.6051,
      "num_input_tokens_seen": 39898016,
      "step": 3180,
      "train_runtime": 33893.2332,
      "train_tokens_per_second": 1177.168
    },
    {
      "epoch": 0.6436944219886823,
      "grad_norm": 4.971250057220459,
      "learning_rate": 1.1286385422854392e-06,
      "loss": 0.5994,
      "num_input_tokens_seen": 39961440,
      "step": 3185,
      "train_runtime": 33946.9046,
      "train_tokens_per_second": 1177.175
    },
    {
      "epoch": 0.6447049312853679,
      "grad_norm": 3.5606610774993896,
      "learning_rate": 1.122928004275914e-06,
      "loss": 0.4855,
      "num_input_tokens_seen": 40024464,
      "step": 3190,
      "train_runtime": 34000.3927,
      "train_tokens_per_second": 1177.177
    },
    {
      "epoch": 0.6457154405820533,
      "grad_norm": 5.8030219078063965,
      "learning_rate": 1.117226305513026e-06,
      "loss": 0.6668,
      "num_input_tokens_seen": 40086112,
      "step": 3195,
      "train_runtime": 34052.7756,
      "train_tokens_per_second": 1177.176
    },
    {
      "epoch": 0.6467259498787389,
      "grad_norm": 4.520718574523926,
      "learning_rate": 1.1115335034592413e-06,
      "loss": 0.6289,
      "num_input_tokens_seen": 40149792,
      "step": 3200,
      "train_runtime": 34106.5402,
      "train_tokens_per_second": 1177.187
    },
    {
      "epoch": 0.6477364591754244,
      "grad_norm": 2.45762300491333,
      "learning_rate": 1.1058496554873678e-06,
      "loss": 0.647,
      "num_input_tokens_seen": 40211936,
      "step": 3205,
      "train_runtime": 34160.006,
      "train_tokens_per_second": 1177.164
    },
    {
      "epoch": 0.64874696847211,
      "grad_norm": 2.2967662811279297,
      "learning_rate": 1.1001748188799692e-06,
      "loss": 0.5822,
      "num_input_tokens_seen": 40275008,
      "step": 3210,
      "train_runtime": 34213.3768,
      "train_tokens_per_second": 1177.171
    },
    {
      "epoch": 0.6497574777687954,
      "grad_norm": 2.585782051086426,
      "learning_rate": 1.0945090508287951e-06,
      "loss": 0.3869,
      "num_input_tokens_seen": 40338128,
      "step": 3215,
      "train_runtime": 34266.9333,
      "train_tokens_per_second": 1177.174
    },
    {
      "epoch": 0.650767987065481,
      "grad_norm": 3.7379424571990967,
      "learning_rate": 1.0888524084341974e-06,
      "loss": 0.5606,
      "num_input_tokens_seen": 40401008,
      "step": 3220,
      "train_runtime": 34320.2791,
      "train_tokens_per_second": 1177.176
    },
    {
      "epoch": 0.6517784963621666,
      "grad_norm": 3.01676082611084,
      "learning_rate": 1.0832049487045622e-06,
      "loss": 0.5528,
      "num_input_tokens_seen": 40463104,
      "step": 3225,
      "train_runtime": 34373.0664,
      "train_tokens_per_second": 1177.175
    },
    {
      "epoch": 0.652789005658852,
      "grad_norm": 3.4768333435058594,
      "learning_rate": 1.0775667285557276e-06,
      "loss": 0.4293,
      "num_input_tokens_seen": 40526208,
      "step": 3230,
      "train_runtime": 34426.6214,
      "train_tokens_per_second": 1177.176
    },
    {
      "epoch": 0.6537995149555376,
      "grad_norm": 3.4596681594848633,
      "learning_rate": 1.0719378048104176e-06,
      "loss": 0.6395,
      "num_input_tokens_seen": 40590624,
      "step": 3235,
      "train_runtime": 34480.9904,
      "train_tokens_per_second": 1177.188
    },
    {
      "epoch": 0.6548100242522231,
      "grad_norm": 5.492783069610596,
      "learning_rate": 1.0663182341976633e-06,
      "loss": 0.6193,
      "num_input_tokens_seen": 40654256,
      "step": 3240,
      "train_runtime": 34534.8171,
      "train_tokens_per_second": 1177.196
    },
    {
      "epoch": 0.6558205335489087,
      "grad_norm": 3.645630359649658,
      "learning_rate": 1.060708073352234e-06,
      "loss": 0.6678,
      "num_input_tokens_seen": 40717424,
      "step": 3245,
      "train_runtime": 34588.4422,
      "train_tokens_per_second": 1177.197
    },
    {
      "epoch": 0.6568310428455941,
      "grad_norm": 3.3326354026794434,
      "learning_rate": 1.0551073788140676e-06,
      "loss": 0.4631,
      "num_input_tokens_seen": 40779328,
      "step": 3250,
      "train_runtime": 34641.0662,
      "train_tokens_per_second": 1177.196
    },
    {
      "epoch": 0.6578415521422797,
      "grad_norm": 3.782985210418701,
      "learning_rate": 1.049516207027699e-06,
      "loss": 0.6347,
      "num_input_tokens_seen": 40841552,
      "step": 3255,
      "train_runtime": 34693.8909,
      "train_tokens_per_second": 1177.197
    },
    {
      "epoch": 0.6588520614389652,
      "grad_norm": 3.1567580699920654,
      "learning_rate": 1.04393461434169e-06,
      "loss": 0.5643,
      "num_input_tokens_seen": 40904048,
      "step": 3260,
      "train_runtime": 34746.9763,
      "train_tokens_per_second": 1177.197
    },
    {
      "epoch": 0.6598625707356508,
      "grad_norm": 3.633094072341919,
      "learning_rate": 1.0383626570080632e-06,
      "loss": 0.5373,
      "num_input_tokens_seen": 40967168,
      "step": 3265,
      "train_runtime": 34800.633,
      "train_tokens_per_second": 1177.196
    },
    {
      "epoch": 0.6608730800323362,
      "grad_norm": 4.974003791809082,
      "learning_rate": 1.0328003911817368e-06,
      "loss": 0.6229,
      "num_input_tokens_seen": 41030432,
      "step": 3270,
      "train_runtime": 34854.1722,
      "train_tokens_per_second": 1177.203
    },
    {
      "epoch": 0.6618835893290218,
      "grad_norm": 4.070038318634033,
      "learning_rate": 1.0272478729199558e-06,
      "loss": 0.5524,
      "num_input_tokens_seen": 41092784,
      "step": 3275,
      "train_runtime": 34907.182,
      "train_tokens_per_second": 1177.201
    },
    {
      "epoch": 0.6628940986257074,
      "grad_norm": 5.321396350860596,
      "learning_rate": 1.021705158181727e-06,
      "loss": 0.5614,
      "num_input_tokens_seen": 41154464,
      "step": 3280,
      "train_runtime": 34959.7204,
      "train_tokens_per_second": 1177.197
    },
    {
      "epoch": 0.6639046079223929,
      "grad_norm": 3.43612003326416,
      "learning_rate": 1.0161723028272562e-06,
      "loss": 0.6903,
      "num_input_tokens_seen": 41218656,
      "step": 3285,
      "train_runtime": 35013.9052,
      "train_tokens_per_second": 1177.208
    },
    {
      "epoch": 0.6649151172190784,
      "grad_norm": 7.337741374969482,
      "learning_rate": 1.010649362617387e-06,
      "loss": 0.7459,
      "num_input_tokens_seen": 41281472,
      "step": 3290,
      "train_runtime": 35067.1108,
      "train_tokens_per_second": 1177.213
    },
    {
      "epoch": 0.6659256265157639,
      "grad_norm": 2.843583345413208,
      "learning_rate": 1.0051363932130336e-06,
      "loss": 0.5193,
      "num_input_tokens_seen": 41342704,
      "step": 3295,
      "train_runtime": 35119.3481,
      "train_tokens_per_second": 1177.206
    },
    {
      "epoch": 0.6669361358124495,
      "grad_norm": 2.2402327060699463,
      "learning_rate": 9.996334501746264e-07,
      "loss": 0.4636,
      "num_input_tokens_seen": 41406384,
      "step": 3300,
      "train_runtime": 35173.2799,
      "train_tokens_per_second": 1177.211
    },
    {
      "epoch": 0.667946645109135,
      "grad_norm": 3.4797136783599854,
      "learning_rate": 9.94140588961546e-07,
      "loss": 0.6987,
      "num_input_tokens_seen": 41468464,
      "step": 3305,
      "train_runtime": 35226.735,
      "train_tokens_per_second": 1177.187
    },
    {
      "epoch": 0.6689571544058205,
      "grad_norm": 3.0444207191467285,
      "learning_rate": 9.886578649315698e-07,
      "loss": 0.5839,
      "num_input_tokens_seen": 41531072,
      "step": 3310,
      "train_runtime": 35279.9528,
      "train_tokens_per_second": 1177.186
    },
    {
      "epoch": 0.669967663702506,
      "grad_norm": 4.548271656036377,
      "learning_rate": 9.831853333403084e-07,
      "loss": 0.4932,
      "num_input_tokens_seen": 41593424,
      "step": 3315,
      "train_runtime": 35332.9275,
      "train_tokens_per_second": 1177.186
    },
    {
      "epoch": 0.6709781729991916,
      "grad_norm": 7.206948280334473,
      "learning_rate": 9.777230493406542e-07,
      "loss": 0.5181,
      "num_input_tokens_seen": 41656352,
      "step": 3320,
      "train_runtime": 35386.4122,
      "train_tokens_per_second": 1177.185
    },
    {
      "epoch": 0.6719886822958772,
      "grad_norm": 2.80816912651062,
      "learning_rate": 9.72271067982221e-07,
      "loss": 0.6788,
      "num_input_tokens_seen": 41717888,
      "step": 3325,
      "train_runtime": 35438.8039,
      "train_tokens_per_second": 1177.181
    },
    {
      "epoch": 0.6729991915925626,
      "grad_norm": 3.195814371109009,
      "learning_rate": 9.668294442107933e-07,
      "loss": 0.5096,
      "num_input_tokens_seen": 41781632,
      "step": 3330,
      "train_runtime": 35492.7831,
      "train_tokens_per_second": 1177.187
    },
    {
      "epoch": 0.6740097008892482,
      "grad_norm": 6.567824363708496,
      "learning_rate": 9.61398232867769e-07,
      "loss": 0.563,
      "num_input_tokens_seen": 41845280,
      "step": 3335,
      "train_runtime": 35546.5997,
      "train_tokens_per_second": 1177.195
    },
    {
      "epoch": 0.6750202101859337,
      "grad_norm": 2.835498332977295,
      "learning_rate": 9.559774886896073e-07,
      "loss": 0.523,
      "num_input_tokens_seen": 41908640,
      "step": 3340,
      "train_runtime": 35600.3998,
      "train_tokens_per_second": 1177.196
    },
    {
      "epoch": 0.6760307194826193,
      "grad_norm": 3.4772181510925293,
      "learning_rate": 9.505672663072802e-07,
      "loss": 0.5946,
      "num_input_tokens_seen": 41972368,
      "step": 3345,
      "train_runtime": 35654.1743,
      "train_tokens_per_second": 1177.208
    },
    {
      "epoch": 0.6770412287793047,
      "grad_norm": 2.6479127407073975,
      "learning_rate": 9.451676202457193e-07,
      "loss": 0.4511,
      "num_input_tokens_seen": 42034416,
      "step": 3350,
      "train_runtime": 35706.9177,
      "train_tokens_per_second": 1177.207
    },
    {
      "epoch": 0.6780517380759903,
      "grad_norm": 5.24006462097168,
      "learning_rate": 9.397786049232653e-07,
      "loss": 0.5872,
      "num_input_tokens_seen": 42096848,
      "step": 3355,
      "train_runtime": 35759.867,
      "train_tokens_per_second": 1177.209
    },
    {
      "epoch": 0.6790622473726758,
      "grad_norm": 3.282853126525879,
      "learning_rate": 9.344002746511208e-07,
      "loss": 0.4942,
      "num_input_tokens_seen": 42159648,
      "step": 3360,
      "train_runtime": 35813.2461,
      "train_tokens_per_second": 1177.208
    },
    {
      "epoch": 0.6800727566693614,
      "grad_norm": 3.2873456478118896,
      "learning_rate": 9.290326836328046e-07,
      "loss": 0.6981,
      "num_input_tokens_seen": 42221968,
      "step": 3365,
      "train_runtime": 35866.041,
      "train_tokens_per_second": 1177.213
    },
    {
      "epoch": 0.6810832659660468,
      "grad_norm": 20.541858673095703,
      "learning_rate": 9.23675885963604e-07,
      "loss": 0.4556,
      "num_input_tokens_seen": 42285872,
      "step": 3370,
      "train_runtime": 35920.082,
      "train_tokens_per_second": 1177.221
    },
    {
      "epoch": 0.6820937752627324,
      "grad_norm": 2.826087474822998,
      "learning_rate": 9.18329935630028e-07,
      "loss": 0.5022,
      "num_input_tokens_seen": 42348384,
      "step": 3375,
      "train_runtime": 35973.147,
      "train_tokens_per_second": 1177.222
    },
    {
      "epoch": 0.683104284559418,
      "grad_norm": 4.526493549346924,
      "learning_rate": 9.129948865092644e-07,
      "loss": 0.6896,
      "num_input_tokens_seen": 42411648,
      "step": 3380,
      "train_runtime": 36026.7614,
      "train_tokens_per_second": 1177.226
    },
    {
      "epoch": 0.6841147938561035,
      "grad_norm": 4.992778778076172,
      "learning_rate": 9.07670792368639e-07,
      "loss": 0.4633,
      "num_input_tokens_seen": 42474064,
      "step": 3385,
      "train_runtime": 36079.6636,
      "train_tokens_per_second": 1177.23
    },
    {
      "epoch": 0.685125303152789,
      "grad_norm": 4.273828506469727,
      "learning_rate": 9.02357706865072e-07,
      "loss": 0.548,
      "num_input_tokens_seen": 42536944,
      "step": 3390,
      "train_runtime": 36132.978,
      "train_tokens_per_second": 1177.233
    },
    {
      "epoch": 0.6861358124494745,
      "grad_norm": 3.504955530166626,
      "learning_rate": 8.970556835445351e-07,
      "loss": 0.6359,
      "num_input_tokens_seen": 42598976,
      "step": 3395,
      "train_runtime": 36185.8002,
      "train_tokens_per_second": 1177.229
    },
    {
      "epoch": 0.6871463217461601,
      "grad_norm": 3.8726885318756104,
      "learning_rate": 8.917647758415141e-07,
      "loss": 0.5933,
      "num_input_tokens_seen": 42661520,
      "step": 3400,
      "train_runtime": 36238.8103,
      "train_tokens_per_second": 1177.233
    },
    {
      "epoch": 0.6881568310428456,
      "grad_norm": 4.007725715637207,
      "learning_rate": 8.86485037078472e-07,
      "loss": 0.5045,
      "num_input_tokens_seen": 42723312,
      "step": 3405,
      "train_runtime": 36292.1257,
      "train_tokens_per_second": 1177.206
    },
    {
      "epoch": 0.6891673403395311,
      "grad_norm": 3.1664772033691406,
      "learning_rate": 8.812165204653097e-07,
      "loss": 0.4488,
      "num_input_tokens_seen": 42785776,
      "step": 3410,
      "train_runtime": 36345.1562,
      "train_tokens_per_second": 1177.207
    },
    {
      "epoch": 0.6901778496362166,
      "grad_norm": 2.4737627506256104,
      "learning_rate": 8.759592790988263e-07,
      "loss": 0.5744,
      "num_input_tokens_seen": 42848880,
      "step": 3415,
      "train_runtime": 36398.6071,
      "train_tokens_per_second": 1177.212
    },
    {
      "epoch": 0.6911883589329022,
      "grad_norm": 3.432920455932617,
      "learning_rate": 8.707133659621915e-07,
      "loss": 0.616,
      "num_input_tokens_seen": 42910928,
      "step": 3420,
      "train_runtime": 36451.3017,
      "train_tokens_per_second": 1177.213
    },
    {
      "epoch": 0.6921988682295878,
      "grad_norm": 3.378561019897461,
      "learning_rate": 8.654788339244057e-07,
      "loss": 0.4614,
      "num_input_tokens_seen": 42972960,
      "step": 3425,
      "train_runtime": 36503.9392,
      "train_tokens_per_second": 1177.214
    },
    {
      "epoch": 0.6932093775262732,
      "grad_norm": 1.9157336950302124,
      "learning_rate": 8.602557357397712e-07,
      "loss": 0.4661,
      "num_input_tokens_seen": 43034928,
      "step": 3430,
      "train_runtime": 36556.5629,
      "train_tokens_per_second": 1177.215
    },
    {
      "epoch": 0.6942198868229588,
      "grad_norm": 4.14323091506958,
      "learning_rate": 8.550441240473538e-07,
      "loss": 0.6438,
      "num_input_tokens_seen": 43098128,
      "step": 3435,
      "train_runtime": 36610.2077,
      "train_tokens_per_second": 1177.216
    },
    {
      "epoch": 0.6952303961196443,
      "grad_norm": 4.172226428985596,
      "learning_rate": 8.498440513704614e-07,
      "loss": 0.5017,
      "num_input_tokens_seen": 43161200,
      "step": 3440,
      "train_runtime": 36663.6055,
      "train_tokens_per_second": 1177.222
    },
    {
      "epoch": 0.6962409054163299,
      "grad_norm": 3.733433723449707,
      "learning_rate": 8.446555701161091e-07,
      "loss": 0.543,
      "num_input_tokens_seen": 43223872,
      "step": 3445,
      "train_runtime": 36716.7737,
      "train_tokens_per_second": 1177.224
    },
    {
      "epoch": 0.6972514147130153,
      "grad_norm": 3.3401072025299072,
      "learning_rate": 8.394787325744912e-07,
      "loss": 0.6309,
      "num_input_tokens_seen": 43287056,
      "step": 3450,
      "train_runtime": 36770.3457,
      "train_tokens_per_second": 1177.227
    },
    {
      "epoch": 0.6982619240097009,
      "grad_norm": 2.7483630180358887,
      "learning_rate": 8.343135909184546e-07,
      "loss": 0.6376,
      "num_input_tokens_seen": 43349680,
      "step": 3455,
      "train_runtime": 36823.4949,
      "train_tokens_per_second": 1177.229
    },
    {
      "epoch": 0.6992724333063864,
      "grad_norm": 3.933647394180298,
      "learning_rate": 8.291601972029756e-07,
      "loss": 0.5828,
      "num_input_tokens_seen": 43411536,
      "step": 3460,
      "train_runtime": 36875.9359,
      "train_tokens_per_second": 1177.232
    },
    {
      "epoch": 0.700282942603072,
      "grad_norm": 5.383896827697754,
      "learning_rate": 8.240186033646325e-07,
      "loss": 0.5697,
      "num_input_tokens_seen": 43473968,
      "step": 3465,
      "train_runtime": 36929.0084,
      "train_tokens_per_second": 1177.231
    },
    {
      "epoch": 0.7012934518997574,
      "grad_norm": 2.711686611175537,
      "learning_rate": 8.188888612210822e-07,
      "loss": 0.6335,
      "num_input_tokens_seen": 43537296,
      "step": 3470,
      "train_runtime": 36982.5367,
      "train_tokens_per_second": 1177.239
    },
    {
      "epoch": 0.702303961196443,
      "grad_norm": 4.049568176269531,
      "learning_rate": 8.137710224705379e-07,
      "loss": 0.7627,
      "num_input_tokens_seen": 43600016,
      "step": 3475,
      "train_runtime": 37035.8158,
      "train_tokens_per_second": 1177.239
    },
    {
      "epoch": 0.7033144704931286,
      "grad_norm": 3.78558087348938,
      "learning_rate": 8.086651386912508e-07,
      "loss": 0.647,
      "num_input_tokens_seen": 43662768,
      "step": 3480,
      "train_runtime": 37089.0701,
      "train_tokens_per_second": 1177.241
    },
    {
      "epoch": 0.7043249797898141,
      "grad_norm": 3.306730031967163,
      "learning_rate": 8.035712613409882e-07,
      "loss": 0.6263,
      "num_input_tokens_seen": 43725456,
      "step": 3485,
      "train_runtime": 37142.3208,
      "train_tokens_per_second": 1177.241
    },
    {
      "epoch": 0.7053354890864996,
      "grad_norm": 4.5161051750183105,
      "learning_rate": 7.984894417565136e-07,
      "loss": 0.5495,
      "num_input_tokens_seen": 43788880,
      "step": 3490,
      "train_runtime": 37195.9749,
      "train_tokens_per_second": 1177.248
    },
    {
      "epoch": 0.7063459983831851,
      "grad_norm": 2.898486852645874,
      "learning_rate": 7.934197311530708e-07,
      "loss": 0.4383,
      "num_input_tokens_seen": 43852176,
      "step": 3495,
      "train_runtime": 37249.6101,
      "train_tokens_per_second": 1177.252
    },
    {
      "epoch": 0.7073565076798707,
      "grad_norm": 3.1274099349975586,
      "learning_rate": 7.88362180623869e-07,
      "loss": 0.6495,
      "num_input_tokens_seen": 43915360,
      "step": 3500,
      "train_runtime": 37303.0244,
      "train_tokens_per_second": 1177.26
    },
    {
      "epoch": 0.7083670169765561,
      "grad_norm": 3.2970054149627686,
      "learning_rate": 7.833168411395665e-07,
      "loss": 0.5638,
      "num_input_tokens_seen": 43978048,
      "step": 3505,
      "train_runtime": 37356.9596,
      "train_tokens_per_second": 1177.238
    },
    {
      "epoch": 0.7093775262732417,
      "grad_norm": 2.9818849563598633,
      "learning_rate": 7.78283763547756e-07,
      "loss": 0.6604,
      "num_input_tokens_seen": 44040992,
      "step": 3510,
      "train_runtime": 37410.3894,
      "train_tokens_per_second": 1177.24
    },
    {
      "epoch": 0.7103880355699272,
      "grad_norm": 2.8797097206115723,
      "learning_rate": 7.73262998572453e-07,
      "loss": 0.6172,
      "num_input_tokens_seen": 44104064,
      "step": 3515,
      "train_runtime": 37463.8379,
      "train_tokens_per_second": 1177.244
    },
    {
      "epoch": 0.7113985448666128,
      "grad_norm": 3.4153482913970947,
      "learning_rate": 7.682545968135863e-07,
      "loss": 0.7003,
      "num_input_tokens_seen": 44167456,
      "step": 3520,
      "train_runtime": 37517.4734,
      "train_tokens_per_second": 1177.25
    },
    {
      "epoch": 0.7124090541632982,
      "grad_norm": 2.605710506439209,
      "learning_rate": 7.632586087464865e-07,
      "loss": 0.4702,
      "num_input_tokens_seen": 44230272,
      "step": 3525,
      "train_runtime": 37570.7875,
      "train_tokens_per_second": 1177.252
    },
    {
      "epoch": 0.7134195634599838,
      "grad_norm": 3.841883897781372,
      "learning_rate": 7.582750847213764e-07,
      "loss": 0.4441,
      "num_input_tokens_seen": 44294576,
      "step": 3530,
      "train_runtime": 37625.0352,
      "train_tokens_per_second": 1177.263
    },
    {
      "epoch": 0.7144300727566694,
      "grad_norm": 4.22552490234375,
      "learning_rate": 7.533040749628639e-07,
      "loss": 0.5483,
      "num_input_tokens_seen": 44356208,
      "step": 3535,
      "train_runtime": 37677.4428,
      "train_tokens_per_second": 1177.262
    },
    {
      "epoch": 0.7154405820533549,
      "grad_norm": 2.598284959793091,
      "learning_rate": 7.483456295694388e-07,
      "loss": 0.7368,
      "num_input_tokens_seen": 44419712,
      "step": 3540,
      "train_runtime": 37731.1682,
      "train_tokens_per_second": 1177.268
    },
    {
      "epoch": 0.7164510913500404,
      "grad_norm": 3.7988698482513428,
      "learning_rate": 7.433997985129632e-07,
      "loss": 0.6301,
      "num_input_tokens_seen": 44482096,
      "step": 3545,
      "train_runtime": 37784.0987,
      "train_tokens_per_second": 1177.27
    },
    {
      "epoch": 0.7174616006467259,
      "grad_norm": 4.044842720031738,
      "learning_rate": 7.384666316381725e-07,
      "loss": 0.7542,
      "num_input_tokens_seen": 44545072,
      "step": 3550,
      "train_runtime": 37837.3746,
      "train_tokens_per_second": 1177.277
    },
    {
      "epoch": 0.7184721099434115,
      "grad_norm": 3.195519208908081,
      "learning_rate": 7.335461786621684e-07,
      "loss": 0.6789,
      "num_input_tokens_seen": 44608016,
      "step": 3555,
      "train_runtime": 37890.7168,
      "train_tokens_per_second": 1177.281
    },
    {
      "epoch": 0.719482619240097,
      "grad_norm": 3.5633373260498047,
      "learning_rate": 7.286384891739228e-07,
      "loss": 0.6683,
      "num_input_tokens_seen": 44671216,
      "step": 3560,
      "train_runtime": 37944.1789,
      "train_tokens_per_second": 1177.288
    },
    {
      "epoch": 0.7204931285367825,
      "grad_norm": 3.6333863735198975,
      "learning_rate": 7.237436126337735e-07,
      "loss": 0.5172,
      "num_input_tokens_seen": 44732736,
      "step": 3565,
      "train_runtime": 37996.5464,
      "train_tokens_per_second": 1177.284
    },
    {
      "epoch": 0.721503637833468,
      "grad_norm": 3.2406394481658936,
      "learning_rate": 7.188615983729276e-07,
      "loss": 0.4798,
      "num_input_tokens_seen": 44794896,
      "step": 3570,
      "train_runtime": 38049.3428,
      "train_tokens_per_second": 1177.284
    },
    {
      "epoch": 0.7225141471301536,
      "grad_norm": 5.87693452835083,
      "learning_rate": 7.139924955929664e-07,
      "loss": 0.719,
      "num_input_tokens_seen": 44857296,
      "step": 3575,
      "train_runtime": 38102.3233,
      "train_tokens_per_second": 1177.285
    },
    {
      "epoch": 0.7235246564268392,
      "grad_norm": 3.829777717590332,
      "learning_rate": 7.09136353365347e-07,
      "loss": 0.4626,
      "num_input_tokens_seen": 44919584,
      "step": 3580,
      "train_runtime": 38155.2257,
      "train_tokens_per_second": 1177.285
    },
    {
      "epoch": 0.7245351657235246,
      "grad_norm": 3.1812169551849365,
      "learning_rate": 7.042932206309069e-07,
      "loss": 0.5956,
      "num_input_tokens_seen": 44982256,
      "step": 3585,
      "train_runtime": 38208.38,
      "train_tokens_per_second": 1177.288
    },
    {
      "epoch": 0.7255456750202102,
      "grad_norm": 3.626399040222168,
      "learning_rate": 6.994631461993728e-07,
      "loss": 0.4898,
      "num_input_tokens_seen": 45044688,
      "step": 3590,
      "train_runtime": 38261.3122,
      "train_tokens_per_second": 1177.291
    },
    {
      "epoch": 0.7265561843168957,
      "grad_norm": 11.455324172973633,
      "learning_rate": 6.946461787488689e-07,
      "loss": 0.6703,
      "num_input_tokens_seen": 45107408,
      "step": 3595,
      "train_runtime": 38314.5054,
      "train_tokens_per_second": 1177.293
    },
    {
      "epoch": 0.7275666936135813,
      "grad_norm": 2.619256019592285,
      "learning_rate": 6.898423668254259e-07,
      "loss": 0.4853,
      "num_input_tokens_seen": 45171200,
      "step": 3600,
      "train_runtime": 38368.371,
      "train_tokens_per_second": 1177.303
    },
    {
      "epoch": 0.7285772029102667,
      "grad_norm": 3.955212116241455,
      "learning_rate": 6.8505175884249e-07,
      "loss": 0.7032,
      "num_input_tokens_seen": 45234464,
      "step": 3605,
      "train_runtime": 38422.831,
      "train_tokens_per_second": 1177.281
    },
    {
      "epoch": 0.7295877122069523,
      "grad_norm": 2.6653530597686768,
      "learning_rate": 6.802744030804363e-07,
      "loss": 0.5463,
      "num_input_tokens_seen": 45296160,
      "step": 3610,
      "train_runtime": 38475.3451,
      "train_tokens_per_second": 1177.278
    },
    {
      "epoch": 0.7305982215036378,
      "grad_norm": 3.325056314468384,
      "learning_rate": 6.755103476860835e-07,
      "loss": 0.6482,
      "num_input_tokens_seen": 45358640,
      "step": 3615,
      "train_runtime": 38528.463,
      "train_tokens_per_second": 1177.276
    },
    {
      "epoch": 0.7316087308003234,
      "grad_norm": 3.2177698612213135,
      "learning_rate": 6.707596406722074e-07,
      "loss": 0.4826,
      "num_input_tokens_seen": 45420704,
      "step": 3620,
      "train_runtime": 38581.2379,
      "train_tokens_per_second": 1177.274
    },
    {
      "epoch": 0.7326192400970089,
      "grad_norm": 3.179515838623047,
      "learning_rate": 6.66022329917056e-07,
      "loss": 0.6016,
      "num_input_tokens_seen": 45483488,
      "step": 3625,
      "train_runtime": 38634.5675,
      "train_tokens_per_second": 1177.274
    },
    {
      "epoch": 0.7336297493936944,
      "grad_norm": 2.758106231689453,
      "learning_rate": 6.612984631638678e-07,
      "loss": 0.6111,
      "num_input_tokens_seen": 45546208,
      "step": 3630,
      "train_runtime": 38687.7197,
      "train_tokens_per_second": 1177.278
    },
    {
      "epoch": 0.73464025869038,
      "grad_norm": 2.8609466552734375,
      "learning_rate": 6.565880880203931e-07,
      "loss": 0.5748,
      "num_input_tokens_seen": 45609264,
      "step": 3635,
      "train_runtime": 38741.0944,
      "train_tokens_per_second": 1177.284
    },
    {
      "epoch": 0.7356507679870655,
      "grad_norm": 3.327646017074585,
      "learning_rate": 6.518912519584092e-07,
      "loss": 0.4465,
      "num_input_tokens_seen": 45671504,
      "step": 3640,
      "train_runtime": 38794.0368,
      "train_tokens_per_second": 1177.282
    },
    {
      "epoch": 0.736661277283751,
      "grad_norm": 5.870704650878906,
      "learning_rate": 6.472080023132475e-07,
      "loss": 0.593,
      "num_input_tokens_seen": 45733904,
      "step": 3645,
      "train_runtime": 38846.9421,
      "train_tokens_per_second": 1177.285
    },
    {
      "epoch": 0.7376717865804365,
      "grad_norm": 2.6839568614959717,
      "learning_rate": 6.425383862833111e-07,
      "loss": 0.4238,
      "num_input_tokens_seen": 45797104,
      "step": 3650,
      "train_runtime": 38900.4256,
      "train_tokens_per_second": 1177.291
    },
    {
      "epoch": 0.7386822958771221,
      "grad_norm": 3.6276535987854004,
      "learning_rate": 6.378824509296039e-07,
      "loss": 0.5303,
      "num_input_tokens_seen": 45859296,
      "step": 3655,
      "train_runtime": 38953.1782,
      "train_tokens_per_second": 1177.293
    },
    {
      "epoch": 0.7396928051738076,
      "grad_norm": 3.490311861038208,
      "learning_rate": 6.332402431752522e-07,
      "loss": 0.5094,
      "num_input_tokens_seen": 45921424,
      "step": 3660,
      "train_runtime": 39005.8956,
      "train_tokens_per_second": 1177.294
    },
    {
      "epoch": 0.7407033144704931,
      "grad_norm": 3.4000535011291504,
      "learning_rate": 6.28611809805036e-07,
      "loss": 0.4835,
      "num_input_tokens_seen": 45983792,
      "step": 3665,
      "train_runtime": 39058.703,
      "train_tokens_per_second": 1177.3
    },
    {
      "epoch": 0.7417138237671786,
      "grad_norm": 2.799246311187744,
      "learning_rate": 6.239971974649125e-07,
      "loss": 0.4257,
      "num_input_tokens_seen": 46046128,
      "step": 3670,
      "train_runtime": 39111.6656,
      "train_tokens_per_second": 1177.299
    },
    {
      "epoch": 0.7427243330638642,
      "grad_norm": 2.9345133304595947,
      "learning_rate": 6.193964526615518e-07,
      "loss": 0.5495,
      "num_input_tokens_seen": 46108464,
      "step": 3675,
      "train_runtime": 39164.6843,
      "train_tokens_per_second": 1177.297
    },
    {
      "epoch": 0.7437348423605498,
      "grad_norm": 5.858893394470215,
      "learning_rate": 6.148096217618624e-07,
      "loss": 0.5801,
      "num_input_tokens_seen": 46170816,
      "step": 3680,
      "train_runtime": 39217.7813,
      "train_tokens_per_second": 1177.293
    },
    {
      "epoch": 0.7447453516572352,
      "grad_norm": 3.2893002033233643,
      "learning_rate": 6.102367509925288e-07,
      "loss": 0.534,
      "num_input_tokens_seen": 46233120,
      "step": 3685,
      "train_runtime": 39270.6579,
      "train_tokens_per_second": 1177.294
    },
    {
      "epoch": 0.7457558609539208,
      "grad_norm": 4.277243614196777,
      "learning_rate": 6.056778864395415e-07,
      "loss": 0.6669,
      "num_input_tokens_seen": 46296544,
      "step": 3690,
      "train_runtime": 39324.416,
      "train_tokens_per_second": 1177.298
    },
    {
      "epoch": 0.7467663702506063,
      "grad_norm": 2.621908664703369,
      "learning_rate": 6.011330740477367e-07,
      "loss": 0.6736,
      "num_input_tokens_seen": 46359664,
      "step": 3695,
      "train_runtime": 39378.0531,
      "train_tokens_per_second": 1177.297
    },
    {
      "epoch": 0.7477768795472919,
      "grad_norm": 4.954636573791504,
      "learning_rate": 5.966023596203289e-07,
      "loss": 0.5876,
      "num_input_tokens_seen": 46423024,
      "step": 3700,
      "train_runtime": 39431.7585,
      "train_tokens_per_second": 1177.3
    },
    {
      "epoch": 0.7487873888439773,
      "grad_norm": 2.8696494102478027,
      "learning_rate": 5.920857888184533e-07,
      "loss": 0.5805,
      "num_input_tokens_seen": 46485984,
      "step": 3705,
      "train_runtime": 39485.8229,
      "train_tokens_per_second": 1177.283
    },
    {
      "epoch": 0.7497978981406629,
      "grad_norm": 3.237751007080078,
      "learning_rate": 5.875834071607022e-07,
      "loss": 0.4498,
      "num_input_tokens_seen": 46548736,
      "step": 3710,
      "train_runtime": 39539.0373,
      "train_tokens_per_second": 1177.286
    },
    {
      "epoch": 0.7508084074373484,
      "grad_norm": 3.4510116577148438,
      "learning_rate": 5.830952600226695e-07,
      "loss": 0.4882,
      "num_input_tokens_seen": 46610800,
      "step": 3715,
      "train_runtime": 39591.773,
      "train_tokens_per_second": 1177.285
    },
    {
      "epoch": 0.751818916734034,
      "grad_norm": 5.102356910705566,
      "learning_rate": 5.786213926364898e-07,
      "loss": 0.8826,
      "num_input_tokens_seen": 46673200,
      "step": 3720,
      "train_runtime": 39644.8413,
      "train_tokens_per_second": 1177.283
    },
    {
      "epoch": 0.7528294260307195,
      "grad_norm": 5.734504222869873,
      "learning_rate": 5.741618500903866e-07,
      "loss": 0.5845,
      "num_input_tokens_seen": 46734720,
      "step": 3725,
      "train_runtime": 39697.0482,
      "train_tokens_per_second": 1177.285
    },
    {
      "epoch": 0.753839935327405,
      "grad_norm": 4.906577110290527,
      "learning_rate": 5.697166773282143e-07,
      "loss": 0.6568,
      "num_input_tokens_seen": 46797264,
      "step": 3730,
      "train_runtime": 39750.0813,
      "train_tokens_per_second": 1177.287
    },
    {
      "epoch": 0.7548504446240906,
      "grad_norm": 5.951771259307861,
      "learning_rate": 5.652859191490065e-07,
      "loss": 0.6739,
      "num_input_tokens_seen": 46859072,
      "step": 3735,
      "train_runtime": 39802.5576,
      "train_tokens_per_second": 1177.288
    },
    {
      "epoch": 0.7558609539207761,
      "grad_norm": 3.021847724914551,
      "learning_rate": 5.608696202065261e-07,
      "loss": 0.5068,
      "num_input_tokens_seen": 46922048,
      "step": 3740,
      "train_runtime": 39855.9018,
      "train_tokens_per_second": 1177.292
    },
    {
      "epoch": 0.7568714632174616,
      "grad_norm": 3.737863779067993,
      "learning_rate": 5.564678250088138e-07,
      "loss": 0.4966,
      "num_input_tokens_seen": 46984288,
      "step": 3745,
      "train_runtime": 39908.764,
      "train_tokens_per_second": 1177.292
    },
    {
      "epoch": 0.7578819725141471,
      "grad_norm": 4.927465915679932,
      "learning_rate": 5.520805779177382e-07,
      "loss": 0.4549,
      "num_input_tokens_seen": 47046944,
      "step": 3750,
      "train_runtime": 39961.9064,
      "train_tokens_per_second": 1177.295
    },
    {
      "epoch": 0.7588924818108327,
      "grad_norm": 5.623347282409668,
      "learning_rate": 5.477079231485506e-07,
      "loss": 0.5349,
      "num_input_tokens_seen": 47109184,
      "step": 3755,
      "train_runtime": 40014.8017,
      "train_tokens_per_second": 1177.294
    },
    {
      "epoch": 0.7599029911075182,
      "grad_norm": 5.4820146560668945,
      "learning_rate": 5.433499047694401e-07,
      "loss": 0.5587,
      "num_input_tokens_seen": 47172336,
      "step": 3760,
      "train_runtime": 40068.2438,
      "train_tokens_per_second": 1177.3
    },
    {
      "epoch": 0.7609135004042037,
      "grad_norm": 4.331453323364258,
      "learning_rate": 5.390065667010884e-07,
      "loss": 0.6844,
      "num_input_tokens_seen": 47234512,
      "step": 3765,
      "train_runtime": 40121.064,
      "train_tokens_per_second": 1177.3
    },
    {
      "epoch": 0.7619240097008892,
      "grad_norm": 4.10953426361084,
      "learning_rate": 5.346779527162253e-07,
      "loss": 0.6869,
      "num_input_tokens_seen": 47297312,
      "step": 3770,
      "train_runtime": 40174.2585,
      "train_tokens_per_second": 1177.304
    },
    {
      "epoch": 0.7629345189975748,
      "grad_norm": 6.006789684295654,
      "learning_rate": 5.303641064391901e-07,
      "loss": 0.5923,
      "num_input_tokens_seen": 47359760,
      "step": 3775,
      "train_runtime": 40227.561,
      "train_tokens_per_second": 1177.296
    },
    {
      "epoch": 0.7639450282942603,
      "grad_norm": 4.177121639251709,
      "learning_rate": 5.260650713454924e-07,
      "loss": 0.5379,
      "num_input_tokens_seen": 47422752,
      "step": 3780,
      "train_runtime": 40280.9981,
      "train_tokens_per_second": 1177.298
    },
    {
      "epoch": 0.7649555375909458,
      "grad_norm": 3.6549599170684814,
      "learning_rate": 5.217808907613719e-07,
      "loss": 0.5754,
      "num_input_tokens_seen": 47485472,
      "step": 3785,
      "train_runtime": 40334.123,
      "train_tokens_per_second": 1177.303
    },
    {
      "epoch": 0.7659660468876314,
      "grad_norm": 3.4336512088775635,
      "learning_rate": 5.175116078633624e-07,
      "loss": 0.5757,
      "num_input_tokens_seen": 47547680,
      "step": 3790,
      "train_runtime": 40387.0341,
      "train_tokens_per_second": 1177.301
    },
    {
      "epoch": 0.7669765561843169,
      "grad_norm": 3.1563706398010254,
      "learning_rate": 5.132572656778562e-07,
      "loss": 0.4428,
      "num_input_tokens_seen": 47610816,
      "step": 3795,
      "train_runtime": 40440.5275,
      "train_tokens_per_second": 1177.305
    },
    {
      "epoch": 0.7679870654810024,
      "grad_norm": 3.634105920791626,
      "learning_rate": 5.090179070806727e-07,
      "loss": 0.5145,
      "num_input_tokens_seen": 47673200,
      "step": 3800,
      "train_runtime": 40493.6857,
      "train_tokens_per_second": 1177.3
    },
    {
      "epoch": 0.7689975747776879,
      "grad_norm": 4.78156042098999,
      "learning_rate": 5.047935747966243e-07,
      "loss": 0.5368,
      "num_input_tokens_seen": 47735472,
      "step": 3805,
      "train_runtime": 40547.3834,
      "train_tokens_per_second": 1177.276
    },
    {
      "epoch": 0.7700080840743735,
      "grad_norm": 3.010324716567993,
      "learning_rate": 5.005843113990858e-07,
      "loss": 0.6496,
      "num_input_tokens_seen": 47797376,
      "step": 3810,
      "train_runtime": 40600.079,
      "train_tokens_per_second": 1177.273
    },
    {
      "epoch": 0.771018593371059,
      "grad_norm": 3.1046078205108643,
      "learning_rate": 4.963901593095649e-07,
      "loss": 0.6123,
      "num_input_tokens_seen": 47859600,
      "step": 3815,
      "train_runtime": 40652.8569,
      "train_tokens_per_second": 1177.275
    },
    {
      "epoch": 0.7720291026677445,
      "grad_norm": 2.853429079055786,
      "learning_rate": 4.922111607972783e-07,
      "loss": 0.4325,
      "num_input_tokens_seen": 47922800,
      "step": 3820,
      "train_runtime": 40706.2983,
      "train_tokens_per_second": 1177.282
    },
    {
      "epoch": 0.77303961196443,
      "grad_norm": 3.4071691036224365,
      "learning_rate": 4.880473579787201e-07,
      "loss": 0.678,
      "num_input_tokens_seen": 47984976,
      "step": 3825,
      "train_runtime": 40759.1182,
      "train_tokens_per_second": 1177.282
    },
    {
      "epoch": 0.7740501212611156,
      "grad_norm": 3.1496245861053467,
      "learning_rate": 4.838987928172413e-07,
      "loss": 0.5607,
      "num_input_tokens_seen": 48047072,
      "step": 3830,
      "train_runtime": 40811.8229,
      "train_tokens_per_second": 1177.283
    },
    {
      "epoch": 0.7750606305578012,
      "grad_norm": 4.577930927276611,
      "learning_rate": 4.797655071226261e-07,
      "loss": 0.6358,
      "num_input_tokens_seen": 48109456,
      "step": 3835,
      "train_runtime": 40864.7011,
      "train_tokens_per_second": 1177.286
    },
    {
      "epoch": 0.7760711398544866,
      "grad_norm": 5.061673641204834,
      "learning_rate": 4.756475425506705e-07,
      "loss": 0.6347,
      "num_input_tokens_seen": 48171376,
      "step": 3840,
      "train_runtime": 40917.226,
      "train_tokens_per_second": 1177.288
    },
    {
      "epoch": 0.7770816491511722,
      "grad_norm": 2.5833048820495605,
      "learning_rate": 4.715449406027614e-07,
      "loss": 0.5734,
      "num_input_tokens_seen": 48234496,
      "step": 3845,
      "train_runtime": 40970.7296,
      "train_tokens_per_second": 1177.292
    },
    {
      "epoch": 0.7780921584478577,
      "grad_norm": 4.467967510223389,
      "learning_rate": 4.6745774262545865e-07,
      "loss": 0.586,
      "num_input_tokens_seen": 48297280,
      "step": 3850,
      "train_runtime": 41024.0945,
      "train_tokens_per_second": 1177.291
    },
    {
      "epoch": 0.7791026677445433,
      "grad_norm": 3.326862335205078,
      "learning_rate": 4.6338598981008027e-07,
      "loss": 0.6066,
      "num_input_tokens_seen": 48359440,
      "step": 3855,
      "train_runtime": 41076.8767,
      "train_tokens_per_second": 1177.291
    },
    {
      "epoch": 0.7801131770412287,
      "grad_norm": 3.8317034244537354,
      "learning_rate": 4.5932972319228593e-07,
      "loss": 0.555,
      "num_input_tokens_seen": 48423328,
      "step": 3860,
      "train_runtime": 41130.9524,
      "train_tokens_per_second": 1177.297
    },
    {
      "epoch": 0.7811236863379143,
      "grad_norm": 3.7686736583709717,
      "learning_rate": 4.552889836516622e-07,
      "loss": 0.5517,
      "num_input_tokens_seen": 48486496,
      "step": 3865,
      "train_runtime": 41184.5618,
      "train_tokens_per_second": 1177.298
    },
    {
      "epoch": 0.7821341956345998,
      "grad_norm": 2.6727774143218994,
      "learning_rate": 4.5126381191131167e-07,
      "loss": 0.6948,
      "num_input_tokens_seen": 48548864,
      "step": 3870,
      "train_runtime": 41237.5606,
      "train_tokens_per_second": 1177.297
    },
    {
      "epoch": 0.7831447049312854,
      "grad_norm": 2.7915477752685547,
      "learning_rate": 4.4725424853744377e-07,
      "loss": 0.4622,
      "num_input_tokens_seen": 48611216,
      "step": 3875,
      "train_runtime": 41290.3548,
      "train_tokens_per_second": 1177.302
    },
    {
      "epoch": 0.7841552142279709,
      "grad_norm": 4.423336982727051,
      "learning_rate": 4.4326033393896467e-07,
      "loss": 0.6363,
      "num_input_tokens_seen": 48672912,
      "step": 3880,
      "train_runtime": 41342.7479,
      "train_tokens_per_second": 1177.302
    },
    {
      "epoch": 0.7851657235246564,
      "grad_norm": 3.0866639614105225,
      "learning_rate": 4.3928210836706927e-07,
      "loss": 0.6629,
      "num_input_tokens_seen": 48735120,
      "step": 3885,
      "train_runtime": 41395.5288,
      "train_tokens_per_second": 1177.304
    },
    {
      "epoch": 0.786176232821342,
      "grad_norm": 4.012517929077148,
      "learning_rate": 4.353196119148361e-07,
      "loss": 0.516,
      "num_input_tokens_seen": 48796864,
      "step": 3890,
      "train_runtime": 41448.0467,
      "train_tokens_per_second": 1177.302
    },
    {
      "epoch": 0.7871867421180275,
      "grad_norm": 4.842701435089111,
      "learning_rate": 4.3137288451682495e-07,
      "loss": 0.5204,
      "num_input_tokens_seen": 48860016,
      "step": 3895,
      "train_runtime": 41501.6102,
      "train_tokens_per_second": 1177.304
    },
    {
      "epoch": 0.788197251414713,
      "grad_norm": 4.452552318572998,
      "learning_rate": 4.274419659486728e-07,
      "loss": 0.545,
      "num_input_tokens_seen": 48923088,
      "step": 3900,
      "train_runtime": 41555.1035,
      "train_tokens_per_second": 1177.306
    },
    {
      "epoch": 0.7892077607113985,
      "grad_norm": 5.510317802429199,
      "learning_rate": 4.23526895826692e-07,
      "loss": 0.5635,
      "num_input_tokens_seen": 48986288,
      "step": 3905,
      "train_runtime": 41609.2924,
      "train_tokens_per_second": 1177.292
    },
    {
      "epoch": 0.7902182700080841,
      "grad_norm": 4.986194610595703,
      "learning_rate": 4.196277136074724e-07,
      "loss": 0.5421,
      "num_input_tokens_seen": 49050464,
      "step": 3910,
      "train_runtime": 41663.4712,
      "train_tokens_per_second": 1177.301
    },
    {
      "epoch": 0.7912287793047696,
      "grad_norm": 3.3006770610809326,
      "learning_rate": 4.1574445858748454e-07,
      "loss": 0.5038,
      "num_input_tokens_seen": 49113296,
      "step": 3915,
      "train_runtime": 41716.7174,
      "train_tokens_per_second": 1177.305
    },
    {
      "epoch": 0.7922392886014551,
      "grad_norm": 2.981992483139038,
      "learning_rate": 4.1187716990268286e-07,
      "loss": 0.5031,
      "num_input_tokens_seen": 49176352,
      "step": 3920,
      "train_runtime": 41770.1599,
      "train_tokens_per_second": 1177.308
    },
    {
      "epoch": 0.7932497978981407,
      "grad_norm": 3.5853819847106934,
      "learning_rate": 4.080258865281081e-07,
      "loss": 0.5392,
      "num_input_tokens_seen": 49238896,
      "step": 3925,
      "train_runtime": 41823.2605,
      "train_tokens_per_second": 1177.309
    },
    {
      "epoch": 0.7942603071948262,
      "grad_norm": 3.1860404014587402,
      "learning_rate": 4.041906472775005e-07,
      "loss": 0.656,
      "num_input_tokens_seen": 49301840,
      "step": 3930,
      "train_runtime": 41876.606,
      "train_tokens_per_second": 1177.312
    },
    {
      "epoch": 0.7952708164915118,
      "grad_norm": 5.786596775054932,
      "learning_rate": 4.003714908029046e-07,
      "loss": 0.5383,
      "num_input_tokens_seen": 49364240,
      "step": 3935,
      "train_runtime": 41929.6646,
      "train_tokens_per_second": 1177.311
    },
    {
      "epoch": 0.7962813257881972,
      "grad_norm": 4.612095832824707,
      "learning_rate": 3.965684555942801e-07,
      "loss": 0.6698,
      "num_input_tokens_seen": 49427888,
      "step": 3940,
      "train_runtime": 41983.4311,
      "train_tokens_per_second": 1177.319
    },
    {
      "epoch": 0.7972918350848828,
      "grad_norm": 3.6710093021392822,
      "learning_rate": 3.927815799791143e-07,
      "loss": 0.6614,
      "num_input_tokens_seen": 49490224,
      "step": 3945,
      "train_runtime": 42036.2127,
      "train_tokens_per_second": 1177.324
    },
    {
      "epoch": 0.7983023443815683,
      "grad_norm": 3.744783401489258,
      "learning_rate": 3.890109021220367e-07,
      "loss": 0.5923,
      "num_input_tokens_seen": 49552416,
      "step": 3950,
      "train_runtime": 42089.3152,
      "train_tokens_per_second": 1177.316
    },
    {
      "epoch": 0.7993128536782539,
      "grad_norm": 7.969940662384033,
      "learning_rate": 3.852564600244341e-07,
      "loss": 0.445,
      "num_input_tokens_seen": 49615536,
      "step": 3955,
      "train_runtime": 42142.7934,
      "train_tokens_per_second": 1177.32
    },
    {
      "epoch": 0.8003233629749393,
      "grad_norm": 3.859337329864502,
      "learning_rate": 3.81518291524066e-07,
      "loss": 0.5023,
      "num_input_tokens_seen": 49677536,
      "step": 3960,
      "train_runtime": 42195.5811,
      "train_tokens_per_second": 1177.316
    },
    {
      "epoch": 0.8013338722716249,
      "grad_norm": 3.3185038566589355,
      "learning_rate": 3.7779643429468445e-07,
      "loss": 0.6202,
      "num_input_tokens_seen": 49739808,
      "step": 3965,
      "train_runtime": 42248.4194,
      "train_tokens_per_second": 1177.318
    },
    {
      "epoch": 0.8023443815683104,
      "grad_norm": 3.2049145698547363,
      "learning_rate": 3.7409092584565525e-07,
      "loss": 0.4702,
      "num_input_tokens_seen": 49800400,
      "step": 3970,
      "train_runtime": 42299.9756,
      "train_tokens_per_second": 1177.315
    },
    {
      "epoch": 0.803354890864996,
      "grad_norm": 3.3126466274261475,
      "learning_rate": 3.7040180352157924e-07,
      "loss": 0.591,
      "num_input_tokens_seen": 49863600,
      "step": 3975,
      "train_runtime": 42353.6699,
      "train_tokens_per_second": 1177.315
    },
    {
      "epoch": 0.8043654001616815,
      "grad_norm": 3.872746229171753,
      "learning_rate": 3.6672910450191497e-07,
      "loss": 0.5719,
      "num_input_tokens_seen": 49926560,
      "step": 3980,
      "train_runtime": 42407.0333,
      "train_tokens_per_second": 1177.318
    },
    {
      "epoch": 0.805375909458367,
      "grad_norm": 3.391008138656616,
      "learning_rate": 3.6307286580060395e-07,
      "loss": 0.5849,
      "num_input_tokens_seen": 49990096,
      "step": 3985,
      "train_runtime": 42460.7456,
      "train_tokens_per_second": 1177.325
    },
    {
      "epoch": 0.8063864187550526,
      "grad_norm": 3.8504889011383057,
      "learning_rate": 3.594331242657005e-07,
      "loss": 0.6877,
      "num_input_tokens_seen": 50052784,
      "step": 3990,
      "train_runtime": 42513.9138,
      "train_tokens_per_second": 1177.327
    },
    {
      "epoch": 0.8073969280517381,
      "grad_norm": 4.00076961517334,
      "learning_rate": 3.558099165789974e-07,
      "loss": 0.4594,
      "num_input_tokens_seen": 50115632,
      "step": 3995,
      "train_runtime": 42567.1347,
      "train_tokens_per_second": 1177.332
    },
    {
      "epoch": 0.8084074373484236,
      "grad_norm": 4.351169109344482,
      "learning_rate": 3.5220327925565686e-07,
      "loss": 0.5042,
      "num_input_tokens_seen": 50178528,
      "step": 4000,
      "train_runtime": 42620.4145,
      "train_tokens_per_second": 1177.336
    },
    {
      "epoch": 0.8094179466451091,
      "grad_norm": 4.29773473739624,
      "learning_rate": 3.486132486438422e-07,
      "loss": 0.6508,
      "num_input_tokens_seen": 50240288,
      "step": 4005,
      "train_runtime": 42673.6688,
      "train_tokens_per_second": 1177.314
    },
    {
      "epoch": 0.8104284559417947,
      "grad_norm": 2.8803224563598633,
      "learning_rate": 3.4503986092435365e-07,
      "loss": 0.5891,
      "num_input_tokens_seen": 50302848,
      "step": 4010,
      "train_runtime": 42726.7662,
      "train_tokens_per_second": 1177.315
    },
    {
      "epoch": 0.8114389652384802,
      "grad_norm": 4.212482452392578,
      "learning_rate": 3.414831521102617e-07,
      "loss": 0.6822,
      "num_input_tokens_seen": 50366192,
      "step": 4015,
      "train_runtime": 42780.4621,
      "train_tokens_per_second": 1177.318
    },
    {
      "epoch": 0.8124494745351657,
      "grad_norm": 2.629336357116699,
      "learning_rate": 3.379431580465442e-07,
      "loss": 0.6311,
      "num_input_tokens_seen": 50428208,
      "step": 4020,
      "train_runtime": 42833.2972,
      "train_tokens_per_second": 1177.313
    },
    {
      "epoch": 0.8134599838318513,
      "grad_norm": 2.531073570251465,
      "learning_rate": 3.344199144097248e-07,
      "loss": 0.5052,
      "num_input_tokens_seen": 50490384,
      "step": 4025,
      "train_runtime": 42886.1112,
      "train_tokens_per_second": 1177.313
    },
    {
      "epoch": 0.8144704931285368,
      "grad_norm": 6.284623146057129,
      "learning_rate": 3.309134567075165e-07,
      "loss": 0.6797,
      "num_input_tokens_seen": 50554608,
      "step": 4030,
      "train_runtime": 42940.394,
      "train_tokens_per_second": 1177.321
    },
    {
      "epoch": 0.8154810024252223,
      "grad_norm": 4.410198211669922,
      "learning_rate": 3.2742382027845874e-07,
      "loss": 0.4845,
      "num_input_tokens_seen": 50617664,
      "step": 4035,
      "train_runtime": 42994.0262,
      "train_tokens_per_second": 1177.319
    },
    {
      "epoch": 0.8164915117219078,
      "grad_norm": 3.6726479530334473,
      "learning_rate": 3.2395104029156637e-07,
      "loss": 0.5037,
      "num_input_tokens_seen": 50680496,
      "step": 4040,
      "train_runtime": 43047.3637,
      "train_tokens_per_second": 1177.319
    },
    {
      "epoch": 0.8175020210185934,
      "grad_norm": 4.015049934387207,
      "learning_rate": 3.204951517459709e-07,
      "loss": 0.7552,
      "num_input_tokens_seen": 50743760,
      "step": 4045,
      "train_runtime": 43100.9675,
      "train_tokens_per_second": 1177.323
    },
    {
      "epoch": 0.8185125303152789,
      "grad_norm": 3.0785200595855713,
      "learning_rate": 3.170561894705712e-07,
      "loss": 0.5181,
      "num_input_tokens_seen": 50806032,
      "step": 4050,
      "train_runtime": 43153.8849,
      "train_tokens_per_second": 1177.322
    },
    {
      "epoch": 0.8195230396119644,
      "grad_norm": 3.240412473678589,
      "learning_rate": 3.1363418812367923e-07,
      "loss": 0.5736,
      "num_input_tokens_seen": 50869312,
      "step": 4055,
      "train_runtime": 43207.4398,
      "train_tokens_per_second": 1177.328
    },
    {
      "epoch": 0.8205335489086499,
      "grad_norm": 2.030541181564331,
      "learning_rate": 3.102291821926748e-07,
      "loss": 0.4529,
      "num_input_tokens_seen": 50930928,
      "step": 4060,
      "train_runtime": 43259.7343,
      "train_tokens_per_second": 1177.329
    },
    {
      "epoch": 0.8215440582053355,
      "grad_norm": 4.060739994049072,
      "learning_rate": 3.068412059936529e-07,
      "loss": 0.5811,
      "num_input_tokens_seen": 50993328,
      "step": 4065,
      "train_runtime": 43312.7472,
      "train_tokens_per_second": 1177.328
    },
    {
      "epoch": 0.822554567502021,
      "grad_norm": 3.428931474685669,
      "learning_rate": 3.0347029367108357e-07,
      "loss": 0.5557,
      "num_input_tokens_seen": 51055936,
      "step": 4070,
      "train_runtime": 43365.906,
      "train_tokens_per_second": 1177.329
    },
    {
      "epoch": 0.8235650767987065,
      "grad_norm": 3.5364294052124023,
      "learning_rate": 3.001164791974624e-07,
      "loss": 0.5354,
      "num_input_tokens_seen": 51118144,
      "step": 4075,
      "train_runtime": 43418.8077,
      "train_tokens_per_second": 1177.327
    },
    {
      "epoch": 0.824575586095392,
      "grad_norm": 3.3202242851257324,
      "learning_rate": 2.9677979637297237e-07,
      "loss": 0.6328,
      "num_input_tokens_seen": 51180656,
      "step": 4080,
      "train_runtime": 43471.9394,
      "train_tokens_per_second": 1177.326
    },
    {
      "epoch": 0.8255860953920776,
      "grad_norm": 5.297423362731934,
      "learning_rate": 2.9346027882513967e-07,
      "loss": 0.6274,
      "num_input_tokens_seen": 51242864,
      "step": 4085,
      "train_runtime": 43524.8254,
      "train_tokens_per_second": 1177.325
    },
    {
      "epoch": 0.8265966046887632,
      "grad_norm": 4.029085159301758,
      "learning_rate": 2.9015796000849914e-07,
      "loss": 0.5654,
      "num_input_tokens_seen": 51305056,
      "step": 4090,
      "train_runtime": 43577.6442,
      "train_tokens_per_second": 1177.325
    },
    {
      "epoch": 0.8276071139854486,
      "grad_norm": 4.242973804473877,
      "learning_rate": 2.86872873204252e-07,
      "loss": 0.5394,
      "num_input_tokens_seen": 51368384,
      "step": 4095,
      "train_runtime": 43631.4135,
      "train_tokens_per_second": 1177.326
    },
    {
      "epoch": 0.8286176232821342,
      "grad_norm": 2.8023030757904053,
      "learning_rate": 2.8360505151993395e-07,
      "loss": 0.6707,
      "num_input_tokens_seen": 51431584,
      "step": 4100,
      "train_runtime": 43684.9948,
      "train_tokens_per_second": 1177.328
    },
    {
      "epoch": 0.8296281325788197,
      "grad_norm": 3.828986883163452,
      "learning_rate": 2.803545278890809e-07,
      "loss": 0.6816,
      "num_input_tokens_seen": 51495056,
      "step": 4105,
      "train_runtime": 43739.4788,
      "train_tokens_per_second": 1177.313
    },
    {
      "epoch": 0.8306386418755053,
      "grad_norm": 3.439495086669922,
      "learning_rate": 2.77121335070897e-07,
      "loss": 0.3959,
      "num_input_tokens_seen": 51557248,
      "step": 4110,
      "train_runtime": 43792.3819,
      "train_tokens_per_second": 1177.311
    },
    {
      "epoch": 0.8316491511721907,
      "grad_norm": 3.5180540084838867,
      "learning_rate": 2.7390550564992287e-07,
      "loss": 0.5186,
      "num_input_tokens_seen": 51619040,
      "step": 4115,
      "train_runtime": 43844.8459,
      "train_tokens_per_second": 1177.311
    },
    {
      "epoch": 0.8326596604688763,
      "grad_norm": 3.9391674995422363,
      "learning_rate": 2.707070720357094e-07,
      "loss": 0.6799,
      "num_input_tokens_seen": 51682000,
      "step": 4120,
      "train_runtime": 43898.4807,
      "train_tokens_per_second": 1177.307
    },
    {
      "epoch": 0.8336701697655619,
      "grad_norm": 4.391687393188477,
      "learning_rate": 2.6752606646249056e-07,
      "loss": 0.5884,
      "num_input_tokens_seen": 51744256,
      "step": 4125,
      "train_runtime": 43951.299,
      "train_tokens_per_second": 1177.309
    },
    {
      "epoch": 0.8346806790622474,
      "grad_norm": 2.896277904510498,
      "learning_rate": 2.6436252098885734e-07,
      "loss": 0.5481,
      "num_input_tokens_seen": 51807600,
      "step": 4130,
      "train_runtime": 44004.8931,
      "train_tokens_per_second": 1177.315
    },
    {
      "epoch": 0.8356911883589329,
      "grad_norm": 5.022926330566406,
      "learning_rate": 2.6121646749743667e-07,
      "loss": 0.4999,
      "num_input_tokens_seen": 51871360,
      "step": 4135,
      "train_runtime": 44058.7703,
      "train_tokens_per_second": 1177.322
    },
    {
      "epoch": 0.8367016976556184,
      "grad_norm": 2.963527202606201,
      "learning_rate": 2.580879376945675e-07,
      "loss": 0.5947,
      "num_input_tokens_seen": 51933920,
      "step": 4140,
      "train_runtime": 44111.7323,
      "train_tokens_per_second": 1177.327
    },
    {
      "epoch": 0.837712206952304,
      "grad_norm": 3.58630633354187,
      "learning_rate": 2.549769631099845e-07,
      "loss": 0.5659,
      "num_input_tokens_seen": 51996592,
      "step": 4145,
      "train_runtime": 44164.8043,
      "train_tokens_per_second": 1177.331
    },
    {
      "epoch": 0.8387227162489895,
      "grad_norm": 3.813584089279175,
      "learning_rate": 2.5188357509649696e-07,
      "loss": 0.6452,
      "num_input_tokens_seen": 52059168,
      "step": 4150,
      "train_runtime": 44217.9075,
      "train_tokens_per_second": 1177.332
    },
    {
      "epoch": 0.839733225545675,
      "grad_norm": 3.6372597217559814,
      "learning_rate": 2.488078048296758e-07,
      "loss": 0.5718,
      "num_input_tokens_seen": 52121504,
      "step": 4155,
      "train_runtime": 44270.8236,
      "train_tokens_per_second": 1177.333
    },
    {
      "epoch": 0.8407437348423605,
      "grad_norm": 4.374914646148682,
      "learning_rate": 2.4574968330753633e-07,
      "loss": 0.5339,
      "num_input_tokens_seen": 52183248,
      "step": 4160,
      "train_runtime": 44323.3344,
      "train_tokens_per_second": 1177.331
    },
    {
      "epoch": 0.8417542441390461,
      "grad_norm": 3.1079819202423096,
      "learning_rate": 2.4270924135022963e-07,
      "loss": 0.5739,
      "num_input_tokens_seen": 52245792,
      "step": 4165,
      "train_runtime": 44376.4109,
      "train_tokens_per_second": 1177.333
    },
    {
      "epoch": 0.8427647534357317,
      "grad_norm": 3.0118348598480225,
      "learning_rate": 2.396865095997276e-07,
      "loss": 0.5553,
      "num_input_tokens_seen": 52307824,
      "step": 4170,
      "train_runtime": 44429.2809,
      "train_tokens_per_second": 1177.328
    },
    {
      "epoch": 0.8437752627324171,
      "grad_norm": 3.992180585861206,
      "learning_rate": 2.366815185195179e-07,
      "loss": 0.5804,
      "num_input_tokens_seen": 52370608,
      "step": 4175,
      "train_runtime": 44482.5541,
      "train_tokens_per_second": 1177.329
    },
    {
      "epoch": 0.8447857720291027,
      "grad_norm": 4.425224781036377,
      "learning_rate": 2.3369429839429465e-07,
      "loss": 0.5378,
      "num_input_tokens_seen": 52434000,
      "step": 4180,
      "train_runtime": 44536.0981,
      "train_tokens_per_second": 1177.337
    },
    {
      "epoch": 0.8457962813257882,
      "grad_norm": 2.227294683456421,
      "learning_rate": 2.3072487932965456e-07,
      "loss": 0.5261,
      "num_input_tokens_seen": 52495872,
      "step": 4185,
      "train_runtime": 44588.5825,
      "train_tokens_per_second": 1177.339
    },
    {
      "epoch": 0.8468067906224738,
      "grad_norm": 3.269984722137451,
      "learning_rate": 2.277732912517918e-07,
      "loss": 0.4827,
      "num_input_tokens_seen": 52559888,
      "step": 4190,
      "train_runtime": 44642.6782,
      "train_tokens_per_second": 1177.346
    },
    {
      "epoch": 0.8478172999191592,
      "grad_norm": 4.080695152282715,
      "learning_rate": 2.2483956390719894e-07,
      "loss": 0.5426,
      "num_input_tokens_seen": 52622736,
      "step": 4195,
      "train_runtime": 44696.0407,
      "train_tokens_per_second": 1177.347
    },
    {
      "epoch": 0.8488278092158448,
      "grad_norm": 2.9317164421081543,
      "learning_rate": 2.219237268623646e-07,
      "loss": 0.4762,
      "num_input_tokens_seen": 52685584,
      "step": 4200,
      "train_runtime": 44749.3553,
      "train_tokens_per_second": 1177.348
    },
    {
      "epoch": 0.8498383185125303,
      "grad_norm": 3.1733241081237793,
      "learning_rate": 2.1902580950347738e-07,
      "loss": 0.5364,
      "num_input_tokens_seen": 52748112,
      "step": 4205,
      "train_runtime": 44803.2342,
      "train_tokens_per_second": 1177.328
    },
    {
      "epoch": 0.8508488278092159,
      "grad_norm": 3.522993326187134,
      "learning_rate": 2.161458410361281e-07,
      "loss": 0.529,
      "num_input_tokens_seen": 52810784,
      "step": 4210,
      "train_runtime": 44856.2961,
      "train_tokens_per_second": 1177.333
    },
    {
      "epoch": 0.8518593371059013,
      "grad_norm": 3.4738011360168457,
      "learning_rate": 2.1328385048501763e-07,
      "loss": 0.582,
      "num_input_tokens_seen": 52873520,
      "step": 4215,
      "train_runtime": 44909.488,
      "train_tokens_per_second": 1177.335
    },
    {
      "epoch": 0.8528698464025869,
      "grad_norm": 3.208278179168701,
      "learning_rate": 2.1043986669366153e-07,
      "loss": 0.5192,
      "num_input_tokens_seen": 52936384,
      "step": 4220,
      "train_runtime": 44962.836,
      "train_tokens_per_second": 1177.336
    },
    {
      "epoch": 0.8538803556992725,
      "grad_norm": 4.759490489959717,
      "learning_rate": 2.0761391832410124e-07,
      "loss": 0.7744,
      "num_input_tokens_seen": 52998336,
      "step": 4225,
      "train_runtime": 45015.3481,
      "train_tokens_per_second": 1177.339
    },
    {
      "epoch": 0.854890864995958,
      "grad_norm": 2.596691608428955,
      "learning_rate": 2.04806033856616e-07,
      "loss": 0.5369,
      "num_input_tokens_seen": 53060368,
      "step": 4230,
      "train_runtime": 45068.0772,
      "train_tokens_per_second": 1177.338
    },
    {
      "epoch": 0.8559013742926435,
      "grad_norm": 3.9744553565979004,
      "learning_rate": 2.0201624158943353e-07,
      "loss": 0.7057,
      "num_input_tokens_seen": 53122848,
      "step": 4235,
      "train_runtime": 45121.1382,
      "train_tokens_per_second": 1177.338
    },
    {
      "epoch": 0.856911883589329,
      "grad_norm": 2.964618444442749,
      "learning_rate": 1.9924456963844638e-07,
      "loss": 0.5632,
      "num_input_tokens_seen": 53186544,
      "step": 4240,
      "train_runtime": 45175.0568,
      "train_tokens_per_second": 1177.343
    },
    {
      "epoch": 0.8579223928860146,
      "grad_norm": 4.56308126449585,
      "learning_rate": 1.9649104593692756e-07,
      "loss": 0.5546,
      "num_input_tokens_seen": 53248912,
      "step": 4245,
      "train_runtime": 45228.0781,
      "train_tokens_per_second": 1177.342
    },
    {
      "epoch": 0.8589329021827001,
      "grad_norm": 3.147707462310791,
      "learning_rate": 1.937556982352506e-07,
      "loss": 0.7459,
      "num_input_tokens_seen": 53311184,
      "step": 4250,
      "train_runtime": 45280.9461,
      "train_tokens_per_second": 1177.343
    },
    {
      "epoch": 0.8599434114793856,
      "grad_norm": 4.411808013916016,
      "learning_rate": 1.9103855410060876e-07,
      "loss": 0.5122,
      "num_input_tokens_seen": 53373520,
      "step": 4255,
      "train_runtime": 45334.0277,
      "train_tokens_per_second": 1177.339
    },
    {
      "epoch": 0.8609539207760711,
      "grad_norm": 4.001165866851807,
      "learning_rate": 1.8833964091673682e-07,
      "loss": 0.6898,
      "num_input_tokens_seen": 53435936,
      "step": 4260,
      "train_runtime": 45387.0829,
      "train_tokens_per_second": 1177.338
    },
    {
      "epoch": 0.8619644300727567,
      "grad_norm": 4.67122745513916,
      "learning_rate": 1.856589858836355e-07,
      "loss": 0.6817,
      "num_input_tokens_seen": 53498528,
      "step": 4265,
      "train_runtime": 45440.3311,
      "train_tokens_per_second": 1177.336
    },
    {
      "epoch": 0.8629749393694423,
      "grad_norm": 5.748219966888428,
      "learning_rate": 1.829966160172982e-07,
      "loss": 0.6331,
      "num_input_tokens_seen": 53562560,
      "step": 4270,
      "train_runtime": 45494.4737,
      "train_tokens_per_second": 1177.342
    },
    {
      "epoch": 0.8639854486661277,
      "grad_norm": 4.742687702178955,
      "learning_rate": 1.8035255814943828e-07,
      "loss": 0.5135,
      "num_input_tokens_seen": 53624400,
      "step": 4275,
      "train_runtime": 45547.2189,
      "train_tokens_per_second": 1177.336
    },
    {
      "epoch": 0.8649959579628133,
      "grad_norm": 3.1594347953796387,
      "learning_rate": 1.777268389272173e-07,
      "loss": 0.6047,
      "num_input_tokens_seen": 53687504,
      "step": 4280,
      "train_runtime": 45600.6258,
      "train_tokens_per_second": 1177.341
    },
    {
      "epoch": 0.8660064672594988,
      "grad_norm": 4.132379531860352,
      "learning_rate": 1.7511948481297778e-07,
      "loss": 0.5573,
      "num_input_tokens_seen": 53750624,
      "step": 4285,
      "train_runtime": 45654.1061,
      "train_tokens_per_second": 1177.345
    },
    {
      "epoch": 0.8670169765561844,
      "grad_norm": 3.163335084915161,
      "learning_rate": 1.725305220839768e-07,
      "loss": 0.6361,
      "num_input_tokens_seen": 53812304,
      "step": 4290,
      "train_runtime": 45706.6537,
      "train_tokens_per_second": 1177.341
    },
    {
      "epoch": 0.8680274858528698,
      "grad_norm": 5.631870269775391,
      "learning_rate": 1.699599768321207e-07,
      "loss": 0.7041,
      "num_input_tokens_seen": 53875584,
      "step": 4295,
      "train_runtime": 45760.5105,
      "train_tokens_per_second": 1177.338
    },
    {
      "epoch": 0.8690379951495554,
      "grad_norm": 3.0659360885620117,
      "learning_rate": 1.6740787496370134e-07,
      "loss": 0.5137,
      "num_input_tokens_seen": 53938016,
      "step": 4300,
      "train_runtime": 45813.5463,
      "train_tokens_per_second": 1177.338
    },
    {
      "epoch": 0.8700485044462409,
      "grad_norm": 5.1752142906188965,
      "learning_rate": 1.6487424219913626e-07,
      "loss": 0.5628,
      "num_input_tokens_seen": 54000176,
      "step": 4305,
      "train_runtime": 45867.1268,
      "train_tokens_per_second": 1177.318
    },
    {
      "epoch": 0.8710590137429264,
      "grad_norm": 3.1198580265045166,
      "learning_rate": 1.6235910407270858e-07,
      "loss": 0.6413,
      "num_input_tokens_seen": 54063744,
      "step": 4310,
      "train_runtime": 45921.0375,
      "train_tokens_per_second": 1177.32
    },
    {
      "epoch": 0.8720695230396119,
      "grad_norm": 4.821471214294434,
      "learning_rate": 1.5986248593231166e-07,
      "loss": 0.6132,
      "num_input_tokens_seen": 54126272,
      "step": 4315,
      "train_runtime": 45974.1807,
      "train_tokens_per_second": 1177.319
    },
    {
      "epoch": 0.8730800323362975,
      "grad_norm": 6.516937255859375,
      "learning_rate": 1.5738441293918858e-07,
      "loss": 0.5611,
      "num_input_tokens_seen": 54190192,
      "step": 4320,
      "train_runtime": 46028.1829,
      "train_tokens_per_second": 1177.326
    },
    {
      "epoch": 0.874090541632983,
      "grad_norm": 3.0954699516296387,
      "learning_rate": 1.5492491006768527e-07,
      "loss": 0.5842,
      "num_input_tokens_seen": 54252960,
      "step": 4325,
      "train_runtime": 46081.4973,
      "train_tokens_per_second": 1177.326
    },
    {
      "epoch": 0.8751010509296685,
      "grad_norm": 3.847485065460205,
      "learning_rate": 1.52484002104994e-07,
      "loss": 0.535,
      "num_input_tokens_seen": 54315968,
      "step": 4330,
      "train_runtime": 46134.8808,
      "train_tokens_per_second": 1177.33
    },
    {
      "epoch": 0.8761115602263541,
      "grad_norm": 3.6992697715759277,
      "learning_rate": 1.5006171365090636e-07,
      "loss": 0.6498,
      "num_input_tokens_seen": 54379376,
      "step": 4335,
      "train_runtime": 46188.4771,
      "train_tokens_per_second": 1177.336
    },
    {
      "epoch": 0.8771220695230396,
      "grad_norm": 3.1576523780822754,
      "learning_rate": 1.4765806911756174e-07,
      "loss": 0.5299,
      "num_input_tokens_seen": 54442624,
      "step": 4340,
      "train_runtime": 46242.1325,
      "train_tokens_per_second": 1177.338
    },
    {
      "epoch": 0.8781325788197252,
      "grad_norm": 9.175505638122559,
      "learning_rate": 1.4527309272920584e-07,
      "loss": 0.5783,
      "num_input_tokens_seen": 54504944,
      "step": 4345,
      "train_runtime": 46295.0625,
      "train_tokens_per_second": 1177.338
    },
    {
      "epoch": 0.8791430881164106,
      "grad_norm": 4.24160623550415,
      "learning_rate": 1.429068085219436e-07,
      "loss": 0.6573,
      "num_input_tokens_seen": 54568176,
      "step": 4350,
      "train_runtime": 46348.6307,
      "train_tokens_per_second": 1177.342
    },
    {
      "epoch": 0.8801535974130962,
      "grad_norm": 4.731931209564209,
      "learning_rate": 1.4055924034349765e-07,
      "loss": 0.5208,
      "num_input_tokens_seen": 54630656,
      "step": 4355,
      "train_runtime": 46401.7418,
      "train_tokens_per_second": 1177.341
    },
    {
      "epoch": 0.8811641067097817,
      "grad_norm": 3.664085865020752,
      "learning_rate": 1.3823041185296779e-07,
      "loss": 0.5035,
      "num_input_tokens_seen": 54693328,
      "step": 4360,
      "train_runtime": 46454.9386,
      "train_tokens_per_second": 1177.342
    },
    {
      "epoch": 0.8821746160064673,
      "grad_norm": 3.897765874862671,
      "learning_rate": 1.3592034652059315e-07,
      "loss": 0.5085,
      "num_input_tokens_seen": 54757200,
      "step": 4365,
      "train_runtime": 46508.8177,
      "train_tokens_per_second": 1177.351
    },
    {
      "epoch": 0.8831851253031527,
      "grad_norm": 2.898092031478882,
      "learning_rate": 1.336290676275158e-07,
      "loss": 0.5308,
      "num_input_tokens_seen": 54819376,
      "step": 4370,
      "train_runtime": 46561.5112,
      "train_tokens_per_second": 1177.354
    },
    {
      "epoch": 0.8841956345998383,
      "grad_norm": 2.8077433109283447,
      "learning_rate": 1.3135659826554446e-07,
      "loss": 0.5558,
      "num_input_tokens_seen": 54882352,
      "step": 4375,
      "train_runtime": 46614.823,
      "train_tokens_per_second": 1177.358
    },
    {
      "epoch": 0.8852061438965239,
      "grad_norm": 3.160156011581421,
      "learning_rate": 1.2910296133692322e-07,
      "loss": 0.7095,
      "num_input_tokens_seen": 54945328,
      "step": 4380,
      "train_runtime": 46668.2629,
      "train_tokens_per_second": 1177.36
    },
    {
      "epoch": 0.8862166531932094,
      "grad_norm": 3.50114107131958,
      "learning_rate": 1.2686817955410112e-07,
      "loss": 0.5274,
      "num_input_tokens_seen": 55008672,
      "step": 4385,
      "train_runtime": 46721.9171,
      "train_tokens_per_second": 1177.363
    },
    {
      "epoch": 0.8872271624898949,
      "grad_norm": 3.4780025482177734,
      "learning_rate": 1.246522754395023e-07,
      "loss": 0.4683,
      "num_input_tokens_seen": 55072048,
      "step": 4390,
      "train_runtime": 46775.5598,
      "train_tokens_per_second": 1177.368
    },
    {
      "epoch": 0.8882376717865804,
      "grad_norm": 3.6449575424194336,
      "learning_rate": 1.2245527132529908e-07,
      "loss": 0.5519,
      "num_input_tokens_seen": 55134896,
      "step": 4395,
      "train_runtime": 46828.8196,
      "train_tokens_per_second": 1177.371
    },
    {
      "epoch": 0.889248181083266,
      "grad_norm": 3.142120838165283,
      "learning_rate": 1.2027718935318644e-07,
      "loss": 0.4969,
      "num_input_tokens_seen": 55197600,
      "step": 4400,
      "train_runtime": 46882.0122,
      "train_tokens_per_second": 1177.373
    },
    {
      "epoch": 0.8902586903799515,
      "grad_norm": 3.252140760421753,
      "learning_rate": 1.181180514741611e-07,
      "loss": 0.6262,
      "num_input_tokens_seen": 55260448,
      "step": 4405,
      "train_runtime": 46936.0439,
      "train_tokens_per_second": 1177.356
    },
    {
      "epoch": 0.891269199676637,
      "grad_norm": 3.12406849861145,
      "learning_rate": 1.1597787944829795e-07,
      "loss": 0.5727,
      "num_input_tokens_seen": 55323552,
      "step": 4410,
      "train_runtime": 46989.4508,
      "train_tokens_per_second": 1177.361
    },
    {
      "epoch": 0.8922797089733225,
      "grad_norm": 3.4848530292510986,
      "learning_rate": 1.1385669484453053e-07,
      "loss": 0.5548,
      "num_input_tokens_seen": 55387504,
      "step": 4415,
      "train_runtime": 47043.5552,
      "train_tokens_per_second": 1177.366
    },
    {
      "epoch": 0.8932902182700081,
      "grad_norm": 3.366884708404541,
      "learning_rate": 1.117545190404361e-07,
      "loss": 0.4938,
      "num_input_tokens_seen": 55448960,
      "step": 4420,
      "train_runtime": 47095.7416,
      "train_tokens_per_second": 1177.367
    },
    {
      "epoch": 0.8943007275666937,
      "grad_norm": 3.5740790367126465,
      "learning_rate": 1.0967137322201802e-07,
      "loss": 0.5172,
      "num_input_tokens_seen": 55512256,
      "step": 4425,
      "train_runtime": 47149.3742,
      "train_tokens_per_second": 1177.37
    },
    {
      "epoch": 0.8953112368633791,
      "grad_norm": 4.514777660369873,
      "learning_rate": 1.0760727838349404e-07,
      "loss": 0.4637,
      "num_input_tokens_seen": 55575472,
      "step": 4430,
      "train_runtime": 47202.975,
      "train_tokens_per_second": 1177.372
    },
    {
      "epoch": 0.8963217461600647,
      "grad_norm": 3.305375337600708,
      "learning_rate": 1.0556225532708118e-07,
      "loss": 0.5979,
      "num_input_tokens_seen": 55638640,
      "step": 4435,
      "train_runtime": 47256.4307,
      "train_tokens_per_second": 1177.377
    },
    {
      "epoch": 0.8973322554567502,
      "grad_norm": 2.6341917514801025,
      "learning_rate": 1.0353632466279094e-07,
      "loss": 0.6648,
      "num_input_tokens_seen": 55701280,
      "step": 4440,
      "train_runtime": 47309.5191,
      "train_tokens_per_second": 1177.38
    },
    {
      "epoch": 0.8983427647534358,
      "grad_norm": 4.041561603546143,
      "learning_rate": 1.015295068082187e-07,
      "loss": 0.5054,
      "num_input_tokens_seen": 55764144,
      "step": 4445,
      "train_runtime": 47362.8966,
      "train_tokens_per_second": 1177.38
    },
    {
      "epoch": 0.8993532740501212,
      "grad_norm": 4.600080966949463,
      "learning_rate": 9.954182198833749e-08,
      "loss": 0.6673,
      "num_input_tokens_seen": 55826736,
      "step": 4450,
      "train_runtime": 47416.1855,
      "train_tokens_per_second": 1177.377
    },
    {
      "epoch": 0.9003637833468068,
      "grad_norm": 4.085545063018799,
      "learning_rate": 9.75732902352957e-08,
      "loss": 0.7695,
      "num_input_tokens_seen": 55889168,
      "step": 4455,
      "train_runtime": 47469.1663,
      "train_tokens_per_second": 1177.378
    },
    {
      "epoch": 0.9013742926434923,
      "grad_norm": 3.7604928016662598,
      "learning_rate": 9.562393138821456e-08,
      "loss": 0.6977,
      "num_input_tokens_seen": 55951840,
      "step": 4460,
      "train_runtime": 47522.5172,
      "train_tokens_per_second": 1177.375
    },
    {
      "epoch": 0.9023848019401779,
      "grad_norm": 3.060983657836914,
      "learning_rate": 9.369376509298898e-08,
      "loss": 0.6052,
      "num_input_tokens_seen": 56014560,
      "step": 4465,
      "train_runtime": 47575.8422,
      "train_tokens_per_second": 1177.374
    },
    {
      "epoch": 0.9033953112368633,
      "grad_norm": 5.526390075683594,
      "learning_rate": 9.17828108020875e-08,
      "loss": 0.5339,
      "num_input_tokens_seen": 56076640,
      "step": 4470,
      "train_runtime": 47628.5943,
      "train_tokens_per_second": 1177.373
    },
    {
      "epoch": 0.9044058205335489,
      "grad_norm": 4.5335917472839355,
      "learning_rate": 8.989108777435772e-08,
      "loss": 0.4626,
      "num_input_tokens_seen": 56139712,
      "step": 4475,
      "train_runtime": 47681.9863,
      "train_tokens_per_second": 1177.378
    },
    {
      "epoch": 0.9054163298302345,
      "grad_norm": 4.1531500816345215,
      "learning_rate": 8.801861507483321e-08,
      "loss": 0.4621,
      "num_input_tokens_seen": 56203216,
      "step": 4480,
      "train_runtime": 47735.7,
      "train_tokens_per_second": 1177.383
    },
    {
      "epoch": 0.90642683912692,
      "grad_norm": 4.682121276855469,
      "learning_rate": 8.61654115745396e-08,
      "loss": 0.5929,
      "num_input_tokens_seen": 56266064,
      "step": 4485,
      "train_runtime": 47789.0603,
      "train_tokens_per_second": 1177.384
    },
    {
      "epoch": 0.9074373484236055,
      "grad_norm": 4.223501205444336,
      "learning_rate": 8.433149595030431e-08,
      "loss": 0.5493,
      "num_input_tokens_seen": 56328784,
      "step": 4490,
      "train_runtime": 47842.3661,
      "train_tokens_per_second": 1177.383
    },
    {
      "epoch": 0.908447857720291,
      "grad_norm": 3.130216598510742,
      "learning_rate": 8.25168866845698e-08,
      "loss": 0.5033,
      "num_input_tokens_seen": 56390576,
      "step": 4495,
      "train_runtime": 47895.0107,
      "train_tokens_per_second": 1177.379
    },
    {
      "epoch": 0.9094583670169766,
      "grad_norm": 3.936145305633545,
      "learning_rate": 8.07216020652064e-08,
      "loss": 0.6044,
      "num_input_tokens_seen": 56453840,
      "step": 4500,
      "train_runtime": 47948.6323,
      "train_tokens_per_second": 1177.382
    },
    {
      "epoch": 0.9104688763136621,
      "grad_norm": 6.154543399810791,
      "learning_rate": 7.8945660185328e-08,
      "loss": 0.6305,
      "num_input_tokens_seen": 56518352,
      "step": 4505,
      "train_runtime": 48003.8306,
      "train_tokens_per_second": 1177.372
    },
    {
      "epoch": 0.9114793856103476,
      "grad_norm": 3.0912466049194336,
      "learning_rate": 7.718907894311e-08,
      "loss": 0.5979,
      "num_input_tokens_seen": 56581040,
      "step": 4510,
      "train_runtime": 48057.028,
      "train_tokens_per_second": 1177.373
    },
    {
      "epoch": 0.9124898949070331,
      "grad_norm": 3.443275213241577,
      "learning_rate": 7.54518760416083e-08,
      "loss": 0.6457,
      "num_input_tokens_seen": 56644944,
      "step": 4515,
      "train_runtime": 48111.094,
      "train_tokens_per_second": 1177.378
    },
    {
      "epoch": 0.9135004042037187,
      "grad_norm": 3.4428553581237793,
      "learning_rate": 7.37340689885817e-08,
      "loss": 0.5419,
      "num_input_tokens_seen": 56708976,
      "step": 4520,
      "train_runtime": 48165.2518,
      "train_tokens_per_second": 1177.384
    },
    {
      "epoch": 0.9145109135004043,
      "grad_norm": 4.19866418838501,
      "learning_rate": 7.203567509631536e-08,
      "loss": 0.4596,
      "num_input_tokens_seen": 56771040,
      "step": 4525,
      "train_runtime": 48218.0445,
      "train_tokens_per_second": 1177.382
    },
    {
      "epoch": 0.9155214227970897,
      "grad_norm": 3.419058322906494,
      "learning_rate": 7.03567114814454e-08,
      "loss": 0.6351,
      "num_input_tokens_seen": 56833296,
      "step": 4530,
      "train_runtime": 48270.9565,
      "train_tokens_per_second": 1177.381
    },
    {
      "epoch": 0.9165319320937753,
      "grad_norm": 5.4103474617004395,
      "learning_rate": 6.869719506478722e-08,
      "loss": 0.6224,
      "num_input_tokens_seen": 56895760,
      "step": 4535,
      "train_runtime": 48323.9291,
      "train_tokens_per_second": 1177.383
    },
    {
      "epoch": 0.9175424413904608,
      "grad_norm": 3.7131264209747314,
      "learning_rate": 6.705714257116569e-08,
      "loss": 0.634,
      "num_input_tokens_seen": 56958144,
      "step": 4540,
      "train_runtime": 48376.8485,
      "train_tokens_per_second": 1177.384
    },
    {
      "epoch": 0.9185529506871464,
      "grad_norm": 3.0511879920959473,
      "learning_rate": 6.543657052924434e-08,
      "loss": 0.5334,
      "num_input_tokens_seen": 57020688,
      "step": 4545,
      "train_runtime": 48429.8495,
      "train_tokens_per_second": 1177.387
    },
    {
      "epoch": 0.9195634599838318,
      "grad_norm": 4.652146339416504,
      "learning_rate": 6.383549527136111e-08,
      "loss": 0.5364,
      "num_input_tokens_seen": 57083264,
      "step": 4550,
      "train_runtime": 48483.0126,
      "train_tokens_per_second": 1177.387
    },
    {
      "epoch": 0.9205739692805174,
      "grad_norm": 3.5214483737945557,
      "learning_rate": 6.225393293336223e-08,
      "loss": 0.6397,
      "num_input_tokens_seen": 57146592,
      "step": 4555,
      "train_runtime": 48536.6092,
      "train_tokens_per_second": 1177.392
    },
    {
      "epoch": 0.9215844785772029,
      "grad_norm": 2.685511589050293,
      "learning_rate": 6.069189945444097e-08,
      "loss": 0.6071,
      "num_input_tokens_seen": 57209776,
      "step": 4560,
      "train_runtime": 48590.143,
      "train_tokens_per_second": 1177.395
    },
    {
      "epoch": 0.9225949878738885,
      "grad_norm": 3.087183713912964,
      "learning_rate": 5.9149410576974975e-08,
      "loss": 0.5397,
      "num_input_tokens_seen": 57272320,
      "step": 4565,
      "train_runtime": 48643.3275,
      "train_tokens_per_second": 1177.393
    },
    {
      "epoch": 0.9236054971705739,
      "grad_norm": 4.742902755737305,
      "learning_rate": 5.762648184637009e-08,
      "loss": 0.535,
      "num_input_tokens_seen": 57334464,
      "step": 4570,
      "train_runtime": 48696.161,
      "train_tokens_per_second": 1177.392
    },
    {
      "epoch": 0.9246160064672595,
      "grad_norm": 6.083349227905273,
      "learning_rate": 5.6123128610901404e-08,
      "loss": 0.7293,
      "num_input_tokens_seen": 57397536,
      "step": 4575,
      "train_runtime": 48749.643,
      "train_tokens_per_second": 1177.394
    },
    {
      "epoch": 0.9256265157639451,
      "grad_norm": 4.292819499969482,
      "learning_rate": 5.46393660215605e-08,
      "loss": 0.5569,
      "num_input_tokens_seen": 57460304,
      "step": 4580,
      "train_runtime": 48802.9281,
      "train_tokens_per_second": 1177.395
    },
    {
      "epoch": 0.9266370250606305,
      "grad_norm": 3.1266369819641113,
      "learning_rate": 5.317520903190087e-08,
      "loss": 0.5535,
      "num_input_tokens_seen": 57521792,
      "step": 4585,
      "train_runtime": 48855.117,
      "train_tokens_per_second": 1177.395
    },
    {
      "epoch": 0.9276475343573161,
      "grad_norm": 3.244715929031372,
      "learning_rate": 5.173067239788942e-08,
      "loss": 0.6041,
      "num_input_tokens_seen": 57584320,
      "step": 4590,
      "train_runtime": 48908.1814,
      "train_tokens_per_second": 1177.396
    },
    {
      "epoch": 0.9286580436540016,
      "grad_norm": 4.510213851928711,
      "learning_rate": 5.030577067775499e-08,
      "loss": 0.5637,
      "num_input_tokens_seen": 57647472,
      "step": 4595,
      "train_runtime": 48961.7133,
      "train_tokens_per_second": 1177.399
    },
    {
      "epoch": 0.9296685529506872,
      "grad_norm": 3.6560375690460205,
      "learning_rate": 4.8900518231844936e-08,
      "loss": 0.7095,
      "num_input_tokens_seen": 57708576,
      "step": 4600,
      "train_runtime": 49013.7464,
      "train_tokens_per_second": 1177.396
    },
    {
      "epoch": 0.9306790622473726,
      "grad_norm": 2.863469123840332,
      "learning_rate": 4.751492922247702e-08,
      "loss": 0.703,
      "num_input_tokens_seen": 57770864,
      "step": 4605,
      "train_runtime": 49067.3455,
      "train_tokens_per_second": 1177.379
    },
    {
      "epoch": 0.9316895715440582,
      "grad_norm": 3.3546698093414307,
      "learning_rate": 4.6149017613799746e-08,
      "loss": 0.5463,
      "num_input_tokens_seen": 57832336,
      "step": 4610,
      "train_runtime": 49119.6844,
      "train_tokens_per_second": 1177.376
    },
    {
      "epoch": 0.9327000808407437,
      "grad_norm": 2.193046808242798,
      "learning_rate": 4.480279717164892e-08,
      "loss": 0.5719,
      "num_input_tokens_seen": 57894368,
      "step": 4615,
      "train_runtime": 49172.3265,
      "train_tokens_per_second": 1177.377
    },
    {
      "epoch": 0.9337105901374293,
      "grad_norm": 5.471553325653076,
      "learning_rate": 4.347628146341109e-08,
      "loss": 0.4954,
      "num_input_tokens_seen": 57957008,
      "step": 4620,
      "train_runtime": 49225.4371,
      "train_tokens_per_second": 1177.379
    },
    {
      "epoch": 0.9347210994341147,
      "grad_norm": 4.423465251922607,
      "learning_rate": 4.216948385788521e-08,
      "loss": 0.5703,
      "num_input_tokens_seen": 58020528,
      "step": 4625,
      "train_runtime": 49279.2586,
      "train_tokens_per_second": 1177.382
    },
    {
      "epoch": 0.9357316087308003,
      "grad_norm": 3.6405770778656006,
      "learning_rate": 4.0882417525149646e-08,
      "loss": 0.6788,
      "num_input_tokens_seen": 58083200,
      "step": 4630,
      "train_runtime": 49332.6584,
      "train_tokens_per_second": 1177.378
    },
    {
      "epoch": 0.9367421180274859,
      "grad_norm": 3.2768712043762207,
      "learning_rate": 3.961509543642738e-08,
      "loss": 0.527,
      "num_input_tokens_seen": 58145152,
      "step": 4635,
      "train_runtime": 49385.2844,
      "train_tokens_per_second": 1177.378
    },
    {
      "epoch": 0.9377526273241714,
      "grad_norm": 3.4590301513671875,
      "learning_rate": 3.8367530363956345e-08,
      "loss": 0.6647,
      "num_input_tokens_seen": 58206656,
      "step": 4640,
      "train_runtime": 49437.5333,
      "train_tokens_per_second": 1177.378
    },
    {
      "epoch": 0.9387631366208569,
      "grad_norm": 4.135631561279297,
      "learning_rate": 3.7139734880861526e-08,
      "loss": 0.4848,
      "num_input_tokens_seen": 58269600,
      "step": 4645,
      "train_runtime": 49490.8349,
      "train_tokens_per_second": 1177.382
    },
    {
      "epoch": 0.9397736459175424,
      "grad_norm": 4.355393886566162,
      "learning_rate": 3.5931721361026625e-08,
      "loss": 0.6238,
      "num_input_tokens_seen": 58332992,
      "step": 4650,
      "train_runtime": 49544.5989,
      "train_tokens_per_second": 1177.383
    },
    {
      "epoch": 0.940784155214228,
      "grad_norm": 15.263273239135742,
      "learning_rate": 3.474350197897058e-08,
      "loss": 0.5687,
      "num_input_tokens_seen": 58395696,
      "step": 4655,
      "train_runtime": 49597.7341,
      "train_tokens_per_second": 1177.386
    },
    {
      "epoch": 0.9417946645109135,
      "grad_norm": 2.8909714221954346,
      "learning_rate": 3.357508870972392e-08,
      "loss": 0.5409,
      "num_input_tokens_seen": 58457760,
      "step": 4660,
      "train_runtime": 49650.4434,
      "train_tokens_per_second": 1177.386
    },
    {
      "epoch": 0.942805173807599,
      "grad_norm": 3.0614173412323,
      "learning_rate": 3.242649332870906e-08,
      "loss": 0.4759,
      "num_input_tokens_seen": 58519824,
      "step": 4665,
      "train_runtime": 49703.0594,
      "train_tokens_per_second": 1177.389
    },
    {
      "epoch": 0.9438156831042845,
      "grad_norm": 3.8563458919525146,
      "learning_rate": 3.1297727411620845e-08,
      "loss": 0.6125,
      "num_input_tokens_seen": 58582912,
      "step": 4670,
      "train_runtime": 49756.559,
      "train_tokens_per_second": 1177.391
    },
    {
      "epoch": 0.9448261924009701,
      "grad_norm": 4.013493537902832,
      "learning_rate": 3.018880233431043e-08,
      "loss": 0.6546,
      "num_input_tokens_seen": 58646304,
      "step": 4675,
      "train_runtime": 49810.3822,
      "train_tokens_per_second": 1177.391
    },
    {
      "epoch": 0.9458367016976557,
      "grad_norm": 4.557616710662842,
      "learning_rate": 2.9099729272670016e-08,
      "loss": 0.6075,
      "num_input_tokens_seen": 58709056,
      "step": 4680,
      "train_runtime": 49863.5691,
      "train_tokens_per_second": 1177.394
    },
    {
      "epoch": 0.9468472109943411,
      "grad_norm": 3.10516357421875,
      "learning_rate": 2.8030519202520975e-08,
      "loss": 0.5526,
      "num_input_tokens_seen": 58771248,
      "step": 4685,
      "train_runtime": 49916.489,
      "train_tokens_per_second": 1177.391
    },
    {
      "epoch": 0.9478577202910267,
      "grad_norm": 3.313930034637451,
      "learning_rate": 2.6981182899502354e-08,
      "loss": 0.7046,
      "num_input_tokens_seen": 58834656,
      "step": 4690,
      "train_runtime": 49970.2687,
      "train_tokens_per_second": 1177.393
    },
    {
      "epoch": 0.9488682295877122,
      "grad_norm": 2.597517251968384,
      "learning_rate": 2.595173093896341e-08,
      "loss": 0.4901,
      "num_input_tokens_seen": 58897568,
      "step": 4695,
      "train_runtime": 50023.524,
      "train_tokens_per_second": 1177.397
    },
    {
      "epoch": 0.9498787388843978,
      "grad_norm": 3.323392152786255,
      "learning_rate": 2.4942173695855716e-08,
      "loss": 0.6634,
      "num_input_tokens_seen": 58960512,
      "step": 4700,
      "train_runtime": 50076.8872,
      "train_tokens_per_second": 1177.4
    },
    {
      "epoch": 0.9508892481810832,
      "grad_norm": 3.7078816890716553,
      "learning_rate": 2.3952521344630107e-08,
      "loss": 0.6439,
      "num_input_tokens_seen": 59022592,
      "step": 4705,
      "train_runtime": 50130.3875,
      "train_tokens_per_second": 1177.382
    },
    {
      "epoch": 0.9518997574777688,
      "grad_norm": 4.135927677154541,
      "learning_rate": 2.298278385913255e-08,
      "loss": 0.5921,
      "num_input_tokens_seen": 59086432,
      "step": 4710,
      "train_runtime": 50184.3623,
      "train_tokens_per_second": 1177.387
    },
    {
      "epoch": 0.9529102667744543,
      "grad_norm": 3.3447277545928955,
      "learning_rate": 2.2032971012504453e-08,
      "loss": 0.6117,
      "num_input_tokens_seen": 59149168,
      "step": 4715,
      "train_runtime": 50237.5803,
      "train_tokens_per_second": 1177.389
    },
    {
      "epoch": 0.9539207760711399,
      "grad_norm": 2.8357951641082764,
      "learning_rate": 2.110309237708452e-08,
      "loss": 0.6108,
      "num_input_tokens_seen": 59211584,
      "step": 4720,
      "train_runtime": 50290.4662,
      "train_tokens_per_second": 1177.392
    },
    {
      "epoch": 0.9549312853678253,
      "grad_norm": 2.3434131145477295,
      "learning_rate": 2.0193157324311705e-08,
      "loss": 0.4609,
      "num_input_tokens_seen": 59274816,
      "step": 4725,
      "train_runtime": 50344.0198,
      "train_tokens_per_second": 1177.395
    },
    {
      "epoch": 0.9559417946645109,
      "grad_norm": 3.3225901126861572,
      "learning_rate": 1.9303175024630637e-08,
      "loss": 0.5176,
      "num_input_tokens_seen": 59337296,
      "step": 4730,
      "train_runtime": 50397.1049,
      "train_tokens_per_second": 1177.395
    },
    {
      "epoch": 0.9569523039611965,
      "grad_norm": 4.551700592041016,
      "learning_rate": 1.843315444739968e-08,
      "loss": 0.6045,
      "num_input_tokens_seen": 59400768,
      "step": 4735,
      "train_runtime": 50450.7466,
      "train_tokens_per_second": 1177.401
    },
    {
      "epoch": 0.957962813257882,
      "grad_norm": 3.4842886924743652,
      "learning_rate": 1.758310436080035e-08,
      "loss": 0.4288,
      "num_input_tokens_seen": 59462848,
      "step": 4740,
      "train_runtime": 50503.4524,
      "train_tokens_per_second": 1177.402
    },
    {
      "epoch": 0.9589733225545675,
      "grad_norm": 2.9660232067108154,
      "learning_rate": 1.675303333174938e-08,
      "loss": 0.5862,
      "num_input_tokens_seen": 59526928,
      "step": 4745,
      "train_runtime": 50557.4717,
      "train_tokens_per_second": 1177.411
    },
    {
      "epoch": 0.959983831851253,
      "grad_norm": 3.073794364929199,
      "learning_rate": 1.5942949725810783e-08,
      "loss": 0.5724,
      "num_input_tokens_seen": 59589616,
      "step": 4750,
      "train_runtime": 50610.6781,
      "train_tokens_per_second": 1177.412
    },
    {
      "epoch": 0.9609943411479386,
      "grad_norm": 2.800058364868164,
      "learning_rate": 1.515286170711394e-08,
      "loss": 0.4079,
      "num_input_tokens_seen": 59652880,
      "step": 4755,
      "train_runtime": 50664.2597,
      "train_tokens_per_second": 1177.415
    },
    {
      "epoch": 0.9620048504446241,
      "grad_norm": 5.470855712890625,
      "learning_rate": 1.4382777238269417e-08,
      "loss": 0.793,
      "num_input_tokens_seen": 59716256,
      "step": 4760,
      "train_runtime": 50717.9435,
      "train_tokens_per_second": 1177.419
    },
    {
      "epoch": 0.9630153597413096,
      "grad_norm": 4.207609176635742,
      "learning_rate": 1.3632704080289492e-08,
      "loss": 0.6354,
      "num_input_tokens_seen": 59778688,
      "step": 4765,
      "train_runtime": 50770.9306,
      "train_tokens_per_second": 1177.42
    },
    {
      "epoch": 0.9640258690379951,
      "grad_norm": 3.13956618309021,
      "learning_rate": 1.2902649792509768e-08,
      "loss": 0.5886,
      "num_input_tokens_seen": 59841408,
      "step": 4770,
      "train_runtime": 50824.1467,
      "train_tokens_per_second": 1177.421
    },
    {
      "epoch": 0.9650363783346807,
      "grad_norm": 4.1571946144104,
      "learning_rate": 1.2192621732513231e-08,
      "loss": 0.5513,
      "num_input_tokens_seen": 59904160,
      "step": 4775,
      "train_runtime": 50877.3703,
      "train_tokens_per_second": 1177.422
    },
    {
      "epoch": 0.9660468876313663,
      "grad_norm": 3.2583110332489014,
      "learning_rate": 1.1502627056055647e-08,
      "loss": 0.5882,
      "num_input_tokens_seen": 59966704,
      "step": 4780,
      "train_runtime": 50930.5753,
      "train_tokens_per_second": 1177.421
    },
    {
      "epoch": 0.9670573969280517,
      "grad_norm": 3.0534508228302,
      "learning_rate": 1.0832672716994285e-08,
      "loss": 0.6614,
      "num_input_tokens_seen": 60029408,
      "step": 4785,
      "train_runtime": 50983.7287,
      "train_tokens_per_second": 1177.423
    },
    {
      "epoch": 0.9680679062247373,
      "grad_norm": 4.110825061798096,
      "learning_rate": 1.0182765467216636e-08,
      "loss": 0.6626,
      "num_input_tokens_seen": 60091648,
      "step": 4790,
      "train_runtime": 51036.6495,
      "train_tokens_per_second": 1177.421
    },
    {
      "epoch": 0.9690784155214228,
      "grad_norm": 2.9038326740264893,
      "learning_rate": 9.552911856573143e-09,
      "loss": 0.5243,
      "num_input_tokens_seen": 60152672,
      "step": 4795,
      "train_runtime": 51088.7734,
      "train_tokens_per_second": 1177.415
    },
    {
      "epoch": 0.9700889248181084,
      "grad_norm": 2.597968101501465,
      "learning_rate": 8.943118232811241e-09,
      "loss": 0.6352,
      "num_input_tokens_seen": 60215952,
      "step": 4800,
      "train_runtime": 51142.2551,
      "train_tokens_per_second": 1177.421
    },
    {
      "epoch": 0.9710994341147938,
      "grad_norm": 8.525559425354004,
      "learning_rate": 8.35339074151098e-09,
      "loss": 0.6341,
      "num_input_tokens_seen": 60280384,
      "step": 4805,
      "train_runtime": 51197.4575,
      "train_tokens_per_second": 1177.41
    },
    {
      "epoch": 0.9721099434114794,
      "grad_norm": 3.5495152473449707,
      "learning_rate": 7.7837353260235e-09,
      "loss": 0.6654,
      "num_input_tokens_seen": 60341856,
      "step": 4810,
      "train_runtime": 51249.6504,
      "train_tokens_per_second": 1177.41
    },
    {
      "epoch": 0.9731204527081649,
      "grad_norm": 3.8737356662750244,
      "learning_rate": 7.234157727410428e-09,
      "loss": 0.6085,
      "num_input_tokens_seen": 60404336,
      "step": 4815,
      "train_runtime": 51302.8064,
      "train_tokens_per_second": 1177.408
    },
    {
      "epoch": 0.9741309620048505,
      "grad_norm": 3.1372792720794678,
      "learning_rate": 6.7046634843874736e-09,
      "loss": 0.6043,
      "num_input_tokens_seen": 60466496,
      "step": 4820,
      "train_runtime": 51355.5909,
      "train_tokens_per_second": 1177.408
    },
    {
      "epoch": 0.975141471301536,
      "grad_norm": 3.0267493724823,
      "learning_rate": 6.195257933266918e-09,
      "loss": 0.55,
      "num_input_tokens_seen": 60530016,
      "step": 4825,
      "train_runtime": 51409.396,
      "train_tokens_per_second": 1177.412
    },
    {
      "epoch": 0.9761519805982215,
      "grad_norm": 3.9055354595184326,
      "learning_rate": 5.705946207904544e-09,
      "loss": 0.6473,
      "num_input_tokens_seen": 60591952,
      "step": 4830,
      "train_runtime": 51461.971,
      "train_tokens_per_second": 1177.412
    },
    {
      "epoch": 0.9771624898949071,
      "grad_norm": 3.2790181636810303,
      "learning_rate": 5.236733239648128e-09,
      "loss": 0.4971,
      "num_input_tokens_seen": 60654000,
      "step": 4835,
      "train_runtime": 51514.7681,
      "train_tokens_per_second": 1177.41
    },
    {
      "epoch": 0.9781729991915926,
      "grad_norm": 3.592308759689331,
      "learning_rate": 4.787623757287696e-09,
      "loss": 0.4796,
      "num_input_tokens_seen": 60717200,
      "step": 4840,
      "train_runtime": 51568.483,
      "train_tokens_per_second": 1177.409
    },
    {
      "epoch": 0.9791835084882781,
      "grad_norm": 2.961463212966919,
      "learning_rate": 4.358622287007341e-09,
      "loss": 0.466,
      "num_input_tokens_seen": 60780496,
      "step": 4845,
      "train_runtime": 51622.108,
      "train_tokens_per_second": 1177.412
    },
    {
      "epoch": 0.9801940177849636,
      "grad_norm": 2.781067132949829,
      "learning_rate": 3.949733152340151e-09,
      "loss": 0.4167,
      "num_input_tokens_seen": 60843616,
      "step": 4850,
      "train_runtime": 51675.7107,
      "train_tokens_per_second": 1177.412
    },
    {
      "epoch": 0.9812045270816492,
      "grad_norm": 3.4371840953826904,
      "learning_rate": 3.5609604741246856e-09,
      "loss": 0.5639,
      "num_input_tokens_seen": 60905824,
      "step": 4855,
      "train_runtime": 51728.5064,
      "train_tokens_per_second": 1177.413
    },
    {
      "epoch": 0.9822150363783346,
      "grad_norm": 3.807415008544922,
      "learning_rate": 3.1923081704630094e-09,
      "loss": 0.566,
      "num_input_tokens_seen": 60967296,
      "step": 4860,
      "train_runtime": 51780.7612,
      "train_tokens_per_second": 1177.412
    },
    {
      "epoch": 0.9832255456750202,
      "grad_norm": 2.907162666320801,
      "learning_rate": 2.8437799566811695e-09,
      "loss": 0.6468,
      "num_input_tokens_seen": 61030384,
      "step": 4865,
      "train_runtime": 51834.2332,
      "train_tokens_per_second": 1177.415
    },
    {
      "epoch": 0.9842360549717057,
      "grad_norm": 3.6534454822540283,
      "learning_rate": 2.5153793452923342e-09,
      "loss": 0.5028,
      "num_input_tokens_seen": 61093712,
      "step": 4870,
      "train_runtime": 51887.8571,
      "train_tokens_per_second": 1177.418
    },
    {
      "epoch": 0.9852465642683913,
      "grad_norm": 4.529309272766113,
      "learning_rate": 2.207109645960825e-09,
      "loss": 0.5371,
      "num_input_tokens_seen": 61156480,
      "step": 4875,
      "train_runtime": 51941.1329,
      "train_tokens_per_second": 1177.419
    },
    {
      "epoch": 0.9862570735650767,
      "grad_norm": 3.7930004596710205,
      "learning_rate": 1.9189739654694724e-09,
      "loss": 0.7868,
      "num_input_tokens_seen": 61218976,
      "step": 4880,
      "train_runtime": 51994.1962,
      "train_tokens_per_second": 1177.419
    },
    {
      "epoch": 0.9872675828617623,
      "grad_norm": 3.5409374237060547,
      "learning_rate": 1.6509752076871997e-09,
      "loss": 0.5457,
      "num_input_tokens_seen": 61280896,
      "step": 4885,
      "train_runtime": 52046.8788,
      "train_tokens_per_second": 1177.417
    },
    {
      "epoch": 0.9882780921584479,
      "grad_norm": 4.232783794403076,
      "learning_rate": 1.4031160735406e-09,
      "loss": 0.7048,
      "num_input_tokens_seen": 61342592,
      "step": 4890,
      "train_runtime": 52099.3107,
      "train_tokens_per_second": 1177.417
    },
    {
      "epoch": 0.9892886014551334,
      "grad_norm": 3.761225938796997,
      "learning_rate": 1.175399060986626e-09,
      "loss": 0.4432,
      "num_input_tokens_seen": 61404912,
      "step": 4895,
      "train_runtime": 52152.2573,
      "train_tokens_per_second": 1177.416
    },
    {
      "epoch": 0.9902991107518189,
      "grad_norm": 3.668656826019287,
      "learning_rate": 9.678264649870538e-10,
      "loss": 0.7731,
      "num_input_tokens_seen": 61468000,
      "step": 4900,
      "train_runtime": 52205.9224,
      "train_tokens_per_second": 1177.414
    },
    {
      "epoch": 0.9913096200485044,
      "grad_norm": 2.943161964416504,
      "learning_rate": 7.804003774860568e-10,
      "loss": 0.5002,
      "num_input_tokens_seen": 61531696,
      "step": 4905,
      "train_runtime": 52260.6625,
      "train_tokens_per_second": 1177.4
    },
    {
      "epoch": 0.99232012934519,
      "grad_norm": 4.782854080200195,
      "learning_rate": 6.131226873882234e-10,
      "loss": 0.9167,
      "num_input_tokens_seen": 61593408,
      "step": 4910,
      "train_runtime": 52313.1011,
      "train_tokens_per_second": 1177.399
    },
    {
      "epoch": 0.9933306386418755,
      "grad_norm": 3.095829725265503,
      "learning_rate": 4.659950805396829e-10,
      "loss": 0.5472,
      "num_input_tokens_seen": 61657104,
      "step": 4915,
      "train_runtime": 52367.1327,
      "train_tokens_per_second": 1177.401
    },
    {
      "epoch": 0.994341147938561,
      "grad_norm": 3.4060921669006348,
      "learning_rate": 3.390190397123405e-10,
      "loss": 0.6805,
      "num_input_tokens_seen": 61718784,
      "step": 4920,
      "train_runtime": 52419.4972,
      "train_tokens_per_second": 1177.401
    },
    {
      "epoch": 0.9953516572352465,
      "grad_norm": 3.472031831741333,
      "learning_rate": 2.321958445872241e-10,
      "loss": 0.682,
      "num_input_tokens_seen": 61782144,
      "step": 4925,
      "train_runtime": 52473.1005,
      "train_tokens_per_second": 1177.406
    },
    {
      "epoch": 0.9963621665319321,
      "grad_norm": 4.349644660949707,
      "learning_rate": 1.4552657174204952e-10,
      "loss": 0.7508,
      "num_input_tokens_seen": 61845104,
      "step": 4930,
      "train_runtime": 52526.5486,
      "train_tokens_per_second": 1177.407
    },
    {
      "epoch": 0.9973726758286177,
      "grad_norm": 3.331804037094116,
      "learning_rate": 7.90120946416728e-11,
      "loss": 0.5257,
      "num_input_tokens_seen": 61906128,
      "step": 4935,
      "train_runtime": 52578.6193,
      "train_tokens_per_second": 1177.401
    },
    {
      "epoch": 0.9983831851253031,
      "grad_norm": 4.204101085662842,
      "learning_rate": 3.2653083627653955e-11,
      "loss": 0.5711,
      "num_input_tokens_seen": 61967440,
      "step": 4940,
      "train_runtime": 52630.7443,
      "train_tokens_per_second": 1177.4
    },
    {
      "epoch": 0.9993936944219887,
      "grad_norm": 4.370904445648193,
      "learning_rate": 6.450005911817768e-12,
      "loss": 0.6072,
      "num_input_tokens_seen": 62030848,
      "step": 4945,
      "train_runtime": 52684.4308,
      "train_tokens_per_second": 1177.404
    },
    {
      "epoch": 1.0,
      "num_input_tokens_seen": 62067712,
      "step": 4948,
      "total_flos": 2.400015299861545e+17,
      "train_loss": 0.6720821277066017,
      "train_runtime": 52716.5477,
      "train_samples_per_second": 1.502,
      "train_steps_per_second": 0.094
    }
  ],
  "logging_steps": 5,
  "max_steps": 4948,
  "num_input_tokens_seen": 62067712,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.400015299861545e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
