[INFO|2025-10-21 17:17:07] tokenization_utils_base.py:2093 >> loading file vocab.json
[INFO|2025-10-21 17:17:07] tokenization_utils_base.py:2093 >> loading file merges.txt
[INFO|2025-10-21 17:17:07] tokenization_utils_base.py:2093 >> loading file tokenizer.json
[INFO|2025-10-21 17:17:07] tokenization_utils_base.py:2093 >> loading file added_tokens.json
[INFO|2025-10-21 17:17:07] tokenization_utils_base.py:2093 >> loading file special_tokens_map.json
[INFO|2025-10-21 17:17:07] tokenization_utils_base.py:2093 >> loading file tokenizer_config.json
[INFO|2025-10-21 17:17:07] tokenization_utils_base.py:2093 >> loading file chat_template.jinja
[INFO|2025-10-21 17:17:07] tokenization_utils_base.py:2364 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-10-21 17:17:07] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 17:17:07] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 17:17:07] tokenization_utils_base.py:2093 >> loading file vocab.json
[INFO|2025-10-21 17:17:07] tokenization_utils_base.py:2093 >> loading file merges.txt
[INFO|2025-10-21 17:17:07] tokenization_utils_base.py:2093 >> loading file tokenizer.json
[INFO|2025-10-21 17:17:07] tokenization_utils_base.py:2093 >> loading file added_tokens.json
[INFO|2025-10-21 17:17:07] tokenization_utils_base.py:2093 >> loading file special_tokens_map.json
[INFO|2025-10-21 17:17:07] tokenization_utils_base.py:2093 >> loading file tokenizer_config.json
[INFO|2025-10-21 17:17:07] tokenization_utils_base.py:2093 >> loading file chat_template.jinja
[INFO|2025-10-21 17:17:08] tokenization_utils_base.py:2364 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-10-21 17:17:08] logging.py:143 >> Loading dataset /media/data/users/liqz/llama_factory/LLaMA-Factory/data/train_top5_alpaca.jsonl...
[INFO|2025-10-21 17:17:09] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 17:17:09] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 17:17:09] logging.py:143 >> Loading 4-bit AWQ-quantized model.
[INFO|2025-10-21 17:17:09] logging.py:143 >> KV cache is disabled during training.
[WARNING|2025-10-21 17:17:09] logging.py:328 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|2025-10-21 17:17:09] auto.py:242 >> 
[WARNING|2025-10-21 17:17:09] quantizer_awq.py:102 >> `torch.bfloat16` is not supported for AWQ CUDA/XPU kernels yet. Casting to `torch.float16`.
[INFO|2025-10-21 17:17:09] modeling_utils.py:1169 >> loading weights file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/model.safetensors.index.json
[INFO|2025-10-21 17:17:09] modeling_utils.py:2341 >> Instantiating Qwen3ForCausalLM model under default dtype torch.float16.
[INFO|2025-10-21 17:17:09] configuration_utils.py:986 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

[INFO|2025-10-21 17:17:36] configuration_utils.py:939 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/generation_config.json
[INFO|2025-10-21 17:17:36] configuration_utils.py:986 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

[INFO|2025-10-21 17:17:36] dynamic_module_utils.py:423 >> Could not locate the custom_generate/generate.py inside /media/data/users/liqz/Qwen/Qwen3-8B-AWQ.
[INFO|2025-10-21 17:17:36] logging.py:143 >> Gradient checkpointing enabled.
[INFO|2025-10-21 17:17:36] logging.py:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-10-21 17:17:36] logging.py:143 >> Upcasting trainable params to float32.
[INFO|2025-10-21 17:17:36] logging.py:143 >> Fine-tuning method: LoRA
[INFO|2025-10-21 17:17:36] logging.py:143 >> Found linear modules: gate_proj,o_proj,k_proj,v_proj,up_proj,down_proj,q_proj
[INFO|2025-10-21 17:17:37] logging.py:143 >> trainable params: 21,823,488 || all params: 1,266,791,424 || trainable%: 1.7227
[WARNING|2025-10-21 17:17:37] trainer.py:906 >> The model is already on multiple devices. Skipping the move to device specified in `args`.
[INFO|2025-10-21 17:17:37] trainer.py:749 >> Using auto half precision backend
[WARNING|2025-10-21 17:17:37] trainer.py:982 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|2025-10-21 17:17:37] trainer.py:2519 >> ***** Running training *****
[INFO|2025-10-21 17:17:37] trainer.py:2520 >>   Num examples = 79,168
[INFO|2025-10-21 17:17:37] trainer.py:2521 >>   Num Epochs = 1
[INFO|2025-10-21 17:17:37] trainer.py:2522 >>   Instantaneous batch size per device = 2
[INFO|2025-10-21 17:17:37] trainer.py:2525 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|2025-10-21 17:17:37] trainer.py:2526 >>   Gradient Accumulation steps = 8
[INFO|2025-10-21 17:17:37] trainer.py:2527 >>   Total optimization steps = 4,948
[INFO|2025-10-21 17:17:37] trainer.py:2528 >>   Number of trainable parameters = 21,823,488
[INFO|2025-10-21 17:18:30] logging.py:143 >> {'loss': 8.0922, 'learning_rate': 4.0000e-06, 'epoch': 0.00, 'throughput': 1162.86}
[INFO|2025-10-21 17:19:24] logging.py:143 >> {'loss': 7.0781, 'learning_rate': 4.0000e-06, 'epoch': 0.00, 'throughput': 1170.41}
[INFO|2025-10-21 17:20:17] logging.py:143 >> {'loss': 7.1881, 'learning_rate': 3.9999e-06, 'epoch': 0.00, 'throughput': 1173.31}
[INFO|2025-10-21 17:21:10] logging.py:143 >> {'loss': 6.4948, 'learning_rate': 3.9999e-06, 'epoch': 0.00, 'throughput': 1174.03}
[INFO|2025-10-21 17:22:04] logging.py:143 >> {'loss': 6.1312, 'learning_rate': 3.9998e-06, 'epoch': 0.01, 'throughput': 1175.16}
[INFO|2025-10-21 17:22:57] logging.py:143 >> {'loss': 5.4979, 'learning_rate': 3.9997e-06, 'epoch': 0.01, 'throughput': 1175.44}
[INFO|2025-10-21 17:23:50] logging.py:143 >> {'loss': 5.2443, 'learning_rate': 3.9995e-06, 'epoch': 0.01, 'throughput': 1175.62}
[INFO|2025-10-21 17:24:43] logging.py:143 >> {'loss': 4.3395, 'learning_rate': 3.9994e-06, 'epoch': 0.01, 'throughput': 1175.87}
[INFO|2025-10-21 17:25:36] logging.py:143 >> {'loss': 4.0471, 'learning_rate': 3.9992e-06, 'epoch': 0.01, 'throughput': 1175.94}
[INFO|2025-10-21 17:26:30] logging.py:143 >> {'loss': 3.4529, 'learning_rate': 3.9990e-06, 'epoch': 0.01, 'throughput': 1176.16}
[INFO|2025-10-21 17:27:23] logging.py:143 >> {'loss': 3.3163, 'learning_rate': 3.9988e-06, 'epoch': 0.01, 'throughput': 1176.44}
[INFO|2025-10-21 17:28:17] logging.py:143 >> {'loss': 2.9471, 'learning_rate': 3.9986e-06, 'epoch': 0.01, 'throughput': 1176.41}
[INFO|2025-10-21 17:29:10] logging.py:143 >> {'loss': 2.4595, 'learning_rate': 3.9983e-06, 'epoch': 0.01, 'throughput': 1176.72}
[INFO|2025-10-21 17:30:03] logging.py:143 >> {'loss': 2.3361, 'learning_rate': 3.9981e-06, 'epoch': 0.01, 'throughput': 1176.61}
[INFO|2025-10-21 17:30:57] logging.py:143 >> {'loss': 2.0035, 'learning_rate': 3.9978e-06, 'epoch': 0.02, 'throughput': 1176.91}
[INFO|2025-10-21 17:31:50] logging.py:143 >> {'loss': 2.1344, 'learning_rate': 3.9975e-06, 'epoch': 0.02, 'throughput': 1176.88}
[INFO|2025-10-21 17:32:43] logging.py:143 >> {'loss': 1.7256, 'learning_rate': 3.9972e-06, 'epoch': 0.02, 'throughput': 1176.84}
[INFO|2025-10-21 17:33:37] logging.py:143 >> {'loss': 1.5091, 'learning_rate': 3.9968e-06, 'epoch': 0.02, 'throughput': 1176.87}
[INFO|2025-10-21 17:34:30] logging.py:143 >> {'loss': 1.2946, 'learning_rate': 3.9964e-06, 'epoch': 0.02, 'throughput': 1176.84}
[INFO|2025-10-21 17:35:24] logging.py:143 >> {'loss': 1.2065, 'learning_rate': 3.9961e-06, 'epoch': 0.02, 'throughput': 1176.86}
[INFO|2025-10-21 17:35:24] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-100
[INFO|2025-10-21 17:35:24] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 17:35:24] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 17:35:24] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-100/chat_template.jinja
[INFO|2025-10-21 17:35:24] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-100/tokenizer_config.json
[INFO|2025-10-21 17:35:24] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-100/special_tokens_map.json
[INFO|2025-10-21 17:36:18] logging.py:143 >> {'loss': 1.2060, 'learning_rate': 3.9956e-06, 'epoch': 0.02, 'throughput': 1176.31}
[INFO|2025-10-21 17:37:11] logging.py:143 >> {'loss': 1.1270, 'learning_rate': 3.9952e-06, 'epoch': 0.02, 'throughput': 1176.35}
[INFO|2025-10-21 17:38:04] logging.py:143 >> {'loss': 1.0300, 'learning_rate': 3.9948e-06, 'epoch': 0.02, 'throughput': 1176.51}
[INFO|2025-10-21 17:38:57] logging.py:143 >> {'loss': 1.0690, 'learning_rate': 3.9943e-06, 'epoch': 0.02, 'throughput': 1176.46}
[INFO|2025-10-21 17:39:51] logging.py:143 >> {'loss': 0.8232, 'learning_rate': 3.9938e-06, 'epoch': 0.03, 'throughput': 1176.28}
[INFO|2025-10-21 17:40:43] logging.py:143 >> {'loss': 1.0445, 'learning_rate': 3.9933e-06, 'epoch': 0.03, 'throughput': 1176.21}
[INFO|2025-10-21 17:41:36] logging.py:143 >> {'loss': 0.9583, 'learning_rate': 3.9928e-06, 'epoch': 0.03, 'throughput': 1176.29}
[INFO|2025-10-21 17:42:29] logging.py:143 >> {'loss': 0.7914, 'learning_rate': 3.9922e-06, 'epoch': 0.03, 'throughput': 1176.19}
[INFO|2025-10-21 17:43:23] logging.py:143 >> {'loss': 0.7615, 'learning_rate': 3.9916e-06, 'epoch': 0.03, 'throughput': 1176.24}
[INFO|2025-10-21 17:44:16] logging.py:143 >> {'loss': 0.8504, 'learning_rate': 3.9911e-06, 'epoch': 0.03, 'throughput': 1176.22}
[INFO|2025-10-21 17:45:09] logging.py:143 >> {'loss': 0.9230, 'learning_rate': 3.9904e-06, 'epoch': 0.03, 'throughput': 1176.16}
[INFO|2025-10-21 17:46:02] logging.py:143 >> {'loss': 0.6843, 'learning_rate': 3.9898e-06, 'epoch': 0.03, 'throughput': 1176.20}
[INFO|2025-10-21 17:46:55] logging.py:143 >> {'loss': 0.6831, 'learning_rate': 3.9892e-06, 'epoch': 0.03, 'throughput': 1176.24}
[INFO|2025-10-21 17:47:49] logging.py:143 >> {'loss': 1.0845, 'learning_rate': 3.9885e-06, 'epoch': 0.03, 'throughput': 1176.28}
[INFO|2025-10-21 17:48:42] logging.py:143 >> {'loss': 0.7050, 'learning_rate': 3.9878e-06, 'epoch': 0.04, 'throughput': 1176.25}
[INFO|2025-10-21 17:49:36] logging.py:143 >> {'loss': 0.7109, 'learning_rate': 3.9871e-06, 'epoch': 0.04, 'throughput': 1176.36}
[INFO|2025-10-21 17:50:30] logging.py:143 >> {'loss': 0.6746, 'learning_rate': 3.9864e-06, 'epoch': 0.04, 'throughput': 1176.34}
[INFO|2025-10-21 17:51:23] logging.py:143 >> {'loss': 0.7602, 'learning_rate': 3.9856e-06, 'epoch': 0.04, 'throughput': 1176.30}
[INFO|2025-10-21 17:52:16] logging.py:143 >> {'loss': 0.7632, 'learning_rate': 3.9848e-06, 'epoch': 0.04, 'throughput': 1176.28}
[INFO|2025-10-21 17:53:09] logging.py:143 >> {'loss': 0.6204, 'learning_rate': 3.9841e-06, 'epoch': 0.04, 'throughput': 1176.32}
[INFO|2025-10-21 17:53:09] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-200
[INFO|2025-10-21 17:53:09] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 17:53:09] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 17:53:09] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-200/chat_template.jinja
[INFO|2025-10-21 17:53:09] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-200/tokenizer_config.json
[INFO|2025-10-21 17:53:09] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-200/special_tokens_map.json
[INFO|2025-10-21 17:54:04] logging.py:143 >> {'loss': 0.7369, 'learning_rate': 3.9832e-06, 'epoch': 0.04, 'throughput': 1176.00}
[INFO|2025-10-21 17:54:57] logging.py:143 >> {'loss': 0.8126, 'learning_rate': 3.9824e-06, 'epoch': 0.04, 'throughput': 1175.94}
[INFO|2025-10-21 17:55:50] logging.py:143 >> {'loss': 0.7262, 'learning_rate': 3.9816e-06, 'epoch': 0.04, 'throughput': 1175.94}
[INFO|2025-10-21 17:56:44] logging.py:143 >> {'loss': 0.6968, 'learning_rate': 3.9807e-06, 'epoch': 0.04, 'throughput': 1175.92}
[INFO|2025-10-21 17:57:37] logging.py:143 >> {'loss': 0.7374, 'learning_rate': 3.9798e-06, 'epoch': 0.05, 'throughput': 1175.90}
[INFO|2025-10-21 17:58:30] logging.py:143 >> {'loss': 0.7831, 'learning_rate': 3.9789e-06, 'epoch': 0.05, 'throughput': 1175.94}
[INFO|2025-10-21 17:59:24] logging.py:143 >> {'loss': 0.5801, 'learning_rate': 3.9780e-06, 'epoch': 0.05, 'throughput': 1175.99}
[INFO|2025-10-21 18:00:17] logging.py:143 >> {'loss': 0.5858, 'learning_rate': 3.9770e-06, 'epoch': 0.05, 'throughput': 1175.94}
[INFO|2025-10-21 18:01:11] logging.py:143 >> {'loss': 0.6123, 'learning_rate': 3.9760e-06, 'epoch': 0.05, 'throughput': 1175.97}
[INFO|2025-10-21 18:02:04] logging.py:143 >> {'loss': 0.6494, 'learning_rate': 3.9751e-06, 'epoch': 0.05, 'throughput': 1175.99}
[INFO|2025-10-21 18:02:57] logging.py:143 >> {'loss': 0.8759, 'learning_rate': 3.9740e-06, 'epoch': 0.05, 'throughput': 1175.97}
[INFO|2025-10-21 18:03:50] logging.py:143 >> {'loss': 0.6397, 'learning_rate': 3.9730e-06, 'epoch': 0.05, 'throughput': 1175.96}
[INFO|2025-10-21 18:04:43] logging.py:143 >> {'loss': 0.5310, 'learning_rate': 3.9720e-06, 'epoch': 0.05, 'throughput': 1175.98}
[INFO|2025-10-21 18:05:36] logging.py:143 >> {'loss': 0.7880, 'learning_rate': 3.9709e-06, 'epoch': 0.05, 'throughput': 1175.97}
[INFO|2025-10-21 18:06:29] logging.py:143 >> {'loss': 0.6924, 'learning_rate': 3.9698e-06, 'epoch': 0.06, 'throughput': 1176.03}
[INFO|2025-10-21 18:07:22] logging.py:143 >> {'loss': 0.7580, 'learning_rate': 3.9687e-06, 'epoch': 0.06, 'throughput': 1176.06}
[INFO|2025-10-21 18:08:15] logging.py:143 >> {'loss': 0.7342, 'learning_rate': 3.9676e-06, 'epoch': 0.06, 'throughput': 1176.10}
[INFO|2025-10-21 18:09:09] logging.py:143 >> {'loss': 0.7686, 'learning_rate': 3.9664e-06, 'epoch': 0.06, 'throughput': 1176.20}
[INFO|2025-10-21 18:10:02] logging.py:143 >> {'loss': 0.6960, 'learning_rate': 3.9653e-06, 'epoch': 0.06, 'throughput': 1176.17}
[INFO|2025-10-21 18:10:55] logging.py:143 >> {'loss': 0.7620, 'learning_rate': 3.9641e-06, 'epoch': 0.06, 'throughput': 1176.13}
[INFO|2025-10-21 18:10:55] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-300
[INFO|2025-10-21 18:10:55] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 18:10:55] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 18:10:56] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-300/chat_template.jinja
[INFO|2025-10-21 18:10:56] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-300/tokenizer_config.json
[INFO|2025-10-21 18:10:56] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-300/special_tokens_map.json
[INFO|2025-10-21 18:11:49] logging.py:143 >> {'loss': 0.6355, 'learning_rate': 3.9629e-06, 'epoch': 0.06, 'throughput': 1175.78}
[INFO|2025-10-21 18:12:42] logging.py:143 >> {'loss': 0.6717, 'learning_rate': 3.9616e-06, 'epoch': 0.06, 'throughput': 1175.76}
[INFO|2025-10-21 18:13:35] logging.py:143 >> {'loss': 0.7500, 'learning_rate': 3.9604e-06, 'epoch': 0.06, 'throughput': 1175.78}
[INFO|2025-10-21 18:14:28] logging.py:143 >> {'loss': 0.6298, 'learning_rate': 3.9591e-06, 'epoch': 0.06, 'throughput': 1175.80}
[INFO|2025-10-21 18:15:21] logging.py:143 >> {'loss': 0.6946, 'learning_rate': 3.9578e-06, 'epoch': 0.07, 'throughput': 1175.74}
[INFO|2025-10-21 18:16:14] logging.py:143 >> {'loss': 0.8503, 'learning_rate': 3.9565e-06, 'epoch': 0.07, 'throughput': 1175.72}
[INFO|2025-10-21 18:17:07] logging.py:143 >> {'loss': 0.7564, 'learning_rate': 3.9552e-06, 'epoch': 0.07, 'throughput': 1175.75}
[INFO|2025-10-21 18:18:01] logging.py:143 >> {'loss': 0.6947, 'learning_rate': 3.9539e-06, 'epoch': 0.07, 'throughput': 1175.75}
[INFO|2025-10-21 18:18:54] logging.py:143 >> {'loss': 0.7288, 'learning_rate': 3.9525e-06, 'epoch': 0.07, 'throughput': 1175.70}
[INFO|2025-10-21 18:19:48] logging.py:143 >> {'loss': 0.5889, 'learning_rate': 3.9511e-06, 'epoch': 0.07, 'throughput': 1175.71}
[INFO|2025-10-21 18:20:41] logging.py:143 >> {'loss': 0.6522, 'learning_rate': 3.9497e-06, 'epoch': 0.07, 'throughput': 1175.69}
[INFO|2025-10-21 18:21:35] logging.py:143 >> {'loss': 0.7199, 'learning_rate': 3.9483e-06, 'epoch': 0.07, 'throughput': 1175.68}
[INFO|2025-10-21 18:22:28] logging.py:143 >> {'loss': 0.4661, 'learning_rate': 3.9468e-06, 'epoch': 0.07, 'throughput': 1175.68}
[INFO|2025-10-21 18:23:21] logging.py:143 >> {'loss': 0.7628, 'learning_rate': 3.9454e-06, 'epoch': 0.07, 'throughput': 1175.62}
[INFO|2025-10-21 18:24:15] logging.py:143 >> {'loss': 0.6188, 'learning_rate': 3.9439e-06, 'epoch': 0.08, 'throughput': 1175.66}
[INFO|2025-10-21 18:25:08] logging.py:143 >> {'loss': 0.6788, 'learning_rate': 3.9424e-06, 'epoch': 0.08, 'throughput': 1175.68}
[INFO|2025-10-21 18:26:01] logging.py:143 >> {'loss': 0.7056, 'learning_rate': 3.9409e-06, 'epoch': 0.08, 'throughput': 1175.66}
[INFO|2025-10-21 18:26:54] logging.py:143 >> {'loss': 0.7405, 'learning_rate': 3.9393e-06, 'epoch': 0.08, 'throughput': 1175.70}
[INFO|2025-10-21 18:27:47] logging.py:143 >> {'loss': 0.6928, 'learning_rate': 3.9377e-06, 'epoch': 0.08, 'throughput': 1175.67}
[INFO|2025-10-21 18:28:41] logging.py:143 >> {'loss': 0.6887, 'learning_rate': 3.9362e-06, 'epoch': 0.08, 'throughput': 1175.71}
[INFO|2025-10-21 18:28:41] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-400
[INFO|2025-10-21 18:28:41] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 18:28:41] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 18:28:41] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-400/chat_template.jinja
[INFO|2025-10-21 18:28:41] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-400/tokenizer_config.json
[INFO|2025-10-21 18:28:41] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-400/special_tokens_map.json
[INFO|2025-10-21 18:29:35] logging.py:143 >> {'loss': 0.5829, 'learning_rate': 3.9346e-06, 'epoch': 0.08, 'throughput': 1175.52}
[INFO|2025-10-21 18:30:29] logging.py:143 >> {'loss': 0.5491, 'learning_rate': 3.9329e-06, 'epoch': 0.08, 'throughput': 1175.49}
[INFO|2025-10-21 18:31:22] logging.py:143 >> {'loss': 0.8809, 'learning_rate': 3.9313e-06, 'epoch': 0.08, 'throughput': 1175.50}
[INFO|2025-10-21 18:32:15] logging.py:143 >> {'loss': 0.8860, 'learning_rate': 3.9296e-06, 'epoch': 0.08, 'throughput': 1175.50}
[INFO|2025-10-21 18:33:08] logging.py:143 >> {'loss': 0.6576, 'learning_rate': 3.9280e-06, 'epoch': 0.09, 'throughput': 1175.47}
[INFO|2025-10-21 18:34:01] logging.py:143 >> {'loss': 0.6753, 'learning_rate': 3.9263e-06, 'epoch': 0.09, 'throughput': 1175.46}
[INFO|2025-10-21 18:34:54] logging.py:143 >> {'loss': 0.6905, 'learning_rate': 3.9245e-06, 'epoch': 0.09, 'throughput': 1175.43}
[INFO|2025-10-21 18:35:48] logging.py:143 >> {'loss': 0.6303, 'learning_rate': 3.9228e-06, 'epoch': 0.09, 'throughput': 1175.45}
[INFO|2025-10-21 18:36:42] logging.py:143 >> {'loss': 0.6763, 'learning_rate': 3.9211e-06, 'epoch': 0.09, 'throughput': 1175.51}
[INFO|2025-10-21 18:37:35] logging.py:143 >> {'loss': 0.6665, 'learning_rate': 3.9193e-06, 'epoch': 0.09, 'throughput': 1175.48}
[INFO|2025-10-21 18:38:28] logging.py:143 >> {'loss': 0.5560, 'learning_rate': 3.9175e-06, 'epoch': 0.09, 'throughput': 1175.45}
[INFO|2025-10-21 18:39:22] logging.py:143 >> {'loss': 0.6183, 'learning_rate': 3.9157e-06, 'epoch': 0.09, 'throughput': 1175.49}
[INFO|2025-10-21 18:40:15] logging.py:143 >> {'loss': 0.7662, 'learning_rate': 3.9138e-06, 'epoch': 0.09, 'throughput': 1175.46}
[INFO|2025-10-21 18:41:08] logging.py:143 >> {'loss': 0.7666, 'learning_rate': 3.9120e-06, 'epoch': 0.09, 'throughput': 1175.48}
[INFO|2025-10-21 18:42:01] logging.py:143 >> {'loss': 0.6372, 'learning_rate': 3.9101e-06, 'epoch': 0.10, 'throughput': 1175.49}
[INFO|2025-10-21 18:42:55] logging.py:143 >> {'loss': 0.6836, 'learning_rate': 3.9082e-06, 'epoch': 0.10, 'throughput': 1175.51}
[INFO|2025-10-21 18:43:49] logging.py:143 >> {'loss': 0.5558, 'learning_rate': 3.9063e-06, 'epoch': 0.10, 'throughput': 1175.51}
[INFO|2025-10-21 18:44:42] logging.py:143 >> {'loss': 0.6917, 'learning_rate': 3.9044e-06, 'epoch': 0.10, 'throughput': 1175.55}
[INFO|2025-10-21 18:45:35] logging.py:143 >> {'loss': 0.7244, 'learning_rate': 3.9024e-06, 'epoch': 0.10, 'throughput': 1175.55}
[INFO|2025-10-21 18:46:28] logging.py:143 >> {'loss': 0.6920, 'learning_rate': 3.9005e-06, 'epoch': 0.10, 'throughput': 1175.55}
[INFO|2025-10-21 18:46:28] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-500
[INFO|2025-10-21 18:46:28] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 18:46:28] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 18:46:29] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-500/chat_template.jinja
[INFO|2025-10-21 18:46:29] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-500/tokenizer_config.json
[INFO|2025-10-21 18:46:29] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-500/special_tokens_map.json
[INFO|2025-10-21 18:47:23] logging.py:143 >> {'loss': 0.6409, 'learning_rate': 3.8985e-06, 'epoch': 0.10, 'throughput': 1175.40}
[INFO|2025-10-21 18:48:16] logging.py:143 >> {'loss': 0.5669, 'learning_rate': 3.8965e-06, 'epoch': 0.10, 'throughput': 1175.36}
[INFO|2025-10-21 18:49:09] logging.py:143 >> {'loss': 0.9010, 'learning_rate': 3.8944e-06, 'epoch': 0.10, 'throughput': 1175.33}
[INFO|2025-10-21 18:50:03] logging.py:143 >> {'loss': 0.5989, 'learning_rate': 3.8924e-06, 'epoch': 0.11, 'throughput': 1175.33}
[INFO|2025-10-21 18:50:55] logging.py:143 >> {'loss': 0.7055, 'learning_rate': 3.8903e-06, 'epoch': 0.11, 'throughput': 1175.25}
[INFO|2025-10-21 18:51:49] logging.py:143 >> {'loss': 0.6084, 'learning_rate': 3.8882e-06, 'epoch': 0.11, 'throughput': 1175.30}
[INFO|2025-10-21 18:52:42] logging.py:143 >> {'loss': 0.7044, 'learning_rate': 3.8861e-06, 'epoch': 0.11, 'throughput': 1175.32}
[INFO|2025-10-21 18:53:36] logging.py:143 >> {'loss': 0.6145, 'learning_rate': 3.8840e-06, 'epoch': 0.11, 'throughput': 1175.35}
[INFO|2025-10-21 18:54:30] logging.py:143 >> {'loss': 0.5508, 'learning_rate': 3.8819e-06, 'epoch': 0.11, 'throughput': 1175.39}
[INFO|2025-10-21 18:55:24] logging.py:143 >> {'loss': 0.5848, 'learning_rate': 3.8797e-06, 'epoch': 0.11, 'throughput': 1175.47}
[INFO|2025-10-21 18:56:17] logging.py:143 >> {'loss': 0.6638, 'learning_rate': 3.8775e-06, 'epoch': 0.11, 'throughput': 1175.45}
[INFO|2025-10-21 18:57:11] logging.py:143 >> {'loss': 0.6280, 'learning_rate': 3.8753e-06, 'epoch': 0.11, 'throughput': 1175.49}
[INFO|2025-10-21 18:58:04] logging.py:143 >> {'loss': 0.6623, 'learning_rate': 3.8731e-06, 'epoch': 0.11, 'throughput': 1175.46}
[INFO|2025-10-21 18:58:58] logging.py:143 >> {'loss': 0.6995, 'learning_rate': 3.8709e-06, 'epoch': 0.12, 'throughput': 1175.46}
[INFO|2025-10-21 18:59:52] logging.py:143 >> {'loss': 0.6548, 'learning_rate': 3.8686e-06, 'epoch': 0.12, 'throughput': 1175.47}
[INFO|2025-10-21 19:00:45] logging.py:143 >> {'loss': 0.5756, 'learning_rate': 3.8664e-06, 'epoch': 0.12, 'throughput': 1175.49}
[INFO|2025-10-21 19:01:39] logging.py:143 >> {'loss': 0.6073, 'learning_rate': 3.8641e-06, 'epoch': 0.12, 'throughput': 1175.51}
[INFO|2025-10-21 19:02:33] logging.py:143 >> {'loss': 0.6611, 'learning_rate': 3.8618e-06, 'epoch': 0.12, 'throughput': 1175.53}
[INFO|2025-10-21 19:03:26] logging.py:143 >> {'loss': 0.6208, 'learning_rate': 3.8594e-06, 'epoch': 0.12, 'throughput': 1175.55}
[INFO|2025-10-21 19:04:19] logging.py:143 >> {'loss': 0.5971, 'learning_rate': 3.8571e-06, 'epoch': 0.12, 'throughput': 1175.50}
[INFO|2025-10-21 19:04:19] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-600
[INFO|2025-10-21 19:04:19] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 19:04:19] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 19:04:19] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-600/chat_template.jinja
[INFO|2025-10-21 19:04:19] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-600/tokenizer_config.json
[INFO|2025-10-21 19:04:19] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-600/special_tokens_map.json
[INFO|2025-10-21 19:05:13] logging.py:143 >> {'loss': 0.7342, 'learning_rate': 3.8547e-06, 'epoch': 0.12, 'throughput': 1175.37}
[INFO|2025-10-21 19:06:06] logging.py:143 >> {'loss': 0.5571, 'learning_rate': 3.8523e-06, 'epoch': 0.12, 'throughput': 1175.39}
[INFO|2025-10-21 19:06:59] logging.py:143 >> {'loss': 0.7672, 'learning_rate': 3.8499e-06, 'epoch': 0.12, 'throughput': 1175.37}
[INFO|2025-10-21 19:07:53] logging.py:143 >> {'loss': 0.6542, 'learning_rate': 3.8475e-06, 'epoch': 0.13, 'throughput': 1175.33}
[INFO|2025-10-21 19:08:46] logging.py:143 >> {'loss': 0.5159, 'learning_rate': 3.8451e-06, 'epoch': 0.13, 'throughput': 1175.33}
[INFO|2025-10-21 19:09:40] logging.py:143 >> {'loss': 0.5643, 'learning_rate': 3.8426e-06, 'epoch': 0.13, 'throughput': 1175.33}
[INFO|2025-10-21 19:10:34] logging.py:143 >> {'loss': 0.6140, 'learning_rate': 3.8401e-06, 'epoch': 0.13, 'throughput': 1175.35}
[INFO|2025-10-21 19:11:28] logging.py:143 >> {'loss': 0.7284, 'learning_rate': 3.8376e-06, 'epoch': 0.13, 'throughput': 1175.37}
[INFO|2025-10-21 19:12:21] logging.py:143 >> {'loss': 0.6209, 'learning_rate': 3.8351e-06, 'epoch': 0.13, 'throughput': 1175.38}
[INFO|2025-10-21 19:13:14] logging.py:143 >> {'loss': 0.7537, 'learning_rate': 3.8326e-06, 'epoch': 0.13, 'throughput': 1175.42}
[INFO|2025-10-21 19:14:08] logging.py:143 >> {'loss': 0.6640, 'learning_rate': 3.8300e-06, 'epoch': 0.13, 'throughput': 1175.42}
[INFO|2025-10-21 19:15:01] logging.py:143 >> {'loss': 0.6216, 'learning_rate': 3.8275e-06, 'epoch': 0.13, 'throughput': 1175.42}
[INFO|2025-10-21 19:15:54] logging.py:143 >> {'loss': 0.7498, 'learning_rate': 3.8249e-06, 'epoch': 0.13, 'throughput': 1175.41}
[INFO|2025-10-21 19:16:47] logging.py:143 >> {'loss': 0.6286, 'learning_rate': 3.8223e-06, 'epoch': 0.14, 'throughput': 1175.44}
[INFO|2025-10-21 19:17:41] logging.py:143 >> {'loss': 0.5110, 'learning_rate': 3.8196e-06, 'epoch': 0.14, 'throughput': 1175.47}
[INFO|2025-10-21 19:18:33] logging.py:143 >> {'loss': 0.4942, 'learning_rate': 3.8170e-06, 'epoch': 0.14, 'throughput': 1175.46}
[INFO|2025-10-21 19:19:26] logging.py:143 >> {'loss': 0.7301, 'learning_rate': 3.8143e-06, 'epoch': 0.14, 'throughput': 1175.43}
[INFO|2025-10-21 19:20:18] logging.py:143 >> {'loss': 0.6492, 'learning_rate': 3.8117e-06, 'epoch': 0.14, 'throughput': 1175.40}
[INFO|2025-10-21 19:21:12] logging.py:143 >> {'loss': 0.5925, 'learning_rate': 3.8090e-06, 'epoch': 0.14, 'throughput': 1175.42}
[INFO|2025-10-21 19:22:06] logging.py:143 >> {'loss': 0.4582, 'learning_rate': 3.8062e-06, 'epoch': 0.14, 'throughput': 1175.44}
[INFO|2025-10-21 19:22:06] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-700
[INFO|2025-10-21 19:22:06] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 19:22:06] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 19:22:06] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-700/chat_template.jinja
[INFO|2025-10-21 19:22:06] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-700/tokenizer_config.json
[INFO|2025-10-21 19:22:06] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-700/special_tokens_map.json
[INFO|2025-10-21 19:23:00] logging.py:143 >> {'loss': 0.6605, 'learning_rate': 3.8035e-06, 'epoch': 0.14, 'throughput': 1175.37}
[INFO|2025-10-21 19:23:54] logging.py:143 >> {'loss': 0.8152, 'learning_rate': 3.8008e-06, 'epoch': 0.14, 'throughput': 1175.41}
[INFO|2025-10-21 19:24:48] logging.py:143 >> {'loss': 0.5439, 'learning_rate': 3.7980e-06, 'epoch': 0.14, 'throughput': 1175.42}
[INFO|2025-10-21 19:25:41] logging.py:143 >> {'loss': 0.5481, 'learning_rate': 3.7952e-06, 'epoch': 0.15, 'throughput': 1175.45}
[INFO|2025-10-21 19:26:35] logging.py:143 >> {'loss': 0.6939, 'learning_rate': 3.7924e-06, 'epoch': 0.15, 'throughput': 1175.45}
[INFO|2025-10-21 19:27:28] logging.py:143 >> {'loss': 0.5407, 'learning_rate': 3.7896e-06, 'epoch': 0.15, 'throughput': 1175.46}
[INFO|2025-10-21 19:28:21] logging.py:143 >> {'loss': 0.5728, 'learning_rate': 3.7867e-06, 'epoch': 0.15, 'throughput': 1175.47}
[INFO|2025-10-21 19:29:14] logging.py:143 >> {'loss': 0.4969, 'learning_rate': 3.7839e-06, 'epoch': 0.15, 'throughput': 1175.50}
[INFO|2025-10-21 19:30:07] logging.py:143 >> {'loss': 0.6154, 'learning_rate': 3.7810e-06, 'epoch': 0.15, 'throughput': 1175.48}
[INFO|2025-10-21 19:31:00] logging.py:143 >> {'loss': 0.5897, 'learning_rate': 3.7781e-06, 'epoch': 0.15, 'throughput': 1175.51}
[INFO|2025-10-21 19:31:53] logging.py:143 >> {'loss': 0.6698, 'learning_rate': 3.7752e-06, 'epoch': 0.15, 'throughput': 1175.48}
[INFO|2025-10-21 19:32:46] logging.py:143 >> {'loss': 0.4517, 'learning_rate': 3.7722e-06, 'epoch': 0.15, 'throughput': 1175.49}
[INFO|2025-10-21 19:33:39] logging.py:143 >> {'loss': 0.6037, 'learning_rate': 3.7693e-06, 'epoch': 0.15, 'throughput': 1175.50}
[INFO|2025-10-21 19:34:32] logging.py:143 >> {'loss': 0.5804, 'learning_rate': 3.7663e-06, 'epoch': 0.16, 'throughput': 1175.49}
[INFO|2025-10-21 19:35:25] logging.py:143 >> {'loss': 0.5664, 'learning_rate': 3.7633e-06, 'epoch': 0.16, 'throughput': 1175.50}
[INFO|2025-10-21 19:36:19] logging.py:143 >> {'loss': 0.6785, 'learning_rate': 3.7603e-06, 'epoch': 0.16, 'throughput': 1175.56}
[INFO|2025-10-21 19:37:13] logging.py:143 >> {'loss': 0.7208, 'learning_rate': 3.7573e-06, 'epoch': 0.16, 'throughput': 1175.56}
[INFO|2025-10-21 19:38:06] logging.py:143 >> {'loss': 0.6304, 'learning_rate': 3.7543e-06, 'epoch': 0.16, 'throughput': 1175.58}
[INFO|2025-10-21 19:39:00] logging.py:143 >> {'loss': 0.7220, 'learning_rate': 3.7512e-06, 'epoch': 0.16, 'throughput': 1175.58}
[INFO|2025-10-21 19:39:52] logging.py:143 >> {'loss': 0.6119, 'learning_rate': 3.7481e-06, 'epoch': 0.16, 'throughput': 1175.55}
[INFO|2025-10-21 19:39:52] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-800
[INFO|2025-10-21 19:39:52] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 19:39:52] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 19:39:52] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-800/chat_template.jinja
[INFO|2025-10-21 19:39:52] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-800/tokenizer_config.json
[INFO|2025-10-21 19:39:52] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-800/special_tokens_map.json
[INFO|2025-10-21 19:40:46] logging.py:143 >> {'loss': 0.7250, 'learning_rate': 3.7450e-06, 'epoch': 0.16, 'throughput': 1175.46}
[INFO|2025-10-21 19:41:40] logging.py:143 >> {'loss': 0.5536, 'learning_rate': 3.7419e-06, 'epoch': 0.16, 'throughput': 1175.47}
[INFO|2025-10-21 19:42:33] logging.py:143 >> {'loss': 0.6998, 'learning_rate': 3.7388e-06, 'epoch': 0.16, 'throughput': 1175.47}
[INFO|2025-10-21 19:43:26] logging.py:143 >> {'loss': 0.8810, 'learning_rate': 3.7356e-06, 'epoch': 0.17, 'throughput': 1175.50}
[INFO|2025-10-21 19:44:19] logging.py:143 >> {'loss': 0.7103, 'learning_rate': 3.7325e-06, 'epoch': 0.17, 'throughput': 1175.54}
[INFO|2025-10-21 19:45:12] logging.py:143 >> {'loss': 0.6177, 'learning_rate': 3.7293e-06, 'epoch': 0.17, 'throughput': 1175.53}
[INFO|2025-10-21 19:46:06] logging.py:143 >> {'loss': 0.7118, 'learning_rate': 3.7261e-06, 'epoch': 0.17, 'throughput': 1175.56}
[INFO|2025-10-21 19:46:58] logging.py:143 >> {'loss': 0.4893, 'learning_rate': 3.7229e-06, 'epoch': 0.17, 'throughput': 1175.56}
[INFO|2025-10-21 19:47:52] logging.py:143 >> {'loss': 0.6717, 'learning_rate': 3.7196e-06, 'epoch': 0.17, 'throughput': 1175.56}
[INFO|2025-10-21 19:48:44] logging.py:143 >> {'loss': 0.6690, 'learning_rate': 3.7164e-06, 'epoch': 0.17, 'throughput': 1175.58}
[INFO|2025-10-21 19:49:38] logging.py:143 >> {'loss': 0.6337, 'learning_rate': 3.7131e-06, 'epoch': 0.17, 'throughput': 1175.55}
[INFO|2025-10-21 19:50:32] logging.py:143 >> {'loss': 0.5397, 'learning_rate': 3.7098e-06, 'epoch': 0.17, 'throughput': 1175.58}
[INFO|2025-10-21 19:51:25] logging.py:143 >> {'loss': 0.7211, 'learning_rate': 3.7065e-06, 'epoch': 0.17, 'throughput': 1175.59}
[INFO|2025-10-21 19:52:18] logging.py:143 >> {'loss': 0.6669, 'learning_rate': 3.7032e-06, 'epoch': 0.18, 'throughput': 1175.58}
[INFO|2025-10-21 19:53:11] logging.py:143 >> {'loss': 0.6623, 'learning_rate': 3.6999e-06, 'epoch': 0.18, 'throughput': 1175.60}
[INFO|2025-10-21 19:54:05] logging.py:143 >> {'loss': 0.5993, 'learning_rate': 3.6965e-06, 'epoch': 0.18, 'throughput': 1175.63}
[INFO|2025-10-21 19:54:58] logging.py:143 >> {'loss': 0.5558, 'learning_rate': 3.6932e-06, 'epoch': 0.18, 'throughput': 1175.64}
[INFO|2025-10-21 19:55:51] logging.py:143 >> {'loss': 0.4670, 'learning_rate': 3.6898e-06, 'epoch': 0.18, 'throughput': 1175.65}
[INFO|2025-10-21 19:56:44] logging.py:143 >> {'loss': 0.5255, 'learning_rate': 3.6864e-06, 'epoch': 0.18, 'throughput': 1175.70}
[INFO|2025-10-21 19:57:37] logging.py:143 >> {'loss': 0.6028, 'learning_rate': 3.6829e-06, 'epoch': 0.18, 'throughput': 1175.71}
[INFO|2025-10-21 19:57:37] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-900
[INFO|2025-10-21 19:57:37] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 19:57:37] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 19:57:38] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-900/chat_template.jinja
[INFO|2025-10-21 19:57:38] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-900/tokenizer_config.json
[INFO|2025-10-21 19:57:38] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-900/special_tokens_map.json
[INFO|2025-10-21 19:58:31] logging.py:143 >> {'loss': 0.8297, 'learning_rate': 3.6795e-06, 'epoch': 0.18, 'throughput': 1175.63}
[INFO|2025-10-21 19:59:24] logging.py:143 >> {'loss': 0.5778, 'learning_rate': 3.6760e-06, 'epoch': 0.18, 'throughput': 1175.64}
[INFO|2025-10-21 20:00:17] logging.py:143 >> {'loss': 0.7150, 'learning_rate': 3.6726e-06, 'epoch': 0.18, 'throughput': 1175.65}
[INFO|2025-10-21 20:01:11] logging.py:143 >> {'loss': 0.6041, 'learning_rate': 3.6691e-06, 'epoch': 0.19, 'throughput': 1175.67}
[INFO|2025-10-21 20:02:04] logging.py:143 >> {'loss': 0.5344, 'learning_rate': 3.6656e-06, 'epoch': 0.19, 'throughput': 1175.70}
[INFO|2025-10-21 20:02:57] logging.py:143 >> {'loss': 0.5915, 'learning_rate': 3.6621e-06, 'epoch': 0.19, 'throughput': 1175.69}
[INFO|2025-10-21 20:03:50] logging.py:143 >> {'loss': 0.7089, 'learning_rate': 3.6585e-06, 'epoch': 0.19, 'throughput': 1175.67}
[INFO|2025-10-21 20:04:44] logging.py:143 >> {'loss': 0.5789, 'learning_rate': 3.6550e-06, 'epoch': 0.19, 'throughput': 1175.68}
[INFO|2025-10-21 20:05:38] logging.py:143 >> {'loss': 0.7485, 'learning_rate': 3.6514e-06, 'epoch': 0.19, 'throughput': 1175.68}
[INFO|2025-10-21 20:06:31] logging.py:143 >> {'loss': 0.4652, 'learning_rate': 3.6478e-06, 'epoch': 0.19, 'throughput': 1175.70}
[INFO|2025-10-21 20:07:24] logging.py:143 >> {'loss': 0.6770, 'learning_rate': 3.6442e-06, 'epoch': 0.19, 'throughput': 1175.69}
[INFO|2025-10-21 20:08:17] logging.py:143 >> {'loss': 0.5958, 'learning_rate': 3.6406e-06, 'epoch': 0.19, 'throughput': 1175.69}
[INFO|2025-10-21 20:09:11] logging.py:143 >> {'loss': 0.4645, 'learning_rate': 3.6369e-06, 'epoch': 0.20, 'throughput': 1175.70}
[INFO|2025-10-21 20:10:04] logging.py:143 >> {'loss': 0.6209, 'learning_rate': 3.6333e-06, 'epoch': 0.20, 'throughput': 1175.68}
[INFO|2025-10-21 20:10:58] logging.py:143 >> {'loss': 0.5899, 'learning_rate': 3.6296e-06, 'epoch': 0.20, 'throughput': 1175.70}
[INFO|2025-10-21 20:11:52] logging.py:143 >> {'loss': 0.6980, 'learning_rate': 3.6259e-06, 'epoch': 0.20, 'throughput': 1175.72}
[INFO|2025-10-21 20:12:45] logging.py:143 >> {'loss': 0.4878, 'learning_rate': 3.6222e-06, 'epoch': 0.20, 'throughput': 1175.72}
[INFO|2025-10-21 20:13:38] logging.py:143 >> {'loss': 0.5052, 'learning_rate': 3.6185e-06, 'epoch': 0.20, 'throughput': 1175.72}
[INFO|2025-10-21 20:14:32] logging.py:143 >> {'loss': 0.6432, 'learning_rate': 3.6147e-06, 'epoch': 0.20, 'throughput': 1175.73}
[INFO|2025-10-21 20:15:26] logging.py:143 >> {'loss': 0.7236, 'learning_rate': 3.6110e-06, 'epoch': 0.20, 'throughput': 1175.76}
[INFO|2025-10-21 20:15:26] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1000
[INFO|2025-10-21 20:15:26] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 20:15:26] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 20:15:26] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1000/chat_template.jinja
[INFO|2025-10-21 20:15:26] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1000/tokenizer_config.json
[INFO|2025-10-21 20:15:26] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1000/special_tokens_map.json
[INFO|2025-10-21 20:16:19] logging.py:143 >> {'loss': 0.7032, 'learning_rate': 3.6072e-06, 'epoch': 0.20, 'throughput': 1175.68}
[INFO|2025-10-21 20:17:13] logging.py:143 >> {'loss': 0.6757, 'learning_rate': 3.6034e-06, 'epoch': 0.20, 'throughput': 1175.69}
[INFO|2025-10-21 20:18:06] logging.py:143 >> {'loss': 0.6012, 'learning_rate': 3.5996e-06, 'epoch': 0.21, 'throughput': 1175.70}
[INFO|2025-10-21 20:19:00] logging.py:143 >> {'loss': 0.7505, 'learning_rate': 3.5958e-06, 'epoch': 0.21, 'throughput': 1175.71}
[INFO|2025-10-21 20:19:53] logging.py:143 >> {'loss': 0.5411, 'learning_rate': 3.5920e-06, 'epoch': 0.21, 'throughput': 1175.70}
[INFO|2025-10-21 20:20:46] logging.py:143 >> {'loss': 0.5092, 'learning_rate': 3.5881e-06, 'epoch': 0.21, 'throughput': 1175.72}
[INFO|2025-10-21 20:21:39] logging.py:143 >> {'loss': 0.6226, 'learning_rate': 3.5843e-06, 'epoch': 0.21, 'throughput': 1175.72}
[INFO|2025-10-21 20:22:32] logging.py:143 >> {'loss': 0.7058, 'learning_rate': 3.5804e-06, 'epoch': 0.21, 'throughput': 1175.73}
[INFO|2025-10-21 20:23:26] logging.py:143 >> {'loss': 0.5577, 'learning_rate': 3.5765e-06, 'epoch': 0.21, 'throughput': 1175.74}
[INFO|2025-10-21 20:24:20] logging.py:143 >> {'loss': 0.6708, 'learning_rate': 3.5726e-06, 'epoch': 0.21, 'throughput': 1175.76}
[INFO|2025-10-21 20:25:13] logging.py:143 >> {'loss': 0.5781, 'learning_rate': 3.5686e-06, 'epoch': 0.21, 'throughput': 1175.78}
[INFO|2025-10-21 20:26:07] logging.py:143 >> {'loss': 0.6473, 'learning_rate': 3.5647e-06, 'epoch': 0.21, 'throughput': 1175.79}
[INFO|2025-10-21 20:27:00] logging.py:143 >> {'loss': 0.7162, 'learning_rate': 3.5607e-06, 'epoch': 0.22, 'throughput': 1175.81}
[INFO|2025-10-21 20:27:54] logging.py:143 >> {'loss': 0.6906, 'learning_rate': 3.5567e-06, 'epoch': 0.22, 'throughput': 1175.82}
[INFO|2025-10-21 20:28:47] logging.py:143 >> {'loss': 0.6976, 'learning_rate': 3.5527e-06, 'epoch': 0.22, 'throughput': 1175.83}
[INFO|2025-10-21 20:29:41] logging.py:143 >> {'loss': 0.7294, 'learning_rate': 3.5487e-06, 'epoch': 0.22, 'throughput': 1175.85}
[INFO|2025-10-21 20:30:35] logging.py:143 >> {'loss': 0.6109, 'learning_rate': 3.5447e-06, 'epoch': 0.22, 'throughput': 1175.88}
[INFO|2025-10-21 20:31:27] logging.py:143 >> {'loss': 0.6002, 'learning_rate': 3.5407e-06, 'epoch': 0.22, 'throughput': 1175.87}
[INFO|2025-10-21 20:32:20] logging.py:143 >> {'loss': 0.5620, 'learning_rate': 3.5366e-06, 'epoch': 0.22, 'throughput': 1175.89}
[INFO|2025-10-21 20:33:13] logging.py:143 >> {'loss': 0.6024, 'learning_rate': 3.5325e-06, 'epoch': 0.22, 'throughput': 1175.89}
[INFO|2025-10-21 20:33:13] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1100
[INFO|2025-10-21 20:33:13] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 20:33:13] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 20:33:13] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1100/chat_template.jinja
[INFO|2025-10-21 20:33:13] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1100/tokenizer_config.json
[INFO|2025-10-21 20:33:13] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1100/special_tokens_map.json
[INFO|2025-10-21 20:34:06] logging.py:143 >> {'loss': 0.6128, 'learning_rate': 3.5285e-06, 'epoch': 0.22, 'throughput': 1175.83}
[INFO|2025-10-21 20:35:01] logging.py:143 >> {'loss': 0.6269, 'learning_rate': 3.5244e-06, 'epoch': 0.22, 'throughput': 1175.87}
[INFO|2025-10-21 20:35:54] logging.py:143 >> {'loss': 0.5975, 'learning_rate': 3.5202e-06, 'epoch': 0.23, 'throughput': 1175.86}
[INFO|2025-10-21 20:36:47] logging.py:143 >> {'loss': 0.6421, 'learning_rate': 3.5161e-06, 'epoch': 0.23, 'throughput': 1175.85}
[INFO|2025-10-21 20:37:40] logging.py:143 >> {'loss': 0.5632, 'learning_rate': 3.5120e-06, 'epoch': 0.23, 'throughput': 1175.84}
[INFO|2025-10-21 20:38:33] logging.py:143 >> {'loss': 0.6126, 'learning_rate': 3.5078e-06, 'epoch': 0.23, 'throughput': 1175.86}
[INFO|2025-10-21 20:39:26] logging.py:143 >> {'loss': 0.5801, 'learning_rate': 3.5036e-06, 'epoch': 0.23, 'throughput': 1175.88}
[INFO|2025-10-21 20:40:19] logging.py:143 >> {'loss': 0.7558, 'learning_rate': 3.4994e-06, 'epoch': 0.23, 'throughput': 1175.88}
[INFO|2025-10-21 20:41:12] logging.py:143 >> {'loss': 0.5368, 'learning_rate': 3.4952e-06, 'epoch': 0.23, 'throughput': 1175.89}
[INFO|2025-10-21 20:42:06] logging.py:143 >> {'loss': 0.5478, 'learning_rate': 3.4910e-06, 'epoch': 0.23, 'throughput': 1175.90}
[INFO|2025-10-21 20:42:58] logging.py:143 >> {'loss': 0.7167, 'learning_rate': 3.4867e-06, 'epoch': 0.23, 'throughput': 1175.89}
[INFO|2025-10-21 20:43:51] logging.py:143 >> {'loss': 0.6356, 'learning_rate': 3.4825e-06, 'epoch': 0.23, 'throughput': 1175.91}
[INFO|2025-10-21 20:44:44] logging.py:143 >> {'loss': 0.6336, 'learning_rate': 3.4782e-06, 'epoch': 0.24, 'throughput': 1175.92}
[INFO|2025-10-21 20:45:38] logging.py:143 >> {'loss': 0.7130, 'learning_rate': 3.4739e-06, 'epoch': 0.24, 'throughput': 1175.93}
[INFO|2025-10-21 20:46:32] logging.py:143 >> {'loss': 0.6477, 'learning_rate': 3.4696e-06, 'epoch': 0.24, 'throughput': 1175.94}
[INFO|2025-10-21 20:47:25] logging.py:143 >> {'loss': 0.7539, 'learning_rate': 3.4653e-06, 'epoch': 0.24, 'throughput': 1175.95}
[INFO|2025-10-21 20:48:19] logging.py:143 >> {'loss': 0.5938, 'learning_rate': 3.4610e-06, 'epoch': 0.24, 'throughput': 1175.96}
[INFO|2025-10-21 20:49:12] logging.py:143 >> {'loss': 0.6628, 'learning_rate': 3.4567e-06, 'epoch': 0.24, 'throughput': 1175.97}
[INFO|2025-10-21 20:50:05] logging.py:143 >> {'loss': 0.6825, 'learning_rate': 3.4523e-06, 'epoch': 0.24, 'throughput': 1175.97}
[INFO|2025-10-21 20:50:58] logging.py:143 >> {'loss': 0.5584, 'learning_rate': 3.4479e-06, 'epoch': 0.24, 'throughput': 1175.97}
[INFO|2025-10-21 20:50:58] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1200
[INFO|2025-10-21 20:50:58] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 20:50:58] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 20:50:58] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1200/chat_template.jinja
[INFO|2025-10-21 20:50:58] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1200/tokenizer_config.json
[INFO|2025-10-21 20:50:58] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1200/special_tokens_map.json
[INFO|2025-10-21 20:51:52] logging.py:143 >> {'loss': 0.6087, 'learning_rate': 3.4435e-06, 'epoch': 0.24, 'throughput': 1175.88}
[INFO|2025-10-21 20:52:46] logging.py:143 >> {'loss': 0.7613, 'learning_rate': 3.4391e-06, 'epoch': 0.24, 'throughput': 1175.90}
[INFO|2025-10-21 20:53:40] logging.py:143 >> {'loss': 0.8090, 'learning_rate': 3.4347e-06, 'epoch': 0.25, 'throughput': 1175.94}
[INFO|2025-10-21 20:54:33] logging.py:143 >> {'loss': 0.4833, 'learning_rate': 3.4303e-06, 'epoch': 0.25, 'throughput': 1175.93}
[INFO|2025-10-21 20:55:27] logging.py:143 >> {'loss': 0.7113, 'learning_rate': 3.4258e-06, 'epoch': 0.25, 'throughput': 1175.97}
[INFO|2025-10-21 20:56:20] logging.py:143 >> {'loss': 0.7355, 'learning_rate': 3.4214e-06, 'epoch': 0.25, 'throughput': 1175.98}
[INFO|2025-10-21 20:57:14] logging.py:143 >> {'loss': 0.7578, 'learning_rate': 3.4169e-06, 'epoch': 0.25, 'throughput': 1176.00}
[INFO|2025-10-21 20:58:06] logging.py:143 >> {'loss': 0.5447, 'learning_rate': 3.4124e-06, 'epoch': 0.25, 'throughput': 1176.00}
[INFO|2025-10-21 20:59:00] logging.py:143 >> {'loss': 0.5981, 'learning_rate': 3.4079e-06, 'epoch': 0.25, 'throughput': 1176.01}
[INFO|2025-10-21 20:59:53] logging.py:143 >> {'loss': 0.6588, 'learning_rate': 3.4034e-06, 'epoch': 0.25, 'throughput': 1176.02}
[INFO|2025-10-21 21:00:47] logging.py:143 >> {'loss': 0.6006, 'learning_rate': 3.3989e-06, 'epoch': 0.25, 'throughput': 1176.03}
[INFO|2025-10-21 21:01:40] logging.py:143 >> {'loss': 0.6275, 'learning_rate': 3.3943e-06, 'epoch': 0.25, 'throughput': 1176.04}
[INFO|2025-10-21 21:02:34] logging.py:143 >> {'loss': 0.4685, 'learning_rate': 3.3898e-06, 'epoch': 0.26, 'throughput': 1176.08}
[INFO|2025-10-21 21:03:27] logging.py:143 >> {'loss': 0.6212, 'learning_rate': 3.3852e-06, 'epoch': 0.26, 'throughput': 1176.09}
[INFO|2025-10-21 21:04:21] logging.py:143 >> {'loss': 0.5108, 'learning_rate': 3.3806e-06, 'epoch': 0.26, 'throughput': 1176.12}
[INFO|2025-10-21 21:05:13] logging.py:143 >> {'loss': 0.5347, 'learning_rate': 3.3760e-06, 'epoch': 0.26, 'throughput': 1176.11}
[INFO|2025-10-21 21:06:07] logging.py:143 >> {'loss': 0.5688, 'learning_rate': 3.3714e-06, 'epoch': 0.26, 'throughput': 1176.13}
[INFO|2025-10-21 21:07:00] logging.py:143 >> {'loss': 0.7579, 'learning_rate': 3.3668e-06, 'epoch': 0.26, 'throughput': 1176.15}
[INFO|2025-10-21 21:07:54] logging.py:143 >> {'loss': 0.5622, 'learning_rate': 3.3621e-06, 'epoch': 0.26, 'throughput': 1176.16}
[INFO|2025-10-21 21:08:47] logging.py:143 >> {'loss': 0.6312, 'learning_rate': 3.3575e-06, 'epoch': 0.26, 'throughput': 1176.16}
[INFO|2025-10-21 21:08:47] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1300
[INFO|2025-10-21 21:08:47] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 21:08:47] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 21:08:47] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1300/chat_template.jinja
[INFO|2025-10-21 21:08:47] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1300/tokenizer_config.json
[INFO|2025-10-21 21:08:47] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1300/special_tokens_map.json
[INFO|2025-10-21 21:09:40] logging.py:143 >> {'loss': 0.5650, 'learning_rate': 3.3528e-06, 'epoch': 0.26, 'throughput': 1176.11}
[INFO|2025-10-21 21:10:34] logging.py:143 >> {'loss': 0.5350, 'learning_rate': 3.3481e-06, 'epoch': 0.26, 'throughput': 1176.12}
[INFO|2025-10-21 21:11:27] logging.py:143 >> {'loss': 0.5921, 'learning_rate': 3.3434e-06, 'epoch': 0.27, 'throughput': 1176.11}
[INFO|2025-10-21 21:12:20] logging.py:143 >> {'loss': 0.6115, 'learning_rate': 3.3387e-06, 'epoch': 0.27, 'throughput': 1176.11}
[INFO|2025-10-21 21:13:12] logging.py:143 >> {'loss': 0.5833, 'learning_rate': 3.3340e-06, 'epoch': 0.27, 'throughput': 1176.11}
[INFO|2025-10-21 21:14:06] logging.py:143 >> {'loss': 0.6707, 'learning_rate': 3.3292e-06, 'epoch': 0.27, 'throughput': 1176.12}
[INFO|2025-10-21 21:14:59] logging.py:143 >> {'loss': 0.6397, 'learning_rate': 3.3245e-06, 'epoch': 0.27, 'throughput': 1176.13}
[INFO|2025-10-21 21:15:52] logging.py:143 >> {'loss': 0.6215, 'learning_rate': 3.3197e-06, 'epoch': 0.27, 'throughput': 1176.15}
[INFO|2025-10-21 21:16:45] logging.py:143 >> {'loss': 0.5628, 'learning_rate': 3.3149e-06, 'epoch': 0.27, 'throughput': 1176.15}
[INFO|2025-10-21 21:17:37] logging.py:143 >> {'loss': 0.5276, 'learning_rate': 3.3102e-06, 'epoch': 0.27, 'throughput': 1176.16}
[INFO|2025-10-21 21:18:30] logging.py:143 >> {'loss': 0.6151, 'learning_rate': 3.3054e-06, 'epoch': 0.27, 'throughput': 1176.17}
[INFO|2025-10-21 21:19:23] logging.py:143 >> {'loss': 0.6502, 'learning_rate': 3.3005e-06, 'epoch': 0.27, 'throughput': 1176.17}
[INFO|2025-10-21 21:20:17] logging.py:143 >> {'loss': 0.6473, 'learning_rate': 3.2957e-06, 'epoch': 0.28, 'throughput': 1176.20}
[INFO|2025-10-21 21:21:11] logging.py:143 >> {'loss': 0.6201, 'learning_rate': 3.2909e-06, 'epoch': 0.28, 'throughput': 1176.22}
[INFO|2025-10-21 21:22:04] logging.py:143 >> {'loss': 0.6332, 'learning_rate': 3.2860e-06, 'epoch': 0.28, 'throughput': 1176.22}
[INFO|2025-10-21 21:22:57] logging.py:143 >> {'loss': 0.7268, 'learning_rate': 3.2811e-06, 'epoch': 0.28, 'throughput': 1176.23}
[INFO|2025-10-21 21:23:51] logging.py:143 >> {'loss': 0.5492, 'learning_rate': 3.2763e-06, 'epoch': 0.28, 'throughput': 1176.23}
[INFO|2025-10-21 21:24:45] logging.py:143 >> {'loss': 0.5729, 'learning_rate': 3.2714e-06, 'epoch': 0.28, 'throughput': 1176.25}
[INFO|2025-10-21 21:25:38] logging.py:143 >> {'loss': 0.5473, 'learning_rate': 3.2665e-06, 'epoch': 0.28, 'throughput': 1176.27}
[INFO|2025-10-21 21:26:32] logging.py:143 >> {'loss': 0.5622, 'learning_rate': 3.2615e-06, 'epoch': 0.28, 'throughput': 1176.30}
[INFO|2025-10-21 21:26:32] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1400
[INFO|2025-10-21 21:26:32] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 21:26:32] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 21:26:32] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1400/chat_template.jinja
[INFO|2025-10-21 21:26:32] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1400/tokenizer_config.json
[INFO|2025-10-21 21:26:32] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1400/special_tokens_map.json
[INFO|2025-10-21 21:27:26] logging.py:143 >> {'loss': 0.5999, 'learning_rate': 3.2566e-06, 'epoch': 0.28, 'throughput': 1176.24}
[INFO|2025-10-21 21:28:20] logging.py:143 >> {'loss': 0.7426, 'learning_rate': 3.2517e-06, 'epoch': 0.28, 'throughput': 1176.25}
[INFO|2025-10-21 21:29:13] logging.py:143 >> {'loss': 0.5530, 'learning_rate': 3.2467e-06, 'epoch': 0.29, 'throughput': 1176.25}
[INFO|2025-10-21 21:30:06] logging.py:143 >> {'loss': 0.6502, 'learning_rate': 3.2417e-06, 'epoch': 0.29, 'throughput': 1176.27}
[INFO|2025-10-21 21:30:59] logging.py:143 >> {'loss': 0.6262, 'learning_rate': 3.2367e-06, 'epoch': 0.29, 'throughput': 1176.27}
[INFO|2025-10-21 21:31:53] logging.py:143 >> {'loss': 0.5802, 'learning_rate': 3.2317e-06, 'epoch': 0.29, 'throughput': 1176.29}
[INFO|2025-10-21 21:32:47] logging.py:143 >> {'loss': 0.6093, 'learning_rate': 3.2267e-06, 'epoch': 0.29, 'throughput': 1176.32}
[INFO|2025-10-21 21:33:40] logging.py:143 >> {'loss': 0.5493, 'learning_rate': 3.2217e-06, 'epoch': 0.29, 'throughput': 1176.33}
[INFO|2025-10-21 21:34:33] logging.py:143 >> {'loss': 0.4534, 'learning_rate': 3.2167e-06, 'epoch': 0.29, 'throughput': 1176.33}
[INFO|2025-10-21 21:35:26] logging.py:143 >> {'loss': 0.6931, 'learning_rate': 3.2116e-06, 'epoch': 0.29, 'throughput': 1176.33}
[INFO|2025-10-21 21:36:19] logging.py:143 >> {'loss': 0.7404, 'learning_rate': 3.2066e-06, 'epoch': 0.29, 'throughput': 1176.32}
[INFO|2025-10-21 21:37:12] logging.py:143 >> {'loss': 0.5767, 'learning_rate': 3.2015e-06, 'epoch': 0.30, 'throughput': 1176.32}
[INFO|2025-10-21 21:38:06] logging.py:143 >> {'loss': 0.6108, 'learning_rate': 3.1964e-06, 'epoch': 0.30, 'throughput': 1176.33}
[INFO|2025-10-21 21:38:59] logging.py:143 >> {'loss': 0.5887, 'learning_rate': 3.1913e-06, 'epoch': 0.30, 'throughput': 1176.35}
[INFO|2025-10-21 21:39:53] logging.py:143 >> {'loss': 0.6058, 'learning_rate': 3.1862e-06, 'epoch': 0.30, 'throughput': 1176.36}
[INFO|2025-10-21 21:40:47] logging.py:143 >> {'loss': 0.6563, 'learning_rate': 3.1811e-06, 'epoch': 0.30, 'throughput': 1176.37}
[INFO|2025-10-21 21:41:40] logging.py:143 >> {'loss': 0.5268, 'learning_rate': 3.1760e-06, 'epoch': 0.30, 'throughput': 1176.37}
[INFO|2025-10-21 21:42:33] logging.py:143 >> {'loss': 0.5945, 'learning_rate': 3.1708e-06, 'epoch': 0.30, 'throughput': 1176.38}
[INFO|2025-10-21 21:43:26] logging.py:143 >> {'loss': 0.7351, 'learning_rate': 3.1657e-06, 'epoch': 0.30, 'throughput': 1176.38}
[INFO|2025-10-21 21:44:20] logging.py:143 >> {'loss': 0.5939, 'learning_rate': 3.1605e-06, 'epoch': 0.30, 'throughput': 1176.39}
[INFO|2025-10-21 21:44:20] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1500
[INFO|2025-10-21 21:44:20] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 21:44:20] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 21:44:20] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1500/chat_template.jinja
[INFO|2025-10-21 21:44:20] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1500/tokenizer_config.json
[INFO|2025-10-21 21:44:20] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1500/special_tokens_map.json
[INFO|2025-10-21 21:45:14] logging.py:143 >> {'loss': 0.7064, 'learning_rate': 3.1553e-06, 'epoch': 0.30, 'throughput': 1176.34}
[INFO|2025-10-21 21:46:07] logging.py:143 >> {'loss': 0.6287, 'learning_rate': 3.1502e-06, 'epoch': 0.31, 'throughput': 1176.35}
[INFO|2025-10-21 21:47:00] logging.py:143 >> {'loss': 0.7107, 'learning_rate': 3.1450e-06, 'epoch': 0.31, 'throughput': 1176.35}
[INFO|2025-10-21 21:47:53] logging.py:143 >> {'loss': 0.6388, 'learning_rate': 3.1397e-06, 'epoch': 0.31, 'throughput': 1176.37}
[INFO|2025-10-21 21:48:47] logging.py:143 >> {'loss': 0.4100, 'learning_rate': 3.1345e-06, 'epoch': 0.31, 'throughput': 1176.38}
[INFO|2025-10-21 21:49:40] logging.py:143 >> {'loss': 0.7457, 'learning_rate': 3.1293e-06, 'epoch': 0.31, 'throughput': 1176.38}
[INFO|2025-10-21 21:50:33] logging.py:143 >> {'loss': 0.6141, 'learning_rate': 3.1240e-06, 'epoch': 0.31, 'throughput': 1176.39}
[INFO|2025-10-21 21:51:26] logging.py:143 >> {'loss': 0.6515, 'learning_rate': 3.1188e-06, 'epoch': 0.31, 'throughput': 1176.39}
[INFO|2025-10-21 21:52:19] logging.py:143 >> {'loss': 0.4877, 'learning_rate': 3.1135e-06, 'epoch': 0.31, 'throughput': 1176.39}
[INFO|2025-10-21 21:53:13] logging.py:143 >> {'loss': 0.4903, 'learning_rate': 3.1082e-06, 'epoch': 0.31, 'throughput': 1176.38}
[INFO|2025-10-21 21:54:06] logging.py:143 >> {'loss': 0.6653, 'learning_rate': 3.1029e-06, 'epoch': 0.31, 'throughput': 1176.39}
[INFO|2025-10-21 21:55:00] logging.py:143 >> {'loss': 0.5501, 'learning_rate': 3.0976e-06, 'epoch': 0.32, 'throughput': 1176.41}
[INFO|2025-10-21 21:55:53] logging.py:143 >> {'loss': 0.6828, 'learning_rate': 3.0923e-06, 'epoch': 0.32, 'throughput': 1176.42}
[INFO|2025-10-21 21:56:47] logging.py:143 >> {'loss': 0.5309, 'learning_rate': 3.0870e-06, 'epoch': 0.32, 'throughput': 1176.43}
[INFO|2025-10-21 21:57:40] logging.py:143 >> {'loss': 0.7835, 'learning_rate': 3.0817e-06, 'epoch': 0.32, 'throughput': 1176.43}
[INFO|2025-10-21 21:58:33] logging.py:143 >> {'loss': 0.7354, 'learning_rate': 3.0763e-06, 'epoch': 0.32, 'throughput': 1176.42}
[INFO|2025-10-21 21:59:26] logging.py:143 >> {'loss': 0.6500, 'learning_rate': 3.0710e-06, 'epoch': 0.32, 'throughput': 1176.43}
[INFO|2025-10-21 22:00:19] logging.py:143 >> {'loss': 0.8998, 'learning_rate': 3.0656e-06, 'epoch': 0.32, 'throughput': 1176.44}
[INFO|2025-10-21 22:01:12] logging.py:143 >> {'loss': 0.7421, 'learning_rate': 3.0602e-06, 'epoch': 0.32, 'throughput': 1176.45}
[INFO|2025-10-21 22:02:05] logging.py:143 >> {'loss': 0.5968, 'learning_rate': 3.0548e-06, 'epoch': 0.32, 'throughput': 1176.46}
[INFO|2025-10-21 22:02:05] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1600
[INFO|2025-10-21 22:02:05] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 22:02:05] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 22:02:05] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1600/chat_template.jinja
[INFO|2025-10-21 22:02:05] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1600/tokenizer_config.json
[INFO|2025-10-21 22:02:05] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1600/special_tokens_map.json
[INFO|2025-10-21 22:02:59] logging.py:143 >> {'loss': 0.6283, 'learning_rate': 3.0494e-06, 'epoch': 0.32, 'throughput': 1176.42}
[INFO|2025-10-21 22:03:53] logging.py:143 >> {'loss': 0.5486, 'learning_rate': 3.0440e-06, 'epoch': 0.33, 'throughput': 1176.43}
[INFO|2025-10-21 22:04:46] logging.py:143 >> {'loss': 0.6774, 'learning_rate': 3.0386e-06, 'epoch': 0.33, 'throughput': 1176.43}
[INFO|2025-10-21 22:05:39] logging.py:143 >> {'loss': 0.6020, 'learning_rate': 3.0332e-06, 'epoch': 0.33, 'throughput': 1176.44}
[INFO|2025-10-21 22:06:32] logging.py:143 >> {'loss': 0.6940, 'learning_rate': 3.0277e-06, 'epoch': 0.33, 'throughput': 1176.45}
[INFO|2025-10-21 22:07:25] logging.py:143 >> {'loss': 0.6293, 'learning_rate': 3.0223e-06, 'epoch': 0.33, 'throughput': 1176.45}
[INFO|2025-10-21 22:08:18] logging.py:143 >> {'loss': 0.9056, 'learning_rate': 3.0168e-06, 'epoch': 0.33, 'throughput': 1176.45}
[INFO|2025-10-21 22:09:11] logging.py:143 >> {'loss': 0.6013, 'learning_rate': 3.0113e-06, 'epoch': 0.33, 'throughput': 1176.45}
[INFO|2025-10-21 22:10:04] logging.py:143 >> {'loss': 0.7281, 'learning_rate': 3.0059e-06, 'epoch': 0.33, 'throughput': 1176.45}
[INFO|2025-10-21 22:10:58] logging.py:143 >> {'loss': 0.7027, 'learning_rate': 3.0004e-06, 'epoch': 0.33, 'throughput': 1176.47}
[INFO|2025-10-21 22:11:52] logging.py:143 >> {'loss': 0.6082, 'learning_rate': 2.9949e-06, 'epoch': 0.33, 'throughput': 1176.49}
[INFO|2025-10-21 22:12:44] logging.py:143 >> {'loss': 0.6896, 'learning_rate': 2.9894e-06, 'epoch': 0.34, 'throughput': 1176.48}
[INFO|2025-10-21 22:13:38] logging.py:143 >> {'loss': 0.5855, 'learning_rate': 2.9838e-06, 'epoch': 0.34, 'throughput': 1176.49}
[INFO|2025-10-21 22:14:30] logging.py:143 >> {'loss': 0.5613, 'learning_rate': 2.9783e-06, 'epoch': 0.34, 'throughput': 1176.49}
[INFO|2025-10-21 22:15:24] logging.py:143 >> {'loss': 0.5806, 'learning_rate': 2.9728e-06, 'epoch': 0.34, 'throughput': 1176.51}
[INFO|2025-10-21 22:16:18] logging.py:143 >> {'loss': 0.5607, 'learning_rate': 2.9672e-06, 'epoch': 0.34, 'throughput': 1176.51}
[INFO|2025-10-21 22:17:11] logging.py:143 >> {'loss': 0.6016, 'learning_rate': 2.9616e-06, 'epoch': 0.34, 'throughput': 1176.51}
[INFO|2025-10-21 22:18:04] logging.py:143 >> {'loss': 0.5951, 'learning_rate': 2.9561e-06, 'epoch': 0.34, 'throughput': 1176.52}
[INFO|2025-10-21 22:18:57] logging.py:143 >> {'loss': 0.4669, 'learning_rate': 2.9505e-06, 'epoch': 0.34, 'throughput': 1176.53}
[INFO|2025-10-21 22:19:51] logging.py:143 >> {'loss': 0.6566, 'learning_rate': 2.9449e-06, 'epoch': 0.34, 'throughput': 1176.55}
[INFO|2025-10-21 22:19:51] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1700
[INFO|2025-10-21 22:19:51] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 22:19:51] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 22:19:51] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1700/chat_template.jinja
[INFO|2025-10-21 22:19:51] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1700/tokenizer_config.json
[INFO|2025-10-21 22:19:51] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1700/special_tokens_map.json
[INFO|2025-10-21 22:20:45] logging.py:143 >> {'loss': 0.7644, 'learning_rate': 2.9393e-06, 'epoch': 0.34, 'throughput': 1176.51}
[INFO|2025-10-21 22:21:38] logging.py:143 >> {'loss': 0.5527, 'learning_rate': 2.9337e-06, 'epoch': 0.35, 'throughput': 1176.51}
[INFO|2025-10-21 22:22:32] logging.py:143 >> {'loss': 0.8091, 'learning_rate': 2.9281e-06, 'epoch': 0.35, 'throughput': 1176.51}
[INFO|2025-10-21 22:23:25] logging.py:143 >> {'loss': 0.5432, 'learning_rate': 2.9224e-06, 'epoch': 0.35, 'throughput': 1176.51}
[INFO|2025-10-21 22:24:19] logging.py:143 >> {'loss': 0.6431, 'learning_rate': 2.9168e-06, 'epoch': 0.35, 'throughput': 1176.53}
[INFO|2025-10-21 22:25:13] logging.py:143 >> {'loss': 0.5834, 'learning_rate': 2.9111e-06, 'epoch': 0.35, 'throughput': 1176.54}
[INFO|2025-10-21 22:26:05] logging.py:143 >> {'loss': 0.5779, 'learning_rate': 2.9055e-06, 'epoch': 0.35, 'throughput': 1176.53}
[INFO|2025-10-21 22:26:58] logging.py:143 >> {'loss': 0.5441, 'learning_rate': 2.8998e-06, 'epoch': 0.35, 'throughput': 1176.54}
[INFO|2025-10-21 22:27:51] logging.py:143 >> {'loss': 0.6264, 'learning_rate': 2.8942e-06, 'epoch': 0.35, 'throughput': 1176.54}
[INFO|2025-10-21 22:28:44] logging.py:143 >> {'loss': 0.5676, 'learning_rate': 2.8885e-06, 'epoch': 0.35, 'throughput': 1176.55}
[INFO|2025-10-21 22:29:37] logging.py:143 >> {'loss': 0.5586, 'learning_rate': 2.8828e-06, 'epoch': 0.35, 'throughput': 1176.55}
[INFO|2025-10-21 22:30:31] logging.py:143 >> {'loss': 0.4766, 'learning_rate': 2.8771e-06, 'epoch': 0.36, 'throughput': 1176.56}
[INFO|2025-10-21 22:31:23] logging.py:143 >> {'loss': 0.6146, 'learning_rate': 2.8714e-06, 'epoch': 0.36, 'throughput': 1176.56}
[INFO|2025-10-21 22:32:16] logging.py:143 >> {'loss': 0.5679, 'learning_rate': 2.8656e-06, 'epoch': 0.36, 'throughput': 1176.56}
[INFO|2025-10-21 22:33:10] logging.py:143 >> {'loss': 0.5745, 'learning_rate': 2.8599e-06, 'epoch': 0.36, 'throughput': 1176.58}
[INFO|2025-10-21 22:34:03] logging.py:143 >> {'loss': 0.6332, 'learning_rate': 2.8542e-06, 'epoch': 0.36, 'throughput': 1176.58}
[INFO|2025-10-21 22:34:56] logging.py:143 >> {'loss': 0.5841, 'learning_rate': 2.8484e-06, 'epoch': 0.36, 'throughput': 1176.59}
[INFO|2025-10-21 22:35:51] logging.py:143 >> {'loss': 0.4888, 'learning_rate': 2.8427e-06, 'epoch': 0.36, 'throughput': 1176.61}
[INFO|2025-10-21 22:36:44] logging.py:143 >> {'loss': 0.5632, 'learning_rate': 2.8369e-06, 'epoch': 0.36, 'throughput': 1176.62}
[INFO|2025-10-21 22:37:37] logging.py:143 >> {'loss': 0.5320, 'learning_rate': 2.8311e-06, 'epoch': 0.36, 'throughput': 1176.62}
[INFO|2025-10-21 22:37:37] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1800
[INFO|2025-10-21 22:37:37] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 22:37:37] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 22:37:37] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1800/chat_template.jinja
[INFO|2025-10-21 22:37:37] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1800/tokenizer_config.json
[INFO|2025-10-21 22:37:37] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1800/special_tokens_map.json
[INFO|2025-10-21 22:38:31] logging.py:143 >> {'loss': 0.6468, 'learning_rate': 2.8254e-06, 'epoch': 0.36, 'throughput': 1176.58}
[INFO|2025-10-21 22:39:24] logging.py:143 >> {'loss': 0.5120, 'learning_rate': 2.8196e-06, 'epoch': 0.37, 'throughput': 1176.59}
[INFO|2025-10-21 22:40:18] logging.py:143 >> {'loss': 0.6492, 'learning_rate': 2.8138e-06, 'epoch': 0.37, 'throughput': 1176.59}
[INFO|2025-10-21 22:41:12] logging.py:143 >> {'loss': 0.4879, 'learning_rate': 2.8080e-06, 'epoch': 0.37, 'throughput': 1176.59}
[INFO|2025-10-21 22:42:05] logging.py:143 >> {'loss': 0.6996, 'learning_rate': 2.8022e-06, 'epoch': 0.37, 'throughput': 1176.60}
[INFO|2025-10-21 22:42:58] logging.py:143 >> {'loss': 0.6387, 'learning_rate': 2.7963e-06, 'epoch': 0.37, 'throughput': 1176.60}
[INFO|2025-10-21 22:43:51] logging.py:143 >> {'loss': 0.6173, 'learning_rate': 2.7905e-06, 'epoch': 0.37, 'throughput': 1176.61}
[INFO|2025-10-21 22:44:45] logging.py:143 >> {'loss': 0.5874, 'learning_rate': 2.7847e-06, 'epoch': 0.37, 'throughput': 1176.61}
[INFO|2025-10-21 22:45:38] logging.py:143 >> {'loss': 0.4590, 'learning_rate': 2.7788e-06, 'epoch': 0.37, 'throughput': 1176.62}
[INFO|2025-10-21 22:46:31] logging.py:143 >> {'loss': 0.5740, 'learning_rate': 2.7730e-06, 'epoch': 0.37, 'throughput': 1176.62}
[INFO|2025-10-21 22:47:24] logging.py:143 >> {'loss': 0.5933, 'learning_rate': 2.7671e-06, 'epoch': 0.37, 'throughput': 1176.61}
[INFO|2025-10-21 22:48:16] logging.py:143 >> {'loss': 0.5003, 'learning_rate': 2.7613e-06, 'epoch': 0.38, 'throughput': 1176.60}
[INFO|2025-10-21 22:49:10] logging.py:143 >> {'loss': 0.5391, 'learning_rate': 2.7554e-06, 'epoch': 0.38, 'throughput': 1176.60}
[INFO|2025-10-21 22:50:03] logging.py:143 >> {'loss': 0.6260, 'learning_rate': 2.7495e-06, 'epoch': 0.38, 'throughput': 1176.60}
[INFO|2025-10-21 22:50:57] logging.py:143 >> {'loss': 0.5976, 'learning_rate': 2.7436e-06, 'epoch': 0.38, 'throughput': 1176.62}
[INFO|2025-10-21 22:51:50] logging.py:143 >> {'loss': 0.5120, 'learning_rate': 2.7377e-06, 'epoch': 0.38, 'throughput': 1176.62}
[INFO|2025-10-21 22:52:42] logging.py:143 >> {'loss': 0.5453, 'learning_rate': 2.7318e-06, 'epoch': 0.38, 'throughput': 1176.61}
[INFO|2025-10-21 22:53:35] logging.py:143 >> {'loss': 0.8227, 'learning_rate': 2.7259e-06, 'epoch': 0.38, 'throughput': 1176.61}
[INFO|2025-10-21 22:54:28] logging.py:143 >> {'loss': 0.5607, 'learning_rate': 2.7200e-06, 'epoch': 0.38, 'throughput': 1176.60}
[INFO|2025-10-21 22:55:20] logging.py:143 >> {'loss': 0.6824, 'learning_rate': 2.7140e-06, 'epoch': 0.38, 'throughput': 1176.60}
[INFO|2025-10-21 22:55:20] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1900
[INFO|2025-10-21 22:55:20] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 22:55:20] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 22:55:21] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1900/chat_template.jinja
[INFO|2025-10-21 22:55:21] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1900/tokenizer_config.json
[INFO|2025-10-21 22:55:21] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-1900/special_tokens_map.json
[INFO|2025-10-21 22:56:15] logging.py:143 >> {'loss': 0.5432, 'learning_rate': 2.7081e-06, 'epoch': 0.39, 'throughput': 1176.57}
[INFO|2025-10-21 22:57:09] logging.py:143 >> {'loss': 0.5470, 'learning_rate': 2.7022e-06, 'epoch': 0.39, 'throughput': 1176.58}
[INFO|2025-10-21 22:58:02] logging.py:143 >> {'loss': 0.6510, 'learning_rate': 2.6962e-06, 'epoch': 0.39, 'throughput': 1176.59}
[INFO|2025-10-21 22:58:55] logging.py:143 >> {'loss': 0.5220, 'learning_rate': 2.6903e-06, 'epoch': 0.39, 'throughput': 1176.58}
[INFO|2025-10-21 22:59:47] logging.py:143 >> {'loss': 0.6065, 'learning_rate': 2.6843e-06, 'epoch': 0.39, 'throughput': 1176.58}
[INFO|2025-10-21 23:00:41] logging.py:143 >> {'loss': 0.4948, 'learning_rate': 2.6783e-06, 'epoch': 0.39, 'throughput': 1176.58}
[INFO|2025-10-21 23:01:35] logging.py:143 >> {'loss': 0.7279, 'learning_rate': 2.6724e-06, 'epoch': 0.39, 'throughput': 1176.58}
[INFO|2025-10-21 23:02:29] logging.py:143 >> {'loss': 0.4619, 'learning_rate': 2.6664e-06, 'epoch': 0.39, 'throughput': 1176.59}
[INFO|2025-10-21 23:03:22] logging.py:143 >> {'loss': 0.5386, 'learning_rate': 2.6604e-06, 'epoch': 0.39, 'throughput': 1176.58}
[INFO|2025-10-21 23:04:15] logging.py:143 >> {'loss': 0.5364, 'learning_rate': 2.6544e-06, 'epoch': 0.39, 'throughput': 1176.58}
[INFO|2025-10-21 23:05:09] logging.py:143 >> {'loss': 0.6136, 'learning_rate': 2.6484e-06, 'epoch': 0.40, 'throughput': 1176.59}
[INFO|2025-10-21 23:06:01] logging.py:143 >> {'loss': 0.6195, 'learning_rate': 2.6424e-06, 'epoch': 0.40, 'throughput': 1176.59}
[INFO|2025-10-21 23:06:55] logging.py:143 >> {'loss': 0.5013, 'learning_rate': 2.6364e-06, 'epoch': 0.40, 'throughput': 1176.60}
[INFO|2025-10-21 23:07:48] logging.py:143 >> {'loss': 0.5886, 'learning_rate': 2.6303e-06, 'epoch': 0.40, 'throughput': 1176.60}
[INFO|2025-10-21 23:08:41] logging.py:143 >> {'loss': 0.5710, 'learning_rate': 2.6243e-06, 'epoch': 0.40, 'throughput': 1176.61}
[INFO|2025-10-21 23:09:34] logging.py:143 >> {'loss': 0.5797, 'learning_rate': 2.6183e-06, 'epoch': 0.40, 'throughput': 1176.62}
[INFO|2025-10-21 23:10:26] logging.py:143 >> {'loss': 0.5936, 'learning_rate': 2.6122e-06, 'epoch': 0.40, 'throughput': 1176.62}
[INFO|2025-10-21 23:11:20] logging.py:143 >> {'loss': 0.4577, 'learning_rate': 2.6062e-06, 'epoch': 0.40, 'throughput': 1176.62}
[INFO|2025-10-21 23:12:13] logging.py:143 >> {'loss': 0.7144, 'learning_rate': 2.6001e-06, 'epoch': 0.40, 'throughput': 1176.63}
[INFO|2025-10-21 23:13:06] logging.py:143 >> {'loss': 0.5053, 'learning_rate': 2.5941e-06, 'epoch': 0.40, 'throughput': 1176.63}
[INFO|2025-10-21 23:13:06] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2000
[INFO|2025-10-21 23:13:06] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 23:13:06] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 23:13:06] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2000/chat_template.jinja
[INFO|2025-10-21 23:13:06] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2000/tokenizer_config.json
[INFO|2025-10-21 23:13:06] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2000/special_tokens_map.json
[INFO|2025-10-21 23:13:59] logging.py:143 >> {'loss': 0.5239, 'learning_rate': 2.5880e-06, 'epoch': 0.41, 'throughput': 1176.60}
[INFO|2025-10-21 23:14:52] logging.py:143 >> {'loss': 0.5039, 'learning_rate': 2.5819e-06, 'epoch': 0.41, 'throughput': 1176.60}
[INFO|2025-10-21 23:15:45] logging.py:143 >> {'loss': 0.5787, 'learning_rate': 2.5759e-06, 'epoch': 0.41, 'throughput': 1176.60}
[INFO|2025-10-21 23:16:38] logging.py:143 >> {'loss': 0.6472, 'learning_rate': 2.5698e-06, 'epoch': 0.41, 'throughput': 1176.61}
[INFO|2025-10-21 23:17:32] logging.py:143 >> {'loss': 0.5386, 'learning_rate': 2.5637e-06, 'epoch': 0.41, 'throughput': 1176.62}
[INFO|2025-10-21 23:18:25] logging.py:143 >> {'loss': 0.4028, 'learning_rate': 2.5576e-06, 'epoch': 0.41, 'throughput': 1176.63}
[INFO|2025-10-21 23:19:19] logging.py:143 >> {'loss': 0.6926, 'learning_rate': 2.5515e-06, 'epoch': 0.41, 'throughput': 1176.64}
[INFO|2025-10-21 23:20:12] logging.py:143 >> {'loss': 0.5582, 'learning_rate': 2.5454e-06, 'epoch': 0.41, 'throughput': 1176.65}
[INFO|2025-10-21 23:21:05] logging.py:143 >> {'loss': 0.5421, 'learning_rate': 2.5393e-06, 'epoch': 0.41, 'throughput': 1176.65}
[INFO|2025-10-21 23:21:59] logging.py:143 >> {'loss': 0.5980, 'learning_rate': 2.5332e-06, 'epoch': 0.41, 'throughput': 1176.66}
[INFO|2025-10-21 23:22:53] logging.py:143 >> {'loss': 0.6110, 'learning_rate': 2.5270e-06, 'epoch': 0.42, 'throughput': 1176.67}
[INFO|2025-10-21 23:23:46] logging.py:143 >> {'loss': 0.5292, 'learning_rate': 2.5209e-06, 'epoch': 0.42, 'throughput': 1176.68}
[INFO|2025-10-21 23:24:39] logging.py:143 >> {'loss': 0.5152, 'learning_rate': 2.5148e-06, 'epoch': 0.42, 'throughput': 1176.68}
[INFO|2025-10-21 23:25:32] logging.py:143 >> {'loss': 0.5862, 'learning_rate': 2.5086e-06, 'epoch': 0.42, 'throughput': 1176.67}
[INFO|2025-10-21 23:26:25] logging.py:143 >> {'loss': 0.6319, 'learning_rate': 2.5025e-06, 'epoch': 0.42, 'throughput': 1176.67}
[INFO|2025-10-21 23:27:18] logging.py:143 >> {'loss': 0.7200, 'learning_rate': 2.4963e-06, 'epoch': 0.42, 'throughput': 1176.69}
[INFO|2025-10-21 23:28:12] logging.py:143 >> {'loss': 0.6548, 'learning_rate': 2.4902e-06, 'epoch': 0.42, 'throughput': 1176.71}
[INFO|2025-10-21 23:29:05] logging.py:143 >> {'loss': 0.5558, 'learning_rate': 2.4840e-06, 'epoch': 0.42, 'throughput': 1176.72}
[INFO|2025-10-21 23:29:58] logging.py:143 >> {'loss': 0.4996, 'learning_rate': 2.4779e-06, 'epoch': 0.42, 'throughput': 1176.72}
[INFO|2025-10-21 23:30:52] logging.py:143 >> {'loss': 0.5691, 'learning_rate': 2.4717e-06, 'epoch': 0.42, 'throughput': 1176.73}
[INFO|2025-10-21 23:30:52] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2100
[INFO|2025-10-21 23:30:52] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 23:30:52] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 23:30:52] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2100/chat_template.jinja
[INFO|2025-10-21 23:30:52] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2100/tokenizer_config.json
[INFO|2025-10-21 23:30:52] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2100/special_tokens_map.json
[INFO|2025-10-21 23:31:46] logging.py:143 >> {'loss': 0.5768, 'learning_rate': 2.4655e-06, 'epoch': 0.43, 'throughput': 1176.69}
[INFO|2025-10-21 23:32:39] logging.py:143 >> {'loss': 0.6315, 'learning_rate': 2.4594e-06, 'epoch': 0.43, 'throughput': 1176.69}
[INFO|2025-10-21 23:33:32] logging.py:143 >> {'loss': 0.5714, 'learning_rate': 2.4532e-06, 'epoch': 0.43, 'throughput': 1176.70}
[INFO|2025-10-21 23:34:26] logging.py:143 >> {'loss': 0.6860, 'learning_rate': 2.4470e-06, 'epoch': 0.43, 'throughput': 1176.70}
[INFO|2025-10-21 23:35:20] logging.py:143 >> {'loss': 0.4842, 'learning_rate': 2.4408e-06, 'epoch': 0.43, 'throughput': 1176.72}
[INFO|2025-10-21 23:36:13] logging.py:143 >> {'loss': 0.5434, 'learning_rate': 2.4346e-06, 'epoch': 0.43, 'throughput': 1176.72}
[INFO|2025-10-21 23:37:06] logging.py:143 >> {'loss': 0.5750, 'learning_rate': 2.4284e-06, 'epoch': 0.43, 'throughput': 1176.72}
[INFO|2025-10-21 23:37:59] logging.py:143 >> {'loss': 0.6142, 'learning_rate': 2.4222e-06, 'epoch': 0.43, 'throughput': 1176.72}
[INFO|2025-10-21 23:38:52] logging.py:143 >> {'loss': 0.4698, 'learning_rate': 2.4160e-06, 'epoch': 0.43, 'throughput': 1176.73}
[INFO|2025-10-21 23:39:46] logging.py:143 >> {'loss': 0.8394, 'learning_rate': 2.4098e-06, 'epoch': 0.43, 'throughput': 1176.74}
[INFO|2025-10-21 23:40:40] logging.py:143 >> {'loss': 0.5828, 'learning_rate': 2.4036e-06, 'epoch': 0.44, 'throughput': 1176.75}
[INFO|2025-10-21 23:41:34] logging.py:143 >> {'loss': 0.6166, 'learning_rate': 2.3973e-06, 'epoch': 0.44, 'throughput': 1176.76}
[INFO|2025-10-21 23:42:27] logging.py:143 >> {'loss': 0.5197, 'learning_rate': 2.3911e-06, 'epoch': 0.44, 'throughput': 1176.77}
[INFO|2025-10-21 23:43:20] logging.py:143 >> {'loss': 0.6045, 'learning_rate': 2.3849e-06, 'epoch': 0.44, 'throughput': 1176.76}
[INFO|2025-10-21 23:44:14] logging.py:143 >> {'loss': 0.5972, 'learning_rate': 2.3787e-06, 'epoch': 0.44, 'throughput': 1176.77}
[INFO|2025-10-21 23:45:07] logging.py:143 >> {'loss': 0.6531, 'learning_rate': 2.3724e-06, 'epoch': 0.44, 'throughput': 1176.78}
[INFO|2025-10-21 23:45:59] logging.py:143 >> {'loss': 0.6343, 'learning_rate': 2.3662e-06, 'epoch': 0.44, 'throughput': 1176.78}
[INFO|2025-10-21 23:46:52] logging.py:143 >> {'loss': 0.5811, 'learning_rate': 2.3599e-06, 'epoch': 0.44, 'throughput': 1176.78}
[INFO|2025-10-21 23:47:45] logging.py:143 >> {'loss': 0.6091, 'learning_rate': 2.3537e-06, 'epoch': 0.44, 'throughput': 1176.79}
[INFO|2025-10-21 23:48:38] logging.py:143 >> {'loss': 0.5612, 'learning_rate': 2.3474e-06, 'epoch': 0.44, 'throughput': 1176.80}
[INFO|2025-10-21 23:48:38] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2200
[INFO|2025-10-21 23:48:38] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-21 23:48:38] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-21 23:48:39] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2200/chat_template.jinja
[INFO|2025-10-21 23:48:39] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2200/tokenizer_config.json
[INFO|2025-10-21 23:48:39] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2200/special_tokens_map.json
[INFO|2025-10-21 23:49:33] logging.py:143 >> {'loss': 0.5389, 'learning_rate': 2.3412e-06, 'epoch': 0.45, 'throughput': 1176.77}
[INFO|2025-10-21 23:50:27] logging.py:143 >> {'loss': 0.5689, 'learning_rate': 2.3349e-06, 'epoch': 0.45, 'throughput': 1176.78}
[INFO|2025-10-21 23:51:20] logging.py:143 >> {'loss': 0.5939, 'learning_rate': 2.3287e-06, 'epoch': 0.45, 'throughput': 1176.79}
[INFO|2025-10-21 23:52:14] logging.py:143 >> {'loss': 0.7729, 'learning_rate': 2.3224e-06, 'epoch': 0.45, 'throughput': 1176.79}
[INFO|2025-10-21 23:53:07] logging.py:143 >> {'loss': 0.6197, 'learning_rate': 2.3161e-06, 'epoch': 0.45, 'throughput': 1176.80}
[INFO|2025-10-21 23:54:01] logging.py:143 >> {'loss': 0.4996, 'learning_rate': 2.3099e-06, 'epoch': 0.45, 'throughput': 1176.82}
[INFO|2025-10-21 23:54:55] logging.py:143 >> {'loss': 0.5526, 'learning_rate': 2.3036e-06, 'epoch': 0.45, 'throughput': 1176.81}
[INFO|2025-10-21 23:55:48] logging.py:143 >> {'loss': 0.5462, 'learning_rate': 2.2973e-06, 'epoch': 0.45, 'throughput': 1176.82}
[INFO|2025-10-21 23:56:41] logging.py:143 >> {'loss': 0.6201, 'learning_rate': 2.2910e-06, 'epoch': 0.45, 'throughput': 1176.82}
[INFO|2025-10-21 23:57:36] logging.py:143 >> {'loss': 0.5204, 'learning_rate': 2.2847e-06, 'epoch': 0.45, 'throughput': 1176.84}
[INFO|2025-10-21 23:58:29] logging.py:143 >> {'loss': 0.5236, 'learning_rate': 2.2785e-06, 'epoch': 0.46, 'throughput': 1176.84}
[INFO|2025-10-21 23:59:22] logging.py:143 >> {'loss': 0.6035, 'learning_rate': 2.2722e-06, 'epoch': 0.46, 'throughput': 1176.84}
[INFO|2025-10-22 00:00:15] logging.py:143 >> {'loss': 0.6140, 'learning_rate': 2.2659e-06, 'epoch': 0.46, 'throughput': 1176.85}
[INFO|2025-10-22 00:01:09] logging.py:143 >> {'loss': 0.6920, 'learning_rate': 2.2596e-06, 'epoch': 0.46, 'throughput': 1176.86}
[INFO|2025-10-22 00:02:03] logging.py:143 >> {'loss': 0.4207, 'learning_rate': 2.2533e-06, 'epoch': 0.46, 'throughput': 1176.87}
[INFO|2025-10-22 00:02:56] logging.py:143 >> {'loss': 0.5122, 'learning_rate': 2.2470e-06, 'epoch': 0.46, 'throughput': 1176.86}
[INFO|2025-10-22 00:03:49] logging.py:143 >> {'loss': 0.6128, 'learning_rate': 2.2407e-06, 'epoch': 0.46, 'throughput': 1176.87}
[INFO|2025-10-22 00:04:43] logging.py:143 >> {'loss': 0.5483, 'learning_rate': 2.2344e-06, 'epoch': 0.46, 'throughput': 1176.88}
[INFO|2025-10-22 00:05:36] logging.py:143 >> {'loss': 0.5788, 'learning_rate': 2.2281e-06, 'epoch': 0.46, 'throughput': 1176.88}
[INFO|2025-10-22 00:06:29] logging.py:143 >> {'loss': 0.6322, 'learning_rate': 2.2218e-06, 'epoch': 0.46, 'throughput': 1176.88}
[INFO|2025-10-22 00:06:29] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2300
[INFO|2025-10-22 00:06:29] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 00:06:29] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 00:06:29] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2300/chat_template.jinja
[INFO|2025-10-22 00:06:29] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2300/tokenizer_config.json
[INFO|2025-10-22 00:06:29] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2300/special_tokens_map.json
[INFO|2025-10-22 00:07:22] logging.py:143 >> {'loss': 0.5923, 'learning_rate': 2.2155e-06, 'epoch': 0.47, 'throughput': 1176.85}
[INFO|2025-10-22 00:08:15] logging.py:143 >> {'loss': 0.5235, 'learning_rate': 2.2091e-06, 'epoch': 0.47, 'throughput': 1176.85}
[INFO|2025-10-22 00:09:08] logging.py:143 >> {'loss': 0.4929, 'learning_rate': 2.2028e-06, 'epoch': 0.47, 'throughput': 1176.85}
[INFO|2025-10-22 00:10:02] logging.py:143 >> {'loss': 0.5742, 'learning_rate': 2.1965e-06, 'epoch': 0.47, 'throughput': 1176.85}
[INFO|2025-10-22 00:10:55] logging.py:143 >> {'loss': 0.5224, 'learning_rate': 2.1902e-06, 'epoch': 0.47, 'throughput': 1176.86}
[INFO|2025-10-22 00:11:49] logging.py:143 >> {'loss': 0.5408, 'learning_rate': 2.1839e-06, 'epoch': 0.47, 'throughput': 1176.86}
[INFO|2025-10-22 00:12:42] logging.py:143 >> {'loss': 0.6340, 'learning_rate': 2.1775e-06, 'epoch': 0.47, 'throughput': 1176.86}
[INFO|2025-10-22 00:13:35] logging.py:143 >> {'loss': 0.6083, 'learning_rate': 2.1712e-06, 'epoch': 0.47, 'throughput': 1176.86}
[INFO|2025-10-22 00:14:29] logging.py:143 >> {'loss': 0.6487, 'learning_rate': 2.1649e-06, 'epoch': 0.47, 'throughput': 1176.87}
[INFO|2025-10-22 00:15:22] logging.py:143 >> {'loss': 0.5801, 'learning_rate': 2.1586e-06, 'epoch': 0.47, 'throughput': 1176.88}
[INFO|2025-10-22 00:16:15] logging.py:143 >> {'loss': 0.5921, 'learning_rate': 2.1522e-06, 'epoch': 0.48, 'throughput': 1176.89}
[INFO|2025-10-22 00:17:08] logging.py:143 >> {'loss': 0.6469, 'learning_rate': 2.1459e-06, 'epoch': 0.48, 'throughput': 1176.89}
[INFO|2025-10-22 00:18:02] logging.py:143 >> {'loss': 0.5856, 'learning_rate': 2.1396e-06, 'epoch': 0.48, 'throughput': 1176.90}
[INFO|2025-10-22 00:18:55] logging.py:143 >> {'loss': 0.5538, 'learning_rate': 2.1332e-06, 'epoch': 0.48, 'throughput': 1176.92}
[INFO|2025-10-22 00:19:48] logging.py:143 >> {'loss': 0.5475, 'learning_rate': 2.1269e-06, 'epoch': 0.48, 'throughput': 1176.92}
[INFO|2025-10-22 00:20:41] logging.py:143 >> {'loss': 0.5856, 'learning_rate': 2.1206e-06, 'epoch': 0.48, 'throughput': 1176.93}
[INFO|2025-10-22 00:21:34] logging.py:143 >> {'loss': 0.6462, 'learning_rate': 2.1142e-06, 'epoch': 0.48, 'throughput': 1176.94}
[INFO|2025-10-22 00:22:28] logging.py:143 >> {'loss': 0.5204, 'learning_rate': 2.1079e-06, 'epoch': 0.48, 'throughput': 1176.95}
[INFO|2025-10-22 00:23:21] logging.py:143 >> {'loss': 0.7895, 'learning_rate': 2.1015e-06, 'epoch': 0.48, 'throughput': 1176.96}
[INFO|2025-10-22 00:24:15] logging.py:143 >> {'loss': 0.6463, 'learning_rate': 2.0952e-06, 'epoch': 0.49, 'throughput': 1176.96}
[INFO|2025-10-22 00:24:15] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2400
[INFO|2025-10-22 00:24:15] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 00:24:15] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 00:24:15] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2400/chat_template.jinja
[INFO|2025-10-22 00:24:15] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2400/tokenizer_config.json
[INFO|2025-10-22 00:24:15] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2400/special_tokens_map.json
[INFO|2025-10-22 00:25:09] logging.py:143 >> {'loss': 0.4849, 'learning_rate': 2.0889e-06, 'epoch': 0.49, 'throughput': 1176.91}
[INFO|2025-10-22 00:26:02] logging.py:143 >> {'loss': 0.6434, 'learning_rate': 2.0825e-06, 'epoch': 0.49, 'throughput': 1176.92}
[INFO|2025-10-22 00:26:55] logging.py:143 >> {'loss': 0.5656, 'learning_rate': 2.0762e-06, 'epoch': 0.49, 'throughput': 1176.92}
[INFO|2025-10-22 00:27:48] logging.py:143 >> {'loss': 0.5289, 'learning_rate': 2.0698e-06, 'epoch': 0.49, 'throughput': 1176.92}
[INFO|2025-10-22 00:28:41] logging.py:143 >> {'loss': 0.4339, 'learning_rate': 2.0635e-06, 'epoch': 0.49, 'throughput': 1176.93}
[INFO|2025-10-22 00:29:33] logging.py:143 >> {'loss': 0.6227, 'learning_rate': 2.0571e-06, 'epoch': 0.49, 'throughput': 1176.93}
[INFO|2025-10-22 00:30:27] logging.py:143 >> {'loss': 0.6725, 'learning_rate': 2.0508e-06, 'epoch': 0.49, 'throughput': 1176.93}
[INFO|2025-10-22 00:31:21] logging.py:143 >> {'loss': 0.5814, 'learning_rate': 2.0444e-06, 'epoch': 0.49, 'throughput': 1176.94}
[INFO|2025-10-22 00:32:14] logging.py:143 >> {'loss': 0.4539, 'learning_rate': 2.0381e-06, 'epoch': 0.49, 'throughput': 1176.94}
[INFO|2025-10-22 00:33:07] logging.py:143 >> {'loss': 0.4290, 'learning_rate': 2.0317e-06, 'epoch': 0.50, 'throughput': 1176.95}
[INFO|2025-10-22 00:34:00] logging.py:143 >> {'loss': 0.5240, 'learning_rate': 2.0254e-06, 'epoch': 0.50, 'throughput': 1176.95}
[INFO|2025-10-22 00:34:54] logging.py:143 >> {'loss': 0.6713, 'learning_rate': 2.0190e-06, 'epoch': 0.50, 'throughput': 1176.96}
[INFO|2025-10-22 00:35:47] logging.py:143 >> {'loss': 0.5240, 'learning_rate': 2.0127e-06, 'epoch': 0.50, 'throughput': 1176.96}
[INFO|2025-10-22 00:36:40] logging.py:143 >> {'loss': 0.6109, 'learning_rate': 2.0063e-06, 'epoch': 0.50, 'throughput': 1176.97}
[INFO|2025-10-22 00:37:35] logging.py:143 >> {'loss': 0.6026, 'learning_rate': 2.0000e-06, 'epoch': 0.50, 'throughput': 1176.99}
[INFO|2025-10-22 00:38:29] logging.py:143 >> {'loss': 0.5379, 'learning_rate': 1.9937e-06, 'epoch': 0.50, 'throughput': 1176.99}
[INFO|2025-10-22 00:39:22] logging.py:143 >> {'loss': 0.5568, 'learning_rate': 1.9873e-06, 'epoch': 0.50, 'throughput': 1177.00}
[INFO|2025-10-22 00:40:16] logging.py:143 >> {'loss': 0.5845, 'learning_rate': 1.9810e-06, 'epoch': 0.50, 'throughput': 1177.00}
[INFO|2025-10-22 00:41:09] logging.py:143 >> {'loss': 0.4871, 'learning_rate': 1.9746e-06, 'epoch': 0.50, 'throughput': 1177.01}
[INFO|2025-10-22 00:42:03] logging.py:143 >> {'loss': 0.6666, 'learning_rate': 1.9683e-06, 'epoch': 0.51, 'throughput': 1177.02}
[INFO|2025-10-22 00:42:03] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2500
[INFO|2025-10-22 00:42:03] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 00:42:03] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 00:42:03] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2500/chat_template.jinja
[INFO|2025-10-22 00:42:03] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2500/tokenizer_config.json
[INFO|2025-10-22 00:42:03] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2500/special_tokens_map.json
[INFO|2025-10-22 00:42:57] logging.py:143 >> {'loss': 0.5230, 'learning_rate': 1.9619e-06, 'epoch': 0.51, 'throughput': 1176.99}
[INFO|2025-10-22 00:43:51] logging.py:143 >> {'loss': 0.5564, 'learning_rate': 1.9556e-06, 'epoch': 0.51, 'throughput': 1177.00}
[INFO|2025-10-22 00:44:45] logging.py:143 >> {'loss': 0.5300, 'learning_rate': 1.9492e-06, 'epoch': 0.51, 'throughput': 1177.01}
[INFO|2025-10-22 00:45:38] logging.py:143 >> {'loss': 0.6632, 'learning_rate': 1.9429e-06, 'epoch': 0.51, 'throughput': 1177.01}
[INFO|2025-10-22 00:46:31] logging.py:143 >> {'loss': 0.6558, 'learning_rate': 1.9365e-06, 'epoch': 0.51, 'throughput': 1177.02}
[INFO|2025-10-22 00:47:25] logging.py:143 >> {'loss': 0.5018, 'learning_rate': 1.9302e-06, 'epoch': 0.51, 'throughput': 1177.02}
[INFO|2025-10-22 00:48:18] logging.py:143 >> {'loss': 0.4454, 'learning_rate': 1.9238e-06, 'epoch': 0.51, 'throughput': 1177.03}
[INFO|2025-10-22 00:49:10] logging.py:143 >> {'loss': 0.5974, 'learning_rate': 1.9175e-06, 'epoch': 0.51, 'throughput': 1177.02}
[INFO|2025-10-22 00:50:03] logging.py:143 >> {'loss': 0.4928, 'learning_rate': 1.9111e-06, 'epoch': 0.51, 'throughput': 1177.02}
[INFO|2025-10-22 00:50:57] logging.py:143 >> {'loss': 0.4805, 'learning_rate': 1.9048e-06, 'epoch': 0.52, 'throughput': 1177.04}
[INFO|2025-10-22 00:51:50] logging.py:143 >> {'loss': 0.5990, 'learning_rate': 1.8985e-06, 'epoch': 0.52, 'throughput': 1177.04}
[INFO|2025-10-22 00:52:43] logging.py:143 >> {'loss': 0.5942, 'learning_rate': 1.8921e-06, 'epoch': 0.52, 'throughput': 1177.04}
[INFO|2025-10-22 00:53:35] logging.py:143 >> {'loss': 0.7289, 'learning_rate': 1.8858e-06, 'epoch': 0.52, 'throughput': 1177.04}
[INFO|2025-10-22 00:54:28] logging.py:143 >> {'loss': 0.6213, 'learning_rate': 1.8794e-06, 'epoch': 0.52, 'throughput': 1177.04}
[INFO|2025-10-22 00:55:21] logging.py:143 >> {'loss': 0.6097, 'learning_rate': 1.8731e-06, 'epoch': 0.52, 'throughput': 1177.05}
[INFO|2025-10-22 00:56:14] logging.py:143 >> {'loss': 0.6956, 'learning_rate': 1.8668e-06, 'epoch': 0.52, 'throughput': 1177.03}
[INFO|2025-10-22 00:57:07] logging.py:143 >> {'loss': 0.6323, 'learning_rate': 1.8604e-06, 'epoch': 0.52, 'throughput': 1177.03}
[INFO|2025-10-22 00:58:00] logging.py:143 >> {'loss': 0.5404, 'learning_rate': 1.8541e-06, 'epoch': 0.52, 'throughput': 1177.04}
[INFO|2025-10-22 00:58:54] logging.py:143 >> {'loss': 0.7468, 'learning_rate': 1.8478e-06, 'epoch': 0.52, 'throughput': 1177.04}
[INFO|2025-10-22 00:59:46] logging.py:143 >> {'loss': 0.6323, 'learning_rate': 1.8414e-06, 'epoch': 0.53, 'throughput': 1177.04}
[INFO|2025-10-22 00:59:46] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2600
[INFO|2025-10-22 00:59:46] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 00:59:46] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 00:59:47] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2600/chat_template.jinja
[INFO|2025-10-22 00:59:47] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2600/tokenizer_config.json
[INFO|2025-10-22 00:59:47] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2600/special_tokens_map.json
[INFO|2025-10-22 01:00:41] logging.py:143 >> {'loss': 0.4728, 'learning_rate': 1.8351e-06, 'epoch': 0.53, 'throughput': 1177.01}
[INFO|2025-10-22 01:01:34] logging.py:143 >> {'loss': 0.6283, 'learning_rate': 1.8288e-06, 'epoch': 0.53, 'throughput': 1177.01}
[INFO|2025-10-22 01:02:27] logging.py:143 >> {'loss': 0.4812, 'learning_rate': 1.8225e-06, 'epoch': 0.53, 'throughput': 1177.02}
[INFO|2025-10-22 01:03:20] logging.py:143 >> {'loss': 0.6574, 'learning_rate': 1.8161e-06, 'epoch': 0.53, 'throughput': 1177.03}
[INFO|2025-10-22 01:04:13] logging.py:143 >> {'loss': 0.6527, 'learning_rate': 1.8098e-06, 'epoch': 0.53, 'throughput': 1177.03}
[INFO|2025-10-22 01:05:06] logging.py:143 >> {'loss': 0.7267, 'learning_rate': 1.8035e-06, 'epoch': 0.53, 'throughput': 1177.03}
[INFO|2025-10-22 01:05:59] logging.py:143 >> {'loss': 0.7937, 'learning_rate': 1.7972e-06, 'epoch': 0.53, 'throughput': 1177.03}
[INFO|2025-10-22 01:06:52] logging.py:143 >> {'loss': 0.5602, 'learning_rate': 1.7909e-06, 'epoch': 0.53, 'throughput': 1177.03}
[INFO|2025-10-22 01:07:46] logging.py:143 >> {'loss': 0.4656, 'learning_rate': 1.7845e-06, 'epoch': 0.53, 'throughput': 1177.04}
[INFO|2025-10-22 01:08:39] logging.py:143 >> {'loss': 0.4950, 'learning_rate': 1.7782e-06, 'epoch': 0.54, 'throughput': 1177.04}
[INFO|2025-10-22 01:09:32] logging.py:143 >> {'loss': 0.6704, 'learning_rate': 1.7719e-06, 'epoch': 0.54, 'throughput': 1177.04}
[INFO|2025-10-22 01:10:25] logging.py:143 >> {'loss': 0.4880, 'learning_rate': 1.7656e-06, 'epoch': 0.54, 'throughput': 1177.04}
[INFO|2025-10-22 01:11:18] logging.py:143 >> {'loss': 0.6028, 'learning_rate': 1.7593e-06, 'epoch': 0.54, 'throughput': 1177.05}
[INFO|2025-10-22 01:12:11] logging.py:143 >> {'loss': 0.6258, 'learning_rate': 1.7530e-06, 'epoch': 0.54, 'throughput': 1177.05}
[INFO|2025-10-22 01:13:03] logging.py:143 >> {'loss': 0.5702, 'learning_rate': 1.7467e-06, 'epoch': 0.54, 'throughput': 1177.05}
[INFO|2025-10-22 01:13:56] logging.py:143 >> {'loss': 0.4773, 'learning_rate': 1.7404e-06, 'epoch': 0.54, 'throughput': 1177.05}
[INFO|2025-10-22 01:14:50] logging.py:143 >> {'loss': 0.7007, 'learning_rate': 1.7341e-06, 'epoch': 0.54, 'throughput': 1177.06}
[INFO|2025-10-22 01:15:43] logging.py:143 >> {'loss': 0.5401, 'learning_rate': 1.7278e-06, 'epoch': 0.54, 'throughput': 1177.06}
[INFO|2025-10-22 01:16:36] logging.py:143 >> {'loss': 0.6514, 'learning_rate': 1.7215e-06, 'epoch': 0.54, 'throughput': 1177.06}
[INFO|2025-10-22 01:17:30] logging.py:143 >> {'loss': 0.6403, 'learning_rate': 1.7153e-06, 'epoch': 0.55, 'throughput': 1177.07}
[INFO|2025-10-22 01:17:30] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2700
[INFO|2025-10-22 01:17:31] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 01:17:31] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 01:17:31] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2700/chat_template.jinja
[INFO|2025-10-22 01:17:31] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2700/tokenizer_config.json
[INFO|2025-10-22 01:17:31] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2700/special_tokens_map.json
[INFO|2025-10-22 01:18:24] logging.py:143 >> {'loss': 0.6461, 'learning_rate': 1.7090e-06, 'epoch': 0.55, 'throughput': 1177.04}
[INFO|2025-10-22 01:19:17] logging.py:143 >> {'loss': 0.5807, 'learning_rate': 1.7027e-06, 'epoch': 0.55, 'throughput': 1177.04}
[INFO|2025-10-22 01:20:10] logging.py:143 >> {'loss': 0.5659, 'learning_rate': 1.6964e-06, 'epoch': 0.55, 'throughput': 1177.05}
[INFO|2025-10-22 01:21:03] logging.py:143 >> {'loss': 0.5318, 'learning_rate': 1.6901e-06, 'epoch': 0.55, 'throughput': 1177.05}
[INFO|2025-10-22 01:21:56] logging.py:143 >> {'loss': 0.4676, 'learning_rate': 1.6839e-06, 'epoch': 0.55, 'throughput': 1177.06}
[INFO|2025-10-22 01:22:49] logging.py:143 >> {'loss': 0.5864, 'learning_rate': 1.6776e-06, 'epoch': 0.55, 'throughput': 1177.06}
[INFO|2025-10-22 01:23:43] logging.py:143 >> {'loss': 0.7664, 'learning_rate': 1.6713e-06, 'epoch': 0.55, 'throughput': 1177.06}
[INFO|2025-10-22 01:24:35] logging.py:143 >> {'loss': 0.6361, 'learning_rate': 1.6651e-06, 'epoch': 0.55, 'throughput': 1177.05}
[INFO|2025-10-22 01:25:29] logging.py:143 >> {'loss': 0.7617, 'learning_rate': 1.6588e-06, 'epoch': 0.55, 'throughput': 1177.05}
[INFO|2025-10-22 01:26:22] logging.py:143 >> {'loss': 0.5832, 'learning_rate': 1.6526e-06, 'epoch': 0.56, 'throughput': 1177.05}
[INFO|2025-10-22 01:27:16] logging.py:143 >> {'loss': 0.4843, 'learning_rate': 1.6463e-06, 'epoch': 0.56, 'throughput': 1177.06}
[INFO|2025-10-22 01:28:09] logging.py:143 >> {'loss': 0.5711, 'learning_rate': 1.6401e-06, 'epoch': 0.56, 'throughput': 1177.07}
[INFO|2025-10-22 01:29:01] logging.py:143 >> {'loss': 0.6620, 'learning_rate': 1.6338e-06, 'epoch': 0.56, 'throughput': 1177.07}
[INFO|2025-10-22 01:29:55] logging.py:143 >> {'loss': 0.4641, 'learning_rate': 1.6276e-06, 'epoch': 0.56, 'throughput': 1177.07}
[INFO|2025-10-22 01:30:48] logging.py:143 >> {'loss': 0.6824, 'learning_rate': 1.6213e-06, 'epoch': 0.56, 'throughput': 1177.07}
[INFO|2025-10-22 01:31:41] logging.py:143 >> {'loss': 0.5922, 'learning_rate': 1.6151e-06, 'epoch': 0.56, 'throughput': 1177.07}
[INFO|2025-10-22 01:32:34] logging.py:143 >> {'loss': 0.4696, 'learning_rate': 1.6089e-06, 'epoch': 0.56, 'throughput': 1177.07}
[INFO|2025-10-22 01:33:28] logging.py:143 >> {'loss': 0.5547, 'learning_rate': 1.6027e-06, 'epoch': 0.56, 'throughput': 1177.08}
[INFO|2025-10-22 01:34:21] logging.py:143 >> {'loss': 0.6393, 'learning_rate': 1.5964e-06, 'epoch': 0.56, 'throughput': 1177.07}
[INFO|2025-10-22 01:35:14] logging.py:143 >> {'loss': 0.6000, 'learning_rate': 1.5902e-06, 'epoch': 0.57, 'throughput': 1177.07}
[INFO|2025-10-22 01:35:14] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2800
[INFO|2025-10-22 01:35:14] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 01:35:14] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 01:35:14] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2800/chat_template.jinja
[INFO|2025-10-22 01:35:14] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2800/tokenizer_config.json
[INFO|2025-10-22 01:35:14] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2800/special_tokens_map.json
[INFO|2025-10-22 01:36:08] logging.py:143 >> {'loss': 0.7674, 'learning_rate': 1.5840e-06, 'epoch': 0.57, 'throughput': 1177.04}
[INFO|2025-10-22 01:37:01] logging.py:143 >> {'loss': 0.6415, 'learning_rate': 1.5778e-06, 'epoch': 0.57, 'throughput': 1177.04}
[INFO|2025-10-22 01:37:54] logging.py:143 >> {'loss': 0.6078, 'learning_rate': 1.5716e-06, 'epoch': 0.57, 'throughput': 1177.04}
[INFO|2025-10-22 01:38:48] logging.py:143 >> {'loss': 0.5550, 'learning_rate': 1.5654e-06, 'epoch': 0.57, 'throughput': 1177.04}
[INFO|2025-10-22 01:39:40] logging.py:143 >> {'loss': 0.8557, 'learning_rate': 1.5592e-06, 'epoch': 0.57, 'throughput': 1177.04}
[INFO|2025-10-22 01:40:34] logging.py:143 >> {'loss': 0.7050, 'learning_rate': 1.5530e-06, 'epoch': 0.57, 'throughput': 1177.05}
[INFO|2025-10-22 01:41:26] logging.py:143 >> {'loss': 0.6634, 'learning_rate': 1.5468e-06, 'epoch': 0.57, 'throughput': 1177.04}
[INFO|2025-10-22 01:42:19] logging.py:143 >> {'loss': 0.4211, 'learning_rate': 1.5406e-06, 'epoch': 0.57, 'throughput': 1177.05}
[INFO|2025-10-22 01:43:12] logging.py:143 >> {'loss': 0.5592, 'learning_rate': 1.5345e-06, 'epoch': 0.57, 'throughput': 1177.05}
[INFO|2025-10-22 01:44:05] logging.py:143 >> {'loss': 0.5074, 'learning_rate': 1.5283e-06, 'epoch': 0.58, 'throughput': 1177.05}
[INFO|2025-10-22 01:44:58] logging.py:143 >> {'loss': 0.5422, 'learning_rate': 1.5221e-06, 'epoch': 0.58, 'throughput': 1177.05}
[INFO|2025-10-22 01:45:51] logging.py:143 >> {'loss': 0.4820, 'learning_rate': 1.5160e-06, 'epoch': 0.58, 'throughput': 1177.06}
[INFO|2025-10-22 01:46:43] logging.py:143 >> {'loss': 0.5526, 'learning_rate': 1.5098e-06, 'epoch': 0.58, 'throughput': 1177.06}
[INFO|2025-10-22 01:47:36] logging.py:143 >> {'loss': 0.5699, 'learning_rate': 1.5037e-06, 'epoch': 0.58, 'throughput': 1177.06}
[INFO|2025-10-22 01:48:29] logging.py:143 >> {'loss': 0.5045, 'learning_rate': 1.4975e-06, 'epoch': 0.58, 'throughput': 1177.07}
[INFO|2025-10-22 01:49:22] logging.py:143 >> {'loss': 0.5496, 'learning_rate': 1.4914e-06, 'epoch': 0.58, 'throughput': 1177.08}
[INFO|2025-10-22 01:50:15] logging.py:143 >> {'loss': 0.4280, 'learning_rate': 1.4852e-06, 'epoch': 0.58, 'throughput': 1177.09}
[INFO|2025-10-22 01:51:08] logging.py:143 >> {'loss': 0.5565, 'learning_rate': 1.4791e-06, 'epoch': 0.58, 'throughput': 1177.09}
[INFO|2025-10-22 01:52:01] logging.py:143 >> {'loss': 0.4282, 'learning_rate': 1.4730e-06, 'epoch': 0.59, 'throughput': 1177.09}
[INFO|2025-10-22 01:52:54] logging.py:143 >> {'loss': 0.6043, 'learning_rate': 1.4668e-06, 'epoch': 0.59, 'throughput': 1177.09}
[INFO|2025-10-22 01:52:54] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2900
[INFO|2025-10-22 01:52:54] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 01:52:54] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 01:52:54] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2900/chat_template.jinja
[INFO|2025-10-22 01:52:54] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2900/tokenizer_config.json
[INFO|2025-10-22 01:52:54] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-2900/special_tokens_map.json
[INFO|2025-10-22 01:53:48] logging.py:143 >> {'loss': 0.6567, 'learning_rate': 1.4607e-06, 'epoch': 0.59, 'throughput': 1177.06}
[INFO|2025-10-22 01:54:42] logging.py:143 >> {'loss': 0.5063, 'learning_rate': 1.4546e-06, 'epoch': 0.59, 'throughput': 1177.07}
[INFO|2025-10-22 01:55:35] logging.py:143 >> {'loss': 0.6809, 'learning_rate': 1.4485e-06, 'epoch': 0.59, 'throughput': 1177.08}
[INFO|2025-10-22 01:56:28] logging.py:143 >> {'loss': 0.6173, 'learning_rate': 1.4424e-06, 'epoch': 0.59, 'throughput': 1177.07}
[INFO|2025-10-22 01:57:20] logging.py:143 >> {'loss': 0.4493, 'learning_rate': 1.4363e-06, 'epoch': 0.59, 'throughput': 1177.07}
[INFO|2025-10-22 01:58:12] logging.py:143 >> {'loss': 0.6481, 'learning_rate': 1.4302e-06, 'epoch': 0.59, 'throughput': 1177.07}
[INFO|2025-10-22 01:59:05] logging.py:143 >> {'loss': 0.6757, 'learning_rate': 1.4241e-06, 'epoch': 0.59, 'throughput': 1177.06}
[INFO|2025-10-22 01:59:58] logging.py:143 >> {'loss': 0.4957, 'learning_rate': 1.4181e-06, 'epoch': 0.59, 'throughput': 1177.07}
[INFO|2025-10-22 02:00:51] logging.py:143 >> {'loss': 0.6078, 'learning_rate': 1.4120e-06, 'epoch': 0.60, 'throughput': 1177.07}
[INFO|2025-10-22 02:01:44] logging.py:143 >> {'loss': 0.5363, 'learning_rate': 1.4059e-06, 'epoch': 0.60, 'throughput': 1177.07}
[INFO|2025-10-22 02:02:37] logging.py:143 >> {'loss': 0.6405, 'learning_rate': 1.3999e-06, 'epoch': 0.60, 'throughput': 1177.08}
[INFO|2025-10-22 02:03:31] logging.py:143 >> {'loss': 0.6563, 'learning_rate': 1.3938e-06, 'epoch': 0.60, 'throughput': 1177.08}
[INFO|2025-10-22 02:04:24] logging.py:143 >> {'loss': 0.5570, 'learning_rate': 1.3878e-06, 'epoch': 0.60, 'throughput': 1177.08}
[INFO|2025-10-22 02:05:16] logging.py:143 >> {'loss': 0.5426, 'learning_rate': 1.3817e-06, 'epoch': 0.60, 'throughput': 1177.08}
[INFO|2025-10-22 02:06:10] logging.py:143 >> {'loss': 0.6128, 'learning_rate': 1.3757e-06, 'epoch': 0.60, 'throughput': 1177.08}
[INFO|2025-10-22 02:07:03] logging.py:143 >> {'loss': 0.4926, 'learning_rate': 1.3697e-06, 'epoch': 0.60, 'throughput': 1177.09}
[INFO|2025-10-22 02:07:56] logging.py:143 >> {'loss': 0.6319, 'learning_rate': 1.3636e-06, 'epoch': 0.60, 'throughput': 1177.09}
[INFO|2025-10-22 02:08:49] logging.py:143 >> {'loss': 0.5711, 'learning_rate': 1.3576e-06, 'epoch': 0.60, 'throughput': 1177.09}
[INFO|2025-10-22 02:09:43] logging.py:143 >> {'loss': 0.4774, 'learning_rate': 1.3516e-06, 'epoch': 0.61, 'throughput': 1177.10}
[INFO|2025-10-22 02:10:37] logging.py:143 >> {'loss': 0.4931, 'learning_rate': 1.3456e-06, 'epoch': 0.61, 'throughput': 1177.11}
[INFO|2025-10-22 02:10:37] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3000
[INFO|2025-10-22 02:10:37] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 02:10:37] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 02:10:37] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3000/chat_template.jinja
[INFO|2025-10-22 02:10:37] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3000/tokenizer_config.json
[INFO|2025-10-22 02:10:37] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3000/special_tokens_map.json
[INFO|2025-10-22 02:11:30] logging.py:143 >> {'loss': 0.7700, 'learning_rate': 1.3396e-06, 'epoch': 0.61, 'throughput': 1177.08}
[INFO|2025-10-22 02:12:24] logging.py:143 >> {'loss': 0.5696, 'learning_rate': 1.3336e-06, 'epoch': 0.61, 'throughput': 1177.08}
[INFO|2025-10-22 02:13:17] logging.py:143 >> {'loss': 0.5238, 'learning_rate': 1.3276e-06, 'epoch': 0.61, 'throughput': 1177.09}
[INFO|2025-10-22 02:14:10] logging.py:143 >> {'loss': 0.7487, 'learning_rate': 1.3217e-06, 'epoch': 0.61, 'throughput': 1177.09}
[INFO|2025-10-22 02:15:03] logging.py:143 >> {'loss': 0.6147, 'learning_rate': 1.3157e-06, 'epoch': 0.61, 'throughput': 1177.10}
[INFO|2025-10-22 02:15:55] logging.py:143 >> {'loss': 0.6934, 'learning_rate': 1.3097e-06, 'epoch': 0.61, 'throughput': 1177.10}
[INFO|2025-10-22 02:16:49] logging.py:143 >> {'loss': 0.5265, 'learning_rate': 1.3038e-06, 'epoch': 0.61, 'throughput': 1177.10}
[INFO|2025-10-22 02:17:42] logging.py:143 >> {'loss': 0.4684, 'learning_rate': 1.2978e-06, 'epoch': 0.61, 'throughput': 1177.11}
[INFO|2025-10-22 02:18:35] logging.py:143 >> {'loss': 0.5557, 'learning_rate': 1.2919e-06, 'epoch': 0.62, 'throughput': 1177.12}
[INFO|2025-10-22 02:19:28] logging.py:143 >> {'loss': 0.5556, 'learning_rate': 1.2860e-06, 'epoch': 0.62, 'throughput': 1177.12}
[INFO|2025-10-22 02:20:21] logging.py:143 >> {'loss': 0.5822, 'learning_rate': 1.2800e-06, 'epoch': 0.62, 'throughput': 1177.12}
[INFO|2025-10-22 02:21:14] logging.py:143 >> {'loss': 0.5602, 'learning_rate': 1.2741e-06, 'epoch': 0.62, 'throughput': 1177.13}
[INFO|2025-10-22 02:22:08] logging.py:143 >> {'loss': 0.4053, 'learning_rate': 1.2682e-06, 'epoch': 0.62, 'throughput': 1177.13}
[INFO|2025-10-22 02:23:01] logging.py:143 >> {'loss': 0.5106, 'learning_rate': 1.2623e-06, 'epoch': 0.62, 'throughput': 1177.13}
[INFO|2025-10-22 02:23:55] logging.py:143 >> {'loss': 0.5898, 'learning_rate': 1.2564e-06, 'epoch': 0.62, 'throughput': 1177.14}
[INFO|2025-10-22 02:24:47] logging.py:143 >> {'loss': 0.6197, 'learning_rate': 1.2505e-06, 'epoch': 0.62, 'throughput': 1177.14}
[INFO|2025-10-22 02:25:41] logging.py:143 >> {'loss': 0.4366, 'learning_rate': 1.2446e-06, 'epoch': 0.62, 'throughput': 1177.15}
[INFO|2025-10-22 02:26:34] logging.py:143 >> {'loss': 0.5344, 'learning_rate': 1.2387e-06, 'epoch': 0.62, 'throughput': 1177.15}
[INFO|2025-10-22 02:27:28] logging.py:143 >> {'loss': 0.7400, 'learning_rate': 1.2329e-06, 'epoch': 0.63, 'throughput': 1177.15}
[INFO|2025-10-22 02:28:21] logging.py:143 >> {'loss': 0.4148, 'learning_rate': 1.2270e-06, 'epoch': 0.63, 'throughput': 1177.15}
[INFO|2025-10-22 02:28:21] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3100
[INFO|2025-10-22 02:28:21] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 02:28:21] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 02:28:21] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3100/chat_template.jinja
[INFO|2025-10-22 02:28:21] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3100/tokenizer_config.json
[INFO|2025-10-22 02:28:21] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3100/special_tokens_map.json
[INFO|2025-10-22 02:29:14] logging.py:143 >> {'loss': 0.5945, 'learning_rate': 1.2212e-06, 'epoch': 0.63, 'throughput': 1177.12}
[INFO|2025-10-22 02:30:07] logging.py:143 >> {'loss': 0.4828, 'learning_rate': 1.2153e-06, 'epoch': 0.63, 'throughput': 1177.12}
[INFO|2025-10-22 02:31:00] logging.py:143 >> {'loss': 0.4927, 'learning_rate': 1.2095e-06, 'epoch': 0.63, 'throughput': 1177.13}
[INFO|2025-10-22 02:31:54] logging.py:143 >> {'loss': 0.6355, 'learning_rate': 1.2037e-06, 'epoch': 0.63, 'throughput': 1177.13}
[INFO|2025-10-22 02:32:47] logging.py:143 >> {'loss': 0.4743, 'learning_rate': 1.1978e-06, 'epoch': 0.63, 'throughput': 1177.14}
[INFO|2025-10-22 02:33:40] logging.py:143 >> {'loss': 0.5942, 'learning_rate': 1.1920e-06, 'epoch': 0.63, 'throughput': 1177.14}
[INFO|2025-10-22 02:34:33] logging.py:143 >> {'loss': 0.5569, 'learning_rate': 1.1862e-06, 'epoch': 0.63, 'throughput': 1177.14}
[INFO|2025-10-22 02:35:26] logging.py:143 >> {'loss': 0.6141, 'learning_rate': 1.1804e-06, 'epoch': 0.63, 'throughput': 1177.15}
[INFO|2025-10-22 02:36:20] logging.py:143 >> {'loss': 0.5607, 'learning_rate': 1.1746e-06, 'epoch': 0.64, 'throughput': 1177.15}
[INFO|2025-10-22 02:37:12] logging.py:143 >> {'loss': 0.6258, 'learning_rate': 1.1689e-06, 'epoch': 0.64, 'throughput': 1177.15}
[INFO|2025-10-22 02:38:05] logging.py:143 >> {'loss': 0.7759, 'learning_rate': 1.1631e-06, 'epoch': 0.64, 'throughput': 1177.15}
[INFO|2025-10-22 02:38:59] logging.py:143 >> {'loss': 0.7127, 'learning_rate': 1.1573e-06, 'epoch': 0.64, 'throughput': 1177.16}
[INFO|2025-10-22 02:39:52] logging.py:143 >> {'loss': 0.5112, 'learning_rate': 1.1516e-06, 'epoch': 0.64, 'throughput': 1177.16}
[INFO|2025-10-22 02:40:44] logging.py:143 >> {'loss': 0.5623, 'learning_rate': 1.1458e-06, 'epoch': 0.64, 'throughput': 1177.16}
[INFO|2025-10-22 02:41:37] logging.py:143 >> {'loss': 0.5660, 'learning_rate': 1.1401e-06, 'epoch': 0.64, 'throughput': 1177.17}
[INFO|2025-10-22 02:42:30] logging.py:143 >> {'loss': 0.6051, 'learning_rate': 1.1344e-06, 'epoch': 0.64, 'throughput': 1177.17}
[INFO|2025-10-22 02:43:24] logging.py:143 >> {'loss': 0.5994, 'learning_rate': 1.1286e-06, 'epoch': 0.64, 'throughput': 1177.17}
[INFO|2025-10-22 02:44:17] logging.py:143 >> {'loss': 0.4855, 'learning_rate': 1.1229e-06, 'epoch': 0.64, 'throughput': 1177.18}
[INFO|2025-10-22 02:45:10] logging.py:143 >> {'loss': 0.6668, 'learning_rate': 1.1172e-06, 'epoch': 0.65, 'throughput': 1177.18}
[INFO|2025-10-22 02:46:04] logging.py:143 >> {'loss': 0.6289, 'learning_rate': 1.1115e-06, 'epoch': 0.65, 'throughput': 1177.19}
[INFO|2025-10-22 02:46:04] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3200
[INFO|2025-10-22 02:46:04] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 02:46:04] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 02:46:04] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3200/chat_template.jinja
[INFO|2025-10-22 02:46:04] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3200/tokenizer_config.json
[INFO|2025-10-22 02:46:04] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3200/special_tokens_map.json
[INFO|2025-10-22 02:46:57] logging.py:143 >> {'loss': 0.6470, 'learning_rate': 1.1058e-06, 'epoch': 0.65, 'throughput': 1177.16}
[INFO|2025-10-22 02:47:50] logging.py:143 >> {'loss': 0.5822, 'learning_rate': 1.1002e-06, 'epoch': 0.65, 'throughput': 1177.17}
[INFO|2025-10-22 02:48:44] logging.py:143 >> {'loss': 0.3869, 'learning_rate': 1.0945e-06, 'epoch': 0.65, 'throughput': 1177.17}
[INFO|2025-10-22 02:49:37] logging.py:143 >> {'loss': 0.5606, 'learning_rate': 1.0889e-06, 'epoch': 0.65, 'throughput': 1177.18}
[INFO|2025-10-22 02:50:30] logging.py:143 >> {'loss': 0.5528, 'learning_rate': 1.0832e-06, 'epoch': 0.65, 'throughput': 1177.17}
[INFO|2025-10-22 02:51:24] logging.py:143 >> {'loss': 0.4293, 'learning_rate': 1.0776e-06, 'epoch': 0.65, 'throughput': 1177.18}
[INFO|2025-10-22 02:52:18] logging.py:143 >> {'loss': 0.6395, 'learning_rate': 1.0719e-06, 'epoch': 0.65, 'throughput': 1177.19}
[INFO|2025-10-22 02:53:12] logging.py:143 >> {'loss': 0.6193, 'learning_rate': 1.0663e-06, 'epoch': 0.65, 'throughput': 1177.20}
[INFO|2025-10-22 02:54:05] logging.py:143 >> {'loss': 0.6678, 'learning_rate': 1.0607e-06, 'epoch': 0.66, 'throughput': 1177.20}
[INFO|2025-10-22 02:54:58] logging.py:143 >> {'loss': 0.4631, 'learning_rate': 1.0551e-06, 'epoch': 0.66, 'throughput': 1177.20}
[INFO|2025-10-22 02:55:51] logging.py:143 >> {'loss': 0.6347, 'learning_rate': 1.0495e-06, 'epoch': 0.66, 'throughput': 1177.20}
[INFO|2025-10-22 02:56:44] logging.py:143 >> {'loss': 0.5643, 'learning_rate': 1.0439e-06, 'epoch': 0.66, 'throughput': 1177.20}
[INFO|2025-10-22 02:57:38] logging.py:143 >> {'loss': 0.5373, 'learning_rate': 1.0384e-06, 'epoch': 0.66, 'throughput': 1177.20}
[INFO|2025-10-22 02:58:31] logging.py:143 >> {'loss': 0.6229, 'learning_rate': 1.0328e-06, 'epoch': 0.66, 'throughput': 1177.20}
[INFO|2025-10-22 02:59:24] logging.py:143 >> {'loss': 0.5524, 'learning_rate': 1.0272e-06, 'epoch': 0.66, 'throughput': 1177.20}
[INFO|2025-10-22 03:00:17] logging.py:143 >> {'loss': 0.5614, 'learning_rate': 1.0217e-06, 'epoch': 0.66, 'throughput': 1177.20}
[INFO|2025-10-22 03:01:11] logging.py:143 >> {'loss': 0.6903, 'learning_rate': 1.0162e-06, 'epoch': 0.66, 'throughput': 1177.21}
[INFO|2025-10-22 03:02:04] logging.py:143 >> {'loss': 0.7459, 'learning_rate': 1.0106e-06, 'epoch': 0.66, 'throughput': 1177.21}
[INFO|2025-10-22 03:02:56] logging.py:143 >> {'loss': 0.5193, 'learning_rate': 1.0051e-06, 'epoch': 0.67, 'throughput': 1177.21}
[INFO|2025-10-22 03:03:50] logging.py:143 >> {'loss': 0.4636, 'learning_rate': 9.9963e-07, 'epoch': 0.67, 'throughput': 1177.21}
[INFO|2025-10-22 03:03:50] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3300
[INFO|2025-10-22 03:03:50] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 03:03:50] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 03:03:50] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3300/chat_template.jinja
[INFO|2025-10-22 03:03:50] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3300/tokenizer_config.json
[INFO|2025-10-22 03:03:50] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3300/special_tokens_map.json
[INFO|2025-10-22 03:04:44] logging.py:143 >> {'loss': 0.6987, 'learning_rate': 9.9414e-07, 'epoch': 0.67, 'throughput': 1177.19}
[INFO|2025-10-22 03:05:37] logging.py:143 >> {'loss': 0.5839, 'learning_rate': 9.8866e-07, 'epoch': 0.67, 'throughput': 1177.19}
[INFO|2025-10-22 03:06:30] logging.py:143 >> {'loss': 0.4932, 'learning_rate': 9.8319e-07, 'epoch': 0.67, 'throughput': 1177.19}
[INFO|2025-10-22 03:07:23] logging.py:143 >> {'loss': 0.5181, 'learning_rate': 9.7772e-07, 'epoch': 0.67, 'throughput': 1177.19}
[INFO|2025-10-22 03:08:16] logging.py:143 >> {'loss': 0.6788, 'learning_rate': 9.7227e-07, 'epoch': 0.67, 'throughput': 1177.18}
[INFO|2025-10-22 03:09:10] logging.py:143 >> {'loss': 0.5096, 'learning_rate': 9.6683e-07, 'epoch': 0.67, 'throughput': 1177.19}
[INFO|2025-10-22 03:10:04] logging.py:143 >> {'loss': 0.5630, 'learning_rate': 9.6140e-07, 'epoch': 0.67, 'throughput': 1177.20}
[INFO|2025-10-22 03:10:57] logging.py:143 >> {'loss': 0.5230, 'learning_rate': 9.5598e-07, 'epoch': 0.68, 'throughput': 1177.20}
[INFO|2025-10-22 03:11:51] logging.py:143 >> {'loss': 0.5946, 'learning_rate': 9.5057e-07, 'epoch': 0.68, 'throughput': 1177.21}
[INFO|2025-10-22 03:12:44] logging.py:143 >> {'loss': 0.4511, 'learning_rate': 9.4517e-07, 'epoch': 0.68, 'throughput': 1177.21}
[INFO|2025-10-22 03:13:37] logging.py:143 >> {'loss': 0.5872, 'learning_rate': 9.3978e-07, 'epoch': 0.68, 'throughput': 1177.21}
[INFO|2025-10-22 03:14:30] logging.py:143 >> {'loss': 0.4942, 'learning_rate': 9.3440e-07, 'epoch': 0.68, 'throughput': 1177.21}
[INFO|2025-10-22 03:15:23] logging.py:143 >> {'loss': 0.6981, 'learning_rate': 9.2903e-07, 'epoch': 0.68, 'throughput': 1177.21}
[INFO|2025-10-22 03:16:17] logging.py:143 >> {'loss': 0.4556, 'learning_rate': 9.2368e-07, 'epoch': 0.68, 'throughput': 1177.22}
[INFO|2025-10-22 03:17:10] logging.py:143 >> {'loss': 0.5022, 'learning_rate': 9.1833e-07, 'epoch': 0.68, 'throughput': 1177.22}
[INFO|2025-10-22 03:18:04] logging.py:143 >> {'loss': 0.6896, 'learning_rate': 9.1299e-07, 'epoch': 0.68, 'throughput': 1177.23}
[INFO|2025-10-22 03:18:57] logging.py:143 >> {'loss': 0.4633, 'learning_rate': 9.0767e-07, 'epoch': 0.68, 'throughput': 1177.23}
[INFO|2025-10-22 03:19:50] logging.py:143 >> {'loss': 0.5480, 'learning_rate': 9.0236e-07, 'epoch': 0.69, 'throughput': 1177.23}
[INFO|2025-10-22 03:20:43] logging.py:143 >> {'loss': 0.6359, 'learning_rate': 8.9706e-07, 'epoch': 0.69, 'throughput': 1177.23}
[INFO|2025-10-22 03:21:36] logging.py:143 >> {'loss': 0.5933, 'learning_rate': 8.9176e-07, 'epoch': 0.69, 'throughput': 1177.23}
[INFO|2025-10-22 03:21:36] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3400
[INFO|2025-10-22 03:21:36] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 03:21:36] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 03:21:36] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3400/chat_template.jinja
[INFO|2025-10-22 03:21:36] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3400/tokenizer_config.json
[INFO|2025-10-22 03:21:36] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3400/special_tokens_map.json
[INFO|2025-10-22 03:22:29] logging.py:143 >> {'loss': 0.5045, 'learning_rate': 8.8649e-07, 'epoch': 0.69, 'throughput': 1177.21}
[INFO|2025-10-22 03:23:22] logging.py:143 >> {'loss': 0.4488, 'learning_rate': 8.8122e-07, 'epoch': 0.69, 'throughput': 1177.21}
[INFO|2025-10-22 03:24:16] logging.py:143 >> {'loss': 0.5744, 'learning_rate': 8.7596e-07, 'epoch': 0.69, 'throughput': 1177.21}
[INFO|2025-10-22 03:25:08] logging.py:143 >> {'loss': 0.6160, 'learning_rate': 8.7071e-07, 'epoch': 0.69, 'throughput': 1177.21}
[INFO|2025-10-22 03:26:01] logging.py:143 >> {'loss': 0.4614, 'learning_rate': 8.6548e-07, 'epoch': 0.69, 'throughput': 1177.21}
[INFO|2025-10-22 03:26:54] logging.py:143 >> {'loss': 0.4661, 'learning_rate': 8.6026e-07, 'epoch': 0.69, 'throughput': 1177.21}
[INFO|2025-10-22 03:27:47] logging.py:143 >> {'loss': 0.6438, 'learning_rate': 8.5504e-07, 'epoch': 0.69, 'throughput': 1177.22}
[INFO|2025-10-22 03:28:41] logging.py:143 >> {'loss': 0.5017, 'learning_rate': 8.4984e-07, 'epoch': 0.70, 'throughput': 1177.22}
[INFO|2025-10-22 03:29:34] logging.py:143 >> {'loss': 0.5430, 'learning_rate': 8.4466e-07, 'epoch': 0.70, 'throughput': 1177.22}
[INFO|2025-10-22 03:30:27] logging.py:143 >> {'loss': 0.6309, 'learning_rate': 8.3948e-07, 'epoch': 0.70, 'throughput': 1177.23}
[INFO|2025-10-22 03:31:20] logging.py:143 >> {'loss': 0.6376, 'learning_rate': 8.3431e-07, 'epoch': 0.70, 'throughput': 1177.23}
[INFO|2025-10-22 03:32:13] logging.py:143 >> {'loss': 0.5828, 'learning_rate': 8.2916e-07, 'epoch': 0.70, 'throughput': 1177.23}
[INFO|2025-10-22 03:33:06] logging.py:143 >> {'loss': 0.5697, 'learning_rate': 8.2402e-07, 'epoch': 0.70, 'throughput': 1177.23}
[INFO|2025-10-22 03:34:00] logging.py:143 >> {'loss': 0.6335, 'learning_rate': 8.1889e-07, 'epoch': 0.70, 'throughput': 1177.24}
[INFO|2025-10-22 03:34:53] logging.py:143 >> {'loss': 0.7627, 'learning_rate': 8.1377e-07, 'epoch': 0.70, 'throughput': 1177.24}
[INFO|2025-10-22 03:35:46] logging.py:143 >> {'loss': 0.6470, 'learning_rate': 8.0867e-07, 'epoch': 0.70, 'throughput': 1177.24}
[INFO|2025-10-22 03:36:39] logging.py:143 >> {'loss': 0.6263, 'learning_rate': 8.0357e-07, 'epoch': 0.70, 'throughput': 1177.24}
[INFO|2025-10-22 03:37:33] logging.py:143 >> {'loss': 0.5495, 'learning_rate': 7.9849e-07, 'epoch': 0.71, 'throughput': 1177.25}
[INFO|2025-10-22 03:38:27] logging.py:143 >> {'loss': 0.4383, 'learning_rate': 7.9342e-07, 'epoch': 0.71, 'throughput': 1177.25}
[INFO|2025-10-22 03:39:20] logging.py:143 >> {'loss': 0.6495, 'learning_rate': 7.8836e-07, 'epoch': 0.71, 'throughput': 1177.26}
[INFO|2025-10-22 03:39:20] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3500
[INFO|2025-10-22 03:39:20] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 03:39:20] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 03:39:20] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3500/chat_template.jinja
[INFO|2025-10-22 03:39:20] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3500/tokenizer_config.json
[INFO|2025-10-22 03:39:20] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3500/special_tokens_map.json
[INFO|2025-10-22 03:40:14] logging.py:143 >> {'loss': 0.5638, 'learning_rate': 7.8332e-07, 'epoch': 0.71, 'throughput': 1177.24}
[INFO|2025-10-22 03:41:07] logging.py:143 >> {'loss': 0.6604, 'learning_rate': 7.7828e-07, 'epoch': 0.71, 'throughput': 1177.24}
[INFO|2025-10-22 03:42:01] logging.py:143 >> {'loss': 0.6172, 'learning_rate': 7.7326e-07, 'epoch': 0.71, 'throughput': 1177.24}
[INFO|2025-10-22 03:42:54] logging.py:143 >> {'loss': 0.7003, 'learning_rate': 7.6825e-07, 'epoch': 0.71, 'throughput': 1177.25}
[INFO|2025-10-22 03:43:48] logging.py:143 >> {'loss': 0.4702, 'learning_rate': 7.6326e-07, 'epoch': 0.71, 'throughput': 1177.25}
[INFO|2025-10-22 03:44:42] logging.py:143 >> {'loss': 0.4441, 'learning_rate': 7.5828e-07, 'epoch': 0.71, 'throughput': 1177.26}
[INFO|2025-10-22 03:45:34] logging.py:143 >> {'loss': 0.5483, 'learning_rate': 7.5330e-07, 'epoch': 0.71, 'throughput': 1177.26}
[INFO|2025-10-22 03:46:28] logging.py:143 >> {'loss': 0.7368, 'learning_rate': 7.4835e-07, 'epoch': 0.72, 'throughput': 1177.27}
[INFO|2025-10-22 03:47:21] logging.py:143 >> {'loss': 0.6301, 'learning_rate': 7.4340e-07, 'epoch': 0.72, 'throughput': 1177.27}
[INFO|2025-10-22 03:48:14] logging.py:143 >> {'loss': 0.7542, 'learning_rate': 7.3847e-07, 'epoch': 0.72, 'throughput': 1177.28}
[INFO|2025-10-22 03:49:08] logging.py:143 >> {'loss': 0.6789, 'learning_rate': 7.3355e-07, 'epoch': 0.72, 'throughput': 1177.28}
[INFO|2025-10-22 03:50:01] logging.py:143 >> {'loss': 0.6683, 'learning_rate': 7.2864e-07, 'epoch': 0.72, 'throughput': 1177.29}
[INFO|2025-10-22 03:50:54] logging.py:143 >> {'loss': 0.5172, 'learning_rate': 7.2374e-07, 'epoch': 0.72, 'throughput': 1177.28}
[INFO|2025-10-22 03:51:46] logging.py:143 >> {'loss': 0.4798, 'learning_rate': 7.1886e-07, 'epoch': 0.72, 'throughput': 1177.28}
[INFO|2025-10-22 03:52:39] logging.py:143 >> {'loss': 0.7190, 'learning_rate': 7.1399e-07, 'epoch': 0.72, 'throughput': 1177.29}
[INFO|2025-10-22 03:53:32] logging.py:143 >> {'loss': 0.4626, 'learning_rate': 7.0914e-07, 'epoch': 0.72, 'throughput': 1177.29}
[INFO|2025-10-22 03:54:25] logging.py:143 >> {'loss': 0.5956, 'learning_rate': 7.0429e-07, 'epoch': 0.72, 'throughput': 1177.29}
[INFO|2025-10-22 03:55:18] logging.py:143 >> {'loss': 0.4898, 'learning_rate': 6.9946e-07, 'epoch': 0.73, 'throughput': 1177.29}
[INFO|2025-10-22 03:56:11] logging.py:143 >> {'loss': 0.6703, 'learning_rate': 6.9465e-07, 'epoch': 0.73, 'throughput': 1177.29}
[INFO|2025-10-22 03:57:05] logging.py:143 >> {'loss': 0.4853, 'learning_rate': 6.8984e-07, 'epoch': 0.73, 'throughput': 1177.30}
[INFO|2025-10-22 03:57:05] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3600
[INFO|2025-10-22 03:57:05] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 03:57:05] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 03:57:06] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3600/chat_template.jinja
[INFO|2025-10-22 03:57:06] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3600/tokenizer_config.json
[INFO|2025-10-22 03:57:06] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3600/special_tokens_map.json
[INFO|2025-10-22 03:58:00] logging.py:143 >> {'loss': 0.7032, 'learning_rate': 6.8505e-07, 'epoch': 0.73, 'throughput': 1177.28}
[INFO|2025-10-22 03:58:52] logging.py:143 >> {'loss': 0.5463, 'learning_rate': 6.8027e-07, 'epoch': 0.73, 'throughput': 1177.28}
[INFO|2025-10-22 03:59:45] logging.py:143 >> {'loss': 0.6482, 'learning_rate': 6.7551e-07, 'epoch': 0.73, 'throughput': 1177.28}
[INFO|2025-10-22 04:00:38] logging.py:143 >> {'loss': 0.4826, 'learning_rate': 6.7076e-07, 'epoch': 0.73, 'throughput': 1177.27}
[INFO|2025-10-22 04:01:32] logging.py:143 >> {'loss': 0.6016, 'learning_rate': 6.6602e-07, 'epoch': 0.73, 'throughput': 1177.27}
[INFO|2025-10-22 04:02:25] logging.py:143 >> {'loss': 0.6111, 'learning_rate': 6.6130e-07, 'epoch': 0.73, 'throughput': 1177.28}
[INFO|2025-10-22 04:03:18] logging.py:143 >> {'loss': 0.5748, 'learning_rate': 6.5659e-07, 'epoch': 0.73, 'throughput': 1177.28}
[INFO|2025-10-22 04:04:11] logging.py:143 >> {'loss': 0.4465, 'learning_rate': 6.5189e-07, 'epoch': 0.74, 'throughput': 1177.28}
[INFO|2025-10-22 04:05:04] logging.py:143 >> {'loss': 0.5930, 'learning_rate': 6.4721e-07, 'epoch': 0.74, 'throughput': 1177.28}
[INFO|2025-10-22 04:05:57] logging.py:143 >> {'loss': 0.4238, 'learning_rate': 6.4254e-07, 'epoch': 0.74, 'throughput': 1177.29}
[INFO|2025-10-22 04:06:50] logging.py:143 >> {'loss': 0.5303, 'learning_rate': 6.3788e-07, 'epoch': 0.74, 'throughput': 1177.29}
[INFO|2025-10-22 04:07:43] logging.py:143 >> {'loss': 0.5094, 'learning_rate': 6.3324e-07, 'epoch': 0.74, 'throughput': 1177.29}
[INFO|2025-10-22 04:08:36] logging.py:143 >> {'loss': 0.4835, 'learning_rate': 6.2861e-07, 'epoch': 0.74, 'throughput': 1177.30}
[INFO|2025-10-22 04:09:29] logging.py:143 >> {'loss': 0.4257, 'learning_rate': 6.2400e-07, 'epoch': 0.74, 'throughput': 1177.30}
[INFO|2025-10-22 04:10:22] logging.py:143 >> {'loss': 0.5495, 'learning_rate': 6.1940e-07, 'epoch': 0.74, 'throughput': 1177.30}
[INFO|2025-10-22 04:11:15] logging.py:143 >> {'loss': 0.5801, 'learning_rate': 6.1481e-07, 'epoch': 0.74, 'throughput': 1177.29}
[INFO|2025-10-22 04:12:08] logging.py:143 >> {'loss': 0.5340, 'learning_rate': 6.1024e-07, 'epoch': 0.74, 'throughput': 1177.29}
[INFO|2025-10-22 04:13:01] logging.py:143 >> {'loss': 0.6669, 'learning_rate': 6.0568e-07, 'epoch': 0.75, 'throughput': 1177.30}
[INFO|2025-10-22 04:13:55] logging.py:143 >> {'loss': 0.6736, 'learning_rate': 6.0113e-07, 'epoch': 0.75, 'throughput': 1177.30}
[INFO|2025-10-22 04:14:49] logging.py:143 >> {'loss': 0.5876, 'learning_rate': 5.9660e-07, 'epoch': 0.75, 'throughput': 1177.30}
[INFO|2025-10-22 04:14:49] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3700
[INFO|2025-10-22 04:14:49] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 04:14:49] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 04:14:49] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3700/chat_template.jinja
[INFO|2025-10-22 04:14:49] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3700/tokenizer_config.json
[INFO|2025-10-22 04:14:49] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3700/special_tokens_map.json
[INFO|2025-10-22 04:15:43] logging.py:143 >> {'loss': 0.5805, 'learning_rate': 5.9209e-07, 'epoch': 0.75, 'throughput': 1177.28}
[INFO|2025-10-22 04:16:36] logging.py:143 >> {'loss': 0.4498, 'learning_rate': 5.8758e-07, 'epoch': 0.75, 'throughput': 1177.29}
[INFO|2025-10-22 04:17:29] logging.py:143 >> {'loss': 0.4882, 'learning_rate': 5.8310e-07, 'epoch': 0.75, 'throughput': 1177.29}
[INFO|2025-10-22 04:18:22] logging.py:143 >> {'loss': 0.8826, 'learning_rate': 5.7862e-07, 'epoch': 0.75, 'throughput': 1177.28}
[INFO|2025-10-22 04:19:14] logging.py:143 >> {'loss': 0.5845, 'learning_rate': 5.7416e-07, 'epoch': 0.75, 'throughput': 1177.28}
[INFO|2025-10-22 04:20:07] logging.py:143 >> {'loss': 0.6568, 'learning_rate': 5.6972e-07, 'epoch': 0.75, 'throughput': 1177.29}
[INFO|2025-10-22 04:21:00] logging.py:143 >> {'loss': 0.6739, 'learning_rate': 5.6529e-07, 'epoch': 0.75, 'throughput': 1177.29}
[INFO|2025-10-22 04:21:53] logging.py:143 >> {'loss': 0.5068, 'learning_rate': 5.6087e-07, 'epoch': 0.76, 'throughput': 1177.29}
[INFO|2025-10-22 04:22:46] logging.py:143 >> {'loss': 0.4966, 'learning_rate': 5.5647e-07, 'epoch': 0.76, 'throughput': 1177.29}
[INFO|2025-10-22 04:23:39] logging.py:143 >> {'loss': 0.4549, 'learning_rate': 5.5208e-07, 'epoch': 0.76, 'throughput': 1177.29}
[INFO|2025-10-22 04:24:32] logging.py:143 >> {'loss': 0.5349, 'learning_rate': 5.4771e-07, 'epoch': 0.76, 'throughput': 1177.29}
[INFO|2025-10-22 04:25:25] logging.py:143 >> {'loss': 0.5587, 'learning_rate': 5.4335e-07, 'epoch': 0.76, 'throughput': 1177.30}
[INFO|2025-10-22 04:26:18] logging.py:143 >> {'loss': 0.6844, 'learning_rate': 5.3901e-07, 'epoch': 0.76, 'throughput': 1177.30}
[INFO|2025-10-22 04:27:11] logging.py:143 >> {'loss': 0.6869, 'learning_rate': 5.3468e-07, 'epoch': 0.76, 'throughput': 1177.30}
[INFO|2025-10-22 04:28:05] logging.py:143 >> {'loss': 0.5923, 'learning_rate': 5.3036e-07, 'epoch': 0.76, 'throughput': 1177.30}
[INFO|2025-10-22 04:28:58] logging.py:143 >> {'loss': 0.5379, 'learning_rate': 5.2607e-07, 'epoch': 0.76, 'throughput': 1177.30}
[INFO|2025-10-22 04:29:51] logging.py:143 >> {'loss': 0.5754, 'learning_rate': 5.2178e-07, 'epoch': 0.76, 'throughput': 1177.30}
[INFO|2025-10-22 04:30:44] logging.py:143 >> {'loss': 0.5757, 'learning_rate': 5.1751e-07, 'epoch': 0.77, 'throughput': 1177.30}
[INFO|2025-10-22 04:31:38] logging.py:143 >> {'loss': 0.4428, 'learning_rate': 5.1326e-07, 'epoch': 0.77, 'throughput': 1177.30}
[INFO|2025-10-22 04:32:31] logging.py:143 >> {'loss': 0.5145, 'learning_rate': 5.0902e-07, 'epoch': 0.77, 'throughput': 1177.30}
[INFO|2025-10-22 04:32:31] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3800
[INFO|2025-10-22 04:32:31] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 04:32:31] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 04:32:31] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3800/chat_template.jinja
[INFO|2025-10-22 04:32:31] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3800/tokenizer_config.json
[INFO|2025-10-22 04:32:31] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3800/special_tokens_map.json
[INFO|2025-10-22 04:33:24] logging.py:143 >> {'loss': 0.5368, 'learning_rate': 5.0479e-07, 'epoch': 0.77, 'throughput': 1177.28}
[INFO|2025-10-22 04:34:17] logging.py:143 >> {'loss': 0.6496, 'learning_rate': 5.0058e-07, 'epoch': 0.77, 'throughput': 1177.27}
[INFO|2025-10-22 04:35:10] logging.py:143 >> {'loss': 0.6123, 'learning_rate': 4.9639e-07, 'epoch': 0.77, 'throughput': 1177.28}
[INFO|2025-10-22 04:36:03] logging.py:143 >> {'loss': 0.4325, 'learning_rate': 4.9221e-07, 'epoch': 0.77, 'throughput': 1177.28}
[INFO|2025-10-22 04:36:56] logging.py:143 >> {'loss': 0.6780, 'learning_rate': 4.8805e-07, 'epoch': 0.77, 'throughput': 1177.28}
[INFO|2025-10-22 04:37:49] logging.py:143 >> {'loss': 0.5607, 'learning_rate': 4.8390e-07, 'epoch': 0.77, 'throughput': 1177.28}
[INFO|2025-10-22 04:38:42] logging.py:143 >> {'loss': 0.6358, 'learning_rate': 4.7977e-07, 'epoch': 0.78, 'throughput': 1177.29}
[INFO|2025-10-22 04:39:34] logging.py:143 >> {'loss': 0.6347, 'learning_rate': 4.7565e-07, 'epoch': 0.78, 'throughput': 1177.29}
[INFO|2025-10-22 04:40:28] logging.py:143 >> {'loss': 0.5734, 'learning_rate': 4.7154e-07, 'epoch': 0.78, 'throughput': 1177.29}
[INFO|2025-10-22 04:41:21] logging.py:143 >> {'loss': 0.5860, 'learning_rate': 4.6746e-07, 'epoch': 0.78, 'throughput': 1177.29}
[INFO|2025-10-22 04:42:14] logging.py:143 >> {'loss': 0.6066, 'learning_rate': 4.6339e-07, 'epoch': 0.78, 'throughput': 1177.29}
[INFO|2025-10-22 04:43:08] logging.py:143 >> {'loss': 0.5550, 'learning_rate': 4.5933e-07, 'epoch': 0.78, 'throughput': 1177.30}
[INFO|2025-10-22 04:44:02] logging.py:143 >> {'loss': 0.5517, 'learning_rate': 4.5529e-07, 'epoch': 0.78, 'throughput': 1177.30}
[INFO|2025-10-22 04:44:55] logging.py:143 >> {'loss': 0.6948, 'learning_rate': 4.5126e-07, 'epoch': 0.78, 'throughput': 1177.30}
[INFO|2025-10-22 04:45:47] logging.py:143 >> {'loss': 0.4622, 'learning_rate': 4.4725e-07, 'epoch': 0.78, 'throughput': 1177.30}
[INFO|2025-10-22 04:46:40] logging.py:143 >> {'loss': 0.6363, 'learning_rate': 4.4326e-07, 'epoch': 0.78, 'throughput': 1177.30}
[INFO|2025-10-22 04:47:33] logging.py:143 >> {'loss': 0.6629, 'learning_rate': 4.3928e-07, 'epoch': 0.79, 'throughput': 1177.30}
[INFO|2025-10-22 04:48:25] logging.py:143 >> {'loss': 0.5160, 'learning_rate': 4.3532e-07, 'epoch': 0.79, 'throughput': 1177.30}
[INFO|2025-10-22 04:49:19] logging.py:143 >> {'loss': 0.5204, 'learning_rate': 4.3137e-07, 'epoch': 0.79, 'throughput': 1177.30}
[INFO|2025-10-22 04:50:12] logging.py:143 >> {'loss': 0.5450, 'learning_rate': 4.2744e-07, 'epoch': 0.79, 'throughput': 1177.31}
[INFO|2025-10-22 04:50:12] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3900
[INFO|2025-10-22 04:50:12] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 04:50:12] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 04:50:12] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3900/chat_template.jinja
[INFO|2025-10-22 04:50:12] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3900/tokenizer_config.json
[INFO|2025-10-22 04:50:12] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-3900/special_tokens_map.json
[INFO|2025-10-22 04:51:06] logging.py:143 >> {'loss': 0.5635, 'learning_rate': 4.2353e-07, 'epoch': 0.79, 'throughput': 1177.29}
[INFO|2025-10-22 04:52:00] logging.py:143 >> {'loss': 0.5421, 'learning_rate': 4.1963e-07, 'epoch': 0.79, 'throughput': 1177.30}
[INFO|2025-10-22 04:52:54] logging.py:143 >> {'loss': 0.5038, 'learning_rate': 4.1574e-07, 'epoch': 0.79, 'throughput': 1177.31}
[INFO|2025-10-22 04:53:47] logging.py:143 >> {'loss': 0.5031, 'learning_rate': 4.1188e-07, 'epoch': 0.79, 'throughput': 1177.31}
[INFO|2025-10-22 04:54:40] logging.py:143 >> {'loss': 0.5392, 'learning_rate': 4.0803e-07, 'epoch': 0.79, 'throughput': 1177.31}
[INFO|2025-10-22 04:55:34] logging.py:143 >> {'loss': 0.6560, 'learning_rate': 4.0419e-07, 'epoch': 0.79, 'throughput': 1177.31}
[INFO|2025-10-22 04:56:27] logging.py:143 >> {'loss': 0.5383, 'learning_rate': 4.0037e-07, 'epoch': 0.80, 'throughput': 1177.31}
[INFO|2025-10-22 04:57:20] logging.py:143 >> {'loss': 0.6698, 'learning_rate': 3.9657e-07, 'epoch': 0.80, 'throughput': 1177.32}
[INFO|2025-10-22 04:58:13] logging.py:143 >> {'loss': 0.6614, 'learning_rate': 3.9278e-07, 'epoch': 0.80, 'throughput': 1177.32}
[INFO|2025-10-22 04:59:06] logging.py:143 >> {'loss': 0.5923, 'learning_rate': 3.8901e-07, 'epoch': 0.80, 'throughput': 1177.32}
[INFO|2025-10-22 05:00:00] logging.py:143 >> {'loss': 0.4450, 'learning_rate': 3.8526e-07, 'epoch': 0.80, 'throughput': 1177.32}
[INFO|2025-10-22 05:00:53] logging.py:143 >> {'loss': 0.5023, 'learning_rate': 3.8152e-07, 'epoch': 0.80, 'throughput': 1177.32}
[INFO|2025-10-22 05:01:45] logging.py:143 >> {'loss': 0.6202, 'learning_rate': 3.7780e-07, 'epoch': 0.80, 'throughput': 1177.32}
[INFO|2025-10-22 05:02:37] logging.py:143 >> {'loss': 0.4702, 'learning_rate': 3.7409e-07, 'epoch': 0.80, 'throughput': 1177.32}
[INFO|2025-10-22 05:03:31] logging.py:143 >> {'loss': 0.5910, 'learning_rate': 3.7040e-07, 'epoch': 0.80, 'throughput': 1177.31}
[INFO|2025-10-22 05:04:24] logging.py:143 >> {'loss': 0.5719, 'learning_rate': 3.6673e-07, 'epoch': 0.80, 'throughput': 1177.32}
[INFO|2025-10-22 05:05:18] logging.py:143 >> {'loss': 0.5849, 'learning_rate': 3.6307e-07, 'epoch': 0.81, 'throughput': 1177.33}
[INFO|2025-10-22 05:06:11] logging.py:143 >> {'loss': 0.6877, 'learning_rate': 3.5943e-07, 'epoch': 0.81, 'throughput': 1177.33}
[INFO|2025-10-22 05:07:04] logging.py:143 >> {'loss': 0.4594, 'learning_rate': 3.5581e-07, 'epoch': 0.81, 'throughput': 1177.33}
[INFO|2025-10-22 05:07:57] logging.py:143 >> {'loss': 0.5042, 'learning_rate': 3.5220e-07, 'epoch': 0.81, 'throughput': 1177.34}
[INFO|2025-10-22 05:07:57] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4000
[INFO|2025-10-22 05:07:57] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 05:07:57] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 05:07:58] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4000/chat_template.jinja
[INFO|2025-10-22 05:07:58] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4000/tokenizer_config.json
[INFO|2025-10-22 05:07:58] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4000/special_tokens_map.json
[INFO|2025-10-22 05:08:51] logging.py:143 >> {'loss': 0.6508, 'learning_rate': 3.4861e-07, 'epoch': 0.81, 'throughput': 1177.31}
[INFO|2025-10-22 05:09:44] logging.py:143 >> {'loss': 0.5891, 'learning_rate': 3.4504e-07, 'epoch': 0.81, 'throughput': 1177.31}
[INFO|2025-10-22 05:10:37] logging.py:143 >> {'loss': 0.6822, 'learning_rate': 3.4148e-07, 'epoch': 0.81, 'throughput': 1177.32}
[INFO|2025-10-22 05:11:30] logging.py:143 >> {'loss': 0.6311, 'learning_rate': 3.3794e-07, 'epoch': 0.81, 'throughput': 1177.31}
[INFO|2025-10-22 05:12:23] logging.py:143 >> {'loss': 0.5052, 'learning_rate': 3.3442e-07, 'epoch': 0.81, 'throughput': 1177.31}
[INFO|2025-10-22 05:13:17] logging.py:143 >> {'loss': 0.6797, 'learning_rate': 3.3091e-07, 'epoch': 0.81, 'throughput': 1177.32}
[INFO|2025-10-22 05:14:11] logging.py:143 >> {'loss': 0.4845, 'learning_rate': 3.2742e-07, 'epoch': 0.82, 'throughput': 1177.32}
[INFO|2025-10-22 05:15:04] logging.py:143 >> {'loss': 0.5037, 'learning_rate': 3.2395e-07, 'epoch': 0.82, 'throughput': 1177.32}
[INFO|2025-10-22 05:15:58] logging.py:143 >> {'loss': 0.7552, 'learning_rate': 3.2050e-07, 'epoch': 0.82, 'throughput': 1177.32}
[INFO|2025-10-22 05:16:51] logging.py:143 >> {'loss': 0.5181, 'learning_rate': 3.1706e-07, 'epoch': 0.82, 'throughput': 1177.32}
[INFO|2025-10-22 05:17:44] logging.py:143 >> {'loss': 0.5736, 'learning_rate': 3.1363e-07, 'epoch': 0.82, 'throughput': 1177.33}
[INFO|2025-10-22 05:18:37] logging.py:143 >> {'loss': 0.4529, 'learning_rate': 3.1023e-07, 'epoch': 0.82, 'throughput': 1177.33}
[INFO|2025-10-22 05:19:30] logging.py:143 >> {'loss': 0.5811, 'learning_rate': 3.0684e-07, 'epoch': 0.82, 'throughput': 1177.33}
[INFO|2025-10-22 05:20:23] logging.py:143 >> {'loss': 0.5557, 'learning_rate': 3.0347e-07, 'epoch': 0.82, 'throughput': 1177.33}
[INFO|2025-10-22 05:21:16] logging.py:143 >> {'loss': 0.5354, 'learning_rate': 3.0012e-07, 'epoch': 0.82, 'throughput': 1177.33}
[INFO|2025-10-22 05:22:09] logging.py:143 >> {'loss': 0.6328, 'learning_rate': 2.9678e-07, 'epoch': 0.82, 'throughput': 1177.33}
[INFO|2025-10-22 05:23:02] logging.py:143 >> {'loss': 0.6274, 'learning_rate': 2.9346e-07, 'epoch': 0.83, 'throughput': 1177.33}
[INFO|2025-10-22 05:23:55] logging.py:143 >> {'loss': 0.5654, 'learning_rate': 2.9016e-07, 'epoch': 0.83, 'throughput': 1177.33}
[INFO|2025-10-22 05:24:48] logging.py:143 >> {'loss': 0.5394, 'learning_rate': 2.8687e-07, 'epoch': 0.83, 'throughput': 1177.33}
[INFO|2025-10-22 05:25:42] logging.py:143 >> {'loss': 0.6707, 'learning_rate': 2.8361e-07, 'epoch': 0.83, 'throughput': 1177.33}
[INFO|2025-10-22 05:25:42] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4100
[INFO|2025-10-22 05:25:42] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 05:25:42] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 05:25:42] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4100/chat_template.jinja
[INFO|2025-10-22 05:25:42] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4100/tokenizer_config.json
[INFO|2025-10-22 05:25:42] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4100/special_tokens_map.json
[INFO|2025-10-22 05:26:36] logging.py:143 >> {'loss': 0.6816, 'learning_rate': 2.8035e-07, 'epoch': 0.83, 'throughput': 1177.31}
[INFO|2025-10-22 05:27:29] logging.py:143 >> {'loss': 0.3959, 'learning_rate': 2.7712e-07, 'epoch': 0.83, 'throughput': 1177.31}
[INFO|2025-10-22 05:28:22] logging.py:143 >> {'loss': 0.5186, 'learning_rate': 2.7391e-07, 'epoch': 0.83, 'throughput': 1177.31}
[INFO|2025-10-22 05:29:15] logging.py:143 >> {'loss': 0.6799, 'learning_rate': 2.7071e-07, 'epoch': 0.83, 'throughput': 1177.31}
[INFO|2025-10-22 05:30:08] logging.py:143 >> {'loss': 0.5884, 'learning_rate': 2.6753e-07, 'epoch': 0.83, 'throughput': 1177.31}
[INFO|2025-10-22 05:31:02] logging.py:143 >> {'loss': 0.5481, 'learning_rate': 2.6436e-07, 'epoch': 0.83, 'throughput': 1177.31}
[INFO|2025-10-22 05:31:56] logging.py:143 >> {'loss': 0.4999, 'learning_rate': 2.6122e-07, 'epoch': 0.84, 'throughput': 1177.32}
[INFO|2025-10-22 05:32:49] logging.py:143 >> {'loss': 0.5947, 'learning_rate': 2.5809e-07, 'epoch': 0.84, 'throughput': 1177.33}
[INFO|2025-10-22 05:33:42] logging.py:143 >> {'loss': 0.5659, 'learning_rate': 2.5498e-07, 'epoch': 0.84, 'throughput': 1177.33}
[INFO|2025-10-22 05:34:35] logging.py:143 >> {'loss': 0.6452, 'learning_rate': 2.5188e-07, 'epoch': 0.84, 'throughput': 1177.33}
[INFO|2025-10-22 05:35:28] logging.py:143 >> {'loss': 0.5718, 'learning_rate': 2.4881e-07, 'epoch': 0.84, 'throughput': 1177.33}
[INFO|2025-10-22 05:36:20] logging.py:143 >> {'loss': 0.5339, 'learning_rate': 2.4575e-07, 'epoch': 0.84, 'throughput': 1177.33}
[INFO|2025-10-22 05:37:13] logging.py:143 >> {'loss': 0.5739, 'learning_rate': 2.4271e-07, 'epoch': 0.84, 'throughput': 1177.33}
[INFO|2025-10-22 05:38:06] logging.py:143 >> {'loss': 0.5553, 'learning_rate': 2.3969e-07, 'epoch': 0.84, 'throughput': 1177.33}
[INFO|2025-10-22 05:39:00] logging.py:143 >> {'loss': 0.5804, 'learning_rate': 2.3668e-07, 'epoch': 0.84, 'throughput': 1177.33}
[INFO|2025-10-22 05:39:53] logging.py:143 >> {'loss': 0.5378, 'learning_rate': 2.3369e-07, 'epoch': 0.84, 'throughput': 1177.34}
[INFO|2025-10-22 05:40:46] logging.py:143 >> {'loss': 0.5261, 'learning_rate': 2.3072e-07, 'epoch': 0.85, 'throughput': 1177.34}
[INFO|2025-10-22 05:41:40] logging.py:143 >> {'loss': 0.4827, 'learning_rate': 2.2777e-07, 'epoch': 0.85, 'throughput': 1177.35}
[INFO|2025-10-22 05:42:33] logging.py:143 >> {'loss': 0.5426, 'learning_rate': 2.2484e-07, 'epoch': 0.85, 'throughput': 1177.35}
[INFO|2025-10-22 05:43:26] logging.py:143 >> {'loss': 0.4762, 'learning_rate': 2.2192e-07, 'epoch': 0.85, 'throughput': 1177.35}
[INFO|2025-10-22 05:43:26] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4200
[INFO|2025-10-22 05:43:26] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 05:43:26] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 05:43:27] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4200/chat_template.jinja
[INFO|2025-10-22 05:43:27] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4200/tokenizer_config.json
[INFO|2025-10-22 05:43:27] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4200/special_tokens_map.json
[INFO|2025-10-22 05:44:20] logging.py:143 >> {'loss': 0.5364, 'learning_rate': 2.1903e-07, 'epoch': 0.85, 'throughput': 1177.33}
[INFO|2025-10-22 05:45:13] logging.py:143 >> {'loss': 0.5290, 'learning_rate': 2.1615e-07, 'epoch': 0.85, 'throughput': 1177.33}
[INFO|2025-10-22 05:46:06] logging.py:143 >> {'loss': 0.5820, 'learning_rate': 2.1328e-07, 'epoch': 0.85, 'throughput': 1177.34}
[INFO|2025-10-22 05:47:00] logging.py:143 >> {'loss': 0.5192, 'learning_rate': 2.1044e-07, 'epoch': 0.85, 'throughput': 1177.34}
[INFO|2025-10-22 05:47:52] logging.py:143 >> {'loss': 0.7744, 'learning_rate': 2.0761e-07, 'epoch': 0.85, 'throughput': 1177.34}
[INFO|2025-10-22 05:48:45] logging.py:143 >> {'loss': 0.5369, 'learning_rate': 2.0481e-07, 'epoch': 0.85, 'throughput': 1177.34}
[INFO|2025-10-22 05:49:38] logging.py:143 >> {'loss': 0.7057, 'learning_rate': 2.0202e-07, 'epoch': 0.86, 'throughput': 1177.34}
[INFO|2025-10-22 05:50:32] logging.py:143 >> {'loss': 0.5632, 'learning_rate': 1.9924e-07, 'epoch': 0.86, 'throughput': 1177.34}
[INFO|2025-10-22 05:51:25] logging.py:143 >> {'loss': 0.5546, 'learning_rate': 1.9649e-07, 'epoch': 0.86, 'throughput': 1177.34}
[INFO|2025-10-22 05:52:18] logging.py:143 >> {'loss': 0.7459, 'learning_rate': 1.9376e-07, 'epoch': 0.86, 'throughput': 1177.34}
[INFO|2025-10-22 05:53:11] logging.py:143 >> {'loss': 0.5122, 'learning_rate': 1.9104e-07, 'epoch': 0.86, 'throughput': 1177.34}
[INFO|2025-10-22 05:54:04] logging.py:143 >> {'loss': 0.6898, 'learning_rate': 1.8834e-07, 'epoch': 0.86, 'throughput': 1177.34}
[INFO|2025-10-22 05:54:57] logging.py:143 >> {'loss': 0.6817, 'learning_rate': 1.8566e-07, 'epoch': 0.86, 'throughput': 1177.34}
[INFO|2025-10-22 05:55:51] logging.py:143 >> {'loss': 0.6331, 'learning_rate': 1.8300e-07, 'epoch': 0.86, 'throughput': 1177.34}
[INFO|2025-10-22 05:56:44] logging.py:143 >> {'loss': 0.5135, 'learning_rate': 1.8035e-07, 'epoch': 0.86, 'throughput': 1177.34}
[INFO|2025-10-22 05:57:38] logging.py:143 >> {'loss': 0.6047, 'learning_rate': 1.7773e-07, 'epoch': 0.86, 'throughput': 1177.34}
[INFO|2025-10-22 05:58:31] logging.py:143 >> {'loss': 0.5573, 'learning_rate': 1.7512e-07, 'epoch': 0.87, 'throughput': 1177.34}
[INFO|2025-10-22 05:59:24] logging.py:143 >> {'loss': 0.6361, 'learning_rate': 1.7253e-07, 'epoch': 0.87, 'throughput': 1177.34}
[INFO|2025-10-22 06:00:17] logging.py:143 >> {'loss': 0.7041, 'learning_rate': 1.6996e-07, 'epoch': 0.87, 'throughput': 1177.34}
[INFO|2025-10-22 06:01:11] logging.py:143 >> {'loss': 0.5137, 'learning_rate': 1.6741e-07, 'epoch': 0.87, 'throughput': 1177.34}
[INFO|2025-10-22 06:01:11] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4300
[INFO|2025-10-22 06:01:11] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 06:01:11] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 06:01:11] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4300/chat_template.jinja
[INFO|2025-10-22 06:01:11] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4300/tokenizer_config.json
[INFO|2025-10-22 06:01:11] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4300/special_tokens_map.json
[INFO|2025-10-22 06:02:04] logging.py:143 >> {'loss': 0.5628, 'learning_rate': 1.6487e-07, 'epoch': 0.87, 'throughput': 1177.32}
[INFO|2025-10-22 06:02:58] logging.py:143 >> {'loss': 0.6413, 'learning_rate': 1.6236e-07, 'epoch': 0.87, 'throughput': 1177.32}
[INFO|2025-10-22 06:03:51] logging.py:143 >> {'loss': 0.6132, 'learning_rate': 1.5986e-07, 'epoch': 0.87, 'throughput': 1177.32}
[INFO|2025-10-22 06:04:45] logging.py:143 >> {'loss': 0.5611, 'learning_rate': 1.5738e-07, 'epoch': 0.87, 'throughput': 1177.33}
[INFO|2025-10-22 06:05:38] logging.py:143 >> {'loss': 0.5842, 'learning_rate': 1.5492e-07, 'epoch': 0.87, 'throughput': 1177.33}
[INFO|2025-10-22 06:06:32] logging.py:143 >> {'loss': 0.5350, 'learning_rate': 1.5248e-07, 'epoch': 0.88, 'throughput': 1177.33}
[INFO|2025-10-22 06:07:25] logging.py:143 >> {'loss': 0.6498, 'learning_rate': 1.5006e-07, 'epoch': 0.88, 'throughput': 1177.34}
[INFO|2025-10-22 06:08:19] logging.py:143 >> {'loss': 0.5299, 'learning_rate': 1.4766e-07, 'epoch': 0.88, 'throughput': 1177.34}
[INFO|2025-10-22 06:09:12] logging.py:143 >> {'loss': 0.5783, 'learning_rate': 1.4527e-07, 'epoch': 0.88, 'throughput': 1177.34}
[INFO|2025-10-22 06:10:06] logging.py:143 >> {'loss': 0.6573, 'learning_rate': 1.4291e-07, 'epoch': 0.88, 'throughput': 1177.34}
[INFO|2025-10-22 06:10:59] logging.py:143 >> {'loss': 0.5208, 'learning_rate': 1.4056e-07, 'epoch': 0.88, 'throughput': 1177.34}
[INFO|2025-10-22 06:11:52] logging.py:143 >> {'loss': 0.5035, 'learning_rate': 1.3823e-07, 'epoch': 0.88, 'throughput': 1177.34}
[INFO|2025-10-22 06:12:46] logging.py:143 >> {'loss': 0.5085, 'learning_rate': 1.3592e-07, 'epoch': 0.88, 'throughput': 1177.35}
[INFO|2025-10-22 06:13:38] logging.py:143 >> {'loss': 0.5308, 'learning_rate': 1.3363e-07, 'epoch': 0.88, 'throughput': 1177.35}
[INFO|2025-10-22 06:14:32] logging.py:143 >> {'loss': 0.5558, 'learning_rate': 1.3136e-07, 'epoch': 0.88, 'throughput': 1177.36}
[INFO|2025-10-22 06:15:25] logging.py:143 >> {'loss': 0.7095, 'learning_rate': 1.2910e-07, 'epoch': 0.89, 'throughput': 1177.36}
[INFO|2025-10-22 06:16:19] logging.py:143 >> {'loss': 0.5274, 'learning_rate': 1.2687e-07, 'epoch': 0.89, 'throughput': 1177.36}
[INFO|2025-10-22 06:17:13] logging.py:143 >> {'loss': 0.4683, 'learning_rate': 1.2465e-07, 'epoch': 0.89, 'throughput': 1177.37}
[INFO|2025-10-22 06:18:06] logging.py:143 >> {'loss': 0.5519, 'learning_rate': 1.2246e-07, 'epoch': 0.89, 'throughput': 1177.37}
[INFO|2025-10-22 06:18:59] logging.py:143 >> {'loss': 0.4969, 'learning_rate': 1.2028e-07, 'epoch': 0.89, 'throughput': 1177.37}
[INFO|2025-10-22 06:18:59] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4400
[INFO|2025-10-22 06:18:59] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 06:18:59] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 06:18:59] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4400/chat_template.jinja
[INFO|2025-10-22 06:18:59] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4400/tokenizer_config.json
[INFO|2025-10-22 06:18:59] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4400/special_tokens_map.json
[INFO|2025-10-22 06:19:53] logging.py:143 >> {'loss': 0.6262, 'learning_rate': 1.1812e-07, 'epoch': 0.89, 'throughput': 1177.36}
[INFO|2025-10-22 06:20:46] logging.py:143 >> {'loss': 0.5727, 'learning_rate': 1.1598e-07, 'epoch': 0.89, 'throughput': 1177.36}
[INFO|2025-10-22 06:21:41] logging.py:143 >> {'loss': 0.5548, 'learning_rate': 1.1386e-07, 'epoch': 0.89, 'throughput': 1177.37}
[INFO|2025-10-22 06:22:33] logging.py:143 >> {'loss': 0.4938, 'learning_rate': 1.1175e-07, 'epoch': 0.89, 'throughput': 1177.37}
[INFO|2025-10-22 06:23:26] logging.py:143 >> {'loss': 0.5172, 'learning_rate': 1.0967e-07, 'epoch': 0.89, 'throughput': 1177.37}
[INFO|2025-10-22 06:24:20] logging.py:143 >> {'loss': 0.4637, 'learning_rate': 1.0761e-07, 'epoch': 0.90, 'throughput': 1177.37}
[INFO|2025-10-22 06:25:13] logging.py:143 >> {'loss': 0.5979, 'learning_rate': 1.0556e-07, 'epoch': 0.90, 'throughput': 1177.38}
[INFO|2025-10-22 06:26:07] logging.py:143 >> {'loss': 0.6648, 'learning_rate': 1.0354e-07, 'epoch': 0.90, 'throughput': 1177.38}
[INFO|2025-10-22 06:27:00] logging.py:143 >> {'loss': 0.5054, 'learning_rate': 1.0153e-07, 'epoch': 0.90, 'throughput': 1177.38}
[INFO|2025-10-22 06:27:53] logging.py:143 >> {'loss': 0.6673, 'learning_rate': 9.9542e-08, 'epoch': 0.90, 'throughput': 1177.38}
[INFO|2025-10-22 06:28:46] logging.py:143 >> {'loss': 0.7695, 'learning_rate': 9.7573e-08, 'epoch': 0.90, 'throughput': 1177.38}
[INFO|2025-10-22 06:29:40] logging.py:143 >> {'loss': 0.6977, 'learning_rate': 9.5624e-08, 'epoch': 0.90, 'throughput': 1177.38}
[INFO|2025-10-22 06:30:33] logging.py:143 >> {'loss': 0.6052, 'learning_rate': 9.3694e-08, 'epoch': 0.90, 'throughput': 1177.37}
[INFO|2025-10-22 06:31:26] logging.py:143 >> {'loss': 0.5339, 'learning_rate': 9.1783e-08, 'epoch': 0.90, 'throughput': 1177.37}
[INFO|2025-10-22 06:32:19] logging.py:143 >> {'loss': 0.4626, 'learning_rate': 8.9891e-08, 'epoch': 0.90, 'throughput': 1177.38}
[INFO|2025-10-22 06:33:13] logging.py:143 >> {'loss': 0.4621, 'learning_rate': 8.8019e-08, 'epoch': 0.91, 'throughput': 1177.38}
[INFO|2025-10-22 06:34:06] logging.py:143 >> {'loss': 0.5929, 'learning_rate': 8.6165e-08, 'epoch': 0.91, 'throughput': 1177.38}
[INFO|2025-10-22 06:34:59] logging.py:143 >> {'loss': 0.5493, 'learning_rate': 8.4331e-08, 'epoch': 0.91, 'throughput': 1177.38}
[INFO|2025-10-22 06:35:52] logging.py:143 >> {'loss': 0.5033, 'learning_rate': 8.2517e-08, 'epoch': 0.91, 'throughput': 1177.38}
[INFO|2025-10-22 06:36:46] logging.py:143 >> {'loss': 0.6044, 'learning_rate': 8.0722e-08, 'epoch': 0.91, 'throughput': 1177.38}
[INFO|2025-10-22 06:36:46] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4500
[INFO|2025-10-22 06:36:46] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 06:36:46] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 06:36:46] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4500/chat_template.jinja
[INFO|2025-10-22 06:36:46] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4500/tokenizer_config.json
[INFO|2025-10-22 06:36:46] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4500/special_tokens_map.json
[INFO|2025-10-22 06:37:41] logging.py:143 >> {'loss': 0.6305, 'learning_rate': 7.8946e-08, 'epoch': 0.91, 'throughput': 1177.37}
[INFO|2025-10-22 06:38:34] logging.py:143 >> {'loss': 0.5979, 'learning_rate': 7.7189e-08, 'epoch': 0.91, 'throughput': 1177.37}
[INFO|2025-10-22 06:39:28] logging.py:143 >> {'loss': 0.6457, 'learning_rate': 7.5452e-08, 'epoch': 0.91, 'throughput': 1177.38}
[INFO|2025-10-22 06:40:22] logging.py:143 >> {'loss': 0.5419, 'learning_rate': 7.3734e-08, 'epoch': 0.91, 'throughput': 1177.38}
[INFO|2025-10-22 06:41:15] logging.py:143 >> {'loss': 0.4596, 'learning_rate': 7.2036e-08, 'epoch': 0.91, 'throughput': 1177.38}
[INFO|2025-10-22 06:42:08] logging.py:143 >> {'loss': 0.6351, 'learning_rate': 7.0357e-08, 'epoch': 0.92, 'throughput': 1177.38}
[INFO|2025-10-22 06:43:01] logging.py:143 >> {'loss': 0.6224, 'learning_rate': 6.8697e-08, 'epoch': 0.92, 'throughput': 1177.38}
[INFO|2025-10-22 06:43:54] logging.py:143 >> {'loss': 0.6340, 'learning_rate': 6.7057e-08, 'epoch': 0.92, 'throughput': 1177.38}
[INFO|2025-10-22 06:44:47] logging.py:143 >> {'loss': 0.5334, 'learning_rate': 6.5437e-08, 'epoch': 0.92, 'throughput': 1177.39}
[INFO|2025-10-22 06:45:40] logging.py:143 >> {'loss': 0.5364, 'learning_rate': 6.3835e-08, 'epoch': 0.92, 'throughput': 1177.39}
[INFO|2025-10-22 06:46:34] logging.py:143 >> {'loss': 0.6397, 'learning_rate': 6.2254e-08, 'epoch': 0.92, 'throughput': 1177.39}
[INFO|2025-10-22 06:47:27] logging.py:143 >> {'loss': 0.6071, 'learning_rate': 6.0692e-08, 'epoch': 0.92, 'throughput': 1177.39}
[INFO|2025-10-22 06:48:20] logging.py:143 >> {'loss': 0.5397, 'learning_rate': 5.9149e-08, 'epoch': 0.92, 'throughput': 1177.39}
[INFO|2025-10-22 06:49:13] logging.py:143 >> {'loss': 0.5350, 'learning_rate': 5.7626e-08, 'epoch': 0.92, 'throughput': 1177.39}
[INFO|2025-10-22 06:50:07] logging.py:143 >> {'loss': 0.7293, 'learning_rate': 5.6123e-08, 'epoch': 0.92, 'throughput': 1177.39}
[INFO|2025-10-22 06:51:00] logging.py:143 >> {'loss': 0.5569, 'learning_rate': 5.4639e-08, 'epoch': 0.93, 'throughput': 1177.39}
[INFO|2025-10-22 06:51:52] logging.py:143 >> {'loss': 0.5535, 'learning_rate': 5.3175e-08, 'epoch': 0.93, 'throughput': 1177.40}
[INFO|2025-10-22 06:52:45] logging.py:143 >> {'loss': 0.6041, 'learning_rate': 5.1731e-08, 'epoch': 0.93, 'throughput': 1177.40}
[INFO|2025-10-22 06:53:39] logging.py:143 >> {'loss': 0.5637, 'learning_rate': 5.0306e-08, 'epoch': 0.93, 'throughput': 1177.40}
[INFO|2025-10-22 06:54:31] logging.py:143 >> {'loss': 0.7095, 'learning_rate': 4.8901e-08, 'epoch': 0.93, 'throughput': 1177.40}
[INFO|2025-10-22 06:54:31] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4600
[INFO|2025-10-22 06:54:31] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 06:54:31] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 06:54:31] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4600/chat_template.jinja
[INFO|2025-10-22 06:54:31] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4600/tokenizer_config.json
[INFO|2025-10-22 06:54:31] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4600/special_tokens_map.json
[INFO|2025-10-22 06:55:24] logging.py:143 >> {'loss': 0.7030, 'learning_rate': 4.7515e-08, 'epoch': 0.93, 'throughput': 1177.38}
[INFO|2025-10-22 06:56:17] logging.py:143 >> {'loss': 0.5463, 'learning_rate': 4.6149e-08, 'epoch': 0.93, 'throughput': 1177.38}
[INFO|2025-10-22 06:57:09] logging.py:143 >> {'loss': 0.5719, 'learning_rate': 4.4803e-08, 'epoch': 0.93, 'throughput': 1177.38}
[INFO|2025-10-22 06:58:02] logging.py:143 >> {'loss': 0.4954, 'learning_rate': 4.3476e-08, 'epoch': 0.93, 'throughput': 1177.38}
[INFO|2025-10-22 06:58:56] logging.py:143 >> {'loss': 0.5703, 'learning_rate': 4.2169e-08, 'epoch': 0.93, 'throughput': 1177.38}
[INFO|2025-10-22 06:59:50] logging.py:143 >> {'loss': 0.6788, 'learning_rate': 4.0882e-08, 'epoch': 0.94, 'throughput': 1177.38}
[INFO|2025-10-22 07:00:42] logging.py:143 >> {'loss': 0.5270, 'learning_rate': 3.9615e-08, 'epoch': 0.94, 'throughput': 1177.38}
[INFO|2025-10-22 07:01:35] logging.py:143 >> {'loss': 0.6647, 'learning_rate': 3.8368e-08, 'epoch': 0.94, 'throughput': 1177.38}
[INFO|2025-10-22 07:02:28] logging.py:143 >> {'loss': 0.4848, 'learning_rate': 3.7140e-08, 'epoch': 0.94, 'throughput': 1177.38}
[INFO|2025-10-22 07:03:22] logging.py:143 >> {'loss': 0.6238, 'learning_rate': 3.5932e-08, 'epoch': 0.94, 'throughput': 1177.38}
[INFO|2025-10-22 07:04:15] logging.py:143 >> {'loss': 0.5687, 'learning_rate': 3.4744e-08, 'epoch': 0.94, 'throughput': 1177.39}
[INFO|2025-10-22 07:05:07] logging.py:143 >> {'loss': 0.5409, 'learning_rate': 3.3575e-08, 'epoch': 0.94, 'throughput': 1177.39}
[INFO|2025-10-22 07:06:00] logging.py:143 >> {'loss': 0.4759, 'learning_rate': 3.2426e-08, 'epoch': 0.94, 'throughput': 1177.39}
[INFO|2025-10-22 07:06:54] logging.py:143 >> {'loss': 0.6125, 'learning_rate': 3.1298e-08, 'epoch': 0.94, 'throughput': 1177.39}
[INFO|2025-10-22 07:07:47] logging.py:143 >> {'loss': 0.6546, 'learning_rate': 3.0189e-08, 'epoch': 0.94, 'throughput': 1177.39}
[INFO|2025-10-22 07:08:41] logging.py:143 >> {'loss': 0.6075, 'learning_rate': 2.9100e-08, 'epoch': 0.95, 'throughput': 1177.39}
[INFO|2025-10-22 07:09:33] logging.py:143 >> {'loss': 0.5526, 'learning_rate': 2.8031e-08, 'epoch': 0.95, 'throughput': 1177.39}
[INFO|2025-10-22 07:10:27] logging.py:143 >> {'loss': 0.7046, 'learning_rate': 2.6981e-08, 'epoch': 0.95, 'throughput': 1177.39}
[INFO|2025-10-22 07:11:21] logging.py:143 >> {'loss': 0.4901, 'learning_rate': 2.5952e-08, 'epoch': 0.95, 'throughput': 1177.40}
[INFO|2025-10-22 07:12:14] logging.py:143 >> {'loss': 0.6634, 'learning_rate': 2.4942e-08, 'epoch': 0.95, 'throughput': 1177.40}
[INFO|2025-10-22 07:12:14] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4700
[INFO|2025-10-22 07:12:14] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 07:12:14] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 07:12:14] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4700/chat_template.jinja
[INFO|2025-10-22 07:12:14] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4700/tokenizer_config.json
[INFO|2025-10-22 07:12:14] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4700/special_tokens_map.json
[INFO|2025-10-22 07:13:07] logging.py:143 >> {'loss': 0.6439, 'learning_rate': 2.3953e-08, 'epoch': 0.95, 'throughput': 1177.38}
[INFO|2025-10-22 07:14:01] logging.py:143 >> {'loss': 0.5921, 'learning_rate': 2.2983e-08, 'epoch': 0.95, 'throughput': 1177.39}
[INFO|2025-10-22 07:14:55] logging.py:143 >> {'loss': 0.6117, 'learning_rate': 2.2033e-08, 'epoch': 0.95, 'throughput': 1177.39}
[INFO|2025-10-22 07:15:47] logging.py:143 >> {'loss': 0.6108, 'learning_rate': 2.1103e-08, 'epoch': 0.95, 'throughput': 1177.39}
[INFO|2025-10-22 07:16:41] logging.py:143 >> {'loss': 0.4609, 'learning_rate': 2.0193e-08, 'epoch': 0.95, 'throughput': 1177.40}
[INFO|2025-10-22 07:17:34] logging.py:143 >> {'loss': 0.5176, 'learning_rate': 1.9303e-08, 'epoch': 0.96, 'throughput': 1177.40}
[INFO|2025-10-22 07:18:28] logging.py:143 >> {'loss': 0.6045, 'learning_rate': 1.8433e-08, 'epoch': 0.96, 'throughput': 1177.40}
[INFO|2025-10-22 07:19:20] logging.py:143 >> {'loss': 0.4288, 'learning_rate': 1.7583e-08, 'epoch': 0.96, 'throughput': 1177.40}
[INFO|2025-10-22 07:20:14] logging.py:143 >> {'loss': 0.5862, 'learning_rate': 1.6753e-08, 'epoch': 0.96, 'throughput': 1177.41}
[INFO|2025-10-22 07:21:08] logging.py:143 >> {'loss': 0.5724, 'learning_rate': 1.5943e-08, 'epoch': 0.96, 'throughput': 1177.41}
[INFO|2025-10-22 07:22:01] logging.py:143 >> {'loss': 0.4079, 'learning_rate': 1.5153e-08, 'epoch': 0.96, 'throughput': 1177.42}
[INFO|2025-10-22 07:22:55] logging.py:143 >> {'loss': 0.7930, 'learning_rate': 1.4383e-08, 'epoch': 0.96, 'throughput': 1177.42}
[INFO|2025-10-22 07:23:48] logging.py:143 >> {'loss': 0.6354, 'learning_rate': 1.3633e-08, 'epoch': 0.96, 'throughput': 1177.42}
[INFO|2025-10-22 07:24:41] logging.py:143 >> {'loss': 0.5886, 'learning_rate': 1.2903e-08, 'epoch': 0.96, 'throughput': 1177.42}
[INFO|2025-10-22 07:25:34] logging.py:143 >> {'loss': 0.5513, 'learning_rate': 1.2193e-08, 'epoch': 0.97, 'throughput': 1177.42}
[INFO|2025-10-22 07:26:28] logging.py:143 >> {'loss': 0.5882, 'learning_rate': 1.1503e-08, 'epoch': 0.97, 'throughput': 1177.42}
[INFO|2025-10-22 07:27:21] logging.py:143 >> {'loss': 0.6614, 'learning_rate': 1.0833e-08, 'epoch': 0.97, 'throughput': 1177.42}
[INFO|2025-10-22 07:28:14] logging.py:143 >> {'loss': 0.6626, 'learning_rate': 1.0183e-08, 'epoch': 0.97, 'throughput': 1177.42}
[INFO|2025-10-22 07:29:06] logging.py:143 >> {'loss': 0.5243, 'learning_rate': 9.5529e-09, 'epoch': 0.97, 'throughput': 1177.41}
[INFO|2025-10-22 07:29:59] logging.py:143 >> {'loss': 0.6352, 'learning_rate': 8.9431e-09, 'epoch': 0.97, 'throughput': 1177.42}
[INFO|2025-10-22 07:29:59] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4800
[INFO|2025-10-22 07:29:59] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 07:29:59] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 07:29:59] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4800/chat_template.jinja
[INFO|2025-10-22 07:29:59] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4800/tokenizer_config.json
[INFO|2025-10-22 07:29:59] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4800/special_tokens_map.json
[INFO|2025-10-22 07:30:54] logging.py:143 >> {'loss': 0.6341, 'learning_rate': 8.3534e-09, 'epoch': 0.97, 'throughput': 1177.41}
[INFO|2025-10-22 07:31:47] logging.py:143 >> {'loss': 0.6654, 'learning_rate': 7.7837e-09, 'epoch': 0.97, 'throughput': 1177.41}
[INFO|2025-10-22 07:32:40] logging.py:143 >> {'loss': 0.6085, 'learning_rate': 7.2342e-09, 'epoch': 0.97, 'throughput': 1177.41}
[INFO|2025-10-22 07:33:33] logging.py:143 >> {'loss': 0.6043, 'learning_rate': 6.7047e-09, 'epoch': 0.97, 'throughput': 1177.41}
[INFO|2025-10-22 07:34:26] logging.py:143 >> {'loss': 0.5500, 'learning_rate': 6.1953e-09, 'epoch': 0.98, 'throughput': 1177.41}
[INFO|2025-10-22 07:35:19] logging.py:143 >> {'loss': 0.6473, 'learning_rate': 5.7059e-09, 'epoch': 0.98, 'throughput': 1177.41}
[INFO|2025-10-22 07:36:12] logging.py:143 >> {'loss': 0.4971, 'learning_rate': 5.2367e-09, 'epoch': 0.98, 'throughput': 1177.41}
[INFO|2025-10-22 07:37:05] logging.py:143 >> {'loss': 0.4796, 'learning_rate': 4.7876e-09, 'epoch': 0.98, 'throughput': 1177.41}
[INFO|2025-10-22 07:37:59] logging.py:143 >> {'loss': 0.4660, 'learning_rate': 4.3586e-09, 'epoch': 0.98, 'throughput': 1177.41}
[INFO|2025-10-22 07:38:53] logging.py:143 >> {'loss': 0.4167, 'learning_rate': 3.9497e-09, 'epoch': 0.98, 'throughput': 1177.41}
[INFO|2025-10-22 07:39:45] logging.py:143 >> {'loss': 0.5639, 'learning_rate': 3.5610e-09, 'epoch': 0.98, 'throughput': 1177.41}
[INFO|2025-10-22 07:40:38] logging.py:143 >> {'loss': 0.5660, 'learning_rate': 3.1923e-09, 'epoch': 0.98, 'throughput': 1177.41}
[INFO|2025-10-22 07:41:31] logging.py:143 >> {'loss': 0.6468, 'learning_rate': 2.8438e-09, 'epoch': 0.98, 'throughput': 1177.41}
[INFO|2025-10-22 07:42:25] logging.py:143 >> {'loss': 0.5028, 'learning_rate': 2.5154e-09, 'epoch': 0.98, 'throughput': 1177.42}
[INFO|2025-10-22 07:43:18] logging.py:143 >> {'loss': 0.5371, 'learning_rate': 2.2071e-09, 'epoch': 0.99, 'throughput': 1177.42}
[INFO|2025-10-22 07:44:11] logging.py:143 >> {'loss': 0.7868, 'learning_rate': 1.9190e-09, 'epoch': 0.99, 'throughput': 1177.42}
[INFO|2025-10-22 07:45:04] logging.py:143 >> {'loss': 0.5457, 'learning_rate': 1.6510e-09, 'epoch': 0.99, 'throughput': 1177.42}
[INFO|2025-10-22 07:45:56] logging.py:143 >> {'loss': 0.7048, 'learning_rate': 1.4031e-09, 'epoch': 0.99, 'throughput': 1177.42}
[INFO|2025-10-22 07:46:49] logging.py:143 >> {'loss': 0.4432, 'learning_rate': 1.1754e-09, 'epoch': 0.99, 'throughput': 1177.42}
[INFO|2025-10-22 07:47:43] logging.py:143 >> {'loss': 0.7731, 'learning_rate': 9.6783e-10, 'epoch': 0.99, 'throughput': 1177.41}
[INFO|2025-10-22 07:47:43] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4900
[INFO|2025-10-22 07:47:43] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 07:47:43] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 07:47:43] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4900/chat_template.jinja
[INFO|2025-10-22 07:47:43] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4900/tokenizer_config.json
[INFO|2025-10-22 07:47:43] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4900/special_tokens_map.json
[INFO|2025-10-22 07:48:38] logging.py:143 >> {'loss': 0.5002, 'learning_rate': 7.8040e-10, 'epoch': 0.99, 'throughput': 1177.40}
[INFO|2025-10-22 07:49:30] logging.py:143 >> {'loss': 0.9167, 'learning_rate': 6.1312e-10, 'epoch': 0.99, 'throughput': 1177.40}
[INFO|2025-10-22 07:50:24] logging.py:143 >> {'loss': 0.5472, 'learning_rate': 4.6600e-10, 'epoch': 0.99, 'throughput': 1177.40}
[INFO|2025-10-22 07:51:16] logging.py:143 >> {'loss': 0.6805, 'learning_rate': 3.3902e-10, 'epoch': 0.99, 'throughput': 1177.40}
[INFO|2025-10-22 07:52:10] logging.py:143 >> {'loss': 0.6820, 'learning_rate': 2.3220e-10, 'epoch': 1.00, 'throughput': 1177.41}
[INFO|2025-10-22 07:53:04] logging.py:143 >> {'loss': 0.7508, 'learning_rate': 1.4553e-10, 'epoch': 1.00, 'throughput': 1177.41}
[INFO|2025-10-22 07:53:56] logging.py:143 >> {'loss': 0.5257, 'learning_rate': 7.9012e-11, 'epoch': 1.00, 'throughput': 1177.40}
[INFO|2025-10-22 07:54:48] logging.py:143 >> {'loss': 0.5711, 'learning_rate': 3.2653e-11, 'epoch': 1.00, 'throughput': 1177.40}
[INFO|2025-10-22 07:55:41] logging.py:143 >> {'loss': 0.6072, 'learning_rate': 6.4500e-12, 'epoch': 1.00, 'throughput': 1177.40}
[INFO|2025-10-22 07:56:13] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4948
[INFO|2025-10-22 07:56:13] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 07:56:13] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 07:56:13] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4948/chat_template.jinja
[INFO|2025-10-22 07:56:13] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4948/tokenizer_config.json
[INFO|2025-10-22 07:56:13] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/checkpoint-4948/special_tokens_map.json
[INFO|2025-10-22 07:56:14] trainer.py:2810 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|2025-10-22 07:56:14] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56
[INFO|2025-10-22 07:56:14] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 07:56:14] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 07:56:14] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/chat_template.jinja
[INFO|2025-10-22 07:56:14] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/tokenizer_config.json
[INFO|2025-10-22 07:56:14] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_2025-10-21-17-15-56/special_tokens_map.json
[WARNING|2025-10-22 07:56:14] logging.py:148 >> No metric eval_loss to plot.
[WARNING|2025-10-22 07:56:14] logging.py:148 >> No metric eval_accuracy to plot.
[INFO|2025-10-22 07:56:14] modelcard.py:456 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
