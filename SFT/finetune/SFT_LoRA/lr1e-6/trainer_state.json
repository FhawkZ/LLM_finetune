{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 100,
  "global_step": 4899,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0010207206287639074,
      "grad_norm": 28.025907516479492,
      "learning_rate": 9.999983550806113e-07,
      "loss": 9.5986,
      "num_input_tokens_seen": 63472,
      "step": 5,
      "train_runtime": 54.7127,
      "train_tokens_per_second": 1160.095
    },
    {
      "epoch": 0.002041441257527815,
      "grad_norm": 36.42958450317383,
      "learning_rate": 9.999916726141438e-07,
      "loss": 10.1343,
      "num_input_tokens_seen": 126432,
      "step": 10,
      "train_runtime": 108.0016,
      "train_tokens_per_second": 1170.649
    },
    {
      "epoch": 0.0030621618862917217,
      "grad_norm": 52.482337951660156,
      "learning_rate": 9.999798498617838e-07,
      "loss": 9.8379,
      "num_input_tokens_seen": 189040,
      "step": 15,
      "train_runtime": 161.3369,
      "train_tokens_per_second": 1171.709
    },
    {
      "epoch": 0.00408288251505563,
      "grad_norm": 47.57538604736328,
      "learning_rate": 9.999628869450779e-07,
      "loss": 10.1201,
      "num_input_tokens_seen": 250224,
      "step": 20,
      "train_runtime": 213.4479,
      "train_tokens_per_second": 1172.296
    },
    {
      "epoch": 0.005103603143819537,
      "grad_norm": 40.62760925292969,
      "learning_rate": 9.999407840384173e-07,
      "loss": 9.0671,
      "num_input_tokens_seen": 313536,
      "step": 25,
      "train_runtime": 267.1024,
      "train_tokens_per_second": 1173.842
    },
    {
      "epoch": 0.0061243237725834435,
      "grad_norm": 40.742103576660156,
      "learning_rate": 9.999135413690362e-07,
      "loss": 10.0013,
      "num_input_tokens_seen": 376112,
      "step": 30,
      "train_runtime": 320.2739,
      "train_tokens_per_second": 1174.345
    },
    {
      "epoch": 0.007145044401347351,
      "grad_norm": 46.00783157348633,
      "learning_rate": 9.998811592170102e-07,
      "loss": 9.5303,
      "num_input_tokens_seen": 438592,
      "step": 35,
      "train_runtime": 373.2872,
      "train_tokens_per_second": 1174.945
    },
    {
      "epoch": 0.00816576503011126,
      "grad_norm": 44.1495246887207,
      "learning_rate": 9.99843637915251e-07,
      "loss": 9.6047,
      "num_input_tokens_seen": 502336,
      "step": 40,
      "train_runtime": 427.3616,
      "train_tokens_per_second": 1175.435
    },
    {
      "epoch": 0.009186485658875166,
      "grad_norm": 36.999656677246094,
      "learning_rate": 9.998009778495063e-07,
      "loss": 8.3994,
      "num_input_tokens_seen": 565456,
      "step": 45,
      "train_runtime": 480.9915,
      "train_tokens_per_second": 1175.605
    },
    {
      "epoch": 0.010207206287639074,
      "grad_norm": 52.39639663696289,
      "learning_rate": 9.997531794583524e-07,
      "loss": 8.8185,
      "num_input_tokens_seen": 629152,
      "step": 50,
      "train_runtime": 534.8778,
      "train_tokens_per_second": 1176.254
    },
    {
      "epoch": 0.01122792691640298,
      "grad_norm": 32.3061637878418,
      "learning_rate": 9.997002432331931e-07,
      "loss": 8.0236,
      "num_input_tokens_seen": 692400,
      "step": 55,
      "train_runtime": 588.5193,
      "train_tokens_per_second": 1176.512
    },
    {
      "epoch": 0.012248647545166887,
      "grad_norm": 48.72734832763672,
      "learning_rate": 9.996421697182516e-07,
      "loss": 7.5213,
      "num_input_tokens_seen": 756272,
      "step": 60,
      "train_runtime": 642.7952,
      "train_tokens_per_second": 1176.536
    },
    {
      "epoch": 0.013269368173930795,
      "grad_norm": 39.11638641357422,
      "learning_rate": 9.995789595105668e-07,
      "loss": 7.69,
      "num_input_tokens_seen": 819136,
      "step": 65,
      "train_runtime": 696.0785,
      "train_tokens_per_second": 1176.787
    },
    {
      "epoch": 0.014290088802694702,
      "grad_norm": 44.781837463378906,
      "learning_rate": 9.995106132599868e-07,
      "loss": 8.1544,
      "num_input_tokens_seen": 882016,
      "step": 70,
      "train_runtime": 749.5142,
      "train_tokens_per_second": 1176.784
    },
    {
      "epoch": 0.01531080943145861,
      "grad_norm": 40.45317459106445,
      "learning_rate": 9.994371316691616e-07,
      "loss": 7.4283,
      "num_input_tokens_seen": 945872,
      "step": 75,
      "train_runtime": 803.5443,
      "train_tokens_per_second": 1177.125
    },
    {
      "epoch": 0.01633153006022252,
      "grad_norm": 48.2558708190918,
      "learning_rate": 9.99358515493537e-07,
      "loss": 7.2821,
      "num_input_tokens_seen": 1008208,
      "step": 80,
      "train_runtime": 856.507,
      "train_tokens_per_second": 1177.116
    },
    {
      "epoch": 0.017352250688986423,
      "grad_norm": 41.745792388916016,
      "learning_rate": 9.992747655413455e-07,
      "loss": 7.3816,
      "num_input_tokens_seen": 1070416,
      "step": 85,
      "train_runtime": 909.321,
      "train_tokens_per_second": 1177.16
    },
    {
      "epoch": 0.01837297131775033,
      "grad_norm": 28.136117935180664,
      "learning_rate": 9.991858826735984e-07,
      "loss": 6.1827,
      "num_input_tokens_seen": 1134464,
      "step": 90,
      "train_runtime": 963.4366,
      "train_tokens_per_second": 1177.518
    },
    {
      "epoch": 0.01939369194651424,
      "grad_norm": 38.29441833496094,
      "learning_rate": 9.990918678040781e-07,
      "loss": 6.948,
      "num_input_tokens_seen": 1196720,
      "step": 95,
      "train_runtime": 1016.3338,
      "train_tokens_per_second": 1177.487
    },
    {
      "epoch": 0.020414412575278148,
      "grad_norm": 39.28352737426758,
      "learning_rate": 9.989927218993271e-07,
      "loss": 6.0614,
      "num_input_tokens_seen": 1260016,
      "step": 100,
      "train_runtime": 1069.9248,
      "train_tokens_per_second": 1177.668
    },
    {
      "epoch": 0.020414412575278148,
      "eval_loss": 6.7553606033325195,
      "eval_runtime": 139.1275,
      "eval_samples_per_second": 5.693,
      "eval_steps_per_second": 2.846,
      "num_input_tokens_seen": 1260016,
      "step": 100
    },
    {
      "epoch": 0.021435133204042053,
      "grad_norm": 45.89613342285156,
      "learning_rate": 9.988884459786392e-07,
      "loss": 5.9971,
      "num_input_tokens_seen": 1322128,
      "step": 105,
      "train_runtime": 1262.5994,
      "train_tokens_per_second": 1047.148
    },
    {
      "epoch": 0.02245585383280596,
      "grad_norm": 32.52590560913086,
      "learning_rate": 9.987790411140483e-07,
      "loss": 5.6988,
      "num_input_tokens_seen": 1385776,
      "step": 110,
      "train_runtime": 1316.3003,
      "train_tokens_per_second": 1052.781
    },
    {
      "epoch": 0.02347657446156987,
      "grad_norm": 52.72541427612305,
      "learning_rate": 9.986645084303172e-07,
      "loss": 6.0975,
      "num_input_tokens_seen": 1447056,
      "step": 115,
      "train_runtime": 1368.3827,
      "train_tokens_per_second": 1057.494
    },
    {
      "epoch": 0.024497295090333774,
      "grad_norm": 55.03352737426758,
      "learning_rate": 9.985448491049277e-07,
      "loss": 5.524,
      "num_input_tokens_seen": 1509312,
      "step": 120,
      "train_runtime": 1421.2367,
      "train_tokens_per_second": 1061.971
    },
    {
      "epoch": 0.025518015719097682,
      "grad_norm": 32.085025787353516,
      "learning_rate": 9.98420064368066e-07,
      "loss": 5.0014,
      "num_input_tokens_seen": 1572064,
      "step": 125,
      "train_runtime": 1474.5877,
      "train_tokens_per_second": 1066.104
    },
    {
      "epoch": 0.02653873634786159,
      "grad_norm": 39.364994049072266,
      "learning_rate": 9.982901555026124e-07,
      "loss": 4.758,
      "num_input_tokens_seen": 1635472,
      "step": 130,
      "train_runtime": 1528.2329,
      "train_tokens_per_second": 1070.172
    },
    {
      "epoch": 0.0275594569766255,
      "grad_norm": 34.759498596191406,
      "learning_rate": 9.981551238441261e-07,
      "loss": 4.5925,
      "num_input_tokens_seen": 1699712,
      "step": 135,
      "train_runtime": 1582.4982,
      "train_tokens_per_second": 1074.069
    },
    {
      "epoch": 0.028580177605389404,
      "grad_norm": 38.350799560546875,
      "learning_rate": 9.980149707808332e-07,
      "loss": 4.7356,
      "num_input_tokens_seen": 1762160,
      "step": 140,
      "train_runtime": 1635.5708,
      "train_tokens_per_second": 1077.398
    },
    {
      "epoch": 0.029600898234153312,
      "grad_norm": 37.32517623901367,
      "learning_rate": 9.978696977536114e-07,
      "loss": 4.1267,
      "num_input_tokens_seen": 1825680,
      "step": 145,
      "train_runtime": 1689.2984,
      "train_tokens_per_second": 1080.733
    },
    {
      "epoch": 0.03062161886291722,
      "grad_norm": 35.663673400878906,
      "learning_rate": 9.97719306255975e-07,
      "loss": 3.5905,
      "num_input_tokens_seen": 1889184,
      "step": 150,
      "train_runtime": 1743.0286,
      "train_tokens_per_second": 1083.851
    },
    {
      "epoch": 0.031642339491681125,
      "grad_norm": 27.306652069091797,
      "learning_rate": 9.975637978340605e-07,
      "loss": 4.1348,
      "num_input_tokens_seen": 1951520,
      "step": 155,
      "train_runtime": 1796.0347,
      "train_tokens_per_second": 1086.571
    },
    {
      "epoch": 0.03266306012044504,
      "grad_norm": 29.858266830444336,
      "learning_rate": 9.974031740866102e-07,
      "loss": 3.4915,
      "num_input_tokens_seen": 2013696,
      "step": 160,
      "train_runtime": 1848.9671,
      "train_tokens_per_second": 1089.092
    },
    {
      "epoch": 0.03368378074920894,
      "grad_norm": 26.11689567565918,
      "learning_rate": 9.972374366649552e-07,
      "loss": 3.5158,
      "num_input_tokens_seen": 2077056,
      "step": 165,
      "train_runtime": 1902.6167,
      "train_tokens_per_second": 1091.684
    },
    {
      "epoch": 0.034704501377972846,
      "grad_norm": 22.825376510620117,
      "learning_rate": 9.970665872729995e-07,
      "loss": 3.2095,
      "num_input_tokens_seen": 2139920,
      "step": 170,
      "train_runtime": 1955.9355,
      "train_tokens_per_second": 1094.065
    },
    {
      "epoch": 0.03572522200673676,
      "grad_norm": 23.604991912841797,
      "learning_rate": 9.968906276672023e-07,
      "loss": 2.9898,
      "num_input_tokens_seen": 2202016,
      "step": 175,
      "train_runtime": 2008.7318,
      "train_tokens_per_second": 1096.222
    },
    {
      "epoch": 0.03674594263550066,
      "grad_norm": 26.140308380126953,
      "learning_rate": 9.967095596565583e-07,
      "loss": 3.3946,
      "num_input_tokens_seen": 2264432,
      "step": 180,
      "train_runtime": 2061.7228,
      "train_tokens_per_second": 1098.32
    },
    {
      "epoch": 0.03776666326426457,
      "grad_norm": 15.895533561706543,
      "learning_rate": 9.965233851025813e-07,
      "loss": 2.6575,
      "num_input_tokens_seen": 2327232,
      "step": 185,
      "train_runtime": 2115.0209,
      "train_tokens_per_second": 1100.335
    },
    {
      "epoch": 0.03878738389302848,
      "grad_norm": 22.55730628967285,
      "learning_rate": 9.963321059192846e-07,
      "loss": 2.602,
      "num_input_tokens_seen": 2389456,
      "step": 190,
      "train_runtime": 2167.8862,
      "train_tokens_per_second": 1102.205
    },
    {
      "epoch": 0.039808104521792384,
      "grad_norm": 11.093121528625488,
      "learning_rate": 9.961357240731598e-07,
      "loss": 2.2301,
      "num_input_tokens_seen": 2452064,
      "step": 195,
      "train_runtime": 2221.0782,
      "train_tokens_per_second": 1103.997
    },
    {
      "epoch": 0.040828825150556296,
      "grad_norm": 15.409443855285645,
      "learning_rate": 9.959342415831583e-07,
      "loss": 2.6531,
      "num_input_tokens_seen": 2514048,
      "step": 200,
      "train_runtime": 2273.6086,
      "train_tokens_per_second": 1105.752
    },
    {
      "epoch": 0.040828825150556296,
      "eval_loss": 2.3708415031433105,
      "eval_runtime": 139.1883,
      "eval_samples_per_second": 5.69,
      "eval_steps_per_second": 2.845,
      "num_input_tokens_seen": 2514048,
      "step": 200
    },
    {
      "epoch": 0.0418495457793202,
      "grad_norm": 10.65818977355957,
      "learning_rate": 9.957276605206701e-07,
      "loss": 2.1212,
      "num_input_tokens_seen": 2576976,
      "step": 205,
      "train_runtime": 2466.6752,
      "train_tokens_per_second": 1044.716
    },
    {
      "epoch": 0.042870266408084105,
      "grad_norm": 10.564105987548828,
      "learning_rate": 9.95515983009502e-07,
      "loss": 1.8144,
      "num_input_tokens_seen": 2639216,
      "step": 210,
      "train_runtime": 2519.5751,
      "train_tokens_per_second": 1047.485
    },
    {
      "epoch": 0.04389098703684802,
      "grad_norm": 11.933798789978027,
      "learning_rate": 9.952992112258554e-07,
      "loss": 2.2061,
      "num_input_tokens_seen": 2701424,
      "step": 215,
      "train_runtime": 2572.4043,
      "train_tokens_per_second": 1050.155
    },
    {
      "epoch": 0.04491170766561192,
      "grad_norm": 8.751349449157715,
      "learning_rate": 9.950773473983061e-07,
      "loss": 1.6785,
      "num_input_tokens_seen": 2763808,
      "step": 220,
      "train_runtime": 2625.3744,
      "train_tokens_per_second": 1052.729
    },
    {
      "epoch": 0.04593242829437583,
      "grad_norm": 12.482976913452148,
      "learning_rate": 9.948503938077786e-07,
      "loss": 2.127,
      "num_input_tokens_seen": 2826416,
      "step": 225,
      "train_runtime": 2678.5443,
      "train_tokens_per_second": 1055.206
    },
    {
      "epoch": 0.04695314892313974,
      "grad_norm": 14.844622611999512,
      "learning_rate": 9.946183527875247e-07,
      "loss": 1.9282,
      "num_input_tokens_seen": 2889488,
      "step": 230,
      "train_runtime": 2731.9387,
      "train_tokens_per_second": 1057.669
    },
    {
      "epoch": 0.04797386955190364,
      "grad_norm": 7.483675956726074,
      "learning_rate": 9.943812267230983e-07,
      "loss": 1.6109,
      "num_input_tokens_seen": 2952768,
      "step": 235,
      "train_runtime": 2785.7264,
      "train_tokens_per_second": 1059.963
    },
    {
      "epoch": 0.04899459018066755,
      "grad_norm": 12.37841510772705,
      "learning_rate": 9.941390180523316e-07,
      "loss": 1.4484,
      "num_input_tokens_seen": 3015632,
      "step": 240,
      "train_runtime": 2839.2717,
      "train_tokens_per_second": 1062.115
    },
    {
      "epoch": 0.05001531080943146,
      "grad_norm": 10.214427947998047,
      "learning_rate": 9.938917292653099e-07,
      "loss": 2.0576,
      "num_input_tokens_seen": 3077984,
      "step": 245,
      "train_runtime": 2892.2152,
      "train_tokens_per_second": 1064.231
    },
    {
      "epoch": 0.051036031438195364,
      "grad_norm": 9.408882141113281,
      "learning_rate": 9.936393629043455e-07,
      "loss": 1.6287,
      "num_input_tokens_seen": 3139760,
      "step": 250,
      "train_runtime": 2944.7371,
      "train_tokens_per_second": 1066.228
    },
    {
      "epoch": 0.052056752066959276,
      "grad_norm": 8.732069969177246,
      "learning_rate": 9.93381921563952e-07,
      "loss": 1.6431,
      "num_input_tokens_seen": 3201952,
      "step": 255,
      "train_runtime": 2997.3818,
      "train_tokens_per_second": 1068.25
    },
    {
      "epoch": 0.05307747269572318,
      "grad_norm": 11.208169937133789,
      "learning_rate": 9.931194078908178e-07,
      "loss": 1.4746,
      "num_input_tokens_seen": 3265072,
      "step": 260,
      "train_runtime": 3050.6813,
      "train_tokens_per_second": 1070.276
    },
    {
      "epoch": 0.054098193324487086,
      "grad_norm": 10.360002517700195,
      "learning_rate": 9.928518245837787e-07,
      "loss": 1.6952,
      "num_input_tokens_seen": 3327440,
      "step": 265,
      "train_runtime": 3103.5026,
      "train_tokens_per_second": 1072.156
    },
    {
      "epoch": 0.055118913953251,
      "grad_norm": 15.776053428649902,
      "learning_rate": 9.925791743937895e-07,
      "loss": 1.5275,
      "num_input_tokens_seen": 3390272,
      "step": 270,
      "train_runtime": 3156.7147,
      "train_tokens_per_second": 1073.987
    },
    {
      "epoch": 0.0561396345820149,
      "grad_norm": 12.088377952575684,
      "learning_rate": 9.92301460123897e-07,
      "loss": 1.4348,
      "num_input_tokens_seen": 3453088,
      "step": 275,
      "train_runtime": 3210.0048,
      "train_tokens_per_second": 1075.727
    },
    {
      "epoch": 0.05716035521077881,
      "grad_norm": 8.262349128723145,
      "learning_rate": 9.9201868462921e-07,
      "loss": 1.6168,
      "num_input_tokens_seen": 3515776,
      "step": 280,
      "train_runtime": 3263.1214,
      "train_tokens_per_second": 1077.427
    },
    {
      "epoch": 0.05818107583954272,
      "grad_norm": 7.082253932952881,
      "learning_rate": 9.91730850816871e-07,
      "loss": 1.9046,
      "num_input_tokens_seen": 3577952,
      "step": 285,
      "train_runtime": 3315.9829,
      "train_tokens_per_second": 1079.002
    },
    {
      "epoch": 0.059201796468306624,
      "grad_norm": 11.644393920898438,
      "learning_rate": 9.914379616460254e-07,
      "loss": 1.4245,
      "num_input_tokens_seen": 3640768,
      "step": 290,
      "train_runtime": 3369.2451,
      "train_tokens_per_second": 1080.589
    },
    {
      "epoch": 0.06022251709707053,
      "grad_norm": 9.477888107299805,
      "learning_rate": 9.91140020127791e-07,
      "loss": 1.5326,
      "num_input_tokens_seen": 3703088,
      "step": 295,
      "train_runtime": 3422.2675,
      "train_tokens_per_second": 1082.057
    },
    {
      "epoch": 0.06124323772583444,
      "grad_norm": 8.373373985290527,
      "learning_rate": 9.908370293252287e-07,
      "loss": 1.098,
      "num_input_tokens_seen": 3766176,
      "step": 300,
      "train_runtime": 3475.8378,
      "train_tokens_per_second": 1083.53
    },
    {
      "epoch": 0.06124323772583444,
      "eval_loss": 1.5039446353912354,
      "eval_runtime": 139.0294,
      "eval_samples_per_second": 5.697,
      "eval_steps_per_second": 2.848,
      "num_input_tokens_seen": 3766176,
      "step": 300
    },
    {
      "epoch": 0.062263958354598345,
      "grad_norm": 5.978115081787109,
      "learning_rate": 9.905289923533089e-07,
      "loss": 1.463,
      "num_input_tokens_seen": 3828832,
      "step": 305,
      "train_runtime": 3668.6224,
      "train_tokens_per_second": 1043.67
    },
    {
      "epoch": 0.06328467898336225,
      "grad_norm": 6.431660175323486,
      "learning_rate": 9.902159123788801e-07,
      "loss": 1.3755,
      "num_input_tokens_seen": 3891152,
      "step": 310,
      "train_runtime": 3721.4239,
      "train_tokens_per_second": 1045.608
    },
    {
      "epoch": 0.06430539961212615,
      "grad_norm": 9.636908531188965,
      "learning_rate": 9.898977926206376e-07,
      "loss": 1.5379,
      "num_input_tokens_seen": 3953056,
      "step": 315,
      "train_runtime": 3774.0513,
      "train_tokens_per_second": 1047.43
    },
    {
      "epoch": 0.06532612024089007,
      "grad_norm": 7.467303276062012,
      "learning_rate": 9.895746363490885e-07,
      "loss": 1.4138,
      "num_input_tokens_seen": 4016368,
      "step": 320,
      "train_runtime": 3827.7596,
      "train_tokens_per_second": 1049.274
    },
    {
      "epoch": 0.06634684086965398,
      "grad_norm": 8.631540298461914,
      "learning_rate": 9.892464468865196e-07,
      "loss": 1.5573,
      "num_input_tokens_seen": 4079984,
      "step": 325,
      "train_runtime": 3881.6527,
      "train_tokens_per_second": 1051.095
    },
    {
      "epoch": 0.06736756149841788,
      "grad_norm": 9.72110652923584,
      "learning_rate": 9.889132276069622e-07,
      "loss": 1.3235,
      "num_input_tokens_seen": 4143200,
      "step": 330,
      "train_runtime": 3935.274,
      "train_tokens_per_second": 1052.836
    },
    {
      "epoch": 0.06838828212718179,
      "grad_norm": 8.313339233398438,
      "learning_rate": 9.885749819361583e-07,
      "loss": 1.2947,
      "num_input_tokens_seen": 4206256,
      "step": 335,
      "train_runtime": 3988.7377,
      "train_tokens_per_second": 1054.533
    },
    {
      "epoch": 0.06940900275594569,
      "grad_norm": 11.221131324768066,
      "learning_rate": 9.882317133515247e-07,
      "loss": 1.5548,
      "num_input_tokens_seen": 4268928,
      "step": 340,
      "train_runtime": 4041.8132,
      "train_tokens_per_second": 1056.191
    },
    {
      "epoch": 0.07042972338470961,
      "grad_norm": 6.4904985427856445,
      "learning_rate": 9.878834253821175e-07,
      "loss": 0.9226,
      "num_input_tokens_seen": 4331424,
      "step": 345,
      "train_runtime": 4094.8051,
      "train_tokens_per_second": 1057.785
    },
    {
      "epoch": 0.07145044401347352,
      "grad_norm": 9.055479049682617,
      "learning_rate": 9.875301216085957e-07,
      "loss": 1.2797,
      "num_input_tokens_seen": 4395104,
      "step": 350,
      "train_runtime": 4148.7563,
      "train_tokens_per_second": 1059.379
    },
    {
      "epoch": 0.07247116464223742,
      "grad_norm": 14.822396278381348,
      "learning_rate": 9.871718056631846e-07,
      "loss": 1.4614,
      "num_input_tokens_seen": 4456736,
      "step": 355,
      "train_runtime": 4201.1671,
      "train_tokens_per_second": 1060.833
    },
    {
      "epoch": 0.07349188527100133,
      "grad_norm": 13.677345275878906,
      "learning_rate": 9.868084812296382e-07,
      "loss": 1.2482,
      "num_input_tokens_seen": 4519728,
      "step": 360,
      "train_runtime": 4254.4736,
      "train_tokens_per_second": 1062.347
    },
    {
      "epoch": 0.07451260589976523,
      "grad_norm": 8.5068359375,
      "learning_rate": 9.864401520432022e-07,
      "loss": 1.4017,
      "num_input_tokens_seen": 4581536,
      "step": 365,
      "train_runtime": 4306.9409,
      "train_tokens_per_second": 1063.756
    },
    {
      "epoch": 0.07553332652852913,
      "grad_norm": 7.721773624420166,
      "learning_rate": 9.860668218905738e-07,
      "loss": 1.0371,
      "num_input_tokens_seen": 4644320,
      "step": 370,
      "train_runtime": 4360.1837,
      "train_tokens_per_second": 1065.166
    },
    {
      "epoch": 0.07655404715729305,
      "grad_norm": 8.279385566711426,
      "learning_rate": 9.856884946098645e-07,
      "loss": 1.4009,
      "num_input_tokens_seen": 4706656,
      "step": 375,
      "train_runtime": 4413.2472,
      "train_tokens_per_second": 1066.484
    },
    {
      "epoch": 0.07757476778605696,
      "grad_norm": 8.024435043334961,
      "learning_rate": 9.853051740905597e-07,
      "loss": 1.39,
      "num_input_tokens_seen": 4769152,
      "step": 380,
      "train_runtime": 4466.2831,
      "train_tokens_per_second": 1067.812
    },
    {
      "epoch": 0.07859548841482086,
      "grad_norm": 8.75695514678955,
      "learning_rate": 9.849168642734795e-07,
      "loss": 1.4861,
      "num_input_tokens_seen": 4832608,
      "step": 385,
      "train_runtime": 4519.967,
      "train_tokens_per_second": 1069.169
    },
    {
      "epoch": 0.07961620904358477,
      "grad_norm": 6.712752342224121,
      "learning_rate": 9.84523569150737e-07,
      "loss": 1.3469,
      "num_input_tokens_seen": 4894752,
      "step": 390,
      "train_runtime": 4572.8424,
      "train_tokens_per_second": 1070.396
    },
    {
      "epoch": 0.08063692967234867,
      "grad_norm": 7.037930965423584,
      "learning_rate": 9.841252927656984e-07,
      "loss": 1.0411,
      "num_input_tokens_seen": 4957504,
      "step": 395,
      "train_runtime": 4626.1584,
      "train_tokens_per_second": 1071.624
    },
    {
      "epoch": 0.08165765030111259,
      "grad_norm": 7.571876525878906,
      "learning_rate": 9.83722039212941e-07,
      "loss": 1.5435,
      "num_input_tokens_seen": 5020560,
      "step": 400,
      "train_runtime": 4679.6408,
      "train_tokens_per_second": 1072.852
    },
    {
      "epoch": 0.08165765030111259,
      "eval_loss": 1.3539012670516968,
      "eval_runtime": 139.0928,
      "eval_samples_per_second": 5.694,
      "eval_steps_per_second": 2.847,
      "num_input_tokens_seen": 5020560,
      "step": 400
    },
    {
      "epoch": 0.0826783709298765,
      "grad_norm": 6.694304943084717,
      "learning_rate": 9.833138126382106e-07,
      "loss": 1.2759,
      "num_input_tokens_seen": 5084416,
      "step": 405,
      "train_runtime": 4873.0601,
      "train_tokens_per_second": 1043.372
    },
    {
      "epoch": 0.0836990915586404,
      "grad_norm": 9.100578308105469,
      "learning_rate": 9.8290061723838e-07,
      "loss": 1.3396,
      "num_input_tokens_seen": 5146368,
      "step": 410,
      "train_runtime": 4925.7692,
      "train_tokens_per_second": 1044.785
    },
    {
      "epoch": 0.0847198121874043,
      "grad_norm": 7.832365036010742,
      "learning_rate": 9.82482457261405e-07,
      "loss": 1.3118,
      "num_input_tokens_seen": 5207824,
      "step": 415,
      "train_runtime": 4978.0456,
      "train_tokens_per_second": 1046.158
    },
    {
      "epoch": 0.08574053281616821,
      "grad_norm": 7.2040791511535645,
      "learning_rate": 9.820593370062806e-07,
      "loss": 1.4598,
      "num_input_tokens_seen": 5271040,
      "step": 420,
      "train_runtime": 5031.6792,
      "train_tokens_per_second": 1047.571
    },
    {
      "epoch": 0.08676125344493212,
      "grad_norm": 7.562097549438477,
      "learning_rate": 9.816312608229973e-07,
      "loss": 1.2585,
      "num_input_tokens_seen": 5333920,
      "step": 425,
      "train_runtime": 5084.9969,
      "train_tokens_per_second": 1048.952
    },
    {
      "epoch": 0.08778197407369603,
      "grad_norm": 8.808877944946289,
      "learning_rate": 9.811982331124968e-07,
      "loss": 1.0566,
      "num_input_tokens_seen": 5396320,
      "step": 430,
      "train_runtime": 5138.0848,
      "train_tokens_per_second": 1050.259
    },
    {
      "epoch": 0.08880269470245994,
      "grad_norm": 8.212002754211426,
      "learning_rate": 9.807602583266253e-07,
      "loss": 1.368,
      "num_input_tokens_seen": 5458704,
      "step": 435,
      "train_runtime": 5191.0381,
      "train_tokens_per_second": 1051.563
    },
    {
      "epoch": 0.08982341533122384,
      "grad_norm": 6.490293025970459,
      "learning_rate": 9.803173409680892e-07,
      "loss": 1.241,
      "num_input_tokens_seen": 5522608,
      "step": 440,
      "train_runtime": 5245.1193,
      "train_tokens_per_second": 1052.904
    },
    {
      "epoch": 0.09084413595998775,
      "grad_norm": 5.90654182434082,
      "learning_rate": 9.798694855904079e-07,
      "loss": 1.2609,
      "num_input_tokens_seen": 5585792,
      "step": 445,
      "train_runtime": 5298.7063,
      "train_tokens_per_second": 1054.18
    },
    {
      "epoch": 0.09186485658875165,
      "grad_norm": 10.57840633392334,
      "learning_rate": 9.794166967978677e-07,
      "loss": 1.5927,
      "num_input_tokens_seen": 5648272,
      "step": 450,
      "train_runtime": 5351.7281,
      "train_tokens_per_second": 1055.411
    },
    {
      "epoch": 0.09288557721751557,
      "grad_norm": 6.002223014831543,
      "learning_rate": 9.789589792454736e-07,
      "loss": 1.3925,
      "num_input_tokens_seen": 5710896,
      "step": 455,
      "train_runtime": 5404.9221,
      "train_tokens_per_second": 1056.61
    },
    {
      "epoch": 0.09390629784627948,
      "grad_norm": 7.861691474914551,
      "learning_rate": 9.784963376389021e-07,
      "loss": 1.1849,
      "num_input_tokens_seen": 5773552,
      "step": 460,
      "train_runtime": 5458.054,
      "train_tokens_per_second": 1057.804
    },
    {
      "epoch": 0.09492701847504338,
      "grad_norm": 9.770747184753418,
      "learning_rate": 9.78028776734453e-07,
      "loss": 1.4223,
      "num_input_tokens_seen": 5835744,
      "step": 465,
      "train_runtime": 5510.8883,
      "train_tokens_per_second": 1058.948
    },
    {
      "epoch": 0.09594773910380729,
      "grad_norm": 9.834554672241211,
      "learning_rate": 9.77556301338999e-07,
      "loss": 1.3283,
      "num_input_tokens_seen": 5897760,
      "step": 470,
      "train_runtime": 5563.9176,
      "train_tokens_per_second": 1060.001
    },
    {
      "epoch": 0.09696845973257119,
      "grad_norm": 7.288888931274414,
      "learning_rate": 9.770789163099392e-07,
      "loss": 1.1056,
      "num_input_tokens_seen": 5960960,
      "step": 475,
      "train_runtime": 5617.3836,
      "train_tokens_per_second": 1061.163
    },
    {
      "epoch": 0.0979891803613351,
      "grad_norm": 9.793152809143066,
      "learning_rate": 9.765966265551458e-07,
      "loss": 1.2825,
      "num_input_tokens_seen": 6023280,
      "step": 480,
      "train_runtime": 5670.3796,
      "train_tokens_per_second": 1062.236
    },
    {
      "epoch": 0.09900990099009901,
      "grad_norm": 11.280585289001465,
      "learning_rate": 9.76109437032916e-07,
      "loss": 1.4229,
      "num_input_tokens_seen": 6085888,
      "step": 485,
      "train_runtime": 5723.3849,
      "train_tokens_per_second": 1063.337
    },
    {
      "epoch": 0.10003062161886292,
      "grad_norm": 9.98671817779541,
      "learning_rate": 9.756173527519201e-07,
      "loss": 1.7782,
      "num_input_tokens_seen": 6149872,
      "step": 490,
      "train_runtime": 5777.4834,
      "train_tokens_per_second": 1064.455
    },
    {
      "epoch": 0.10105134224762682,
      "grad_norm": 8.996870040893555,
      "learning_rate": 9.7512037877115e-07,
      "loss": 1.2517,
      "num_input_tokens_seen": 6212256,
      "step": 495,
      "train_runtime": 5830.4715,
      "train_tokens_per_second": 1065.481
    },
    {
      "epoch": 0.10207206287639073,
      "grad_norm": 11.687556266784668,
      "learning_rate": 9.746185201998676e-07,
      "loss": 1.1338,
      "num_input_tokens_seen": 6275104,
      "step": 500,
      "train_runtime": 5883.8526,
      "train_tokens_per_second": 1066.496
    },
    {
      "epoch": 0.10207206287639073,
      "eval_loss": 1.2726125717163086,
      "eval_runtime": 139.3282,
      "eval_samples_per_second": 5.684,
      "eval_steps_per_second": 2.842,
      "num_input_tokens_seen": 6275104,
      "step": 500
    },
    {
      "epoch": 0.10309278350515463,
      "grad_norm": 8.550349235534668,
      "learning_rate": 9.741117821975519e-07,
      "loss": 1.0663,
      "num_input_tokens_seen": 6338320,
      "step": 505,
      "train_runtime": 6076.9697,
      "train_tokens_per_second": 1043.007
    },
    {
      "epoch": 0.10411350413391855,
      "grad_norm": 7.412899971008301,
      "learning_rate": 9.736001699738458e-07,
      "loss": 1.1005,
      "num_input_tokens_seen": 6401632,
      "step": 510,
      "train_runtime": 6130.5513,
      "train_tokens_per_second": 1044.218
    },
    {
      "epoch": 0.10513422476268246,
      "grad_norm": 8.661384582519531,
      "learning_rate": 9.730836887885036e-07,
      "loss": 1.3177,
      "num_input_tokens_seen": 6464384,
      "step": 515,
      "train_runtime": 6183.7684,
      "train_tokens_per_second": 1045.379
    },
    {
      "epoch": 0.10615494539144636,
      "grad_norm": 7.591640949249268,
      "learning_rate": 9.725623439513352e-07,
      "loss": 1.1643,
      "num_input_tokens_seen": 6526800,
      "step": 520,
      "train_runtime": 6236.7759,
      "train_tokens_per_second": 1046.502
    },
    {
      "epoch": 0.10717566602021027,
      "grad_norm": 5.49584436416626,
      "learning_rate": 9.720361408221532e-07,
      "loss": 1.6688,
      "num_input_tokens_seen": 6590016,
      "step": 525,
      "train_runtime": 6290.3796,
      "train_tokens_per_second": 1047.634
    },
    {
      "epoch": 0.10819638664897417,
      "grad_norm": 8.452892303466797,
      "learning_rate": 9.715050848107168e-07,
      "loss": 1.3887,
      "num_input_tokens_seen": 6651936,
      "step": 530,
      "train_runtime": 6343.0343,
      "train_tokens_per_second": 1048.699
    },
    {
      "epoch": 0.10921710727773808,
      "grad_norm": 5.9897074699401855,
      "learning_rate": 9.709691813766758e-07,
      "loss": 1.5607,
      "num_input_tokens_seen": 6714496,
      "step": 535,
      "train_runtime": 6396.1803,
      "train_tokens_per_second": 1049.767
    },
    {
      "epoch": 0.110237827906502,
      "grad_norm": 13.879707336425781,
      "learning_rate": 9.704284360295162e-07,
      "loss": 0.9418,
      "num_input_tokens_seen": 6776976,
      "step": 540,
      "train_runtime": 6449.2069,
      "train_tokens_per_second": 1050.823
    },
    {
      "epoch": 0.1112585485352659,
      "grad_norm": 6.536754131317139,
      "learning_rate": 9.69882854328502e-07,
      "loss": 1.2726,
      "num_input_tokens_seen": 6839504,
      "step": 545,
      "train_runtime": 6502.2018,
      "train_tokens_per_second": 1051.875
    },
    {
      "epoch": 0.1122792691640298,
      "grad_norm": 5.59879732131958,
      "learning_rate": 9.693324418826177e-07,
      "loss": 1.3147,
      "num_input_tokens_seen": 6902480,
      "step": 550,
      "train_runtime": 6555.5618,
      "train_tokens_per_second": 1052.92
    },
    {
      "epoch": 0.11329998979279371,
      "grad_norm": 7.151510238647461,
      "learning_rate": 9.68777204350513e-07,
      "loss": 1.3069,
      "num_input_tokens_seen": 6965408,
      "step": 555,
      "train_runtime": 6608.9253,
      "train_tokens_per_second": 1053.94
    },
    {
      "epoch": 0.11432071042155761,
      "grad_norm": 8.572395324707031,
      "learning_rate": 9.682171474404425e-07,
      "loss": 0.9346,
      "num_input_tokens_seen": 7030256,
      "step": 560,
      "train_runtime": 6663.6475,
      "train_tokens_per_second": 1055.016
    },
    {
      "epoch": 0.11534143105032153,
      "grad_norm": 7.660543441772461,
      "learning_rate": 9.676522769102063e-07,
      "loss": 1.1065,
      "num_input_tokens_seen": 7093760,
      "step": 565,
      "train_runtime": 6717.4895,
      "train_tokens_per_second": 1056.014
    },
    {
      "epoch": 0.11636215167908544,
      "grad_norm": 10.18319320678711,
      "learning_rate": 9.67082598567094e-07,
      "loss": 1.2214,
      "num_input_tokens_seen": 7156608,
      "step": 570,
      "train_runtime": 6770.6691,
      "train_tokens_per_second": 1057.002
    },
    {
      "epoch": 0.11738287230784934,
      "grad_norm": 7.782949447631836,
      "learning_rate": 9.66508118267822e-07,
      "loss": 1.3618,
      "num_input_tokens_seen": 7218784,
      "step": 575,
      "train_runtime": 6823.4577,
      "train_tokens_per_second": 1057.936
    },
    {
      "epoch": 0.11840359293661325,
      "grad_norm": 6.692528247833252,
      "learning_rate": 9.659288419184747e-07,
      "loss": 1.1541,
      "num_input_tokens_seen": 7281216,
      "step": 580,
      "train_runtime": 6876.4512,
      "train_tokens_per_second": 1058.862
    },
    {
      "epoch": 0.11942431356537715,
      "grad_norm": 6.850427627563477,
      "learning_rate": 9.653447754744433e-07,
      "loss": 1.377,
      "num_input_tokens_seen": 7343904,
      "step": 585,
      "train_runtime": 6929.5625,
      "train_tokens_per_second": 1059.793
    },
    {
      "epoch": 0.12044503419414106,
      "grad_norm": 7.95970344543457,
      "learning_rate": 9.647559249403648e-07,
      "loss": 1.5244,
      "num_input_tokens_seen": 7407600,
      "step": 590,
      "train_runtime": 6983.2586,
      "train_tokens_per_second": 1060.766
    },
    {
      "epoch": 0.12146575482290498,
      "grad_norm": 6.550267219543457,
      "learning_rate": 9.6416229637006e-07,
      "loss": 1.1618,
      "num_input_tokens_seen": 7470992,
      "step": 595,
      "train_runtime": 7037.0329,
      "train_tokens_per_second": 1061.668
    },
    {
      "epoch": 0.12248647545166888,
      "grad_norm": 7.288854598999023,
      "learning_rate": 9.635638958664721e-07,
      "loss": 1.2851,
      "num_input_tokens_seen": 7533056,
      "step": 600,
      "train_runtime": 7089.7449,
      "train_tokens_per_second": 1062.529
    },
    {
      "epoch": 0.12248647545166888,
      "eval_loss": 1.2235581874847412,
      "eval_runtime": 139.0759,
      "eval_samples_per_second": 5.695,
      "eval_steps_per_second": 2.847,
      "num_input_tokens_seen": 7533056,
      "step": 600
    },
    {
      "epoch": 0.12350719608043279,
      "grad_norm": 11.591001510620117,
      "learning_rate": 9.629607295816028e-07,
      "loss": 1.1964,
      "num_input_tokens_seen": 7595216,
      "step": 605,
      "train_runtime": 7282.0992,
      "train_tokens_per_second": 1042.998
    },
    {
      "epoch": 0.12452791670919669,
      "grad_norm": 6.276949882507324,
      "learning_rate": 9.623528037164491e-07,
      "loss": 1.213,
      "num_input_tokens_seen": 7658176,
      "step": 610,
      "train_runtime": 7335.6045,
      "train_tokens_per_second": 1043.973
    },
    {
      "epoch": 0.1255486373379606,
      "grad_norm": 5.732554912567139,
      "learning_rate": 9.617401245209415e-07,
      "loss": 1.3921,
      "num_input_tokens_seen": 7720608,
      "step": 615,
      "train_runtime": 7388.8309,
      "train_tokens_per_second": 1044.903
    },
    {
      "epoch": 0.1265693579667245,
      "grad_norm": 9.627758026123047,
      "learning_rate": 9.611226982938768e-07,
      "loss": 1.5748,
      "num_input_tokens_seen": 7783472,
      "step": 620,
      "train_runtime": 7442.1647,
      "train_tokens_per_second": 1045.861
    },
    {
      "epoch": 0.1275900785954884,
      "grad_norm": 5.56364107131958,
      "learning_rate": 9.605005313828554e-07,
      "loss": 1.1757,
      "num_input_tokens_seen": 7845040,
      "step": 625,
      "train_runtime": 7494.5869,
      "train_tokens_per_second": 1046.761
    },
    {
      "epoch": 0.1286107992242523,
      "grad_norm": 6.635815620422363,
      "learning_rate": 9.598736301842157e-07,
      "loss": 1.1708,
      "num_input_tokens_seen": 7907856,
      "step": 630,
      "train_runtime": 7547.8626,
      "train_tokens_per_second": 1047.695
    },
    {
      "epoch": 0.12963151985301624,
      "grad_norm": 6.79207706451416,
      "learning_rate": 9.592420011429674e-07,
      "loss": 1.1875,
      "num_input_tokens_seen": 7971360,
      "step": 635,
      "train_runtime": 7601.5989,
      "train_tokens_per_second": 1048.643
    },
    {
      "epoch": 0.13065224048178015,
      "grad_norm": 5.98827600479126,
      "learning_rate": 9.586056507527264e-07,
      "loss": 1.2142,
      "num_input_tokens_seen": 8032816,
      "step": 640,
      "train_runtime": 7653.9405,
      "train_tokens_per_second": 1049.501
    },
    {
      "epoch": 0.13167296111054405,
      "grad_norm": 11.795638084411621,
      "learning_rate": 9.57964585555648e-07,
      "loss": 1.3467,
      "num_input_tokens_seen": 8096368,
      "step": 645,
      "train_runtime": 7707.6933,
      "train_tokens_per_second": 1050.427
    },
    {
      "epoch": 0.13269368173930796,
      "grad_norm": 11.216376304626465,
      "learning_rate": 9.573188121423583e-07,
      "loss": 1.2338,
      "num_input_tokens_seen": 8159392,
      "step": 650,
      "train_runtime": 7761.1747,
      "train_tokens_per_second": 1051.309
    },
    {
      "epoch": 0.13371440236807186,
      "grad_norm": 6.662441730499268,
      "learning_rate": 9.566683371518877e-07,
      "loss": 1.1653,
      "num_input_tokens_seen": 8222256,
      "step": 655,
      "train_runtime": 7814.5784,
      "train_tokens_per_second": 1052.169
    },
    {
      "epoch": 0.13473512299683577,
      "grad_norm": 5.408988952636719,
      "learning_rate": 9.56013167271603e-07,
      "loss": 0.908,
      "num_input_tokens_seen": 8285392,
      "step": 660,
      "train_runtime": 7868.0366,
      "train_tokens_per_second": 1053.044
    },
    {
      "epoch": 0.13575584362559967,
      "grad_norm": 6.123251914978027,
      "learning_rate": 9.55353309237137e-07,
      "loss": 1.1425,
      "num_input_tokens_seen": 8347440,
      "step": 665,
      "train_runtime": 7920.6803,
      "train_tokens_per_second": 1053.879
    },
    {
      "epoch": 0.13677656425436358,
      "grad_norm": 9.141559600830078,
      "learning_rate": 9.546887698323204e-07,
      "loss": 1.0613,
      "num_input_tokens_seen": 8409104,
      "step": 670,
      "train_runtime": 7973.0229,
      "train_tokens_per_second": 1054.695
    },
    {
      "epoch": 0.13779728488312748,
      "grad_norm": 9.393915176391602,
      "learning_rate": 9.54019555889112e-07,
      "loss": 1.1924,
      "num_input_tokens_seen": 8471968,
      "step": 675,
      "train_runtime": 8026.2732,
      "train_tokens_per_second": 1055.529
    },
    {
      "epoch": 0.13881800551189138,
      "grad_norm": 4.687939643859863,
      "learning_rate": 9.53345674287529e-07,
      "loss": 1.206,
      "num_input_tokens_seen": 8534288,
      "step": 680,
      "train_runtime": 8079.333,
      "train_tokens_per_second": 1056.311
    },
    {
      "epoch": 0.1398387261406553,
      "grad_norm": 10.37313175201416,
      "learning_rate": 9.526671319555744e-07,
      "loss": 1.0452,
      "num_input_tokens_seen": 8596240,
      "step": 685,
      "train_runtime": 8132.0554,
      "train_tokens_per_second": 1057.081
    },
    {
      "epoch": 0.14085944676941922,
      "grad_norm": 6.119033336639404,
      "learning_rate": 9.519839358691676e-07,
      "loss": 1.2383,
      "num_input_tokens_seen": 8657856,
      "step": 690,
      "train_runtime": 8184.5247,
      "train_tokens_per_second": 1057.832
    },
    {
      "epoch": 0.14188016739818313,
      "grad_norm": 7.781008720397949,
      "learning_rate": 9.512960930520721e-07,
      "loss": 1.0131,
      "num_input_tokens_seen": 8721392,
      "step": 695,
      "train_runtime": 8238.4795,
      "train_tokens_per_second": 1058.617
    },
    {
      "epoch": 0.14290088802694703,
      "grad_norm": 7.959659099578857,
      "learning_rate": 9.506036105758232e-07,
      "loss": 1.1784,
      "num_input_tokens_seen": 8784560,
      "step": 700,
      "train_runtime": 8292.0258,
      "train_tokens_per_second": 1059.399
    },
    {
      "epoch": 0.14290088802694703,
      "eval_loss": 1.188793420791626,
      "eval_runtime": 139.0743,
      "eval_samples_per_second": 5.695,
      "eval_steps_per_second": 2.847,
      "num_input_tokens_seen": 8784560,
      "step": 700
    },
    {
      "epoch": 0.14392160865571094,
      "grad_norm": 7.906778335571289,
      "learning_rate": 9.499064955596551e-07,
      "loss": 1.2195,
      "num_input_tokens_seen": 8846096,
      "step": 705,
      "train_runtime": 8483.8443,
      "train_tokens_per_second": 1042.699
    },
    {
      "epoch": 0.14494232928447484,
      "grad_norm": 4.7568182945251465,
      "learning_rate": 9.492047551704283e-07,
      "loss": 1.0803,
      "num_input_tokens_seen": 8908464,
      "step": 710,
      "train_runtime": 8536.7185,
      "train_tokens_per_second": 1043.547
    },
    {
      "epoch": 0.14596304991323875,
      "grad_norm": 5.917076110839844,
      "learning_rate": 9.484983966225551e-07,
      "loss": 1.1747,
      "num_input_tokens_seen": 8970784,
      "step": 715,
      "train_runtime": 8589.4931,
      "train_tokens_per_second": 1044.39
    },
    {
      "epoch": 0.14698377054200265,
      "grad_norm": 5.892233848571777,
      "learning_rate": 9.477874271779267e-07,
      "loss": 1.1775,
      "num_input_tokens_seen": 9033328,
      "step": 720,
      "train_runtime": 8642.5468,
      "train_tokens_per_second": 1045.216
    },
    {
      "epoch": 0.14800449117076656,
      "grad_norm": 7.0861968994140625,
      "learning_rate": 9.470718541458368e-07,
      "loss": 1.3119,
      "num_input_tokens_seen": 9095776,
      "step": 725,
      "train_runtime": 8695.6968,
      "train_tokens_per_second": 1046.009
    },
    {
      "epoch": 0.14902521179953046,
      "grad_norm": 6.551034450531006,
      "learning_rate": 9.463516848829081e-07,
      "loss": 1.3282,
      "num_input_tokens_seen": 9158880,
      "step": 730,
      "train_runtime": 8749.2579,
      "train_tokens_per_second": 1046.818
    },
    {
      "epoch": 0.15004593242829437,
      "grad_norm": 7.5726423263549805,
      "learning_rate": 9.456269267930155e-07,
      "loss": 1.2177,
      "num_input_tokens_seen": 9221472,
      "step": 735,
      "train_runtime": 8802.5876,
      "train_tokens_per_second": 1047.587
    },
    {
      "epoch": 0.15106665305705827,
      "grad_norm": 6.85040807723999,
      "learning_rate": 9.448975873272109e-07,
      "loss": 1.1414,
      "num_input_tokens_seen": 9285824,
      "step": 740,
      "train_runtime": 8857.1033,
      "train_tokens_per_second": 1048.404
    },
    {
      "epoch": 0.1520873736858222,
      "grad_norm": 6.347311973571777,
      "learning_rate": 9.441636739836457e-07,
      "loss": 1.3332,
      "num_input_tokens_seen": 9348192,
      "step": 745,
      "train_runtime": 8910.2857,
      "train_tokens_per_second": 1049.146
    },
    {
      "epoch": 0.1531080943145861,
      "grad_norm": 6.41624116897583,
      "learning_rate": 9.434251943074945e-07,
      "loss": 1.0126,
      "num_input_tokens_seen": 9410192,
      "step": 750,
      "train_runtime": 8962.9982,
      "train_tokens_per_second": 1049.893
    },
    {
      "epoch": 0.15412881494335,
      "grad_norm": 10.97081470489502,
      "learning_rate": 9.426821558908772e-07,
      "loss": 1.2056,
      "num_input_tokens_seen": 9473040,
      "step": 755,
      "train_runtime": 9016.3783,
      "train_tokens_per_second": 1050.648
    },
    {
      "epoch": 0.15514953557211392,
      "grad_norm": 8.197928428649902,
      "learning_rate": 9.419345663727804e-07,
      "loss": 1.1842,
      "num_input_tokens_seen": 9536400,
      "step": 760,
      "train_runtime": 9070.1458,
      "train_tokens_per_second": 1051.405
    },
    {
      "epoch": 0.15617025620087782,
      "grad_norm": 8.909884452819824,
      "learning_rate": 9.411824334389804e-07,
      "loss": 1.1914,
      "num_input_tokens_seen": 9599744,
      "step": 765,
      "train_runtime": 9123.8394,
      "train_tokens_per_second": 1052.161
    },
    {
      "epoch": 0.15719097682964173,
      "grad_norm": 5.357950210571289,
      "learning_rate": 9.40425764821962e-07,
      "loss": 1.2519,
      "num_input_tokens_seen": 9662416,
      "step": 770,
      "train_runtime": 9177.0043,
      "train_tokens_per_second": 1052.894
    },
    {
      "epoch": 0.15821169745840563,
      "grad_norm": 6.031096458435059,
      "learning_rate": 9.396645683008411e-07,
      "loss": 1.014,
      "num_input_tokens_seen": 9725040,
      "step": 775,
      "train_runtime": 9230.2858,
      "train_tokens_per_second": 1053.601
    },
    {
      "epoch": 0.15923241808716954,
      "grad_norm": 4.718717575073242,
      "learning_rate": 9.388988517012835e-07,
      "loss": 0.9698,
      "num_input_tokens_seen": 9787824,
      "step": 780,
      "train_runtime": 9283.8003,
      "train_tokens_per_second": 1054.291
    },
    {
      "epoch": 0.16025313871593344,
      "grad_norm": 6.381723880767822,
      "learning_rate": 9.381286228954248e-07,
      "loss": 1.0227,
      "num_input_tokens_seen": 9850192,
      "step": 785,
      "train_runtime": 9336.6337,
      "train_tokens_per_second": 1055.005
    },
    {
      "epoch": 0.16127385934469735,
      "grad_norm": 6.094330787658691,
      "learning_rate": 9.373538898017895e-07,
      "loss": 1.1936,
      "num_input_tokens_seen": 9912336,
      "step": 790,
      "train_runtime": 9389.5837,
      "train_tokens_per_second": 1055.674
    },
    {
      "epoch": 0.16229457997346125,
      "grad_norm": 7.803999423980713,
      "learning_rate": 9.365746603852089e-07,
      "loss": 1.1236,
      "num_input_tokens_seen": 9974736,
      "step": 795,
      "train_runtime": 9442.6115,
      "train_tokens_per_second": 1056.354
    },
    {
      "epoch": 0.16331530060222518,
      "grad_norm": 6.542364120483398,
      "learning_rate": 9.35790942656741e-07,
      "loss": 0.9174,
      "num_input_tokens_seen": 10037376,
      "step": 800,
      "train_runtime": 9495.7977,
      "train_tokens_per_second": 1057.033
    },
    {
      "epoch": 0.16331530060222518,
      "eval_loss": 1.1589635610580444,
      "eval_runtime": 139.0213,
      "eval_samples_per_second": 5.697,
      "eval_steps_per_second": 2.848,
      "num_input_tokens_seen": 10037376,
      "step": 800
    },
    {
      "epoch": 0.1643360212309891,
      "grad_norm": 6.2754011154174805,
      "learning_rate": 9.350027446735862e-07,
      "loss": 1.1303,
      "num_input_tokens_seen": 10099440,
      "step": 805,
      "train_runtime": 9688.0492,
      "train_tokens_per_second": 1042.464
    },
    {
      "epoch": 0.165356741859753,
      "grad_norm": 5.435734272003174,
      "learning_rate": 9.342100745390054e-07,
      "loss": 1.0125,
      "num_input_tokens_seen": 10161936,
      "step": 810,
      "train_runtime": 9741.2601,
      "train_tokens_per_second": 1043.185
    },
    {
      "epoch": 0.1663774624885169,
      "grad_norm": 8.067197799682617,
      "learning_rate": 9.334129404022362e-07,
      "loss": 1.2042,
      "num_input_tokens_seen": 10224160,
      "step": 815,
      "train_runtime": 9794.1618,
      "train_tokens_per_second": 1043.904
    },
    {
      "epoch": 0.1673981831172808,
      "grad_norm": 7.987349510192871,
      "learning_rate": 9.326113504584104e-07,
      "loss": 1.0708,
      "num_input_tokens_seen": 10286640,
      "step": 820,
      "train_runtime": 9847.3557,
      "train_tokens_per_second": 1044.609
    },
    {
      "epoch": 0.1684189037460447,
      "grad_norm": 9.919955253601074,
      "learning_rate": 9.318053129484676e-07,
      "loss": 1.3239,
      "num_input_tokens_seen": 10349952,
      "step": 825,
      "train_runtime": 9900.9857,
      "train_tokens_per_second": 1045.346
    },
    {
      "epoch": 0.1694396243748086,
      "grad_norm": 5.404414176940918,
      "learning_rate": 9.309948361590728e-07,
      "loss": 1.0108,
      "num_input_tokens_seen": 10412944,
      "step": 830,
      "train_runtime": 9954.5815,
      "train_tokens_per_second": 1046.045
    },
    {
      "epoch": 0.17046034500357252,
      "grad_norm": 6.2316131591796875,
      "learning_rate": 9.301799284225292e-07,
      "loss": 1.0299,
      "num_input_tokens_seen": 10475600,
      "step": 835,
      "train_runtime": 10007.8301,
      "train_tokens_per_second": 1046.74
    },
    {
      "epoch": 0.17148106563233642,
      "grad_norm": 6.060240745544434,
      "learning_rate": 9.293605981166941e-07,
      "loss": 1.4128,
      "num_input_tokens_seen": 10539296,
      "step": 840,
      "train_runtime": 10061.7779,
      "train_tokens_per_second": 1047.459
    },
    {
      "epoch": 0.17250178626110033,
      "grad_norm": 6.830105781555176,
      "learning_rate": 9.285368536648918e-07,
      "loss": 1.0328,
      "num_input_tokens_seen": 10601808,
      "step": 845,
      "train_runtime": 10115.0008,
      "train_tokens_per_second": 1048.127
    },
    {
      "epoch": 0.17352250688986423,
      "grad_norm": 7.70769739151001,
      "learning_rate": 9.277087035358271e-07,
      "loss": 1.249,
      "num_input_tokens_seen": 10665584,
      "step": 850,
      "train_runtime": 10169.0668,
      "train_tokens_per_second": 1048.826
    },
    {
      "epoch": 0.17454322751862816,
      "grad_norm": 7.652599334716797,
      "learning_rate": 9.268761562434988e-07,
      "loss": 1.2023,
      "num_input_tokens_seen": 10727904,
      "step": 855,
      "train_runtime": 10221.8538,
      "train_tokens_per_second": 1049.507
    },
    {
      "epoch": 0.17556394814739207,
      "grad_norm": 6.301430702209473,
      "learning_rate": 9.260392203471115e-07,
      "loss": 1.2658,
      "num_input_tokens_seen": 10790256,
      "step": 860,
      "train_runtime": 10275.0537,
      "train_tokens_per_second": 1050.141
    },
    {
      "epoch": 0.17658466877615597,
      "grad_norm": 5.4363627433776855,
      "learning_rate": 9.251979044509885e-07,
      "loss": 1.1157,
      "num_input_tokens_seen": 10853664,
      "step": 865,
      "train_runtime": 10328.9316,
      "train_tokens_per_second": 1050.802
    },
    {
      "epoch": 0.17760538940491988,
      "grad_norm": 8.271255493164062,
      "learning_rate": 9.243522172044818e-07,
      "loss": 1.0096,
      "num_input_tokens_seen": 10916800,
      "step": 870,
      "train_runtime": 10382.6473,
      "train_tokens_per_second": 1051.447
    },
    {
      "epoch": 0.17862611003368378,
      "grad_norm": 5.91877555847168,
      "learning_rate": 9.235021673018848e-07,
      "loss": 1.1479,
      "num_input_tokens_seen": 10979280,
      "step": 875,
      "train_runtime": 10435.7152,
      "train_tokens_per_second": 1052.087
    },
    {
      "epoch": 0.1796468306624477,
      "grad_norm": 5.829956531524658,
      "learning_rate": 9.226477634823422e-07,
      "loss": 0.8663,
      "num_input_tokens_seen": 11042416,
      "step": 880,
      "train_runtime": 10489.2209,
      "train_tokens_per_second": 1052.739
    },
    {
      "epoch": 0.1806675512912116,
      "grad_norm": 7.370555400848389,
      "learning_rate": 9.217890145297601e-07,
      "loss": 0.8719,
      "num_input_tokens_seen": 11104496,
      "step": 885,
      "train_runtime": 10541.9124,
      "train_tokens_per_second": 1053.366
    },
    {
      "epoch": 0.1816882719199755,
      "grad_norm": 4.804141044616699,
      "learning_rate": 9.209259292727156e-07,
      "loss": 1.2305,
      "num_input_tokens_seen": 11167248,
      "step": 890,
      "train_runtime": 10595.2537,
      "train_tokens_per_second": 1053.986
    },
    {
      "epoch": 0.1827089925487394,
      "grad_norm": 7.851797103881836,
      "learning_rate": 9.200585165843667e-07,
      "loss": 1.0534,
      "num_input_tokens_seen": 11229184,
      "step": 895,
      "train_runtime": 10647.9169,
      "train_tokens_per_second": 1054.59
    },
    {
      "epoch": 0.1837297131775033,
      "grad_norm": 7.075560092926025,
      "learning_rate": 9.191867853823601e-07,
      "loss": 1.0962,
      "num_input_tokens_seen": 11291680,
      "step": 900,
      "train_runtime": 10701.1146,
      "train_tokens_per_second": 1055.187
    },
    {
      "epoch": 0.1837297131775033,
      "eval_loss": 1.1368789672851562,
      "eval_runtime": 139.2079,
      "eval_samples_per_second": 5.689,
      "eval_steps_per_second": 2.845,
      "num_input_tokens_seen": 11291680,
      "step": 900
    },
    {
      "epoch": 0.1847504338062672,
      "grad_norm": 4.429067134857178,
      "learning_rate": 9.183107446287405e-07,
      "loss": 1.0396,
      "num_input_tokens_seen": 11354096,
      "step": 905,
      "train_runtime": 10893.9102,
      "train_tokens_per_second": 1042.242
    },
    {
      "epoch": 0.18577115443503114,
      "grad_norm": 8.186694145202637,
      "learning_rate": 9.174304033298575e-07,
      "loss": 1.002,
      "num_input_tokens_seen": 11416384,
      "step": 910,
      "train_runtime": 10947.0924,
      "train_tokens_per_second": 1042.869
    },
    {
      "epoch": 0.18679187506379505,
      "grad_norm": 5.2099690437316895,
      "learning_rate": 9.165457705362738e-07,
      "loss": 0.9816,
      "num_input_tokens_seen": 11479184,
      "step": 915,
      "train_runtime": 11000.2584,
      "train_tokens_per_second": 1043.538
    },
    {
      "epoch": 0.18781259569255895,
      "grad_norm": 5.783928394317627,
      "learning_rate": 9.156568553426718e-07,
      "loss": 1.3295,
      "num_input_tokens_seen": 11542736,
      "step": 920,
      "train_runtime": 11054.1145,
      "train_tokens_per_second": 1044.203
    },
    {
      "epoch": 0.18883331632132286,
      "grad_norm": 4.720699787139893,
      "learning_rate": 9.147636668877603e-07,
      "loss": 1.3334,
      "num_input_tokens_seen": 11604864,
      "step": 925,
      "train_runtime": 11107.2208,
      "train_tokens_per_second": 1044.804
    },
    {
      "epoch": 0.18985403695008676,
      "grad_norm": 5.821187973022461,
      "learning_rate": 9.138662143541803e-07,
      "loss": 1.1544,
      "num_input_tokens_seen": 11666608,
      "step": 930,
      "train_runtime": 11159.9525,
      "train_tokens_per_second": 1045.399
    },
    {
      "epoch": 0.19087475757885067,
      "grad_norm": 5.915224552154541,
      "learning_rate": 9.129645069684104e-07,
      "loss": 1.0667,
      "num_input_tokens_seen": 11729312,
      "step": 935,
      "train_runtime": 11213.1074,
      "train_tokens_per_second": 1046.036
    },
    {
      "epoch": 0.19189547820761457,
      "grad_norm": 5.4953508377075195,
      "learning_rate": 9.120585540006727e-07,
      "loss": 1.1873,
      "num_input_tokens_seen": 11791968,
      "step": 940,
      "train_runtime": 11266.4264,
      "train_tokens_per_second": 1046.647
    },
    {
      "epoch": 0.19291619883637848,
      "grad_norm": 7.321846008300781,
      "learning_rate": 9.111483647648368e-07,
      "loss": 1.0437,
      "num_input_tokens_seen": 11854864,
      "step": 945,
      "train_runtime": 11319.6975,
      "train_tokens_per_second": 1047.277
    },
    {
      "epoch": 0.19393691946514238,
      "grad_norm": 6.568910121917725,
      "learning_rate": 9.102339486183244e-07,
      "loss": 1.175,
      "num_input_tokens_seen": 11917408,
      "step": 950,
      "train_runtime": 11372.8841,
      "train_tokens_per_second": 1047.879
    },
    {
      "epoch": 0.1949576400939063,
      "grad_norm": 5.082115650177002,
      "learning_rate": 9.093153149620127e-07,
      "loss": 0.9467,
      "num_input_tokens_seen": 11979616,
      "step": 955,
      "train_runtime": 11425.949,
      "train_tokens_per_second": 1048.457
    },
    {
      "epoch": 0.1959783607226702,
      "grad_norm": 8.895987510681152,
      "learning_rate": 9.083924732401384e-07,
      "loss": 1.1159,
      "num_input_tokens_seen": 12044016,
      "step": 960,
      "train_runtime": 11480.3266,
      "train_tokens_per_second": 1049.1
    },
    {
      "epoch": 0.19699908135143412,
      "grad_norm": 7.6629743576049805,
      "learning_rate": 9.074654329402e-07,
      "loss": 0.9868,
      "num_input_tokens_seen": 12106864,
      "step": 965,
      "train_runtime": 11533.6448,
      "train_tokens_per_second": 1049.7
    },
    {
      "epoch": 0.19801980198019803,
      "grad_norm": 5.188982963562012,
      "learning_rate": 9.065342035928605e-07,
      "loss": 0.8944,
      "num_input_tokens_seen": 12169696,
      "step": 970,
      "train_runtime": 11587.0644,
      "train_tokens_per_second": 1050.283
    },
    {
      "epoch": 0.19904052260896193,
      "grad_norm": 6.395224094390869,
      "learning_rate": 9.055987947718498e-07,
      "loss": 0.8976,
      "num_input_tokens_seen": 12232528,
      "step": 975,
      "train_runtime": 11640.4652,
      "train_tokens_per_second": 1050.862
    },
    {
      "epoch": 0.20006124323772584,
      "grad_norm": 5.307852268218994,
      "learning_rate": 9.046592160938653e-07,
      "loss": 1.1117,
      "num_input_tokens_seen": 12296112,
      "step": 980,
      "train_runtime": 11694.2369,
      "train_tokens_per_second": 1051.468
    },
    {
      "epoch": 0.20108196386648974,
      "grad_norm": 5.5024566650390625,
      "learning_rate": 9.037154772184741e-07,
      "loss": 1.1038,
      "num_input_tokens_seen": 12359440,
      "step": 985,
      "train_runtime": 11747.8184,
      "train_tokens_per_second": 1052.063
    },
    {
      "epoch": 0.20210268449525365,
      "grad_norm": 5.4425482749938965,
      "learning_rate": 9.02767587848013e-07,
      "loss": 1.1534,
      "num_input_tokens_seen": 12422720,
      "step": 990,
      "train_runtime": 11801.3299,
      "train_tokens_per_second": 1052.654
    },
    {
      "epoch": 0.20312340512401755,
      "grad_norm": 4.459273338317871,
      "learning_rate": 9.018155577274891e-07,
      "loss": 1.082,
      "num_input_tokens_seen": 12485776,
      "step": 995,
      "train_runtime": 11854.9017,
      "train_tokens_per_second": 1053.216
    },
    {
      "epoch": 0.20414412575278146,
      "grad_norm": 71.91731262207031,
      "learning_rate": 9.008593966444792e-07,
      "loss": 1.0201,
      "num_input_tokens_seen": 12549120,
      "step": 1000,
      "train_runtime": 11908.6653,
      "train_tokens_per_second": 1053.781
    },
    {
      "epoch": 0.20414412575278146,
      "eval_loss": 1.1231725215911865,
      "eval_runtime": 139.2254,
      "eval_samples_per_second": 5.689,
      "eval_steps_per_second": 2.844,
      "num_input_tokens_seen": 12549120,
      "step": 1000
    },
    {
      "epoch": 0.20516484638154536,
      "grad_norm": 4.641617774963379,
      "learning_rate": 8.998991144290297e-07,
      "loss": 1.2772,
      "num_input_tokens_seen": 12612080,
      "step": 1005,
      "train_runtime": 12101.6181,
      "train_tokens_per_second": 1042.181
    },
    {
      "epoch": 0.20618556701030927,
      "grad_norm": 5.9531569480896,
      "learning_rate": 8.989347209535555e-07,
      "loss": 1.1944,
      "num_input_tokens_seen": 12674352,
      "step": 1010,
      "train_runtime": 12154.6797,
      "train_tokens_per_second": 1042.755
    },
    {
      "epoch": 0.20720628763907317,
      "grad_norm": 6.325902462005615,
      "learning_rate": 8.979662261327379e-07,
      "loss": 1.4113,
      "num_input_tokens_seen": 12736608,
      "step": 1015,
      "train_runtime": 12207.6125,
      "train_tokens_per_second": 1043.333
    },
    {
      "epoch": 0.2082270082678371,
      "grad_norm": 7.057168483734131,
      "learning_rate": 8.969936399234232e-07,
      "loss": 1.1101,
      "num_input_tokens_seen": 12799920,
      "step": 1020,
      "train_runtime": 12261.2201,
      "train_tokens_per_second": 1043.935
    },
    {
      "epoch": 0.209247728896601,
      "grad_norm": 7.315577030181885,
      "learning_rate": 8.960169723245206e-07,
      "loss": 1.0409,
      "num_input_tokens_seen": 12862736,
      "step": 1025,
      "train_runtime": 12314.6257,
      "train_tokens_per_second": 1044.509
    },
    {
      "epoch": 0.21026844952536491,
      "grad_norm": 7.1096906661987305,
      "learning_rate": 8.950362333768982e-07,
      "loss": 1.2364,
      "num_input_tokens_seen": 12925568,
      "step": 1030,
      "train_runtime": 12368.0882,
      "train_tokens_per_second": 1045.074
    },
    {
      "epoch": 0.21128917015412882,
      "grad_norm": 7.402352809906006,
      "learning_rate": 8.940514331632813e-07,
      "loss": 1.0499,
      "num_input_tokens_seen": 12988320,
      "step": 1035,
      "train_runtime": 12421.3104,
      "train_tokens_per_second": 1045.648
    },
    {
      "epoch": 0.21230989078289272,
      "grad_norm": 9.4207181930542,
      "learning_rate": 8.930625818081481e-07,
      "loss": 1.1662,
      "num_input_tokens_seen": 13050224,
      "step": 1040,
      "train_runtime": 12474.0283,
      "train_tokens_per_second": 1046.192
    },
    {
      "epoch": 0.21333061141165663,
      "grad_norm": 10.207138061523438,
      "learning_rate": 8.920696894776249e-07,
      "loss": 0.9263,
      "num_input_tokens_seen": 13113056,
      "step": 1045,
      "train_runtime": 12527.3965,
      "train_tokens_per_second": 1046.75
    },
    {
      "epoch": 0.21435133204042053,
      "grad_norm": 11.639552116394043,
      "learning_rate": 8.910727663793827e-07,
      "loss": 0.9584,
      "num_input_tokens_seen": 13176208,
      "step": 1050,
      "train_runtime": 12580.9634,
      "train_tokens_per_second": 1047.313
    },
    {
      "epoch": 0.21537205266918444,
      "grad_norm": 8.531935691833496,
      "learning_rate": 8.900718227625315e-07,
      "loss": 1.0123,
      "num_input_tokens_seen": 13238848,
      "step": 1055,
      "train_runtime": 12634.0724,
      "train_tokens_per_second": 1047.869
    },
    {
      "epoch": 0.21639277329794834,
      "grad_norm": 6.078029632568359,
      "learning_rate": 8.890668689175157e-07,
      "loss": 1.012,
      "num_input_tokens_seen": 13301712,
      "step": 1060,
      "train_runtime": 12687.4871,
      "train_tokens_per_second": 1048.412
    },
    {
      "epoch": 0.21741349392671225,
      "grad_norm": 6.422128677368164,
      "learning_rate": 8.880579151760072e-07,
      "loss": 1.1102,
      "num_input_tokens_seen": 13363696,
      "step": 1065,
      "train_runtime": 12740.2158,
      "train_tokens_per_second": 1048.938
    },
    {
      "epoch": 0.21843421455547615,
      "grad_norm": 5.592443466186523,
      "learning_rate": 8.870449719108006e-07,
      "loss": 1.0363,
      "num_input_tokens_seen": 13426736,
      "step": 1070,
      "train_runtime": 12793.8377,
      "train_tokens_per_second": 1049.469
    },
    {
      "epoch": 0.21945493518424009,
      "grad_norm": 7.5010905265808105,
      "learning_rate": 8.860280495357047e-07,
      "loss": 1.2636,
      "num_input_tokens_seen": 13489600,
      "step": 1075,
      "train_runtime": 12847.3145,
      "train_tokens_per_second": 1049.994
    },
    {
      "epoch": 0.220475655813004,
      "grad_norm": 6.221243381500244,
      "learning_rate": 8.850071585054375e-07,
      "loss": 0.8607,
      "num_input_tokens_seen": 13552560,
      "step": 1080,
      "train_runtime": 12900.7643,
      "train_tokens_per_second": 1050.524
    },
    {
      "epoch": 0.2214963764417679,
      "grad_norm": 6.953230857849121,
      "learning_rate": 8.839823093155172e-07,
      "loss": 1.0705,
      "num_input_tokens_seen": 13615216,
      "step": 1085,
      "train_runtime": 12953.9624,
      "train_tokens_per_second": 1051.046
    },
    {
      "epoch": 0.2225170970705318,
      "grad_norm": 6.3489909172058105,
      "learning_rate": 8.829535125021549e-07,
      "loss": 1.2862,
      "num_input_tokens_seen": 13679424,
      "step": 1090,
      "train_runtime": 13008.6078,
      "train_tokens_per_second": 1051.567
    },
    {
      "epoch": 0.2235378176992957,
      "grad_norm": 7.414649486541748,
      "learning_rate": 8.819207786421463e-07,
      "loss": 0.9468,
      "num_input_tokens_seen": 13743168,
      "step": 1095,
      "train_runtime": 13062.391,
      "train_tokens_per_second": 1052.117
    },
    {
      "epoch": 0.2245585383280596,
      "grad_norm": 5.3378472328186035,
      "learning_rate": 8.808841183527629e-07,
      "loss": 1.2598,
      "num_input_tokens_seen": 13805744,
      "step": 1100,
      "train_runtime": 13115.5377,
      "train_tokens_per_second": 1052.625
    },
    {
      "epoch": 0.2245585383280596,
      "eval_loss": 1.111137866973877,
      "eval_runtime": 138.9664,
      "eval_samples_per_second": 5.699,
      "eval_steps_per_second": 2.85,
      "num_input_tokens_seen": 13805744,
      "step": 1100
    },
    {
      "epoch": 0.22557925895682351,
      "grad_norm": 5.554300785064697,
      "learning_rate": 8.798435422916423e-07,
      "loss": 1.0957,
      "num_input_tokens_seen": 13867840,
      "step": 1105,
      "train_runtime": 13307.7053,
      "train_tokens_per_second": 1042.091
    },
    {
      "epoch": 0.22659997958558742,
      "grad_norm": 6.073756694793701,
      "learning_rate": 8.787990611566803e-07,
      "loss": 1.0672,
      "num_input_tokens_seen": 13931088,
      "step": 1110,
      "train_runtime": 13361.2884,
      "train_tokens_per_second": 1042.646
    },
    {
      "epoch": 0.22762070021435132,
      "grad_norm": 11.055717468261719,
      "learning_rate": 8.777506856859185e-07,
      "loss": 1.119,
      "num_input_tokens_seen": 13992960,
      "step": 1115,
      "train_runtime": 13414.0884,
      "train_tokens_per_second": 1043.154
    },
    {
      "epoch": 0.22864142084311523,
      "grad_norm": 6.062966346740723,
      "learning_rate": 8.766984266574356e-07,
      "loss": 0.8959,
      "num_input_tokens_seen": 14054448,
      "step": 1120,
      "train_runtime": 13466.4217,
      "train_tokens_per_second": 1043.666
    },
    {
      "epoch": 0.22966214147187913,
      "grad_norm": 8.189120292663574,
      "learning_rate": 8.756422948892367e-07,
      "loss": 1.0281,
      "num_input_tokens_seen": 14117040,
      "step": 1125,
      "train_runtime": 13519.5005,
      "train_tokens_per_second": 1044.198
    },
    {
      "epoch": 0.23068286210064307,
      "grad_norm": 7.552305698394775,
      "learning_rate": 8.745823012391408e-07,
      "loss": 0.8625,
      "num_input_tokens_seen": 14178992,
      "step": 1130,
      "train_runtime": 13572.1735,
      "train_tokens_per_second": 1044.71
    },
    {
      "epoch": 0.23170358272940697,
      "grad_norm": 9.199482917785645,
      "learning_rate": 8.735184566046702e-07,
      "loss": 1.0561,
      "num_input_tokens_seen": 14241040,
      "step": 1135,
      "train_runtime": 13624.9505,
      "train_tokens_per_second": 1045.218
    },
    {
      "epoch": 0.23272430335817088,
      "grad_norm": 6.610415935516357,
      "learning_rate": 8.724507719229383e-07,
      "loss": 1.2368,
      "num_input_tokens_seen": 14304688,
      "step": 1140,
      "train_runtime": 13678.9058,
      "train_tokens_per_second": 1045.748
    },
    {
      "epoch": 0.23374502398693478,
      "grad_norm": 5.808502674102783,
      "learning_rate": 8.713792581705368e-07,
      "loss": 1.1507,
      "num_input_tokens_seen": 14367536,
      "step": 1145,
      "train_runtime": 13732.2363,
      "train_tokens_per_second": 1046.263
    },
    {
      "epoch": 0.23476574461569868,
      "grad_norm": 4.936896800994873,
      "learning_rate": 8.703039263634233e-07,
      "loss": 1.1966,
      "num_input_tokens_seen": 14430576,
      "step": 1150,
      "train_runtime": 13785.9034,
      "train_tokens_per_second": 1046.763
    },
    {
      "epoch": 0.2357864652444626,
      "grad_norm": 5.389993667602539,
      "learning_rate": 8.692247875568077e-07,
      "loss": 1.0299,
      "num_input_tokens_seen": 14492816,
      "step": 1155,
      "train_runtime": 13838.7985,
      "train_tokens_per_second": 1047.26
    },
    {
      "epoch": 0.2368071858732265,
      "grad_norm": 6.730190277099609,
      "learning_rate": 8.68141852845039e-07,
      "loss": 1.0653,
      "num_input_tokens_seen": 14556144,
      "step": 1160,
      "train_runtime": 13892.2988,
      "train_tokens_per_second": 1047.785
    },
    {
      "epoch": 0.2378279065019904,
      "grad_norm": 6.16684103012085,
      "learning_rate": 8.670551333614904e-07,
      "loss": 1.1387,
      "num_input_tokens_seen": 14617040,
      "step": 1165,
      "train_runtime": 13944.0633,
      "train_tokens_per_second": 1048.263
    },
    {
      "epoch": 0.2388486271307543,
      "grad_norm": 6.474839210510254,
      "learning_rate": 8.659646402784455e-07,
      "loss": 1.1085,
      "num_input_tokens_seen": 14679840,
      "step": 1170,
      "train_runtime": 13997.2505,
      "train_tokens_per_second": 1048.766
    },
    {
      "epoch": 0.2398693477595182,
      "grad_norm": 9.976819038391113,
      "learning_rate": 8.648703848069839e-07,
      "loss": 1.0877,
      "num_input_tokens_seen": 14743152,
      "step": 1175,
      "train_runtime": 14050.9337,
      "train_tokens_per_second": 1049.265
    },
    {
      "epoch": 0.2408900683882821,
      "grad_norm": 5.531381130218506,
      "learning_rate": 8.637723781968643e-07,
      "loss": 1.1449,
      "num_input_tokens_seen": 14805680,
      "step": 1180,
      "train_runtime": 14104.104,
      "train_tokens_per_second": 1049.743
    },
    {
      "epoch": 0.24191078901704605,
      "grad_norm": 9.001465797424316,
      "learning_rate": 8.62670631736411e-07,
      "loss": 1.1019,
      "num_input_tokens_seen": 14867424,
      "step": 1185,
      "train_runtime": 14156.7291,
      "train_tokens_per_second": 1050.202
    },
    {
      "epoch": 0.24293150964580995,
      "grad_norm": 5.741117477416992,
      "learning_rate": 8.61565156752396e-07,
      "loss": 1.0045,
      "num_input_tokens_seen": 14930752,
      "step": 1190,
      "train_runtime": 14210.5107,
      "train_tokens_per_second": 1050.684
    },
    {
      "epoch": 0.24395223027457386,
      "grad_norm": 7.150263786315918,
      "learning_rate": 8.604559646099235e-07,
      "loss": 0.9486,
      "num_input_tokens_seen": 14993888,
      "step": 1195,
      "train_runtime": 14264.104,
      "train_tokens_per_second": 1051.162
    },
    {
      "epoch": 0.24497295090333776,
      "grad_norm": 5.672297954559326,
      "learning_rate": 8.593430667123129e-07,
      "loss": 0.9701,
      "num_input_tokens_seen": 15056080,
      "step": 1200,
      "train_runtime": 14316.8218,
      "train_tokens_per_second": 1051.636
    },
    {
      "epoch": 0.24497295090333776,
      "eval_loss": 1.1004010438919067,
      "eval_runtime": 139.1991,
      "eval_samples_per_second": 5.69,
      "eval_steps_per_second": 2.845,
      "num_input_tokens_seen": 15056080,
      "step": 1200
    },
    {
      "epoch": 0.24599367153210167,
      "grad_norm": 8.46828842163086,
      "learning_rate": 8.582264745009816e-07,
      "loss": 1.1477,
      "num_input_tokens_seen": 15119392,
      "step": 1205,
      "train_runtime": 14510.1219,
      "train_tokens_per_second": 1041.989
    },
    {
      "epoch": 0.24701439216086557,
      "grad_norm": 5.360185146331787,
      "learning_rate": 8.57106199455327e-07,
      "loss": 1.189,
      "num_input_tokens_seen": 15181696,
      "step": 1210,
      "train_runtime": 14563.1738,
      "train_tokens_per_second": 1042.472
    },
    {
      "epoch": 0.24803511278962947,
      "grad_norm": 9.605740547180176,
      "learning_rate": 8.559822530926089e-07,
      "loss": 0.9723,
      "num_input_tokens_seen": 15243712,
      "step": 1215,
      "train_runtime": 14616.0416,
      "train_tokens_per_second": 1042.944
    },
    {
      "epoch": 0.24905583341839338,
      "grad_norm": 8.093973159790039,
      "learning_rate": 8.54854646967831e-07,
      "loss": 1.2245,
      "num_input_tokens_seen": 15305872,
      "step": 1220,
      "train_runtime": 14668.8584,
      "train_tokens_per_second": 1043.426
    },
    {
      "epoch": 0.2500765540471573,
      "grad_norm": 8.472535133361816,
      "learning_rate": 8.537233926736225e-07,
      "loss": 1.0377,
      "num_input_tokens_seen": 15370544,
      "step": 1225,
      "train_runtime": 14723.6167,
      "train_tokens_per_second": 1043.938
    },
    {
      "epoch": 0.2510972746759212,
      "grad_norm": 5.355071067810059,
      "learning_rate": 8.525885018401176e-07,
      "loss": 0.7135,
      "num_input_tokens_seen": 15432928,
      "step": 1230,
      "train_runtime": 14776.7644,
      "train_tokens_per_second": 1044.405
    },
    {
      "epoch": 0.2521179953046851,
      "grad_norm": 7.905393123626709,
      "learning_rate": 8.514499861348373e-07,
      "loss": 0.9305,
      "num_input_tokens_seen": 15496656,
      "step": 1235,
      "train_runtime": 14830.5878,
      "train_tokens_per_second": 1044.912
    },
    {
      "epoch": 0.253138715933449,
      "grad_norm": 5.526048183441162,
      "learning_rate": 8.503078572625694e-07,
      "loss": 1.086,
      "num_input_tokens_seen": 15559872,
      "step": 1240,
      "train_runtime": 14884.128,
      "train_tokens_per_second": 1045.4
    },
    {
      "epoch": 0.2541594365622129,
      "grad_norm": 4.798346996307373,
      "learning_rate": 8.491621269652469e-07,
      "loss": 1.1317,
      "num_input_tokens_seen": 15623856,
      "step": 1245,
      "train_runtime": 14938.34,
      "train_tokens_per_second": 1045.89
    },
    {
      "epoch": 0.2551801571909768,
      "grad_norm": 6.228325366973877,
      "learning_rate": 8.480128070218287e-07,
      "loss": 1.0576,
      "num_input_tokens_seen": 15687520,
      "step": 1250,
      "train_runtime": 14992.2828,
      "train_tokens_per_second": 1046.373
    },
    {
      "epoch": 0.2562008778197407,
      "grad_norm": 8.92538070678711,
      "learning_rate": 8.468599092481778e-07,
      "loss": 1.1721,
      "num_input_tokens_seen": 15750720,
      "step": 1255,
      "train_runtime": 15045.8564,
      "train_tokens_per_second": 1046.848
    },
    {
      "epoch": 0.2572215984485046,
      "grad_norm": 6.995358467102051,
      "learning_rate": 8.457034454969401e-07,
      "loss": 0.9528,
      "num_input_tokens_seen": 15812976,
      "step": 1260,
      "train_runtime": 15098.6565,
      "train_tokens_per_second": 1047.31
    },
    {
      "epoch": 0.2582423190772686,
      "grad_norm": 6.279018402099609,
      "learning_rate": 8.445434276574217e-07,
      "loss": 1.2471,
      "num_input_tokens_seen": 15875136,
      "step": 1265,
      "train_runtime": 15151.5587,
      "train_tokens_per_second": 1047.756
    },
    {
      "epoch": 0.2592630397060325,
      "grad_norm": 6.180663585662842,
      "learning_rate": 8.433798676554685e-07,
      "loss": 1.3446,
      "num_input_tokens_seen": 15938624,
      "step": 1270,
      "train_runtime": 15205.3056,
      "train_tokens_per_second": 1048.228
    },
    {
      "epoch": 0.2602837603347964,
      "grad_norm": 5.859625816345215,
      "learning_rate": 8.422127774533414e-07,
      "loss": 1.0005,
      "num_input_tokens_seen": 16002192,
      "step": 1275,
      "train_runtime": 15259.1225,
      "train_tokens_per_second": 1048.697
    },
    {
      "epoch": 0.2613044809635603,
      "grad_norm": 5.373728275299072,
      "learning_rate": 8.410421690495951e-07,
      "loss": 1.0221,
      "num_input_tokens_seen": 16064832,
      "step": 1280,
      "train_runtime": 15312.219,
      "train_tokens_per_second": 1049.151
    },
    {
      "epoch": 0.2623252015923242,
      "grad_norm": 6.784321308135986,
      "learning_rate": 8.398680544789536e-07,
      "loss": 1.0263,
      "num_input_tokens_seen": 16127008,
      "step": 1285,
      "train_runtime": 15364.9805,
      "train_tokens_per_second": 1049.595
    },
    {
      "epoch": 0.2633459222210881,
      "grad_norm": 5.393935680389404,
      "learning_rate": 8.386904458121873e-07,
      "loss": 0.8811,
      "num_input_tokens_seen": 16188736,
      "step": 1290,
      "train_runtime": 15417.421,
      "train_tokens_per_second": 1050.029
    },
    {
      "epoch": 0.264366642849852,
      "grad_norm": 5.98054313659668,
      "learning_rate": 8.37509355155988e-07,
      "loss": 0.8169,
      "num_input_tokens_seen": 16251824,
      "step": 1295,
      "train_runtime": 15470.7947,
      "train_tokens_per_second": 1050.484
    },
    {
      "epoch": 0.2653873634786159,
      "grad_norm": 5.024791717529297,
      "learning_rate": 8.363247946528453e-07,
      "loss": 0.9417,
      "num_input_tokens_seen": 16315776,
      "step": 1300,
      "train_runtime": 15524.97,
      "train_tokens_per_second": 1050.938
    },
    {
      "epoch": 0.2653873634786159,
      "eval_loss": 1.0918478965759277,
      "eval_runtime": 139.2579,
      "eval_samples_per_second": 5.687,
      "eval_steps_per_second": 2.844,
      "num_input_tokens_seen": 16315776,
      "step": 1300
    },
    {
      "epoch": 0.2664080841073798,
      "grad_norm": 6.617653846740723,
      "learning_rate": 8.351367764809213e-07,
      "loss": 0.9465,
      "num_input_tokens_seen": 16378176,
      "step": 1305,
      "train_runtime": 15717.7338,
      "train_tokens_per_second": 1042.019
    },
    {
      "epoch": 0.2674288047361437,
      "grad_norm": 7.413365364074707,
      "learning_rate": 8.339453128539256e-07,
      "loss": 1.0023,
      "num_input_tokens_seen": 16441040,
      "step": 1310,
      "train_runtime": 15771.1764,
      "train_tokens_per_second": 1042.474
    },
    {
      "epoch": 0.2684495253649076,
      "grad_norm": 6.274526596069336,
      "learning_rate": 8.327504160209895e-07,
      "loss": 0.9669,
      "num_input_tokens_seen": 16502256,
      "step": 1315,
      "train_runtime": 15823.4145,
      "train_tokens_per_second": 1042.901
    },
    {
      "epoch": 0.26947024599367153,
      "grad_norm": 7.044246673583984,
      "learning_rate": 8.315520982665403e-07,
      "loss": 1.0117,
      "num_input_tokens_seen": 16564704,
      "step": 1320,
      "train_runtime": 15876.3542,
      "train_tokens_per_second": 1043.357
    },
    {
      "epoch": 0.27049096662243544,
      "grad_norm": 5.593325614929199,
      "learning_rate": 8.303503719101747e-07,
      "loss": 0.9753,
      "num_input_tokens_seen": 16626512,
      "step": 1325,
      "train_runtime": 15928.863,
      "train_tokens_per_second": 1043.798
    },
    {
      "epoch": 0.27151168725119934,
      "grad_norm": 8.144051551818848,
      "learning_rate": 8.291452493065327e-07,
      "loss": 1.2425,
      "num_input_tokens_seen": 16689088,
      "step": 1330,
      "train_runtime": 15982.1483,
      "train_tokens_per_second": 1044.233
    },
    {
      "epoch": 0.27253240787996325,
      "grad_norm": 4.9639787673950195,
      "learning_rate": 8.279367428451701e-07,
      "loss": 0.8846,
      "num_input_tokens_seen": 16752656,
      "step": 1335,
      "train_runtime": 16035.9975,
      "train_tokens_per_second": 1044.691
    },
    {
      "epoch": 0.27355312850872715,
      "grad_norm": 6.342640399932861,
      "learning_rate": 8.267248649504313e-07,
      "loss": 1.0828,
      "num_input_tokens_seen": 16814864,
      "step": 1340,
      "train_runtime": 16088.78,
      "train_tokens_per_second": 1045.13
    },
    {
      "epoch": 0.27457384913749106,
      "grad_norm": 5.619985103607178,
      "learning_rate": 8.255096280813212e-07,
      "loss": 1.0386,
      "num_input_tokens_seen": 16879120,
      "step": 1345,
      "train_runtime": 16143.1324,
      "train_tokens_per_second": 1045.591
    },
    {
      "epoch": 0.27559456976625496,
      "grad_norm": 6.635769844055176,
      "learning_rate": 8.24291044731378e-07,
      "loss": 0.958,
      "num_input_tokens_seen": 16940848,
      "step": 1350,
      "train_runtime": 16195.4978,
      "train_tokens_per_second": 1046.022
    },
    {
      "epoch": 0.27661529039501886,
      "grad_norm": 4.842052459716797,
      "learning_rate": 8.23069127428544e-07,
      "loss": 1.0801,
      "num_input_tokens_seen": 17003840,
      "step": 1355,
      "train_runtime": 16248.8916,
      "train_tokens_per_second": 1046.462
    },
    {
      "epoch": 0.27763601102378277,
      "grad_norm": 6.09724760055542,
      "learning_rate": 8.218438887350367e-07,
      "loss": 0.9979,
      "num_input_tokens_seen": 17066320,
      "step": 1360,
      "train_runtime": 16301.8516,
      "train_tokens_per_second": 1046.895
    },
    {
      "epoch": 0.2786567316525467,
      "grad_norm": 4.32177734375,
      "learning_rate": 8.206153412472204e-07,
      "loss": 1.0546,
      "num_input_tokens_seen": 17129536,
      "step": 1365,
      "train_runtime": 16355.5316,
      "train_tokens_per_second": 1047.324
    },
    {
      "epoch": 0.2796774522813106,
      "grad_norm": 5.197371006011963,
      "learning_rate": 8.193834975954762e-07,
      "loss": 1.3725,
      "num_input_tokens_seen": 17191728,
      "step": 1370,
      "train_runtime": 16408.4777,
      "train_tokens_per_second": 1047.734
    },
    {
      "epoch": 0.28069817291007454,
      "grad_norm": 7.203885555267334,
      "learning_rate": 8.18148370444072e-07,
      "loss": 1.0338,
      "num_input_tokens_seen": 17255424,
      "step": 1375,
      "train_runtime": 16462.3884,
      "train_tokens_per_second": 1048.173
    },
    {
      "epoch": 0.28171889353883844,
      "grad_norm": 5.404114246368408,
      "learning_rate": 8.169099724910325e-07,
      "loss": 1.2381,
      "num_input_tokens_seen": 17318256,
      "step": 1380,
      "train_runtime": 16515.8653,
      "train_tokens_per_second": 1048.583
    },
    {
      "epoch": 0.28273961416760235,
      "grad_norm": 6.389730453491211,
      "learning_rate": 8.156683164680091e-07,
      "loss": 1.1654,
      "num_input_tokens_seen": 17380256,
      "step": 1385,
      "train_runtime": 16568.7131,
      "train_tokens_per_second": 1048.98
    },
    {
      "epoch": 0.28376033479636625,
      "grad_norm": 5.904038906097412,
      "learning_rate": 8.144234151401481e-07,
      "loss": 1.0128,
      "num_input_tokens_seen": 17443856,
      "step": 1390,
      "train_runtime": 16622.7656,
      "train_tokens_per_second": 1049.396
    },
    {
      "epoch": 0.28478105542513016,
      "grad_norm": 5.21544885635376,
      "learning_rate": 8.131752813059601e-07,
      "loss": 0.9013,
      "num_input_tokens_seen": 17506352,
      "step": 1395,
      "train_runtime": 16675.8861,
      "train_tokens_per_second": 1049.8
    },
    {
      "epoch": 0.28580177605389406,
      "grad_norm": 6.284928798675537,
      "learning_rate": 8.119239277971883e-07,
      "loss": 0.9842,
      "num_input_tokens_seen": 17568320,
      "step": 1400,
      "train_runtime": 16728.7124,
      "train_tokens_per_second": 1050.19
    },
    {
      "epoch": 0.28580177605389406,
      "eval_loss": 1.0853328704833984,
      "eval_runtime": 139.0041,
      "eval_samples_per_second": 5.698,
      "eval_steps_per_second": 2.849,
      "num_input_tokens_seen": 17568320,
      "step": 1400
    },
    {
      "epoch": 0.28682249668265797,
      "grad_norm": 6.344027042388916,
      "learning_rate": 8.106693674786765e-07,
      "loss": 0.9814,
      "num_input_tokens_seen": 17630672,
      "step": 1405,
      "train_runtime": 16920.9914,
      "train_tokens_per_second": 1041.941
    },
    {
      "epoch": 0.2878432173114219,
      "grad_norm": 6.1329569816589355,
      "learning_rate": 8.094116132482371e-07,
      "loss": 1.0149,
      "num_input_tokens_seen": 17692848,
      "step": 1410,
      "train_runtime": 16973.8777,
      "train_tokens_per_second": 1042.357
    },
    {
      "epoch": 0.2888639379401858,
      "grad_norm": 5.934677600860596,
      "learning_rate": 8.081506780365179e-07,
      "loss": 0.9884,
      "num_input_tokens_seen": 17754864,
      "step": 1415,
      "train_runtime": 17026.5365,
      "train_tokens_per_second": 1042.776
    },
    {
      "epoch": 0.2898846585689497,
      "grad_norm": 5.720710277557373,
      "learning_rate": 8.068865748068699e-07,
      "loss": 1.0598,
      "num_input_tokens_seen": 17818240,
      "step": 1420,
      "train_runtime": 17080.0929,
      "train_tokens_per_second": 1043.217
    },
    {
      "epoch": 0.2909053791977136,
      "grad_norm": 5.19024658203125,
      "learning_rate": 8.056193165552134e-07,
      "loss": 1.0783,
      "num_input_tokens_seen": 17880704,
      "step": 1425,
      "train_runtime": 17133.0987,
      "train_tokens_per_second": 1043.635
    },
    {
      "epoch": 0.2919260998264775,
      "grad_norm": 4.554210662841797,
      "learning_rate": 8.043489163099051e-07,
      "loss": 0.8756,
      "num_input_tokens_seen": 17943840,
      "step": 1430,
      "train_runtime": 17186.776,
      "train_tokens_per_second": 1044.049
    },
    {
      "epoch": 0.2929468204552414,
      "grad_norm": 5.508745193481445,
      "learning_rate": 8.030753871316031e-07,
      "loss": 1.1578,
      "num_input_tokens_seen": 18005456,
      "step": 1435,
      "train_runtime": 17239.3007,
      "train_tokens_per_second": 1044.442
    },
    {
      "epoch": 0.2939675410840053,
      "grad_norm": 11.23600959777832,
      "learning_rate": 8.017987421131338e-07,
      "loss": 1.5409,
      "num_input_tokens_seen": 18068368,
      "step": 1440,
      "train_runtime": 17292.6537,
      "train_tokens_per_second": 1044.858
    },
    {
      "epoch": 0.2949882617127692,
      "grad_norm": 6.535470485687256,
      "learning_rate": 8.005189943793564e-07,
      "loss": 0.914,
      "num_input_tokens_seen": 18130448,
      "step": 1445,
      "train_runtime": 17345.5256,
      "train_tokens_per_second": 1045.252
    },
    {
      "epoch": 0.2960089823415331,
      "grad_norm": 5.0012593269348145,
      "learning_rate": 7.992361570870287e-07,
      "loss": 1.1646,
      "num_input_tokens_seen": 18192064,
      "step": 1450,
      "train_runtime": 17397.9849,
      "train_tokens_per_second": 1045.642
    },
    {
      "epoch": 0.297029702970297,
      "grad_norm": 8.644357681274414,
      "learning_rate": 7.97950243424671e-07,
      "loss": 1.0132,
      "num_input_tokens_seen": 18254384,
      "step": 1455,
      "train_runtime": 17450.8327,
      "train_tokens_per_second": 1046.047
    },
    {
      "epoch": 0.2980504235990609,
      "grad_norm": 5.696107387542725,
      "learning_rate": 7.966612666124313e-07,
      "loss": 1.0263,
      "num_input_tokens_seen": 18316736,
      "step": 1460,
      "train_runtime": 17503.6737,
      "train_tokens_per_second": 1046.451
    },
    {
      "epoch": 0.2990711442278248,
      "grad_norm": 6.012519359588623,
      "learning_rate": 7.953692399019488e-07,
      "loss": 0.7597,
      "num_input_tokens_seen": 18379984,
      "step": 1465,
      "train_runtime": 17557.2008,
      "train_tokens_per_second": 1046.863
    },
    {
      "epoch": 0.30009186485658873,
      "grad_norm": 6.0560197830200195,
      "learning_rate": 7.940741765762185e-07,
      "loss": 0.7266,
      "num_input_tokens_seen": 18442272,
      "step": 1470,
      "train_runtime": 17610.1304,
      "train_tokens_per_second": 1047.254
    },
    {
      "epoch": 0.30111258548535264,
      "grad_norm": 4.497702121734619,
      "learning_rate": 7.92776089949453e-07,
      "loss": 0.9457,
      "num_input_tokens_seen": 18505888,
      "step": 1475,
      "train_runtime": 17664.0489,
      "train_tokens_per_second": 1047.658
    },
    {
      "epoch": 0.30213330611411654,
      "grad_norm": 4.444602966308594,
      "learning_rate": 7.914749933669478e-07,
      "loss": 0.9719,
      "num_input_tokens_seen": 18569056,
      "step": 1480,
      "train_runtime": 17717.6743,
      "train_tokens_per_second": 1048.053
    },
    {
      "epoch": 0.3031540267428805,
      "grad_norm": 4.5880889892578125,
      "learning_rate": 7.901709002049422e-07,
      "loss": 1.0129,
      "num_input_tokens_seen": 18632864,
      "step": 1485,
      "train_runtime": 17771.6328,
      "train_tokens_per_second": 1048.461
    },
    {
      "epoch": 0.3041747473716444,
      "grad_norm": 10.942383766174316,
      "learning_rate": 7.888638238704832e-07,
      "loss": 1.2546,
      "num_input_tokens_seen": 18695792,
      "step": 1490,
      "train_runtime": 17824.9893,
      "train_tokens_per_second": 1048.853
    },
    {
      "epoch": 0.3051954680004083,
      "grad_norm": 7.175558090209961,
      "learning_rate": 7.875537778012864e-07,
      "loss": 0.8799,
      "num_input_tokens_seen": 18759456,
      "step": 1495,
      "train_runtime": 17878.7881,
      "train_tokens_per_second": 1049.258
    },
    {
      "epoch": 0.3062161886291722,
      "grad_norm": 7.146203994750977,
      "learning_rate": 7.862407754655987e-07,
      "loss": 1.0169,
      "num_input_tokens_seen": 18821808,
      "step": 1500,
      "train_runtime": 17931.7287,
      "train_tokens_per_second": 1049.637
    },
    {
      "epoch": 0.3062161886291722,
      "eval_loss": 1.0783014297485352,
      "eval_runtime": 139.0401,
      "eval_samples_per_second": 5.696,
      "eval_steps_per_second": 2.848,
      "num_input_tokens_seen": 18821808,
      "step": 1500
    },
    {
      "epoch": 0.3072369092579361,
      "grad_norm": 5.199761867523193,
      "learning_rate": 7.849248303620601e-07,
      "loss": 1.1004,
      "num_input_tokens_seen": 18884224,
      "step": 1505,
      "train_runtime": 18124.1276,
      "train_tokens_per_second": 1041.938
    },
    {
      "epoch": 0.3082576298867,
      "grad_norm": 7.739002227783203,
      "learning_rate": 7.836059560195636e-07,
      "loss": 1.018,
      "num_input_tokens_seen": 18947616,
      "step": 1510,
      "train_runtime": 18177.7634,
      "train_tokens_per_second": 1042.351
    },
    {
      "epoch": 0.30927835051546393,
      "grad_norm": 5.897204875946045,
      "learning_rate": 7.822841659971175e-07,
      "loss": 0.7695,
      "num_input_tokens_seen": 19010272,
      "step": 1515,
      "train_runtime": 18231.0054,
      "train_tokens_per_second": 1042.744
    },
    {
      "epoch": 0.31029907114422783,
      "grad_norm": 4.4070281982421875,
      "learning_rate": 7.809594738837054e-07,
      "loss": 1.0941,
      "num_input_tokens_seen": 19072960,
      "step": 1520,
      "train_runtime": 18284.1952,
      "train_tokens_per_second": 1043.139
    },
    {
      "epoch": 0.31131979177299174,
      "grad_norm": 5.465912818908691,
      "learning_rate": 7.796318932981465e-07,
      "loss": 1.2319,
      "num_input_tokens_seen": 19135872,
      "step": 1525,
      "train_runtime": 18337.6326,
      "train_tokens_per_second": 1043.53
    },
    {
      "epoch": 0.31234051240175564,
      "grad_norm": 5.936097621917725,
      "learning_rate": 7.783014378889554e-07,
      "loss": 0.994,
      "num_input_tokens_seen": 19198656,
      "step": 1530,
      "train_runtime": 18390.8485,
      "train_tokens_per_second": 1043.924
    },
    {
      "epoch": 0.31336123303051955,
      "grad_norm": 10.649106979370117,
      "learning_rate": 7.769681213342028e-07,
      "loss": 0.7388,
      "num_input_tokens_seen": 19261296,
      "step": 1535,
      "train_runtime": 18443.987,
      "train_tokens_per_second": 1044.313
    },
    {
      "epoch": 0.31438195365928345,
      "grad_norm": 6.802757263183594,
      "learning_rate": 7.756319573413727e-07,
      "loss": 0.9228,
      "num_input_tokens_seen": 19324752,
      "step": 1540,
      "train_runtime": 18497.9445,
      "train_tokens_per_second": 1044.697
    },
    {
      "epoch": 0.31540267428804736,
      "grad_norm": 5.079766273498535,
      "learning_rate": 7.742929596472245e-07,
      "loss": 0.9755,
      "num_input_tokens_seen": 19387680,
      "step": 1545,
      "train_runtime": 18551.3321,
      "train_tokens_per_second": 1045.083
    },
    {
      "epoch": 0.31642339491681126,
      "grad_norm": 6.700004577636719,
      "learning_rate": 7.729511420176488e-07,
      "loss": 1.1448,
      "num_input_tokens_seen": 19450080,
      "step": 1550,
      "train_runtime": 18604.473,
      "train_tokens_per_second": 1045.452
    },
    {
      "epoch": 0.31744411554557517,
      "grad_norm": 7.708950996398926,
      "learning_rate": 7.716065182475284e-07,
      "loss": 1.0397,
      "num_input_tokens_seen": 19512208,
      "step": 1555,
      "train_runtime": 18657.2284,
      "train_tokens_per_second": 1045.826
    },
    {
      "epoch": 0.31846483617433907,
      "grad_norm": 4.882232189178467,
      "learning_rate": 7.702591021605942e-07,
      "loss": 1.0535,
      "num_input_tokens_seen": 19574032,
      "step": 1560,
      "train_runtime": 18709.6792,
      "train_tokens_per_second": 1046.198
    },
    {
      "epoch": 0.319485556803103,
      "grad_norm": 6.0892767906188965,
      "learning_rate": 7.68908907609285e-07,
      "loss": 1.1016,
      "num_input_tokens_seen": 19636656,
      "step": 1565,
      "train_runtime": 18762.8612,
      "train_tokens_per_second": 1046.57
    },
    {
      "epoch": 0.3205062774318669,
      "grad_norm": 8.472524642944336,
      "learning_rate": 7.675559484746039e-07,
      "loss": 0.8962,
      "num_input_tokens_seen": 19698992,
      "step": 1570,
      "train_runtime": 18815.6299,
      "train_tokens_per_second": 1046.948
    },
    {
      "epoch": 0.3215269980606308,
      "grad_norm": 5.665879726409912,
      "learning_rate": 7.662002386659761e-07,
      "loss": 0.9167,
      "num_input_tokens_seen": 19761008,
      "step": 1575,
      "train_runtime": 18868.4279,
      "train_tokens_per_second": 1047.305
    },
    {
      "epoch": 0.3225477186893947,
      "grad_norm": 4.951329231262207,
      "learning_rate": 7.648417921211059e-07,
      "loss": 1.0814,
      "num_input_tokens_seen": 19823568,
      "step": 1580,
      "train_runtime": 18921.5595,
      "train_tokens_per_second": 1047.671
    },
    {
      "epoch": 0.3235684393181586,
      "grad_norm": 6.194657802581787,
      "learning_rate": 7.634806228058326e-07,
      "loss": 1.2117,
      "num_input_tokens_seen": 19886512,
      "step": 1585,
      "train_runtime": 18975.0,
      "train_tokens_per_second": 1048.038
    },
    {
      "epoch": 0.3245891599469225,
      "grad_norm": 7.161766529083252,
      "learning_rate": 7.621167447139887e-07,
      "loss": 1.0034,
      "num_input_tokens_seen": 19948912,
      "step": 1590,
      "train_runtime": 19028.0321,
      "train_tokens_per_second": 1048.396
    },
    {
      "epoch": 0.32560988057568646,
      "grad_norm": 5.094991683959961,
      "learning_rate": 7.60750171867254e-07,
      "loss": 1.1208,
      "num_input_tokens_seen": 20012272,
      "step": 1595,
      "train_runtime": 19081.6827,
      "train_tokens_per_second": 1048.769
    },
    {
      "epoch": 0.32663060120445037,
      "grad_norm": 10.945178031921387,
      "learning_rate": 7.59380918315013e-07,
      "loss": 1.0352,
      "num_input_tokens_seen": 20074880,
      "step": 1600,
      "train_runtime": 19134.7966,
      "train_tokens_per_second": 1049.13
    },
    {
      "epoch": 0.32663060120445037,
      "eval_loss": 1.07244873046875,
      "eval_runtime": 139.0501,
      "eval_samples_per_second": 5.696,
      "eval_steps_per_second": 2.848,
      "num_input_tokens_seen": 20074880,
      "step": 1600
    },
    {
      "epoch": 0.32765132183321427,
      "grad_norm": 10.109759330749512,
      "learning_rate": 7.580089981342093e-07,
      "loss": 0.9776,
      "num_input_tokens_seen": 20137664,
      "step": 1605,
      "train_runtime": 19327.5398,
      "train_tokens_per_second": 1041.916
    },
    {
      "epoch": 0.3286720424619782,
      "grad_norm": 6.0225443840026855,
      "learning_rate": 7.566344254292021e-07,
      "loss": 1.0998,
      "num_input_tokens_seen": 20200288,
      "step": 1610,
      "train_runtime": 19380.7669,
      "train_tokens_per_second": 1042.285
    },
    {
      "epoch": 0.3296927630907421,
      "grad_norm": 6.264907360076904,
      "learning_rate": 7.552572143316195e-07,
      "loss": 0.9538,
      "num_input_tokens_seen": 20262352,
      "step": 1615,
      "train_runtime": 19433.5067,
      "train_tokens_per_second": 1042.65
    },
    {
      "epoch": 0.330713483719506,
      "grad_norm": 9.565278053283691,
      "learning_rate": 7.538773790002156e-07,
      "loss": 1.0532,
      "num_input_tokens_seen": 20325968,
      "step": 1620,
      "train_runtime": 19487.3483,
      "train_tokens_per_second": 1043.034
    },
    {
      "epoch": 0.3317342043482699,
      "grad_norm": 4.72737979888916,
      "learning_rate": 7.524949336207224e-07,
      "loss": 1.1181,
      "num_input_tokens_seen": 20388768,
      "step": 1625,
      "train_runtime": 19540.6526,
      "train_tokens_per_second": 1043.403
    },
    {
      "epoch": 0.3327549249770338,
      "grad_norm": 6.319767951965332,
      "learning_rate": 7.511098924057056e-07,
      "loss": 1.1795,
      "num_input_tokens_seen": 20451712,
      "step": 1630,
      "train_runtime": 19594.1521,
      "train_tokens_per_second": 1043.766
    },
    {
      "epoch": 0.3337756456057977,
      "grad_norm": 3.990690231323242,
      "learning_rate": 7.497222695944179e-07,
      "loss": 1.088,
      "num_input_tokens_seen": 20513824,
      "step": 1635,
      "train_runtime": 19646.8983,
      "train_tokens_per_second": 1044.125
    },
    {
      "epoch": 0.3347963662345616,
      "grad_norm": 12.210511207580566,
      "learning_rate": 7.483320794526531e-07,
      "loss": 1.0704,
      "num_input_tokens_seen": 20576736,
      "step": 1640,
      "train_runtime": 19700.1692,
      "train_tokens_per_second": 1044.495
    },
    {
      "epoch": 0.3358170868633255,
      "grad_norm": 6.118156433105469,
      "learning_rate": 7.469393362725987e-07,
      "loss": 0.9455,
      "num_input_tokens_seen": 20640272,
      "step": 1645,
      "train_runtime": 19753.8393,
      "train_tokens_per_second": 1044.874
    },
    {
      "epoch": 0.3368378074920894,
      "grad_norm": 6.240097522735596,
      "learning_rate": 7.455440543726891e-07,
      "loss": 0.9585,
      "num_input_tokens_seen": 20702864,
      "step": 1650,
      "train_runtime": 19806.892,
      "train_tokens_per_second": 1045.235
    },
    {
      "epoch": 0.3378585281208533,
      "grad_norm": 5.69447660446167,
      "learning_rate": 7.441462480974593e-07,
      "loss": 0.8266,
      "num_input_tokens_seen": 20765728,
      "step": 1655,
      "train_runtime": 19860.3178,
      "train_tokens_per_second": 1045.589
    },
    {
      "epoch": 0.3388792487496172,
      "grad_norm": 5.519064903259277,
      "learning_rate": 7.427459318173963e-07,
      "loss": 0.9388,
      "num_input_tokens_seen": 20828864,
      "step": 1660,
      "train_runtime": 19913.977,
      "train_tokens_per_second": 1045.942
    },
    {
      "epoch": 0.33989996937838113,
      "grad_norm": 10.460068702697754,
      "learning_rate": 7.41343119928792e-07,
      "loss": 1.4145,
      "num_input_tokens_seen": 20891072,
      "step": 1665,
      "train_runtime": 19966.8544,
      "train_tokens_per_second": 1046.288
    },
    {
      "epoch": 0.34092069000714503,
      "grad_norm": 5.777851581573486,
      "learning_rate": 7.399378268535947e-07,
      "loss": 0.9753,
      "num_input_tokens_seen": 20953504,
      "step": 1670,
      "train_runtime": 20019.9026,
      "train_tokens_per_second": 1046.634
    },
    {
      "epoch": 0.34194141063590894,
      "grad_norm": 6.563647747039795,
      "learning_rate": 7.385300670392614e-07,
      "loss": 0.923,
      "num_input_tokens_seen": 21016048,
      "step": 1675,
      "train_runtime": 20073.137,
      "train_tokens_per_second": 1046.974
    },
    {
      "epoch": 0.34296213126467284,
      "grad_norm": 7.388986110687256,
      "learning_rate": 7.37119854958609e-07,
      "loss": 0.9876,
      "num_input_tokens_seen": 21078992,
      "step": 1680,
      "train_runtime": 20126.4783,
      "train_tokens_per_second": 1047.326
    },
    {
      "epoch": 0.34398285189343675,
      "grad_norm": 5.244178295135498,
      "learning_rate": 7.357072051096657e-07,
      "loss": 0.9719,
      "num_input_tokens_seen": 21142320,
      "step": 1685,
      "train_runtime": 20180.1736,
      "train_tokens_per_second": 1047.678
    },
    {
      "epoch": 0.34500357252220065,
      "grad_norm": 5.2661638259887695,
      "learning_rate": 7.342921320155213e-07,
      "loss": 0.9784,
      "num_input_tokens_seen": 21205024,
      "step": 1690,
      "train_runtime": 20233.2829,
      "train_tokens_per_second": 1048.027
    },
    {
      "epoch": 0.34602429315096456,
      "grad_norm": 5.163333892822266,
      "learning_rate": 7.328746502241784e-07,
      "loss": 1.1203,
      "num_input_tokens_seen": 21267040,
      "step": 1695,
      "train_runtime": 20286.0735,
      "train_tokens_per_second": 1048.357
    },
    {
      "epoch": 0.34704501377972846,
      "grad_norm": 4.895970344543457,
      "learning_rate": 7.314547743084036e-07,
      "loss": 0.9325,
      "num_input_tokens_seen": 21331136,
      "step": 1700,
      "train_runtime": 20340.2537,
      "train_tokens_per_second": 1048.715
    },
    {
      "epoch": 0.34704501377972846,
      "eval_loss": 1.0666950941085815,
      "eval_runtime": 139.3046,
      "eval_samples_per_second": 5.685,
      "eval_steps_per_second": 2.843,
      "num_input_tokens_seen": 21331136,
      "step": 1700
    },
    {
      "epoch": 0.3480657344084924,
      "grad_norm": 7.198530197143555,
      "learning_rate": 7.30032518865576e-07,
      "loss": 1.1657,
      "num_input_tokens_seen": 21393760,
      "step": 1705,
      "train_runtime": 20533.1712,
      "train_tokens_per_second": 1041.912
    },
    {
      "epoch": 0.3490864550372563,
      "grad_norm": 5.203395843505859,
      "learning_rate": 7.286078985175384e-07,
      "loss": 0.907,
      "num_input_tokens_seen": 21457664,
      "step": 1710,
      "train_runtime": 20587.2676,
      "train_tokens_per_second": 1042.278
    },
    {
      "epoch": 0.35010717566602023,
      "grad_norm": 16.309343338012695,
      "learning_rate": 7.271809279104461e-07,
      "loss": 0.9725,
      "num_input_tokens_seen": 21520224,
      "step": 1715,
      "train_runtime": 20640.3243,
      "train_tokens_per_second": 1042.63
    },
    {
      "epoch": 0.35112789629478414,
      "grad_norm": 8.281571388244629,
      "learning_rate": 7.257516217146176e-07,
      "loss": 1.0485,
      "num_input_tokens_seen": 21583984,
      "step": 1720,
      "train_runtime": 20694.3055,
      "train_tokens_per_second": 1042.991
    },
    {
      "epoch": 0.35214861692354804,
      "grad_norm": 6.432028293609619,
      "learning_rate": 7.243199946243825e-07,
      "loss": 0.8721,
      "num_input_tokens_seen": 21647696,
      "step": 1725,
      "train_runtime": 20748.1886,
      "train_tokens_per_second": 1043.354
    },
    {
      "epoch": 0.35316933755231195,
      "grad_norm": 5.3462815284729,
      "learning_rate": 7.228860613579311e-07,
      "loss": 0.8712,
      "num_input_tokens_seen": 21710368,
      "step": 1730,
      "train_runtime": 20801.3366,
      "train_tokens_per_second": 1043.701
    },
    {
      "epoch": 0.35419005818107585,
      "grad_norm": 7.650034427642822,
      "learning_rate": 7.214498366571624e-07,
      "loss": 0.9553,
      "num_input_tokens_seen": 21772656,
      "step": 1735,
      "train_runtime": 20854.1223,
      "train_tokens_per_second": 1044.046
    },
    {
      "epoch": 0.35521077880983976,
      "grad_norm": 8.04316234588623,
      "learning_rate": 7.200113352875339e-07,
      "loss": 0.9564,
      "num_input_tokens_seen": 21835472,
      "step": 1740,
      "train_runtime": 20907.4256,
      "train_tokens_per_second": 1044.388
    },
    {
      "epoch": 0.35623149943860366,
      "grad_norm": 6.568754196166992,
      "learning_rate": 7.185705720379081e-07,
      "loss": 1.0089,
      "num_input_tokens_seen": 21898368,
      "step": 1745,
      "train_runtime": 20960.5358,
      "train_tokens_per_second": 1044.743
    },
    {
      "epoch": 0.35725222006736757,
      "grad_norm": 10.297517776489258,
      "learning_rate": 7.17127561720402e-07,
      "loss": 1.0446,
      "num_input_tokens_seen": 21961968,
      "step": 1750,
      "train_runtime": 21014.2553,
      "train_tokens_per_second": 1045.099
    },
    {
      "epoch": 0.35827294069613147,
      "grad_norm": 5.546505451202393,
      "learning_rate": 7.156823191702338e-07,
      "loss": 1.1582,
      "num_input_tokens_seen": 22024784,
      "step": 1755,
      "train_runtime": 21067.4201,
      "train_tokens_per_second": 1045.443
    },
    {
      "epoch": 0.3592936613248954,
      "grad_norm": 5.243874549865723,
      "learning_rate": 7.142348592455707e-07,
      "loss": 0.8331,
      "num_input_tokens_seen": 22087056,
      "step": 1760,
      "train_runtime": 21120.2056,
      "train_tokens_per_second": 1045.778
    },
    {
      "epoch": 0.3603143819536593,
      "grad_norm": 8.095311164855957,
      "learning_rate": 7.127851968273761e-07,
      "loss": 1.0491,
      "num_input_tokens_seen": 22148848,
      "step": 1765,
      "train_runtime": 21172.8015,
      "train_tokens_per_second": 1046.099
    },
    {
      "epoch": 0.3613351025824232,
      "grad_norm": 7.792939186096191,
      "learning_rate": 7.11333346819257e-07,
      "loss": 1.1038,
      "num_input_tokens_seen": 22211728,
      "step": 1770,
      "train_runtime": 21226.0259,
      "train_tokens_per_second": 1046.438
    },
    {
      "epoch": 0.3623558232111871,
      "grad_norm": 7.402597904205322,
      "learning_rate": 7.0987932414731e-07,
      "loss": 0.8801,
      "num_input_tokens_seen": 22274256,
      "step": 1775,
      "train_runtime": 21279.0951,
      "train_tokens_per_second": 1046.767
    },
    {
      "epoch": 0.363376543839951,
      "grad_norm": 6.314855575561523,
      "learning_rate": 7.084231437599687e-07,
      "loss": 1.1868,
      "num_input_tokens_seen": 22337056,
      "step": 1780,
      "train_runtime": 21332.3,
      "train_tokens_per_second": 1047.1
    },
    {
      "epoch": 0.3643972644687149,
      "grad_norm": 5.555235385894775,
      "learning_rate": 7.069648206278493e-07,
      "loss": 1.1606,
      "num_input_tokens_seen": 22400448,
      "step": 1785,
      "train_runtime": 21385.8631,
      "train_tokens_per_second": 1047.442
    },
    {
      "epoch": 0.3654179850974788,
      "grad_norm": 5.431410789489746,
      "learning_rate": 7.055043697435969e-07,
      "loss": 1.1343,
      "num_input_tokens_seen": 22462720,
      "step": 1790,
      "train_runtime": 21438.6675,
      "train_tokens_per_second": 1047.767
    },
    {
      "epoch": 0.3664387057262427,
      "grad_norm": 4.8865275382995605,
      "learning_rate": 7.040418061217324e-07,
      "loss": 0.9881,
      "num_input_tokens_seen": 22524672,
      "step": 1795,
      "train_runtime": 21491.2476,
      "train_tokens_per_second": 1048.086
    },
    {
      "epoch": 0.3674594263550066,
      "grad_norm": 3.687481641769409,
      "learning_rate": 7.025771447984961e-07,
      "loss": 1.1896,
      "num_input_tokens_seen": 22587792,
      "step": 1800,
      "train_runtime": 21544.7039,
      "train_tokens_per_second": 1048.415
    },
    {
      "epoch": 0.3674594263550066,
      "eval_loss": 1.0623548030853271,
      "eval_runtime": 139.0976,
      "eval_samples_per_second": 5.694,
      "eval_steps_per_second": 2.847,
      "num_input_tokens_seen": 22587792,
      "step": 1800
    },
    {
      "epoch": 0.3684801469837705,
      "grad_norm": 4.95028018951416,
      "learning_rate": 7.011104008316946e-07,
      "loss": 1.0538,
      "num_input_tokens_seen": 22651200,
      "step": 1805,
      "train_runtime": 21737.6829,
      "train_tokens_per_second": 1042.025
    },
    {
      "epoch": 0.3695008676125344,
      "grad_norm": 8.450555801391602,
      "learning_rate": 6.996415893005457e-07,
      "loss": 1.2133,
      "num_input_tokens_seen": 22714112,
      "step": 1810,
      "train_runtime": 21791.0456,
      "train_tokens_per_second": 1042.36
    },
    {
      "epoch": 0.3705215882412984,
      "grad_norm": 7.889845848083496,
      "learning_rate": 6.981707253055233e-07,
      "loss": 1.1914,
      "num_input_tokens_seen": 22777168,
      "step": 1815,
      "train_runtime": 21844.582,
      "train_tokens_per_second": 1042.692
    },
    {
      "epoch": 0.3715423088700623,
      "grad_norm": 6.48744535446167,
      "learning_rate": 6.966978239682025e-07,
      "loss": 0.8921,
      "num_input_tokens_seen": 22840400,
      "step": 1820,
      "train_runtime": 21898.1144,
      "train_tokens_per_second": 1043.03
    },
    {
      "epoch": 0.3725630294988262,
      "grad_norm": 5.223031997680664,
      "learning_rate": 6.952229004311029e-07,
      "loss": 1.2736,
      "num_input_tokens_seen": 22903344,
      "step": 1825,
      "train_runtime": 21951.5748,
      "train_tokens_per_second": 1043.358
    },
    {
      "epoch": 0.3735837501275901,
      "grad_norm": 5.293354511260986,
      "learning_rate": 6.937459698575345e-07,
      "loss": 1.2774,
      "num_input_tokens_seen": 22966384,
      "step": 1830,
      "train_runtime": 22005.1009,
      "train_tokens_per_second": 1043.685
    },
    {
      "epoch": 0.374604470756354,
      "grad_norm": 5.915149211883545,
      "learning_rate": 6.92267047431441e-07,
      "loss": 0.9,
      "num_input_tokens_seen": 23030016,
      "step": 1835,
      "train_runtime": 22058.982,
      "train_tokens_per_second": 1044.02
    },
    {
      "epoch": 0.3756251913851179,
      "grad_norm": 6.405385971069336,
      "learning_rate": 6.90786148357244e-07,
      "loss": 1.079,
      "num_input_tokens_seen": 23093072,
      "step": 1840,
      "train_runtime": 22112.4797,
      "train_tokens_per_second": 1044.346
    },
    {
      "epoch": 0.3766459120138818,
      "grad_norm": 5.656139850616455,
      "learning_rate": 6.893032878596861e-07,
      "loss": 0.8975,
      "num_input_tokens_seen": 23154896,
      "step": 1845,
      "train_runtime": 22164.9899,
      "train_tokens_per_second": 1044.661
    },
    {
      "epoch": 0.3776666326426457,
      "grad_norm": 4.324532985687256,
      "learning_rate": 6.878184811836749e-07,
      "loss": 1.2429,
      "num_input_tokens_seen": 23219088,
      "step": 1850,
      "train_runtime": 22219.282,
      "train_tokens_per_second": 1044.997
    },
    {
      "epoch": 0.3786873532714096,
      "grad_norm": 4.191282749176025,
      "learning_rate": 6.863317435941265e-07,
      "loss": 0.8476,
      "num_input_tokens_seen": 23281136,
      "step": 1855,
      "train_runtime": 22272.1203,
      "train_tokens_per_second": 1045.304
    },
    {
      "epoch": 0.3797080739001735,
      "grad_norm": 5.193295955657959,
      "learning_rate": 6.848430903758078e-07,
      "loss": 1.0341,
      "num_input_tokens_seen": 23344080,
      "step": 1860,
      "train_runtime": 22325.5481,
      "train_tokens_per_second": 1045.622
    },
    {
      "epoch": 0.38072879452893743,
      "grad_norm": 8.297348976135254,
      "learning_rate": 6.833525368331799e-07,
      "loss": 0.8869,
      "num_input_tokens_seen": 23406896,
      "step": 1865,
      "train_runtime": 22378.9175,
      "train_tokens_per_second": 1045.935
    },
    {
      "epoch": 0.38174951515770134,
      "grad_norm": 6.610518932342529,
      "learning_rate": 6.818600982902409e-07,
      "loss": 1.0505,
      "num_input_tokens_seen": 23470480,
      "step": 1870,
      "train_runtime": 22432.8525,
      "train_tokens_per_second": 1046.255
    },
    {
      "epoch": 0.38277023578646524,
      "grad_norm": 4.428318023681641,
      "learning_rate": 6.80365790090368e-07,
      "loss": 0.9419,
      "num_input_tokens_seen": 23533680,
      "step": 1875,
      "train_runtime": 22486.489,
      "train_tokens_per_second": 1046.57
    },
    {
      "epoch": 0.38379095641522915,
      "grad_norm": 6.002419471740723,
      "learning_rate": 6.788696275961594e-07,
      "loss": 1.054,
      "num_input_tokens_seen": 23596432,
      "step": 1880,
      "train_runtime": 22539.6837,
      "train_tokens_per_second": 1046.884
    },
    {
      "epoch": 0.38481167704399305,
      "grad_norm": 5.9656524658203125,
      "learning_rate": 6.773716261892776e-07,
      "loss": 0.9194,
      "num_input_tokens_seen": 23657504,
      "step": 1885,
      "train_runtime": 22591.6125,
      "train_tokens_per_second": 1047.181
    },
    {
      "epoch": 0.38583239767275695,
      "grad_norm": 8.441079139709473,
      "learning_rate": 6.758718012702897e-07,
      "loss": 0.8349,
      "num_input_tokens_seen": 23719936,
      "step": 1890,
      "train_runtime": 22644.5689,
      "train_tokens_per_second": 1047.489
    },
    {
      "epoch": 0.38685311830152086,
      "grad_norm": 6.272989749908447,
      "learning_rate": 6.743701682585107e-07,
      "loss": 0.9589,
      "num_input_tokens_seen": 23782608,
      "step": 1895,
      "train_runtime": 22697.8271,
      "train_tokens_per_second": 1047.792
    },
    {
      "epoch": 0.38787383893028476,
      "grad_norm": 4.380527019500732,
      "learning_rate": 6.728667425918433e-07,
      "loss": 0.8863,
      "num_input_tokens_seen": 23846176,
      "step": 1900,
      "train_runtime": 22751.5475,
      "train_tokens_per_second": 1048.112
    },
    {
      "epoch": 0.38787383893028476,
      "eval_loss": 1.0585455894470215,
      "eval_runtime": 139.0708,
      "eval_samples_per_second": 5.695,
      "eval_steps_per_second": 2.847,
      "num_input_tokens_seen": 23846176,
      "step": 1900
    },
    {
      "epoch": 0.38889455955904867,
      "grad_norm": 10.083523750305176,
      "learning_rate": 6.713615397266206e-07,
      "loss": 1.1109,
      "num_input_tokens_seen": 23908864,
      "step": 1905,
      "train_runtime": 22944.1157,
      "train_tokens_per_second": 1042.048
    },
    {
      "epoch": 0.3899152801878126,
      "grad_norm": 5.492186069488525,
      "learning_rate": 6.698545751374463e-07,
      "loss": 0.9661,
      "num_input_tokens_seen": 23970976,
      "step": 1910,
      "train_runtime": 22996.9954,
      "train_tokens_per_second": 1042.353
    },
    {
      "epoch": 0.3909360008165765,
      "grad_norm": 7.096139430999756,
      "learning_rate": 6.683458643170363e-07,
      "loss": 1.0663,
      "num_input_tokens_seen": 24033104,
      "step": 1915,
      "train_runtime": 23049.6893,
      "train_tokens_per_second": 1042.665
    },
    {
      "epoch": 0.3919567214453404,
      "grad_norm": 6.763495445251465,
      "learning_rate": 6.668354227760588e-07,
      "loss": 1.144,
      "num_input_tokens_seen": 24094960,
      "step": 1920,
      "train_runtime": 23102.3624,
      "train_tokens_per_second": 1042.965
    },
    {
      "epoch": 0.39297744207410434,
      "grad_norm": 8.048849105834961,
      "learning_rate": 6.653232660429748e-07,
      "loss": 1.0181,
      "num_input_tokens_seen": 24157088,
      "step": 1925,
      "train_runtime": 23155.2159,
      "train_tokens_per_second": 1043.268
    },
    {
      "epoch": 0.39399816270286825,
      "grad_norm": 7.605035305023193,
      "learning_rate": 6.638094096638793e-07,
      "loss": 0.8789,
      "num_input_tokens_seen": 24219968,
      "step": 1930,
      "train_runtime": 23208.5275,
      "train_tokens_per_second": 1043.581
    },
    {
      "epoch": 0.39501888333163215,
      "grad_norm": 5.578488826751709,
      "learning_rate": 6.622938692023405e-07,
      "loss": 1.3359,
      "num_input_tokens_seen": 24282464,
      "step": 1935,
      "train_runtime": 23261.6032,
      "train_tokens_per_second": 1043.886
    },
    {
      "epoch": 0.39603960396039606,
      "grad_norm": 6.109207630157471,
      "learning_rate": 6.607766602392409e-07,
      "loss": 1.1086,
      "num_input_tokens_seen": 24345152,
      "step": 1940,
      "train_runtime": 23314.7796,
      "train_tokens_per_second": 1044.194
    },
    {
      "epoch": 0.39706032458915996,
      "grad_norm": 4.044875144958496,
      "learning_rate": 6.592577983726153e-07,
      "loss": 1.0226,
      "num_input_tokens_seen": 24408208,
      "step": 1945,
      "train_runtime": 23368.2138,
      "train_tokens_per_second": 1044.505
    },
    {
      "epoch": 0.39808104521792387,
      "grad_norm": 4.766841411590576,
      "learning_rate": 6.577372992174924e-07,
      "loss": 1.0139,
      "num_input_tokens_seen": 24470240,
      "step": 1950,
      "train_runtime": 23420.7035,
      "train_tokens_per_second": 1044.812
    },
    {
      "epoch": 0.3991017658466878,
      "grad_norm": 5.578009605407715,
      "learning_rate": 6.562151784057337e-07,
      "loss": 1.0039,
      "num_input_tokens_seen": 24532752,
      "step": 1955,
      "train_runtime": 23473.8279,
      "train_tokens_per_second": 1045.111
    },
    {
      "epoch": 0.4001224864754517,
      "grad_norm": 6.302968502044678,
      "learning_rate": 6.546914515858716e-07,
      "loss": 1.1525,
      "num_input_tokens_seen": 24594832,
      "step": 1960,
      "train_runtime": 23526.6132,
      "train_tokens_per_second": 1045.405
    },
    {
      "epoch": 0.4011432071042156,
      "grad_norm": 5.4015703201293945,
      "learning_rate": 6.531661344229499e-07,
      "loss": 0.788,
      "num_input_tokens_seen": 24658288,
      "step": 1965,
      "train_runtime": 23580.2253,
      "train_tokens_per_second": 1045.719
    },
    {
      "epoch": 0.4021639277329795,
      "grad_norm": 7.026062965393066,
      "learning_rate": 6.516392425983629e-07,
      "loss": 1.1022,
      "num_input_tokens_seen": 24721360,
      "step": 1970,
      "train_runtime": 23633.6958,
      "train_tokens_per_second": 1046.022
    },
    {
      "epoch": 0.4031846483617434,
      "grad_norm": 4.147274494171143,
      "learning_rate": 6.501107918096926e-07,
      "loss": 1.0142,
      "num_input_tokens_seen": 24783008,
      "step": 1975,
      "train_runtime": 23685.9111,
      "train_tokens_per_second": 1046.319
    },
    {
      "epoch": 0.4042053689905073,
      "grad_norm": 8.539856910705566,
      "learning_rate": 6.485807977705489e-07,
      "loss": 1.1866,
      "num_input_tokens_seen": 24845920,
      "step": 1980,
      "train_runtime": 23739.1536,
      "train_tokens_per_second": 1046.622
    },
    {
      "epoch": 0.4052260896192712,
      "grad_norm": 6.4967265129089355,
      "learning_rate": 6.470492762104071e-07,
      "loss": 0.9087,
      "num_input_tokens_seen": 24908576,
      "step": 1985,
      "train_runtime": 23792.0927,
      "train_tokens_per_second": 1046.927
    },
    {
      "epoch": 0.4062468102480351,
      "grad_norm": 4.210155487060547,
      "learning_rate": 6.455162428744472e-07,
      "loss": 1.0124,
      "num_input_tokens_seen": 24971632,
      "step": 1990,
      "train_runtime": 23845.6023,
      "train_tokens_per_second": 1047.222
    },
    {
      "epoch": 0.407267530876799,
      "grad_norm": 4.8076863288879395,
      "learning_rate": 6.439817135233905e-07,
      "loss": 0.8369,
      "num_input_tokens_seen": 25034768,
      "step": 1995,
      "train_runtime": 23899.1188,
      "train_tokens_per_second": 1047.518
    },
    {
      "epoch": 0.4082882515055629,
      "grad_norm": 5.783966541290283,
      "learning_rate": 6.424457039333393e-07,
      "loss": 1.1709,
      "num_input_tokens_seen": 25097520,
      "step": 2000,
      "train_runtime": 23952.4026,
      "train_tokens_per_second": 1047.808
    },
    {
      "epoch": 0.4082882515055629,
      "eval_loss": 1.0532909631729126,
      "eval_runtime": 139.2459,
      "eval_samples_per_second": 5.688,
      "eval_steps_per_second": 2.844,
      "num_input_tokens_seen": 25097520,
      "step": 2000
    },
    {
      "epoch": 0.4093089721343268,
      "grad_norm": 9.297202110290527,
      "learning_rate": 6.409082298956132e-07,
      "loss": 0.9597,
      "num_input_tokens_seen": 25159504,
      "step": 2005,
      "train_runtime": 24144.9478,
      "train_tokens_per_second": 1042.019
    },
    {
      "epoch": 0.4103296927630907,
      "grad_norm": 6.770421028137207,
      "learning_rate": 6.393693072165878e-07,
      "loss": 1.1645,
      "num_input_tokens_seen": 25223088,
      "step": 2010,
      "train_runtime": 24198.7845,
      "train_tokens_per_second": 1042.329
    },
    {
      "epoch": 0.41135041339185463,
      "grad_norm": 5.086892604827881,
      "learning_rate": 6.378289517175315e-07,
      "loss": 1.1233,
      "num_input_tokens_seen": 25284592,
      "step": 2015,
      "train_runtime": 24251.2129,
      "train_tokens_per_second": 1042.611
    },
    {
      "epoch": 0.41237113402061853,
      "grad_norm": 4.059338092803955,
      "learning_rate": 6.362871792344435e-07,
      "loss": 0.8682,
      "num_input_tokens_seen": 25346928,
      "step": 2020,
      "train_runtime": 24304.3965,
      "train_tokens_per_second": 1042.895
    },
    {
      "epoch": 0.41339185464938244,
      "grad_norm": 5.413133144378662,
      "learning_rate": 6.347440056178903e-07,
      "loss": 1.1038,
      "num_input_tokens_seen": 25409328,
      "step": 2025,
      "train_runtime": 24357.5664,
      "train_tokens_per_second": 1043.18
    },
    {
      "epoch": 0.41441257527814634,
      "grad_norm": 8.665266990661621,
      "learning_rate": 6.331994467328436e-07,
      "loss": 1.3148,
      "num_input_tokens_seen": 25471584,
      "step": 2030,
      "train_runtime": 24410.5219,
      "train_tokens_per_second": 1043.467
    },
    {
      "epoch": 0.4154332959069103,
      "grad_norm": 6.59625768661499,
      "learning_rate": 6.316535184585161e-07,
      "loss": 1.0323,
      "num_input_tokens_seen": 25534112,
      "step": 2035,
      "train_runtime": 24463.5741,
      "train_tokens_per_second": 1043.76
    },
    {
      "epoch": 0.4164540165356742,
      "grad_norm": 5.771824836730957,
      "learning_rate": 6.30106236688199e-07,
      "loss": 1.0603,
      "num_input_tokens_seen": 25597824,
      "step": 2040,
      "train_runtime": 24517.4845,
      "train_tokens_per_second": 1044.064
    },
    {
      "epoch": 0.4174747371644381,
      "grad_norm": 4.365814208984375,
      "learning_rate": 6.285576173290988e-07,
      "loss": 1.0977,
      "num_input_tokens_seen": 25660784,
      "step": 2045,
      "train_runtime": 24570.8745,
      "train_tokens_per_second": 1044.358
    },
    {
      "epoch": 0.418495457793202,
      "grad_norm": 5.471062183380127,
      "learning_rate": 6.270076763021729e-07,
      "loss": 0.9059,
      "num_input_tokens_seen": 25723664,
      "step": 2050,
      "train_runtime": 24624.2257,
      "train_tokens_per_second": 1044.649
    },
    {
      "epoch": 0.4195161784219659,
      "grad_norm": 5.455268859863281,
      "learning_rate": 6.254564295419668e-07,
      "loss": 0.802,
      "num_input_tokens_seen": 25786960,
      "step": 2055,
      "train_runtime": 24677.9443,
      "train_tokens_per_second": 1044.94
    },
    {
      "epoch": 0.42053689905072983,
      "grad_norm": 3.5446012020111084,
      "learning_rate": 6.2390389299645e-07,
      "loss": 0.7879,
      "num_input_tokens_seen": 25849936,
      "step": 2060,
      "train_runtime": 24731.5,
      "train_tokens_per_second": 1045.223
    },
    {
      "epoch": 0.42155761967949373,
      "grad_norm": 7.071827411651611,
      "learning_rate": 6.223500826268513e-07,
      "loss": 1.0665,
      "num_input_tokens_seen": 25912128,
      "step": 2065,
      "train_runtime": 24784.3059,
      "train_tokens_per_second": 1045.505
    },
    {
      "epoch": 0.42257834030825764,
      "grad_norm": 6.106618404388428,
      "learning_rate": 6.207950144074962e-07,
      "loss": 0.9241,
      "num_input_tokens_seen": 25975248,
      "step": 2070,
      "train_runtime": 24837.7756,
      "train_tokens_per_second": 1045.796
    },
    {
      "epoch": 0.42359906093702154,
      "grad_norm": 5.582944393157959,
      "learning_rate": 6.192387043256412e-07,
      "loss": 1.0607,
      "num_input_tokens_seen": 26039184,
      "step": 2075,
      "train_runtime": 24891.7815,
      "train_tokens_per_second": 1046.096
    },
    {
      "epoch": 0.42461978156578545,
      "grad_norm": 4.7650933265686035,
      "learning_rate": 6.176811683813103e-07,
      "loss": 0.8482,
      "num_input_tokens_seen": 26101792,
      "step": 2080,
      "train_runtime": 24945.0366,
      "train_tokens_per_second": 1046.372
    },
    {
      "epoch": 0.42564050219454935,
      "grad_norm": 5.632542133331299,
      "learning_rate": 6.161224225871302e-07,
      "loss": 0.8877,
      "num_input_tokens_seen": 26164800,
      "step": 2085,
      "train_runtime": 24998.3976,
      "train_tokens_per_second": 1046.659
    },
    {
      "epoch": 0.42666122282331326,
      "grad_norm": 5.542108535766602,
      "learning_rate": 6.14562482968166e-07,
      "loss": 1.1654,
      "num_input_tokens_seen": 26227312,
      "step": 2090,
      "train_runtime": 25051.3803,
      "train_tokens_per_second": 1046.941
    },
    {
      "epoch": 0.42768194345207716,
      "grad_norm": 5.445756912231445,
      "learning_rate": 6.130013655617561e-07,
      "loss": 0.9624,
      "num_input_tokens_seen": 26290288,
      "step": 2095,
      "train_runtime": 25104.7587,
      "train_tokens_per_second": 1047.223
    },
    {
      "epoch": 0.42870266408084107,
      "grad_norm": 6.468879699707031,
      "learning_rate": 6.114390864173472e-07,
      "loss": 1.2518,
      "num_input_tokens_seen": 26351968,
      "step": 2100,
      "train_runtime": 25157.1818,
      "train_tokens_per_second": 1047.493
    },
    {
      "epoch": 0.42870266408084107,
      "eval_loss": 1.0502557754516602,
      "eval_runtime": 139.1788,
      "eval_samples_per_second": 5.691,
      "eval_steps_per_second": 2.845,
      "num_input_tokens_seen": 26351968,
      "step": 2100
    },
    {
      "epoch": 0.42972338470960497,
      "grad_norm": 5.360573768615723,
      "learning_rate": 6.098756615963296e-07,
      "loss": 0.9762,
      "num_input_tokens_seen": 26414768,
      "step": 2105,
      "train_runtime": 25349.8899,
      "train_tokens_per_second": 1042.007
    },
    {
      "epoch": 0.4307441053383689,
      "grad_norm": 4.7843780517578125,
      "learning_rate": 6.083111071718723e-07,
      "loss": 0.816,
      "num_input_tokens_seen": 26477232,
      "step": 2110,
      "train_runtime": 25402.8895,
      "train_tokens_per_second": 1042.292
    },
    {
      "epoch": 0.4317648259671328,
      "grad_norm": 12.480283737182617,
      "learning_rate": 6.067454392287572e-07,
      "loss": 1.1025,
      "num_input_tokens_seen": 26539200,
      "step": 2115,
      "train_runtime": 25455.4701,
      "train_tokens_per_second": 1042.574
    },
    {
      "epoch": 0.4327855465958967,
      "grad_norm": 5.124603271484375,
      "learning_rate": 6.051786738632141e-07,
      "loss": 1.0581,
      "num_input_tokens_seen": 26601744,
      "step": 2120,
      "train_runtime": 25508.7272,
      "train_tokens_per_second": 1042.849
    },
    {
      "epoch": 0.4338062672246606,
      "grad_norm": 6.083224296569824,
      "learning_rate": 6.036108271827548e-07,
      "loss": 1.1524,
      "num_input_tokens_seen": 26664416,
      "step": 2125,
      "train_runtime": 25561.8534,
      "train_tokens_per_second": 1043.133
    },
    {
      "epoch": 0.4348269878534245,
      "grad_norm": 5.817627429962158,
      "learning_rate": 6.020419153060085e-07,
      "loss": 1.2915,
      "num_input_tokens_seen": 26726768,
      "step": 2130,
      "train_runtime": 25614.8386,
      "train_tokens_per_second": 1043.41
    },
    {
      "epoch": 0.4358477084821884,
      "grad_norm": 6.042919635772705,
      "learning_rate": 6.004719543625547e-07,
      "loss": 1.0366,
      "num_input_tokens_seen": 26789616,
      "step": 2135,
      "train_runtime": 25668.2361,
      "train_tokens_per_second": 1043.687
    },
    {
      "epoch": 0.4368684291109523,
      "grad_norm": 4.042465686798096,
      "learning_rate": 5.989009604927586e-07,
      "loss": 0.8525,
      "num_input_tokens_seen": 26852480,
      "step": 2140,
      "train_runtime": 25721.525,
      "train_tokens_per_second": 1043.969
    },
    {
      "epoch": 0.43788914973971627,
      "grad_norm": 5.170300006866455,
      "learning_rate": 5.973289498476044e-07,
      "loss": 0.8329,
      "num_input_tokens_seen": 26915440,
      "step": 2145,
      "train_runtime": 25775.0516,
      "train_tokens_per_second": 1044.244
    },
    {
      "epoch": 0.43890987036848017,
      "grad_norm": 6.909649848937988,
      "learning_rate": 5.957559385885294e-07,
      "loss": 1.255,
      "num_input_tokens_seen": 26978912,
      "step": 2150,
      "train_runtime": 25828.7622,
      "train_tokens_per_second": 1044.53
    },
    {
      "epoch": 0.4399305909972441,
      "grad_norm": 6.108521461486816,
      "learning_rate": 5.941819428872583e-07,
      "loss": 1.0816,
      "num_input_tokens_seen": 27041424,
      "step": 2155,
      "train_runtime": 25881.7433,
      "train_tokens_per_second": 1044.807
    },
    {
      "epoch": 0.440951311626008,
      "grad_norm": 7.163941860198975,
      "learning_rate": 5.926069789256363e-07,
      "loss": 1.1206,
      "num_input_tokens_seen": 27104496,
      "step": 2160,
      "train_runtime": 25935.4616,
      "train_tokens_per_second": 1045.075
    },
    {
      "epoch": 0.4419720322547719,
      "grad_norm": 5.620089530944824,
      "learning_rate": 5.910310628954631e-07,
      "loss": 1.0948,
      "num_input_tokens_seen": 27167984,
      "step": 2165,
      "train_runtime": 25989.0439,
      "train_tokens_per_second": 1045.363
    },
    {
      "epoch": 0.4429927528835358,
      "grad_norm": 5.716185092926025,
      "learning_rate": 5.894542109983266e-07,
      "loss": 1.0576,
      "num_input_tokens_seen": 27229152,
      "step": 2170,
      "train_runtime": 26041.1026,
      "train_tokens_per_second": 1045.622
    },
    {
      "epoch": 0.4440134735122997,
      "grad_norm": 5.072196960449219,
      "learning_rate": 5.878764394454357e-07,
      "loss": 1.1917,
      "num_input_tokens_seen": 27291600,
      "step": 2175,
      "train_runtime": 26094.2166,
      "train_tokens_per_second": 1045.887
    },
    {
      "epoch": 0.4450341941410636,
      "grad_norm": 8.910542488098145,
      "learning_rate": 5.862977644574542e-07,
      "loss": 0.8717,
      "num_input_tokens_seen": 27354752,
      "step": 2180,
      "train_runtime": 26147.8275,
      "train_tokens_per_second": 1046.158
    },
    {
      "epoch": 0.4460549147698275,
      "grad_norm": 4.462408065795898,
      "learning_rate": 5.847182022643343e-07,
      "loss": 0.6681,
      "num_input_tokens_seen": 27416208,
      "step": 2185,
      "train_runtime": 26200.1474,
      "train_tokens_per_second": 1046.414
    },
    {
      "epoch": 0.4470756353985914,
      "grad_norm": 3.9006361961364746,
      "learning_rate": 5.831377691051486e-07,
      "loss": 0.8024,
      "num_input_tokens_seen": 27479696,
      "step": 2190,
      "train_runtime": 26253.8287,
      "train_tokens_per_second": 1046.693
    },
    {
      "epoch": 0.4480963560273553,
      "grad_norm": 3.991114616394043,
      "learning_rate": 5.815564812279247e-07,
      "loss": 0.999,
      "num_input_tokens_seen": 27543408,
      "step": 2195,
      "train_runtime": 26307.7203,
      "train_tokens_per_second": 1046.971
    },
    {
      "epoch": 0.4491170766561192,
      "grad_norm": 6.303785800933838,
      "learning_rate": 5.799743548894762e-07,
      "loss": 1.0195,
      "num_input_tokens_seen": 27605648,
      "step": 2200,
      "train_runtime": 26360.5513,
      "train_tokens_per_second": 1047.233
    },
    {
      "epoch": 0.4491170766561192,
      "eval_loss": 1.04597806930542,
      "eval_runtime": 139.1713,
      "eval_samples_per_second": 5.691,
      "eval_steps_per_second": 2.845,
      "num_input_tokens_seen": 27605648,
      "step": 2200
    },
    {
      "epoch": 0.4501377972848831,
      "grad_norm": 6.372260570526123,
      "learning_rate": 5.78391406355238e-07,
      "loss": 1.0821,
      "num_input_tokens_seen": 27668256,
      "step": 2205,
      "train_runtime": 26553.4881,
      "train_tokens_per_second": 1041.982
    },
    {
      "epoch": 0.45115851791364703,
      "grad_norm": 5.693017959594727,
      "learning_rate": 5.76807651899097e-07,
      "loss": 0.9579,
      "num_input_tokens_seen": 27731200,
      "step": 2210,
      "train_runtime": 26606.9362,
      "train_tokens_per_second": 1042.255
    },
    {
      "epoch": 0.45217923854241093,
      "grad_norm": 3.9768874645233154,
      "learning_rate": 5.752231078032258e-07,
      "loss": 1.136,
      "num_input_tokens_seen": 27795040,
      "step": 2215,
      "train_runtime": 26660.8378,
      "train_tokens_per_second": 1042.542
    },
    {
      "epoch": 0.45319995917117484,
      "grad_norm": 5.081507682800293,
      "learning_rate": 5.736377903579148e-07,
      "loss": 1.246,
      "num_input_tokens_seen": 27856832,
      "step": 2220,
      "train_runtime": 26713.4122,
      "train_tokens_per_second": 1042.803
    },
    {
      "epoch": 0.45422067979993874,
      "grad_norm": 6.677011489868164,
      "learning_rate": 5.720517158614055e-07,
      "loss": 1.1585,
      "num_input_tokens_seen": 27919312,
      "step": 2225,
      "train_runtime": 26766.3728,
      "train_tokens_per_second": 1043.074
    },
    {
      "epoch": 0.45524140042870265,
      "grad_norm": 5.888448238372803,
      "learning_rate": 5.704649006197219e-07,
      "loss": 1.1864,
      "num_input_tokens_seen": 27981456,
      "step": 2230,
      "train_runtime": 26819.1679,
      "train_tokens_per_second": 1043.338
    },
    {
      "epoch": 0.45626212105746655,
      "grad_norm": 7.534964561462402,
      "learning_rate": 5.688773609465042e-07,
      "loss": 0.9287,
      "num_input_tokens_seen": 28044256,
      "step": 2235,
      "train_runtime": 26872.4108,
      "train_tokens_per_second": 1043.608
    },
    {
      "epoch": 0.45728284168623046,
      "grad_norm": 7.12473726272583,
      "learning_rate": 5.672891131628392e-07,
      "loss": 1.012,
      "num_input_tokens_seen": 28106848,
      "step": 2240,
      "train_runtime": 26925.5857,
      "train_tokens_per_second": 1043.871
    },
    {
      "epoch": 0.45830356231499436,
      "grad_norm": 7.3981781005859375,
      "learning_rate": 5.657001735970946e-07,
      "loss": 0.9679,
      "num_input_tokens_seen": 28170688,
      "step": 2245,
      "train_runtime": 26979.592,
      "train_tokens_per_second": 1044.148
    },
    {
      "epoch": 0.45932428294375827,
      "grad_norm": 8.834365844726562,
      "learning_rate": 5.641105585847497e-07,
      "loss": 1.1017,
      "num_input_tokens_seen": 28233040,
      "step": 2250,
      "train_runtime": 27032.5522,
      "train_tokens_per_second": 1044.409
    },
    {
      "epoch": 0.4603450035725222,
      "grad_norm": 5.247727394104004,
      "learning_rate": 5.62520284468228e-07,
      "loss": 1.0213,
      "num_input_tokens_seen": 28294928,
      "step": 2255,
      "train_runtime": 27085.1096,
      "train_tokens_per_second": 1044.667
    },
    {
      "epoch": 0.46136572420128613,
      "grad_norm": 5.840823650360107,
      "learning_rate": 5.609293675967285e-07,
      "loss": 0.9343,
      "num_input_tokens_seen": 28357312,
      "step": 2260,
      "train_runtime": 27138.0862,
      "train_tokens_per_second": 1044.927
    },
    {
      "epoch": 0.46238644483005004,
      "grad_norm": 5.570422649383545,
      "learning_rate": 5.593378243260593e-07,
      "loss": 1.0181,
      "num_input_tokens_seen": 28420464,
      "step": 2265,
      "train_runtime": 27191.5445,
      "train_tokens_per_second": 1045.195
    },
    {
      "epoch": 0.46340716545881394,
      "grad_norm": 6.747886657714844,
      "learning_rate": 5.577456710184674e-07,
      "loss": 1.0523,
      "num_input_tokens_seen": 28483248,
      "step": 2270,
      "train_runtime": 27244.7995,
      "train_tokens_per_second": 1045.456
    },
    {
      "epoch": 0.46442788608757785,
      "grad_norm": 6.760429382324219,
      "learning_rate": 5.561529240424719e-07,
      "loss": 0.9987,
      "num_input_tokens_seen": 28545456,
      "step": 2275,
      "train_runtime": 27297.5953,
      "train_tokens_per_second": 1045.713
    },
    {
      "epoch": 0.46544860671634175,
      "grad_norm": 5.682442665100098,
      "learning_rate": 5.54559599772695e-07,
      "loss": 1.0277,
      "num_input_tokens_seen": 28608320,
      "step": 2280,
      "train_runtime": 27350.964,
      "train_tokens_per_second": 1045.971
    },
    {
      "epoch": 0.46646932734510566,
      "grad_norm": 7.092230319976807,
      "learning_rate": 5.529657145896945e-07,
      "loss": 0.919,
      "num_input_tokens_seen": 28669632,
      "step": 2285,
      "train_runtime": 27403.1494,
      "train_tokens_per_second": 1046.217
    },
    {
      "epoch": 0.46749004797386956,
      "grad_norm": 5.132143020629883,
      "learning_rate": 5.513712848797938e-07,
      "loss": 1.1417,
      "num_input_tokens_seen": 28732928,
      "step": 2290,
      "train_runtime": 27456.6187,
      "train_tokens_per_second": 1046.485
    },
    {
      "epoch": 0.46851076860263347,
      "grad_norm": 5.481903076171875,
      "learning_rate": 5.497763270349152e-07,
      "loss": 1.1677,
      "num_input_tokens_seen": 28795088,
      "step": 2295,
      "train_runtime": 27509.4984,
      "train_tokens_per_second": 1046.733
    },
    {
      "epoch": 0.46953148923139737,
      "grad_norm": 6.824049949645996,
      "learning_rate": 5.481808574524106e-07,
      "loss": 1.1781,
      "num_input_tokens_seen": 28857040,
      "step": 2300,
      "train_runtime": 27562.1825,
      "train_tokens_per_second": 1046.979
    },
    {
      "epoch": 0.46953148923139737,
      "eval_loss": 1.0426998138427734,
      "eval_runtime": 138.9902,
      "eval_samples_per_second": 5.698,
      "eval_steps_per_second": 2.849,
      "num_input_tokens_seen": 28857040,
      "step": 2300
    },
    {
      "epoch": 0.4705522098601613,
      "grad_norm": 5.543666839599609,
      "learning_rate": 5.465848925348924e-07,
      "loss": 1.0602,
      "num_input_tokens_seen": 28919392,
      "step": 2305,
      "train_runtime": 27754.7115,
      "train_tokens_per_second": 1041.963
    },
    {
      "epoch": 0.4715729304889252,
      "grad_norm": 6.675838947296143,
      "learning_rate": 5.449884486900662e-07,
      "loss": 0.8754,
      "num_input_tokens_seen": 28982176,
      "step": 2310,
      "train_runtime": 27808.0195,
      "train_tokens_per_second": 1042.224
    },
    {
      "epoch": 0.4725936511176891,
      "grad_norm": 6.944748401641846,
      "learning_rate": 5.433915423305605e-07,
      "loss": 0.9987,
      "num_input_tokens_seen": 29044176,
      "step": 2315,
      "train_runtime": 27860.9342,
      "train_tokens_per_second": 1042.47
    },
    {
      "epoch": 0.473614371746453,
      "grad_norm": 4.933085918426514,
      "learning_rate": 5.417941898737593e-07,
      "loss": 0.8658,
      "num_input_tokens_seen": 29106240,
      "step": 2320,
      "train_runtime": 27913.6114,
      "train_tokens_per_second": 1042.726
    },
    {
      "epoch": 0.4746350923752169,
      "grad_norm": 5.189825057983398,
      "learning_rate": 5.40196407741633e-07,
      "loss": 1.173,
      "num_input_tokens_seen": 29168320,
      "step": 2325,
      "train_runtime": 27966.259,
      "train_tokens_per_second": 1042.983
    },
    {
      "epoch": 0.4756558130039808,
      "grad_norm": 7.220056056976318,
      "learning_rate": 5.385982123605687e-07,
      "loss": 1.0846,
      "num_input_tokens_seen": 29230864,
      "step": 2330,
      "train_runtime": 28019.2987,
      "train_tokens_per_second": 1043.24
    },
    {
      "epoch": 0.4766765336327447,
      "grad_norm": 5.845276355743408,
      "learning_rate": 5.369996201612023e-07,
      "loss": 0.8456,
      "num_input_tokens_seen": 29293408,
      "step": 2335,
      "train_runtime": 28072.3243,
      "train_tokens_per_second": 1043.498
    },
    {
      "epoch": 0.4776972542615086,
      "grad_norm": 9.625162124633789,
      "learning_rate": 5.354006475782499e-07,
      "loss": 1.0864,
      "num_input_tokens_seen": 29356192,
      "step": 2340,
      "train_runtime": 28125.541,
      "train_tokens_per_second": 1043.756
    },
    {
      "epoch": 0.4787179748902725,
      "grad_norm": 6.361910343170166,
      "learning_rate": 5.338013110503373e-07,
      "loss": 0.9268,
      "num_input_tokens_seen": 29418784,
      "step": 2345,
      "train_runtime": 28178.8045,
      "train_tokens_per_second": 1044.004
    },
    {
      "epoch": 0.4797386955190364,
      "grad_norm": 5.423088550567627,
      "learning_rate": 5.322016270198325e-07,
      "loss": 0.9986,
      "num_input_tokens_seen": 29481568,
      "step": 2350,
      "train_runtime": 28232.0161,
      "train_tokens_per_second": 1044.26
    },
    {
      "epoch": 0.4807594161478003,
      "grad_norm": 3.848550319671631,
      "learning_rate": 5.306016119326757e-07,
      "loss": 1.0729,
      "num_input_tokens_seen": 29544880,
      "step": 2355,
      "train_runtime": 28285.6576,
      "train_tokens_per_second": 1044.518
    },
    {
      "epoch": 0.4817801367765642,
      "grad_norm": 7.5392069816589355,
      "learning_rate": 5.290012822382111e-07,
      "loss": 1.0069,
      "num_input_tokens_seen": 29607488,
      "step": 2360,
      "train_runtime": 28338.7402,
      "train_tokens_per_second": 1044.771
    },
    {
      "epoch": 0.4828008574053282,
      "grad_norm": 4.685451507568359,
      "learning_rate": 5.274006543890168e-07,
      "loss": 0.9939,
      "num_input_tokens_seen": 29670608,
      "step": 2365,
      "train_runtime": 28392.2351,
      "train_tokens_per_second": 1045.025
    },
    {
      "epoch": 0.4838215780340921,
      "grad_norm": 6.601507186889648,
      "learning_rate": 5.257997448407366e-07,
      "loss": 1.0955,
      "num_input_tokens_seen": 29732160,
      "step": 2370,
      "train_runtime": 28444.5777,
      "train_tokens_per_second": 1045.266
    },
    {
      "epoch": 0.484842298662856,
      "grad_norm": 4.982137203216553,
      "learning_rate": 5.241985700519098e-07,
      "loss": 1.0197,
      "num_input_tokens_seen": 29793936,
      "step": 2375,
      "train_runtime": 28497.1393,
      "train_tokens_per_second": 1045.506
    },
    {
      "epoch": 0.4858630192916199,
      "grad_norm": 5.835064888000488,
      "learning_rate": 5.225971464838033e-07,
      "loss": 0.9831,
      "num_input_tokens_seen": 29856400,
      "step": 2380,
      "train_runtime": 28550.1701,
      "train_tokens_per_second": 1045.752
    },
    {
      "epoch": 0.4868837399203838,
      "grad_norm": 5.704997539520264,
      "learning_rate": 5.209954906002409e-07,
      "loss": 0.9834,
      "num_input_tokens_seen": 29918784,
      "step": 2385,
      "train_runtime": 28603.1528,
      "train_tokens_per_second": 1045.996
    },
    {
      "epoch": 0.4879044605491477,
      "grad_norm": 5.765879154205322,
      "learning_rate": 5.193936188674355e-07,
      "loss": 0.8965,
      "num_input_tokens_seen": 29981584,
      "step": 2390,
      "train_runtime": 28656.3986,
      "train_tokens_per_second": 1046.244
    },
    {
      "epoch": 0.4889251811779116,
      "grad_norm": 4.252723217010498,
      "learning_rate": 5.177915477538183e-07,
      "loss": 0.8017,
      "num_input_tokens_seen": 30044016,
      "step": 2395,
      "train_runtime": 28709.4641,
      "train_tokens_per_second": 1046.485
    },
    {
      "epoch": 0.4899459018066755,
      "grad_norm": 6.210324287414551,
      "learning_rate": 5.161892937298711e-07,
      "loss": 0.6951,
      "num_input_tokens_seen": 30106992,
      "step": 2400,
      "train_runtime": 28762.8348,
      "train_tokens_per_second": 1046.732
    },
    {
      "epoch": 0.4899459018066755,
      "eval_loss": 1.039351463317871,
      "eval_runtime": 138.96,
      "eval_samples_per_second": 5.699,
      "eval_steps_per_second": 2.85,
      "num_input_tokens_seen": 30106992,
      "step": 2400
    },
    {
      "epoch": 0.4909666224354394,
      "grad_norm": 6.441654205322266,
      "learning_rate": 5.145868732679554e-07,
      "loss": 1.0059,
      "num_input_tokens_seen": 30169984,
      "step": 2405,
      "train_runtime": 28955.742,
      "train_tokens_per_second": 1041.934
    },
    {
      "epoch": 0.49198734306420333,
      "grad_norm": 4.830658912658691,
      "learning_rate": 5.129843028421445e-07,
      "loss": 1.016,
      "num_input_tokens_seen": 30233632,
      "step": 2410,
      "train_runtime": 29009.4563,
      "train_tokens_per_second": 1042.199
    },
    {
      "epoch": 0.49300806369296724,
      "grad_norm": 5.452207088470459,
      "learning_rate": 5.113815989280528e-07,
      "loss": 0.9791,
      "num_input_tokens_seen": 30296784,
      "step": 2415,
      "train_runtime": 29062.9002,
      "train_tokens_per_second": 1042.456
    },
    {
      "epoch": 0.49402878432173114,
      "grad_norm": 6.1681928634643555,
      "learning_rate": 5.097787780026677e-07,
      "loss": 1.0246,
      "num_input_tokens_seen": 30358912,
      "step": 2420,
      "train_runtime": 29115.6456,
      "train_tokens_per_second": 1042.701
    },
    {
      "epoch": 0.49504950495049505,
      "grad_norm": 4.7145771980285645,
      "learning_rate": 5.081758565441787e-07,
      "loss": 0.93,
      "num_input_tokens_seen": 30421904,
      "step": 2425,
      "train_runtime": 29168.9357,
      "train_tokens_per_second": 1042.956
    },
    {
      "epoch": 0.49607022557925895,
      "grad_norm": 7.3452677726745605,
      "learning_rate": 5.065728510318099e-07,
      "loss": 1.0642,
      "num_input_tokens_seen": 30484784,
      "step": 2430,
      "train_runtime": 29222.1716,
      "train_tokens_per_second": 1043.207
    },
    {
      "epoch": 0.49709094620802285,
      "grad_norm": 5.267432689666748,
      "learning_rate": 5.049697779456487e-07,
      "loss": 1.2029,
      "num_input_tokens_seen": 30548352,
      "step": 2435,
      "train_runtime": 29276.1218,
      "train_tokens_per_second": 1043.456
    },
    {
      "epoch": 0.49811166683678676,
      "grad_norm": 4.77575159072876,
      "learning_rate": 5.033666537664778e-07,
      "loss": 1.1218,
      "num_input_tokens_seen": 30611872,
      "step": 2440,
      "train_runtime": 29329.91,
      "train_tokens_per_second": 1043.708
    },
    {
      "epoch": 0.49913238746555066,
      "grad_norm": 8.366584777832031,
      "learning_rate": 5.017634949756046e-07,
      "loss": 1.0655,
      "num_input_tokens_seen": 30674624,
      "step": 2445,
      "train_runtime": 29383.0935,
      "train_tokens_per_second": 1043.955
    },
    {
      "epoch": 0.5001531080943146,
      "grad_norm": 6.040102958679199,
      "learning_rate": 5.001603180546929e-07,
      "loss": 1.0043,
      "num_input_tokens_seen": 30736384,
      "step": 2450,
      "train_runtime": 29435.5165,
      "train_tokens_per_second": 1044.194
    },
    {
      "epoch": 0.5011738287230785,
      "grad_norm": 5.47639274597168,
      "learning_rate": 4.985571394855923e-07,
      "loss": 1.0714,
      "num_input_tokens_seen": 30798240,
      "step": 2455,
      "train_runtime": 29488.0769,
      "train_tokens_per_second": 1044.43
    },
    {
      "epoch": 0.5021945493518424,
      "grad_norm": 7.100369930267334,
      "learning_rate": 4.969539757501699e-07,
      "loss": 1.2512,
      "num_input_tokens_seen": 30860576,
      "step": 2460,
      "train_runtime": 29541.0914,
      "train_tokens_per_second": 1044.666
    },
    {
      "epoch": 0.5032152699806063,
      "grad_norm": 5.406302452087402,
      "learning_rate": 4.953508433301399e-07,
      "loss": 1.5282,
      "num_input_tokens_seen": 30924176,
      "step": 2465,
      "train_runtime": 29595.24,
      "train_tokens_per_second": 1044.904
    },
    {
      "epoch": 0.5042359906093702,
      "grad_norm": 6.369935035705566,
      "learning_rate": 4.93747758706895e-07,
      "loss": 1.0207,
      "num_input_tokens_seen": 30987280,
      "step": 2470,
      "train_runtime": 29648.8219,
      "train_tokens_per_second": 1045.144
    },
    {
      "epoch": 0.5052567112381341,
      "grad_norm": 4.299724578857422,
      "learning_rate": 4.92144738361336e-07,
      "loss": 1.0322,
      "num_input_tokens_seen": 31050480,
      "step": 2475,
      "train_runtime": 29702.3042,
      "train_tokens_per_second": 1045.39
    },
    {
      "epoch": 0.506277431866898,
      "grad_norm": 8.254528045654297,
      "learning_rate": 4.905417987737032e-07,
      "loss": 0.9872,
      "num_input_tokens_seen": 31112800,
      "step": 2480,
      "train_runtime": 29755.3032,
      "train_tokens_per_second": 1045.622
    },
    {
      "epoch": 0.5072981524956619,
      "grad_norm": 5.931041717529297,
      "learning_rate": 4.889389564234066e-07,
      "loss": 0.9632,
      "num_input_tokens_seen": 31175760,
      "step": 2485,
      "train_runtime": 29808.7438,
      "train_tokens_per_second": 1045.86
    },
    {
      "epoch": 0.5083188731244258,
      "grad_norm": 5.778702259063721,
      "learning_rate": 4.873362277888564e-07,
      "loss": 1.067,
      "num_input_tokens_seen": 31239024,
      "step": 2490,
      "train_runtime": 29862.5246,
      "train_tokens_per_second": 1046.095
    },
    {
      "epoch": 0.5093395937531897,
      "grad_norm": 8.427371978759766,
      "learning_rate": 4.857336293472937e-07,
      "loss": 1.0345,
      "num_input_tokens_seen": 31301424,
      "step": 2495,
      "train_runtime": 29915.5337,
      "train_tokens_per_second": 1046.327
    },
    {
      "epoch": 0.5103603143819536,
      "grad_norm": 4.802748680114746,
      "learning_rate": 4.841311775746215e-07,
      "loss": 1.0146,
      "num_input_tokens_seen": 31363648,
      "step": 2500,
      "train_runtime": 29968.3574,
      "train_tokens_per_second": 1046.559
    },
    {
      "epoch": 0.5103603143819536,
      "eval_loss": 1.036899447441101,
      "eval_runtime": 139.0619,
      "eval_samples_per_second": 5.695,
      "eval_steps_per_second": 2.848,
      "num_input_tokens_seen": 31363648,
      "step": 2500
    },
    {
      "epoch": 0.5113810350107175,
      "grad_norm": 6.359377384185791,
      "learning_rate": 4.825288889452341e-07,
      "loss": 0.99,
      "num_input_tokens_seen": 31426576,
      "step": 2505,
      "train_runtime": 30161.0677,
      "train_tokens_per_second": 1041.958
    },
    {
      "epoch": 0.5124017556394814,
      "grad_norm": 6.661026954650879,
      "learning_rate": 4.8092677993185e-07,
      "loss": 1.12,
      "num_input_tokens_seen": 31487856,
      "step": 2510,
      "train_runtime": 30213.2349,
      "train_tokens_per_second": 1042.188
    },
    {
      "epoch": 0.5134224762682453,
      "grad_norm": 6.053446292877197,
      "learning_rate": 4.793248670053396e-07,
      "loss": 1.1548,
      "num_input_tokens_seen": 31549536,
      "step": 2515,
      "train_runtime": 30265.6522,
      "train_tokens_per_second": 1042.42
    },
    {
      "epoch": 0.5144431968970092,
      "grad_norm": 8.072811126708984,
      "learning_rate": 4.777231666345582e-07,
      "loss": 1.0202,
      "num_input_tokens_seen": 31612224,
      "step": 2520,
      "train_runtime": 30318.7286,
      "train_tokens_per_second": 1042.663
    },
    {
      "epoch": 0.5154639175257731,
      "grad_norm": 4.776825428009033,
      "learning_rate": 4.7612169528617576e-07,
      "loss": 0.8816,
      "num_input_tokens_seen": 31673888,
      "step": 2525,
      "train_runtime": 30371.1478,
      "train_tokens_per_second": 1042.894
    },
    {
      "epoch": 0.5164846381545372,
      "grad_norm": 7.44138765335083,
      "learning_rate": 4.745204694245075e-07,
      "loss": 1.1419,
      "num_input_tokens_seen": 31736064,
      "step": 2530,
      "train_runtime": 30423.9997,
      "train_tokens_per_second": 1043.126
    },
    {
      "epoch": 0.5175053587833011,
      "grad_norm": 5.486195087432861,
      "learning_rate": 4.7291950551134514e-07,
      "loss": 0.9766,
      "num_input_tokens_seen": 31799328,
      "step": 2535,
      "train_runtime": 30477.5976,
      "train_tokens_per_second": 1043.367
    },
    {
      "epoch": 0.518526079412065,
      "grad_norm": 5.558590412139893,
      "learning_rate": 4.71318820005787e-07,
      "loss": 0.8112,
      "num_input_tokens_seen": 31862016,
      "step": 2540,
      "train_runtime": 30530.7436,
      "train_tokens_per_second": 1043.604
    },
    {
      "epoch": 0.5195468000408289,
      "grad_norm": 3.600163698196411,
      "learning_rate": 4.6971842936406944e-07,
      "loss": 0.8253,
      "num_input_tokens_seen": 31925712,
      "step": 2545,
      "train_runtime": 30584.556,
      "train_tokens_per_second": 1043.851
    },
    {
      "epoch": 0.5205675206695928,
      "grad_norm": 5.059237480163574,
      "learning_rate": 4.6811835003939776e-07,
      "loss": 1.0604,
      "num_input_tokens_seen": 31988112,
      "step": 2550,
      "train_runtime": 30637.5063,
      "train_tokens_per_second": 1044.083
    },
    {
      "epoch": 0.5215882412983567,
      "grad_norm": 4.2453460693359375,
      "learning_rate": 4.66518598481776e-07,
      "loss": 0.9312,
      "num_input_tokens_seen": 32050592,
      "step": 2555,
      "train_runtime": 30690.5874,
      "train_tokens_per_second": 1044.313
    },
    {
      "epoch": 0.5226089619271206,
      "grad_norm": 6.244878768920898,
      "learning_rate": 4.649191911378388e-07,
      "loss": 0.8817,
      "num_input_tokens_seen": 32114752,
      "step": 2560,
      "train_runtime": 30744.8346,
      "train_tokens_per_second": 1044.558
    },
    {
      "epoch": 0.5236296825558845,
      "grad_norm": 7.216942310333252,
      "learning_rate": 4.6332014445068203e-07,
      "loss": 1.2083,
      "num_input_tokens_seen": 32177760,
      "step": 2565,
      "train_runtime": 30798.1231,
      "train_tokens_per_second": 1044.796
    },
    {
      "epoch": 0.5246504031846484,
      "grad_norm": 6.984803199768066,
      "learning_rate": 4.6172147485969374e-07,
      "loss": 0.9428,
      "num_input_tokens_seen": 32240560,
      "step": 2570,
      "train_runtime": 30851.4246,
      "train_tokens_per_second": 1045.027
    },
    {
      "epoch": 0.5256711238134123,
      "grad_norm": 6.110620021820068,
      "learning_rate": 4.6012319880038523e-07,
      "loss": 0.8883,
      "num_input_tokens_seen": 32302016,
      "step": 2575,
      "train_runtime": 30903.8482,
      "train_tokens_per_second": 1045.243
    },
    {
      "epoch": 0.5266918444421762,
      "grad_norm": 5.748082160949707,
      "learning_rate": 4.585253327042217e-07,
      "loss": 1.051,
      "num_input_tokens_seen": 32364864,
      "step": 2580,
      "train_runtime": 30957.0607,
      "train_tokens_per_second": 1045.476
    },
    {
      "epoch": 0.5277125650709401,
      "grad_norm": 7.785414695739746,
      "learning_rate": 4.5692789299845415e-07,
      "loss": 1.1854,
      "num_input_tokens_seen": 32426784,
      "step": 2585,
      "train_runtime": 31009.767,
      "train_tokens_per_second": 1045.696
    },
    {
      "epoch": 0.528733285699704,
      "grad_norm": 5.79042911529541,
      "learning_rate": 4.5533089610594943e-07,
      "loss": 1.0989,
      "num_input_tokens_seen": 32489504,
      "step": 2590,
      "train_runtime": 31063.1028,
      "train_tokens_per_second": 1045.919
    },
    {
      "epoch": 0.5297540063284679,
      "grad_norm": 5.01813268661499,
      "learning_rate": 4.537343584450224e-07,
      "loss": 1.1496,
      "num_input_tokens_seen": 32551040,
      "step": 2595,
      "train_runtime": 31115.2989,
      "train_tokens_per_second": 1046.143
    },
    {
      "epoch": 0.5307747269572318,
      "grad_norm": 4.3176960945129395,
      "learning_rate": 4.521382964292663e-07,
      "loss": 0.9981,
      "num_input_tokens_seen": 32612560,
      "step": 2600,
      "train_runtime": 31167.7786,
      "train_tokens_per_second": 1046.355
    },
    {
      "epoch": 0.5307747269572318,
      "eval_loss": 1.0333915948867798,
      "eval_runtime": 139.1219,
      "eval_samples_per_second": 5.693,
      "eval_steps_per_second": 2.846,
      "num_input_tokens_seen": 32612560,
      "step": 2600
    },
    {
      "epoch": 0.5317954475859957,
      "grad_norm": 8.34347152709961,
      "learning_rate": 4.5054272646738457e-07,
      "loss": 1.02,
      "num_input_tokens_seen": 32674544,
      "step": 2605,
      "train_runtime": 31359.9904,
      "train_tokens_per_second": 1041.918
    },
    {
      "epoch": 0.5328161682147596,
      "grad_norm": 4.381896495819092,
      "learning_rate": 4.4894766496302183e-07,
      "loss": 0.9095,
      "num_input_tokens_seen": 32738080,
      "step": 2610,
      "train_runtime": 31414.0633,
      "train_tokens_per_second": 1042.147
    },
    {
      "epoch": 0.5338368888435235,
      "grad_norm": 5.291217803955078,
      "learning_rate": 4.473531283145958e-07,
      "loss": 0.9451,
      "num_input_tokens_seen": 32800272,
      "step": 2615,
      "train_runtime": 31466.9376,
      "train_tokens_per_second": 1042.373
    },
    {
      "epoch": 0.5348576094722874,
      "grad_norm": 6.262387752532959,
      "learning_rate": 4.457591329151274e-07,
      "loss": 0.9615,
      "num_input_tokens_seen": 32862352,
      "step": 2620,
      "train_runtime": 31519.6255,
      "train_tokens_per_second": 1042.6
    },
    {
      "epoch": 0.5358783301010513,
      "grad_norm": 5.753005027770996,
      "learning_rate": 4.4416569515207443e-07,
      "loss": 1.0836,
      "num_input_tokens_seen": 32924944,
      "step": 2625,
      "train_runtime": 31572.6067,
      "train_tokens_per_second": 1042.833
    },
    {
      "epoch": 0.5368990507298153,
      "grad_norm": 6.440984725952148,
      "learning_rate": 4.4257283140716056e-07,
      "loss": 1.2479,
      "num_input_tokens_seen": 32987632,
      "step": 2630,
      "train_runtime": 31625.7617,
      "train_tokens_per_second": 1043.062
    },
    {
      "epoch": 0.5379197713585792,
      "grad_norm": 4.714015007019043,
      "learning_rate": 4.4098055805620866e-07,
      "loss": 0.942,
      "num_input_tokens_seen": 33050016,
      "step": 2635,
      "train_runtime": 31678.7607,
      "train_tokens_per_second": 1043.286
    },
    {
      "epoch": 0.5389404919873431,
      "grad_norm": 8.083740234375,
      "learning_rate": 4.39388891468972e-07,
      "loss": 1.0554,
      "num_input_tokens_seen": 33112976,
      "step": 2640,
      "train_runtime": 31732.2205,
      "train_tokens_per_second": 1043.513
    },
    {
      "epoch": 0.539961212616107,
      "grad_norm": 6.927300930023193,
      "learning_rate": 4.377978480089656e-07,
      "loss": 1.1399,
      "num_input_tokens_seen": 33175568,
      "step": 2645,
      "train_runtime": 31785.3707,
      "train_tokens_per_second": 1043.737
    },
    {
      "epoch": 0.5409819332448709,
      "grad_norm": 8.721567153930664,
      "learning_rate": 4.3620744403329846e-07,
      "loss": 1.2524,
      "num_input_tokens_seen": 33239280,
      "step": 2650,
      "train_runtime": 31839.3885,
      "train_tokens_per_second": 1043.967
    },
    {
      "epoch": 0.5420026538736348,
      "grad_norm": 4.163532257080078,
      "learning_rate": 4.346176958925049e-07,
      "loss": 0.7667,
      "num_input_tokens_seen": 33301504,
      "step": 2655,
      "train_runtime": 31892.1115,
      "train_tokens_per_second": 1044.193
    },
    {
      "epoch": 0.5430233745023987,
      "grad_norm": 8.02316665649414,
      "learning_rate": 4.3302861993037715e-07,
      "loss": 0.8542,
      "num_input_tokens_seen": 33364304,
      "step": 2660,
      "train_runtime": 31945.3698,
      "train_tokens_per_second": 1044.418
    },
    {
      "epoch": 0.5440440951311626,
      "grad_norm": 6.683812618255615,
      "learning_rate": 4.31440232483797e-07,
      "loss": 1.0071,
      "num_input_tokens_seen": 33426864,
      "step": 2665,
      "train_runtime": 31998.508,
      "train_tokens_per_second": 1044.638
    },
    {
      "epoch": 0.5450648157599265,
      "grad_norm": 5.740721225738525,
      "learning_rate": 4.298525498825674e-07,
      "loss": 1.0819,
      "num_input_tokens_seen": 33489600,
      "step": 2670,
      "train_runtime": 32051.7376,
      "train_tokens_per_second": 1044.861
    },
    {
      "epoch": 0.5460855363886904,
      "grad_norm": 4.846177101135254,
      "learning_rate": 4.282655884492452e-07,
      "loss": 1.3632,
      "num_input_tokens_seen": 33551792,
      "step": 2675,
      "train_runtime": 32104.4127,
      "train_tokens_per_second": 1045.083
    },
    {
      "epoch": 0.5471062570174543,
      "grad_norm": 5.034626007080078,
      "learning_rate": 4.2667936449897295e-07,
      "loss": 1.023,
      "num_input_tokens_seen": 33613952,
      "step": 2680,
      "train_runtime": 32157.1831,
      "train_tokens_per_second": 1045.302
    },
    {
      "epoch": 0.5481269776462182,
      "grad_norm": 5.3916850090026855,
      "learning_rate": 4.2509389433931165e-07,
      "loss": 1.0693,
      "num_input_tokens_seen": 33676032,
      "step": 2685,
      "train_runtime": 32209.9589,
      "train_tokens_per_second": 1045.516
    },
    {
      "epoch": 0.5491476982749821,
      "grad_norm": 8.593524932861328,
      "learning_rate": 4.235091942700724e-07,
      "loss": 1.0562,
      "num_input_tokens_seen": 33738608,
      "step": 2690,
      "train_runtime": 32263.1333,
      "train_tokens_per_second": 1045.733
    },
    {
      "epoch": 0.550168418903746,
      "grad_norm": 7.127155303955078,
      "learning_rate": 4.2192528058314925e-07,
      "loss": 0.9766,
      "num_input_tokens_seen": 33800912,
      "step": 2695,
      "train_runtime": 32316.0137,
      "train_tokens_per_second": 1045.949
    },
    {
      "epoch": 0.5511891395325099,
      "grad_norm": 8.042290687561035,
      "learning_rate": 4.203421695623519e-07,
      "loss": 1.1426,
      "num_input_tokens_seen": 33863472,
      "step": 2700,
      "train_runtime": 32369.2015,
      "train_tokens_per_second": 1046.163
    },
    {
      "epoch": 0.5511891395325099,
      "eval_loss": 1.0303195714950562,
      "eval_runtime": 139.2591,
      "eval_samples_per_second": 5.687,
      "eval_steps_per_second": 2.844,
      "num_input_tokens_seen": 33863472,
      "step": 2700
    },
    {
      "epoch": 0.5522098601612738,
      "grad_norm": 4.160548210144043,
      "learning_rate": 4.187598774832379e-07,
      "loss": 1.0808,
      "num_input_tokens_seen": 33925728,
      "step": 2705,
      "train_runtime": 32561.6437,
      "train_tokens_per_second": 1041.892
    },
    {
      "epoch": 0.5532305807900377,
      "grad_norm": 7.423404693603516,
      "learning_rate": 4.171784206129456e-07,
      "loss": 0.8191,
      "num_input_tokens_seen": 33986960,
      "step": 2710,
      "train_runtime": 32613.7682,
      "train_tokens_per_second": 1042.105
    },
    {
      "epoch": 0.5542513014188016,
      "grad_norm": 3.6874470710754395,
      "learning_rate": 4.155978152100266e-07,
      "loss": 0.8109,
      "num_input_tokens_seen": 34050368,
      "step": 2715,
      "train_runtime": 32667.3647,
      "train_tokens_per_second": 1042.336
    },
    {
      "epoch": 0.5552720220475655,
      "grad_norm": 7.357337951660156,
      "learning_rate": 4.1401807752427873e-07,
      "loss": 0.9989,
      "num_input_tokens_seen": 34113184,
      "step": 2720,
      "train_runtime": 32720.7715,
      "train_tokens_per_second": 1042.554
    },
    {
      "epoch": 0.5562927426763294,
      "grad_norm": 8.160189628601074,
      "learning_rate": 4.124392237965792e-07,
      "loss": 1.0287,
      "num_input_tokens_seen": 34175488,
      "step": 2725,
      "train_runtime": 32773.7222,
      "train_tokens_per_second": 1042.771
    },
    {
      "epoch": 0.5573134633050933,
      "grad_norm": 6.2046284675598145,
      "learning_rate": 4.1086127025871755e-07,
      "loss": 1.1871,
      "num_input_tokens_seen": 34238240,
      "step": 2730,
      "train_runtime": 32826.9708,
      "train_tokens_per_second": 1042.991
    },
    {
      "epoch": 0.5583341839338573,
      "grad_norm": 6.357048511505127,
      "learning_rate": 4.0928423313322846e-07,
      "loss": 0.9574,
      "num_input_tokens_seen": 34301632,
      "step": 2735,
      "train_runtime": 32880.519,
      "train_tokens_per_second": 1043.221
    },
    {
      "epoch": 0.5593549045626212,
      "grad_norm": 6.347194671630859,
      "learning_rate": 4.0770812863322513e-07,
      "loss": 0.9469,
      "num_input_tokens_seen": 34365104,
      "step": 2740,
      "train_runtime": 32934.2092,
      "train_tokens_per_second": 1043.447
    },
    {
      "epoch": 0.5603756251913851,
      "grad_norm": 6.41135835647583,
      "learning_rate": 4.061329729622334e-07,
      "loss": 0.8507,
      "num_input_tokens_seen": 34427088,
      "step": 2745,
      "train_runtime": 32986.6948,
      "train_tokens_per_second": 1043.666
    },
    {
      "epoch": 0.5613963458201491,
      "grad_norm": 9.745905876159668,
      "learning_rate": 4.0455878231402343e-07,
      "loss": 1.1882,
      "num_input_tokens_seen": 34488736,
      "step": 2750,
      "train_runtime": 33039.0449,
      "train_tokens_per_second": 1043.878
    },
    {
      "epoch": 0.562417066448913,
      "grad_norm": 5.431407928466797,
      "learning_rate": 4.0298557287244506e-07,
      "loss": 0.9223,
      "num_input_tokens_seen": 34552592,
      "step": 2755,
      "train_runtime": 33093.1255,
      "train_tokens_per_second": 1044.102
    },
    {
      "epoch": 0.5634377870776769,
      "grad_norm": 4.76399040222168,
      "learning_rate": 4.0141336081126016e-07,
      "loss": 1.2756,
      "num_input_tokens_seen": 34615024,
      "step": 2760,
      "train_runtime": 33146.1579,
      "train_tokens_per_second": 1044.315
    },
    {
      "epoch": 0.5644585077064408,
      "grad_norm": 8.127968788146973,
      "learning_rate": 3.9984216229397673e-07,
      "loss": 0.9607,
      "num_input_tokens_seen": 34677360,
      "step": 2765,
      "train_runtime": 33199.164,
      "train_tokens_per_second": 1044.525
    },
    {
      "epoch": 0.5654792283352047,
      "grad_norm": 5.258107662200928,
      "learning_rate": 3.9827199347368317e-07,
      "loss": 1.0963,
      "num_input_tokens_seen": 34739936,
      "step": 2770,
      "train_runtime": 33252.2484,
      "train_tokens_per_second": 1044.739
    },
    {
      "epoch": 0.5664999489639686,
      "grad_norm": 4.032856464385986,
      "learning_rate": 3.9670287049288125e-07,
      "loss": 1.3391,
      "num_input_tokens_seen": 34802720,
      "step": 2775,
      "train_runtime": 33305.4929,
      "train_tokens_per_second": 1044.954
    },
    {
      "epoch": 0.5675206695927325,
      "grad_norm": 6.990543365478516,
      "learning_rate": 3.9513480948332155e-07,
      "loss": 1.0144,
      "num_input_tokens_seen": 34864800,
      "step": 2780,
      "train_runtime": 33358.4814,
      "train_tokens_per_second": 1045.155
    },
    {
      "epoch": 0.5685413902214964,
      "grad_norm": 6.4073486328125,
      "learning_rate": 3.9356782656583617e-07,
      "loss": 0.9616,
      "num_input_tokens_seen": 34927248,
      "step": 2785,
      "train_runtime": 33411.4526,
      "train_tokens_per_second": 1045.368
    },
    {
      "epoch": 0.5695621108502603,
      "grad_norm": 9.64809513092041,
      "learning_rate": 3.9200193785017364e-07,
      "loss": 1.0045,
      "num_input_tokens_seen": 34989360,
      "step": 2790,
      "train_runtime": 33464.1289,
      "train_tokens_per_second": 1045.578
    },
    {
      "epoch": 0.5705828314790242,
      "grad_norm": 8.132524490356445,
      "learning_rate": 3.904371594348335e-07,
      "loss": 0.9721,
      "num_input_tokens_seen": 35052672,
      "step": 2795,
      "train_runtime": 33517.7428,
      "train_tokens_per_second": 1045.795
    },
    {
      "epoch": 0.5716035521077881,
      "grad_norm": 8.633302688598633,
      "learning_rate": 3.888735074069005e-07,
      "loss": 0.9509,
      "num_input_tokens_seen": 35115472,
      "step": 2800,
      "train_runtime": 33571.0756,
      "train_tokens_per_second": 1046.004
    },
    {
      "epoch": 0.5716035521077881,
      "eval_loss": 1.0291732549667358,
      "eval_runtime": 139.0052,
      "eval_samples_per_second": 5.698,
      "eval_steps_per_second": 2.849,
      "num_input_tokens_seen": 35115472,
      "step": 2800
    },
    {
      "epoch": 0.572624272736552,
      "grad_norm": 4.4752678871154785,
      "learning_rate": 3.873109978418794e-07,
      "loss": 0.9644,
      "num_input_tokens_seen": 35178144,
      "step": 2805,
      "train_runtime": 33763.6339,
      "train_tokens_per_second": 1041.894
    },
    {
      "epoch": 0.5736449933653159,
      "grad_norm": 5.313394069671631,
      "learning_rate": 3.8574964680352924e-07,
      "loss": 0.8759,
      "num_input_tokens_seen": 35241056,
      "step": 2810,
      "train_runtime": 33817.0505,
      "train_tokens_per_second": 1042.109
    },
    {
      "epoch": 0.5746657139940798,
      "grad_norm": 8.424595832824707,
      "learning_rate": 3.8418947034369896e-07,
      "loss": 0.8811,
      "num_input_tokens_seen": 35304256,
      "step": 2815,
      "train_runtime": 33870.5345,
      "train_tokens_per_second": 1042.329
    },
    {
      "epoch": 0.5756864346228437,
      "grad_norm": 5.641561508178711,
      "learning_rate": 3.826304845021617e-07,
      "loss": 0.817,
      "num_input_tokens_seen": 35366864,
      "step": 2820,
      "train_runtime": 33923.7439,
      "train_tokens_per_second": 1042.54
    },
    {
      "epoch": 0.5767071552516077,
      "grad_norm": 5.055230617523193,
      "learning_rate": 3.8107270530645057e-07,
      "loss": 0.8543,
      "num_input_tokens_seen": 35429776,
      "step": 2825,
      "train_runtime": 33976.9309,
      "train_tokens_per_second": 1042.76
    },
    {
      "epoch": 0.5777278758803716,
      "grad_norm": 5.39760160446167,
      "learning_rate": 3.795161487716928e-07,
      "loss": 0.829,
      "num_input_tokens_seen": 35492896,
      "step": 2830,
      "train_runtime": 34030.3277,
      "train_tokens_per_second": 1042.978
    },
    {
      "epoch": 0.5787485965091355,
      "grad_norm": 3.6651408672332764,
      "learning_rate": 3.779608309004463e-07,
      "loss": 0.7701,
      "num_input_tokens_seen": 35555872,
      "step": 2835,
      "train_runtime": 34083.8034,
      "train_tokens_per_second": 1043.19
    },
    {
      "epoch": 0.5797693171378994,
      "grad_norm": 34.2191276550293,
      "learning_rate": 3.764067676825341e-07,
      "loss": 1.142,
      "num_input_tokens_seen": 35618096,
      "step": 2840,
      "train_runtime": 34136.5825,
      "train_tokens_per_second": 1043.4
    },
    {
      "epoch": 0.5807900377666633,
      "grad_norm": 5.168379783630371,
      "learning_rate": 3.7485397509488115e-07,
      "loss": 0.9015,
      "num_input_tokens_seen": 35680256,
      "step": 2845,
      "train_runtime": 34189.4761,
      "train_tokens_per_second": 1043.603
    },
    {
      "epoch": 0.5818107583954272,
      "grad_norm": 5.3019700050354,
      "learning_rate": 3.7330246910134876e-07,
      "loss": 1.1505,
      "num_input_tokens_seen": 35742736,
      "step": 2850,
      "train_runtime": 34242.4849,
      "train_tokens_per_second": 1043.813
    },
    {
      "epoch": 0.5828314790241911,
      "grad_norm": 7.426358222961426,
      "learning_rate": 3.7175226565257116e-07,
      "loss": 1.1457,
      "num_input_tokens_seen": 35804752,
      "step": 2855,
      "train_runtime": 34295.3854,
      "train_tokens_per_second": 1044.011
    },
    {
      "epoch": 0.583852199652955,
      "grad_norm": 4.776027202606201,
      "learning_rate": 3.70203380685792e-07,
      "loss": 1.1,
      "num_input_tokens_seen": 35867600,
      "step": 2860,
      "train_runtime": 34348.4581,
      "train_tokens_per_second": 1044.227
    },
    {
      "epoch": 0.5848729202817189,
      "grad_norm": 5.108145236968994,
      "learning_rate": 3.686558301246995e-07,
      "loss": 0.9936,
      "num_input_tokens_seen": 35930416,
      "step": 2865,
      "train_runtime": 34401.751,
      "train_tokens_per_second": 1044.436
    },
    {
      "epoch": 0.5858936409104828,
      "grad_norm": 7.2866716384887695,
      "learning_rate": 3.6710962987926307e-07,
      "loss": 0.9274,
      "num_input_tokens_seen": 35992960,
      "step": 2870,
      "train_runtime": 34454.7553,
      "train_tokens_per_second": 1044.644
    },
    {
      "epoch": 0.5869143615392467,
      "grad_norm": 4.686606407165527,
      "learning_rate": 3.6556479584557026e-07,
      "loss": 0.8224,
      "num_input_tokens_seen": 36054736,
      "step": 2875,
      "train_runtime": 34507.2675,
      "train_tokens_per_second": 1044.845
    },
    {
      "epoch": 0.5879350821680106,
      "grad_norm": 6.209048748016357,
      "learning_rate": 3.6402134390566256e-07,
      "loss": 0.9255,
      "num_input_tokens_seen": 36116352,
      "step": 2880,
      "train_runtime": 34559.7304,
      "train_tokens_per_second": 1045.041
    },
    {
      "epoch": 0.5889558027967745,
      "grad_norm": 9.30255126953125,
      "learning_rate": 3.62479289927373e-07,
      "loss": 1.0466,
      "num_input_tokens_seen": 36178256,
      "step": 2885,
      "train_runtime": 34612.4321,
      "train_tokens_per_second": 1045.239
    },
    {
      "epoch": 0.5899765234255384,
      "grad_norm": 5.587095260620117,
      "learning_rate": 3.60938649764162e-07,
      "loss": 0.9332,
      "num_input_tokens_seen": 36241184,
      "step": 2890,
      "train_runtime": 34665.7792,
      "train_tokens_per_second": 1045.446
    },
    {
      "epoch": 0.5909972440543023,
      "grad_norm": 9.701558113098145,
      "learning_rate": 3.59399439254955e-07,
      "loss": 1.3036,
      "num_input_tokens_seen": 36304096,
      "step": 2895,
      "train_runtime": 34719.0154,
      "train_tokens_per_second": 1045.655
    },
    {
      "epoch": 0.5920179646830662,
      "grad_norm": 4.61555814743042,
      "learning_rate": 3.5786167422398017e-07,
      "loss": 0.8919,
      "num_input_tokens_seen": 36367632,
      "step": 2900,
      "train_runtime": 34772.6742,
      "train_tokens_per_second": 1045.868
    },
    {
      "epoch": 0.5920179646830662,
      "eval_loss": 1.0268430709838867,
      "eval_runtime": 138.9902,
      "eval_samples_per_second": 5.698,
      "eval_steps_per_second": 2.849,
      "num_input_tokens_seen": 36367632,
      "step": 2900
    },
    {
      "epoch": 0.5930386853118301,
      "grad_norm": 6.882617950439453,
      "learning_rate": 3.5632537048060427e-07,
      "loss": 1.0323,
      "num_input_tokens_seen": 36430000,
      "step": 2905,
      "train_runtime": 34965.0442,
      "train_tokens_per_second": 1041.898
    },
    {
      "epoch": 0.594059405940594,
      "grad_norm": 7.598732948303223,
      "learning_rate": 3.5479054381917125e-07,
      "loss": 1.0334,
      "num_input_tokens_seen": 36492832,
      "step": 2910,
      "train_runtime": 35018.3623,
      "train_tokens_per_second": 1042.106
    },
    {
      "epoch": 0.5950801265693579,
      "grad_norm": 4.54393196105957,
      "learning_rate": 3.532572100188397e-07,
      "loss": 1.2214,
      "num_input_tokens_seen": 36555552,
      "step": 2915,
      "train_runtime": 35071.5854,
      "train_tokens_per_second": 1042.313
    },
    {
      "epoch": 0.5961008471981218,
      "grad_norm": 4.879239082336426,
      "learning_rate": 3.5172538484342043e-07,
      "loss": 0.949,
      "num_input_tokens_seen": 36619120,
      "step": 2920,
      "train_runtime": 35125.3686,
      "train_tokens_per_second": 1042.526
    },
    {
      "epoch": 0.5971215678268857,
      "grad_norm": 5.82370662689209,
      "learning_rate": 3.5019508404121414e-07,
      "loss": 1.0155,
      "num_input_tokens_seen": 36682320,
      "step": 2925,
      "train_runtime": 35178.8887,
      "train_tokens_per_second": 1042.737
    },
    {
      "epoch": 0.5981422884556497,
      "grad_norm": 6.148874759674072,
      "learning_rate": 3.4866632334485037e-07,
      "loss": 0.866,
      "num_input_tokens_seen": 36745040,
      "step": 2930,
      "train_runtime": 35232.1114,
      "train_tokens_per_second": 1042.942
    },
    {
      "epoch": 0.5991630090844136,
      "grad_norm": 5.231231689453125,
      "learning_rate": 3.471391184711246e-07,
      "loss": 0.9342,
      "num_input_tokens_seen": 36807264,
      "step": 2935,
      "train_runtime": 35285.1875,
      "train_tokens_per_second": 1043.136
    },
    {
      "epoch": 0.6001837297131775,
      "grad_norm": 6.936364650726318,
      "learning_rate": 3.4561348512083817e-07,
      "loss": 1.115,
      "num_input_tokens_seen": 36871568,
      "step": 2940,
      "train_runtime": 35339.4107,
      "train_tokens_per_second": 1043.355
    },
    {
      "epoch": 0.6012044503419414,
      "grad_norm": 7.948878765106201,
      "learning_rate": 3.4408943897863516e-07,
      "loss": 1.1219,
      "num_input_tokens_seen": 36934320,
      "step": 2945,
      "train_runtime": 35392.7784,
      "train_tokens_per_second": 1043.555
    },
    {
      "epoch": 0.6022251709707053,
      "grad_norm": 6.663166522979736,
      "learning_rate": 3.425669957128423e-07,
      "loss": 1.0141,
      "num_input_tokens_seen": 36997264,
      "step": 2950,
      "train_runtime": 35446.2333,
      "train_tokens_per_second": 1043.757
    },
    {
      "epoch": 0.6032458915994692,
      "grad_norm": 5.877442836761475,
      "learning_rate": 3.4104617097530775e-07,
      "loss": 0.9419,
      "num_input_tokens_seen": 37060880,
      "step": 2955,
      "train_runtime": 35500.1046,
      "train_tokens_per_second": 1043.965
    },
    {
      "epoch": 0.6042666122282331,
      "grad_norm": 5.133551120758057,
      "learning_rate": 3.3952698040123953e-07,
      "loss": 0.9297,
      "num_input_tokens_seen": 37123584,
      "step": 2960,
      "train_runtime": 35553.5525,
      "train_tokens_per_second": 1044.16
    },
    {
      "epoch": 0.605287332856997,
      "grad_norm": 11.802494049072266,
      "learning_rate": 3.380094396090457e-07,
      "loss": 1.131,
      "num_input_tokens_seen": 37186112,
      "step": 2965,
      "train_runtime": 35606.4898,
      "train_tokens_per_second": 1044.363
    },
    {
      "epoch": 0.606308053485761,
      "grad_norm": 5.296082496643066,
      "learning_rate": 3.364935642001729e-07,
      "loss": 1.0161,
      "num_input_tokens_seen": 37248864,
      "step": 2970,
      "train_runtime": 35659.617,
      "train_tokens_per_second": 1044.567
    },
    {
      "epoch": 0.6073287741145249,
      "grad_norm": 5.611044406890869,
      "learning_rate": 3.34979369758947e-07,
      "loss": 1.1799,
      "num_input_tokens_seen": 37310880,
      "step": 2975,
      "train_runtime": 35712.3525,
      "train_tokens_per_second": 1044.761
    },
    {
      "epoch": 0.6083494947432888,
      "grad_norm": 5.047722816467285,
      "learning_rate": 3.334668718524116e-07,
      "loss": 1.0429,
      "num_input_tokens_seen": 37373536,
      "step": 2980,
      "train_runtime": 35765.4825,
      "train_tokens_per_second": 1044.961
    },
    {
      "epoch": 0.6093702153720527,
      "grad_norm": 6.154183864593506,
      "learning_rate": 3.3195608603016936e-07,
      "loss": 0.9938,
      "num_input_tokens_seen": 37436352,
      "step": 2985,
      "train_runtime": 35818.7148,
      "train_tokens_per_second": 1045.162
    },
    {
      "epoch": 0.6103909360008166,
      "grad_norm": 5.221545696258545,
      "learning_rate": 3.3044702782422067e-07,
      "loss": 0.6821,
      "num_input_tokens_seen": 37499952,
      "step": 2990,
      "train_runtime": 35872.5784,
      "train_tokens_per_second": 1045.365
    },
    {
      "epoch": 0.6114116566295805,
      "grad_norm": 6.1034464836120605,
      "learning_rate": 3.2893971274880523e-07,
      "loss": 1.1425,
      "num_input_tokens_seen": 37561952,
      "step": 2995,
      "train_runtime": 35925.2374,
      "train_tokens_per_second": 1045.559
    },
    {
      "epoch": 0.6124323772583444,
      "grad_norm": 6.461740970611572,
      "learning_rate": 3.2743415630024205e-07,
      "loss": 0.8978,
      "num_input_tokens_seen": 37624144,
      "step": 3000,
      "train_runtime": 35978.0004,
      "train_tokens_per_second": 1045.754
    },
    {
      "epoch": 0.6124323772583444,
      "eval_loss": 1.0249617099761963,
      "eval_runtime": 139.0395,
      "eval_samples_per_second": 5.696,
      "eval_steps_per_second": 2.848,
      "num_input_tokens_seen": 37624144,
      "step": 3000
    },
    {
      "epoch": 0.6134530978871083,
      "grad_norm": 6.030832767486572,
      "learning_rate": 3.259303739567701e-07,
      "loss": 1.0552,
      "num_input_tokens_seen": 37686544,
      "step": 3005,
      "train_runtime": 36170.6274,
      "train_tokens_per_second": 1041.91
    },
    {
      "epoch": 0.6144738185158722,
      "grad_norm": 6.20921516418457,
      "learning_rate": 3.244283811783889e-07,
      "loss": 1.0603,
      "num_input_tokens_seen": 37749584,
      "step": 3010,
      "train_runtime": 36224.0763,
      "train_tokens_per_second": 1042.113
    },
    {
      "epoch": 0.6154945391446361,
      "grad_norm": 6.574428558349609,
      "learning_rate": 3.2292819340670065e-07,
      "loss": 1.3695,
      "num_input_tokens_seen": 37812768,
      "step": 3015,
      "train_runtime": 36277.5934,
      "train_tokens_per_second": 1042.317
    },
    {
      "epoch": 0.6165152597734,
      "grad_norm": 5.753381252288818,
      "learning_rate": 3.2142982606475005e-07,
      "loss": 1.1944,
      "num_input_tokens_seen": 37875904,
      "step": 3020,
      "train_runtime": 36331.1725,
      "train_tokens_per_second": 1042.518
    },
    {
      "epoch": 0.617535980402164,
      "grad_norm": 6.615638256072998,
      "learning_rate": 3.199332945568666e-07,
      "loss": 1.2924,
      "num_input_tokens_seen": 37939760,
      "step": 3025,
      "train_runtime": 36385.202,
      "train_tokens_per_second": 1042.725
    },
    {
      "epoch": 0.6185567010309279,
      "grad_norm": 6.07702112197876,
      "learning_rate": 3.184386142685063e-07,
      "loss": 0.9714,
      "num_input_tokens_seen": 38002304,
      "step": 3030,
      "train_runtime": 36438.3988,
      "train_tokens_per_second": 1042.919
    },
    {
      "epoch": 0.6195774216596918,
      "grad_norm": 5.9478349685668945,
      "learning_rate": 3.1694580056609303e-07,
      "loss": 1.0811,
      "num_input_tokens_seen": 38066144,
      "step": 3035,
      "train_runtime": 36492.5203,
      "train_tokens_per_second": 1043.122
    },
    {
      "epoch": 0.6205981422884557,
      "grad_norm": 4.982393741607666,
      "learning_rate": 3.1545486879686066e-07,
      "loss": 1.0379,
      "num_input_tokens_seen": 38128656,
      "step": 3040,
      "train_runtime": 36545.4296,
      "train_tokens_per_second": 1043.322
    },
    {
      "epoch": 0.6216188629172196,
      "grad_norm": 6.043096542358398,
      "learning_rate": 3.139658342886959e-07,
      "loss": 1.0558,
      "num_input_tokens_seen": 38192464,
      "step": 3045,
      "train_runtime": 36599.2681,
      "train_tokens_per_second": 1043.531
    },
    {
      "epoch": 0.6226395835459835,
      "grad_norm": 7.7097578048706055,
      "learning_rate": 3.1247871234997934e-07,
      "loss": 0.9163,
      "num_input_tokens_seen": 38254176,
      "step": 3050,
      "train_runtime": 36651.8465,
      "train_tokens_per_second": 1043.718
    },
    {
      "epoch": 0.6236603041747474,
      "grad_norm": 5.688064098358154,
      "learning_rate": 3.1099351826943e-07,
      "loss": 0.9913,
      "num_input_tokens_seen": 38316560,
      "step": 3055,
      "train_runtime": 36704.8085,
      "train_tokens_per_second": 1043.911
    },
    {
      "epoch": 0.6246810248035113,
      "grad_norm": 6.594097137451172,
      "learning_rate": 3.095102673159463e-07,
      "loss": 0.8022,
      "num_input_tokens_seen": 38378352,
      "step": 3060,
      "train_runtime": 36757.2738,
      "train_tokens_per_second": 1044.102
    },
    {
      "epoch": 0.6257017454322752,
      "grad_norm": 5.843066692352295,
      "learning_rate": 3.080289747384501e-07,
      "loss": 0.9699,
      "num_input_tokens_seen": 38441152,
      "step": 3065,
      "train_runtime": 36810.4417,
      "train_tokens_per_second": 1044.3
    },
    {
      "epoch": 0.6267224660610391,
      "grad_norm": 4.264003276824951,
      "learning_rate": 3.0654965576572977e-07,
      "loss": 0.9015,
      "num_input_tokens_seen": 38504272,
      "step": 3070,
      "train_runtime": 36863.9865,
      "train_tokens_per_second": 1044.496
    },
    {
      "epoch": 0.627743186689803,
      "grad_norm": 4.253250598907471,
      "learning_rate": 3.0507232560628345e-07,
      "loss": 1.1532,
      "num_input_tokens_seen": 38567360,
      "step": 3075,
      "train_runtime": 36917.5869,
      "train_tokens_per_second": 1044.688
    },
    {
      "epoch": 0.6287639073185669,
      "grad_norm": 4.948118686676025,
      "learning_rate": 3.0359699944816287e-07,
      "loss": 1.266,
      "num_input_tokens_seen": 38629920,
      "step": 3080,
      "train_runtime": 36970.6101,
      "train_tokens_per_second": 1044.882
    },
    {
      "epoch": 0.6297846279473308,
      "grad_norm": 5.576965808868408,
      "learning_rate": 3.0212369245881695e-07,
      "loss": 1.3037,
      "num_input_tokens_seen": 38693536,
      "step": 3085,
      "train_runtime": 37024.5663,
      "train_tokens_per_second": 1045.077
    },
    {
      "epoch": 0.6308053485760947,
      "grad_norm": 6.210402011871338,
      "learning_rate": 3.006524197849364e-07,
      "loss": 1.3764,
      "num_input_tokens_seen": 38755840,
      "step": 3090,
      "train_runtime": 37077.4946,
      "train_tokens_per_second": 1045.266
    },
    {
      "epoch": 0.6318260692048586,
      "grad_norm": 5.163219451904297,
      "learning_rate": 2.9918319655229747e-07,
      "loss": 1.2018,
      "num_input_tokens_seen": 38818288,
      "step": 3095,
      "train_runtime": 37130.5336,
      "train_tokens_per_second": 1045.455
    },
    {
      "epoch": 0.6328467898336225,
      "grad_norm": 5.732316493988037,
      "learning_rate": 2.9771603786560665e-07,
      "loss": 1.2021,
      "num_input_tokens_seen": 38880336,
      "step": 3100,
      "train_runtime": 37183.2034,
      "train_tokens_per_second": 1045.642
    },
    {
      "epoch": 0.6328467898336225,
      "eval_loss": 1.0243333578109741,
      "eval_runtime": 139.2506,
      "eval_samples_per_second": 5.688,
      "eval_steps_per_second": 2.844,
      "num_input_tokens_seen": 38880336,
      "step": 3100
    },
    {
      "epoch": 0.6338675104623864,
      "grad_norm": 6.70726203918457,
      "learning_rate": 2.962509588083454e-07,
      "loss": 0.8902,
      "num_input_tokens_seen": 38944016,
      "step": 3105,
      "train_runtime": 37376.6128,
      "train_tokens_per_second": 1041.935
    },
    {
      "epoch": 0.6348882310911503,
      "grad_norm": 4.32145357131958,
      "learning_rate": 2.947879744426147e-07,
      "loss": 1.0952,
      "num_input_tokens_seen": 39006272,
      "step": 3110,
      "train_runtime": 37429.4897,
      "train_tokens_per_second": 1042.127
    },
    {
      "epoch": 0.6359089517199142,
      "grad_norm": 6.362078666687012,
      "learning_rate": 2.933270998089812e-07,
      "loss": 1.0066,
      "num_input_tokens_seen": 39068656,
      "step": 3115,
      "train_runtime": 37482.4127,
      "train_tokens_per_second": 1042.32
    },
    {
      "epoch": 0.6369296723486781,
      "grad_norm": 5.046336650848389,
      "learning_rate": 2.918683499263215e-07,
      "loss": 1.024,
      "num_input_tokens_seen": 39131024,
      "step": 3120,
      "train_runtime": 37535.3102,
      "train_tokens_per_second": 1042.512
    },
    {
      "epoch": 0.637950392977442,
      "grad_norm": 4.572744369506836,
      "learning_rate": 2.904117397916681e-07,
      "loss": 1.0501,
      "num_input_tokens_seen": 39194208,
      "step": 3125,
      "train_runtime": 37588.7703,
      "train_tokens_per_second": 1042.711
    },
    {
      "epoch": 0.638971113606206,
      "grad_norm": 7.088102340698242,
      "learning_rate": 2.889572843800553e-07,
      "loss": 1.1136,
      "num_input_tokens_seen": 39256912,
      "step": 3130,
      "train_runtime": 37641.9518,
      "train_tokens_per_second": 1042.903
    },
    {
      "epoch": 0.6399918342349699,
      "grad_norm": 4.843656063079834,
      "learning_rate": 2.8750499864436594e-07,
      "loss": 1.0298,
      "num_input_tokens_seen": 39320032,
      "step": 3135,
      "train_runtime": 37695.3988,
      "train_tokens_per_second": 1043.099
    },
    {
      "epoch": 0.6410125548637338,
      "grad_norm": 6.410877704620361,
      "learning_rate": 2.860548975151761e-07,
      "loss": 0.9073,
      "num_input_tokens_seen": 39381712,
      "step": 3140,
      "train_runtime": 37747.8381,
      "train_tokens_per_second": 1043.284
    },
    {
      "epoch": 0.6420332754924977,
      "grad_norm": 5.21064567565918,
      "learning_rate": 2.8460699590060266e-07,
      "loss": 0.9469,
      "num_input_tokens_seen": 39443936,
      "step": 3145,
      "train_runtime": 37800.6852,
      "train_tokens_per_second": 1043.471
    },
    {
      "epoch": 0.6430539961212616,
      "grad_norm": 6.5915751457214355,
      "learning_rate": 2.8316130868615046e-07,
      "loss": 1.2873,
      "num_input_tokens_seen": 39507600,
      "step": 3150,
      "train_runtime": 37854.6523,
      "train_tokens_per_second": 1043.666
    },
    {
      "epoch": 0.6440747167500255,
      "grad_norm": 6.860096454620361,
      "learning_rate": 2.8171785073455803e-07,
      "loss": 0.9757,
      "num_input_tokens_seen": 39570528,
      "step": 3155,
      "train_runtime": 37908.0393,
      "train_tokens_per_second": 1043.856
    },
    {
      "epoch": 0.6450954373787894,
      "grad_norm": 8.101746559143066,
      "learning_rate": 2.802766368856455e-07,
      "loss": 0.9534,
      "num_input_tokens_seen": 39633152,
      "step": 3160,
      "train_runtime": 37961.1884,
      "train_tokens_per_second": 1044.044
    },
    {
      "epoch": 0.6461161580075533,
      "grad_norm": 8.131298065185547,
      "learning_rate": 2.7883768195616195e-07,
      "loss": 1.1536,
      "num_input_tokens_seen": 39695568,
      "step": 3165,
      "train_runtime": 38014.1167,
      "train_tokens_per_second": 1044.232
    },
    {
      "epoch": 0.6471368786363172,
      "grad_norm": 9.837028503417969,
      "learning_rate": 2.774010007396337e-07,
      "loss": 1.3029,
      "num_input_tokens_seen": 39757888,
      "step": 3170,
      "train_runtime": 38067.1895,
      "train_tokens_per_second": 1044.414
    },
    {
      "epoch": 0.6481575992650811,
      "grad_norm": 5.00350284576416,
      "learning_rate": 2.7596660800621074e-07,
      "loss": 1.0947,
      "num_input_tokens_seen": 39820512,
      "step": 3175,
      "train_runtime": 38120.4021,
      "train_tokens_per_second": 1044.598
    },
    {
      "epoch": 0.649178319893845,
      "grad_norm": 5.276729106903076,
      "learning_rate": 2.745345185025163e-07,
      "loss": 0.8653,
      "num_input_tokens_seen": 39883296,
      "step": 3180,
      "train_runtime": 38173.6473,
      "train_tokens_per_second": 1044.786
    },
    {
      "epoch": 0.6501990405226089,
      "grad_norm": 6.300775051116943,
      "learning_rate": 2.7310474695149444e-07,
      "loss": 1.0945,
      "num_input_tokens_seen": 39947072,
      "step": 3185,
      "train_runtime": 38227.4522,
      "train_tokens_per_second": 1044.984
    },
    {
      "epoch": 0.6512197611513729,
      "grad_norm": 6.123531818389893,
      "learning_rate": 2.716773080522589e-07,
      "loss": 1.3297,
      "num_input_tokens_seen": 40009312,
      "step": 3190,
      "train_runtime": 38280.2723,
      "train_tokens_per_second": 1045.168
    },
    {
      "epoch": 0.6522404817801368,
      "grad_norm": 5.28840446472168,
      "learning_rate": 2.702522164799425e-07,
      "loss": 0.8808,
      "num_input_tokens_seen": 40071760,
      "step": 3195,
      "train_runtime": 38333.2346,
      "train_tokens_per_second": 1045.353
    },
    {
      "epoch": 0.6532612024089007,
      "grad_norm": 6.079819202423096,
      "learning_rate": 2.688294868855453e-07,
      "loss": 1.0809,
      "num_input_tokens_seen": 40134064,
      "step": 3200,
      "train_runtime": 38386.0267,
      "train_tokens_per_second": 1045.538
    },
    {
      "epoch": 0.6532612024089007,
      "eval_loss": 1.0224865674972534,
      "eval_runtime": 139.3015,
      "eval_samples_per_second": 5.686,
      "eval_steps_per_second": 2.843,
      "num_input_tokens_seen": 40134064,
      "step": 3200
    },
    {
      "epoch": 0.6542819230376646,
      "grad_norm": 4.022045612335205,
      "learning_rate": 2.674091338957844e-07,
      "loss": 0.907,
      "num_input_tokens_seen": 40196576,
      "step": 3205,
      "train_runtime": 38578.6947,
      "train_tokens_per_second": 1041.937
    },
    {
      "epoch": 0.6553026436664285,
      "grad_norm": 6.858551502227783,
      "learning_rate": 2.659911721129443e-07,
      "loss": 0.9036,
      "num_input_tokens_seen": 40260640,
      "step": 3210,
      "train_runtime": 38632.9262,
      "train_tokens_per_second": 1042.133
    },
    {
      "epoch": 0.6563233642951924,
      "grad_norm": 7.644737720489502,
      "learning_rate": 2.6457561611472546e-07,
      "loss": 1.1227,
      "num_input_tokens_seen": 40323376,
      "step": 3215,
      "train_runtime": 38686.2956,
      "train_tokens_per_second": 1042.317
    },
    {
      "epoch": 0.6573440849239564,
      "grad_norm": 7.235320091247559,
      "learning_rate": 2.631624804540953e-07,
      "loss": 0.9542,
      "num_input_tokens_seen": 40384928,
      "step": 3220,
      "train_runtime": 38738.6098,
      "train_tokens_per_second": 1042.498
    },
    {
      "epoch": 0.6583648055527203,
      "grad_norm": 5.993038177490234,
      "learning_rate": 2.617517796591384e-07,
      "loss": 0.8012,
      "num_input_tokens_seen": 40447296,
      "step": 3225,
      "train_runtime": 38791.5919,
      "train_tokens_per_second": 1042.682
    },
    {
      "epoch": 0.6593855261814842,
      "grad_norm": 6.099856376647949,
      "learning_rate": 2.6034352823290717e-07,
      "loss": 0.7998,
      "num_input_tokens_seen": 40510048,
      "step": 3230,
      "train_runtime": 38844.8972,
      "train_tokens_per_second": 1042.867
    },
    {
      "epoch": 0.6604062468102481,
      "grad_norm": 7.321577548980713,
      "learning_rate": 2.589377406532723e-07,
      "loss": 1.25,
      "num_input_tokens_seen": 40572432,
      "step": 3235,
      "train_runtime": 38897.8966,
      "train_tokens_per_second": 1043.05
    },
    {
      "epoch": 0.661426967439012,
      "grad_norm": 5.25341796875,
      "learning_rate": 2.575344313727751e-07,
      "loss": 1.0685,
      "num_input_tokens_seen": 40634480,
      "step": 3240,
      "train_runtime": 38950.5605,
      "train_tokens_per_second": 1043.232
    },
    {
      "epoch": 0.6624476880677759,
      "grad_norm": 3.6095452308654785,
      "learning_rate": 2.561336148184772e-07,
      "loss": 0.8447,
      "num_input_tokens_seen": 40697696,
      "step": 3245,
      "train_runtime": 39004.2882,
      "train_tokens_per_second": 1043.416
    },
    {
      "epoch": 0.6634684086965398,
      "grad_norm": 3.835134506225586,
      "learning_rate": 2.547353053918139e-07,
      "loss": 0.8089,
      "num_input_tokens_seen": 40760032,
      "step": 3250,
      "train_runtime": 39057.1586,
      "train_tokens_per_second": 1043.6
    },
    {
      "epoch": 0.6644891293253037,
      "grad_norm": 4.900205135345459,
      "learning_rate": 2.5333951746844507e-07,
      "loss": 1.119,
      "num_input_tokens_seen": 40822560,
      "step": 3255,
      "train_runtime": 39110.3353,
      "train_tokens_per_second": 1043.779
    },
    {
      "epoch": 0.6655098499540676,
      "grad_norm": 7.199527740478516,
      "learning_rate": 2.519462653981076e-07,
      "loss": 0.9037,
      "num_input_tokens_seen": 40885136,
      "step": 3260,
      "train_runtime": 39163.4691,
      "train_tokens_per_second": 1043.961
    },
    {
      "epoch": 0.6665305705828315,
      "grad_norm": 4.559431552886963,
      "learning_rate": 2.505555635044678e-07,
      "loss": 1.2472,
      "num_input_tokens_seen": 40949200,
      "step": 3265,
      "train_runtime": 39217.4927,
      "train_tokens_per_second": 1044.157
    },
    {
      "epoch": 0.6675512912115954,
      "grad_norm": 4.83593225479126,
      "learning_rate": 2.491674260849746e-07,
      "loss": 0.7704,
      "num_input_tokens_seen": 41012064,
      "step": 3270,
      "train_runtime": 39270.9374,
      "train_tokens_per_second": 1044.336
    },
    {
      "epoch": 0.6685720118403593,
      "grad_norm": 6.749897003173828,
      "learning_rate": 2.4778186741071204e-07,
      "loss": 0.8498,
      "num_input_tokens_seen": 41074336,
      "step": 3275,
      "train_runtime": 39323.9542,
      "train_tokens_per_second": 1044.512
    },
    {
      "epoch": 0.6695927324691232,
      "grad_norm": 5.164090633392334,
      "learning_rate": 2.463989017262526e-07,
      "loss": 1.1359,
      "num_input_tokens_seen": 41138464,
      "step": 3280,
      "train_runtime": 39378.2631,
      "train_tokens_per_second": 1044.7
    },
    {
      "epoch": 0.6706134530978871,
      "grad_norm": 8.460702896118164,
      "learning_rate": 2.4501854324951115e-07,
      "loss": 0.8906,
      "num_input_tokens_seen": 41200960,
      "step": 3285,
      "train_runtime": 39431.3541,
      "train_tokens_per_second": 1044.878
    },
    {
      "epoch": 0.671634173726651,
      "grad_norm": 8.70578670501709,
      "learning_rate": 2.436408061715988e-07,
      "loss": 1.0148,
      "num_input_tokens_seen": 41264160,
      "step": 3290,
      "train_runtime": 39484.893,
      "train_tokens_per_second": 1045.062
    },
    {
      "epoch": 0.6726548943554149,
      "grad_norm": 5.340545177459717,
      "learning_rate": 2.4226570465667605e-07,
      "loss": 1.0379,
      "num_input_tokens_seen": 41326320,
      "step": 3295,
      "train_runtime": 39537.6779,
      "train_tokens_per_second": 1045.239
    },
    {
      "epoch": 0.6736756149841788,
      "grad_norm": 7.941256046295166,
      "learning_rate": 2.408932528418083e-07,
      "loss": 0.9673,
      "num_input_tokens_seen": 41389008,
      "step": 3300,
      "train_runtime": 39590.754,
      "train_tokens_per_second": 1045.421
    },
    {
      "epoch": 0.6736756149841788,
      "eval_loss": 1.0216190814971924,
      "eval_runtime": 139.2471,
      "eval_samples_per_second": 5.688,
      "eval_steps_per_second": 2.844,
      "num_input_tokens_seen": 41389008,
      "step": 3300
    },
    {
      "epoch": 0.6746963356129427,
      "grad_norm": 6.0854339599609375,
      "learning_rate": 2.395234648368199e-07,
      "loss": 0.9222,
      "num_input_tokens_seen": 41452304,
      "step": 3305,
      "train_runtime": 39784.1085,
      "train_tokens_per_second": 1041.931
    },
    {
      "epoch": 0.6757170562417066,
      "grad_norm": 5.806652069091797,
      "learning_rate": 2.381563547241493e-07,
      "loss": 1.0621,
      "num_input_tokens_seen": 41515840,
      "step": 3310,
      "train_runtime": 39837.9007,
      "train_tokens_per_second": 1042.119
    },
    {
      "epoch": 0.6767377768704705,
      "grad_norm": 10.718896865844727,
      "learning_rate": 2.3679193655870423e-07,
      "loss": 1.3506,
      "num_input_tokens_seen": 41578256,
      "step": 3315,
      "train_runtime": 39890.8814,
      "train_tokens_per_second": 1042.3
    },
    {
      "epoch": 0.6777584974992344,
      "grad_norm": 5.768531322479248,
      "learning_rate": 2.354302243677172e-07,
      "loss": 0.921,
      "num_input_tokens_seen": 41640992,
      "step": 3320,
      "train_runtime": 39944.0441,
      "train_tokens_per_second": 1042.483
    },
    {
      "epoch": 0.6787792181279984,
      "grad_norm": 8.216185569763184,
      "learning_rate": 2.3407123215060116e-07,
      "loss": 1.0729,
      "num_input_tokens_seen": 41704656,
      "step": 3325,
      "train_runtime": 39997.8811,
      "train_tokens_per_second": 1042.672
    },
    {
      "epoch": 0.6797999387567623,
      "grad_norm": 5.573665618896484,
      "learning_rate": 2.3271497387880596e-07,
      "loss": 0.7786,
      "num_input_tokens_seen": 41767152,
      "step": 3330,
      "train_runtime": 40050.8468,
      "train_tokens_per_second": 1042.853
    },
    {
      "epoch": 0.6808206593855262,
      "grad_norm": 5.47996187210083,
      "learning_rate": 2.313614634956746e-07,
      "loss": 1.0294,
      "num_input_tokens_seen": 41830112,
      "step": 3335,
      "train_runtime": 40104.2507,
      "train_tokens_per_second": 1043.034
    },
    {
      "epoch": 0.6818413800142901,
      "grad_norm": 5.314586639404297,
      "learning_rate": 2.3001071491629925e-07,
      "loss": 0.7863,
      "num_input_tokens_seen": 41891952,
      "step": 3340,
      "train_runtime": 40156.7827,
      "train_tokens_per_second": 1043.21
    },
    {
      "epoch": 0.682862100643054,
      "grad_norm": 8.268125534057617,
      "learning_rate": 2.2866274202737894e-07,
      "loss": 0.9283,
      "num_input_tokens_seen": 41955600,
      "step": 3345,
      "train_runtime": 40210.6616,
      "train_tokens_per_second": 1043.395
    },
    {
      "epoch": 0.6838828212718179,
      "grad_norm": 5.183356761932373,
      "learning_rate": 2.2731755868707653e-07,
      "loss": 0.8548,
      "num_input_tokens_seen": 42017296,
      "step": 3350,
      "train_runtime": 40263.1575,
      "train_tokens_per_second": 1043.567
    },
    {
      "epoch": 0.6849035419005818,
      "grad_norm": 5.1430745124816895,
      "learning_rate": 2.2597517872487626e-07,
      "loss": 1.0978,
      "num_input_tokens_seen": 42080064,
      "step": 3355,
      "train_runtime": 40316.3774,
      "train_tokens_per_second": 1043.746
    },
    {
      "epoch": 0.6859242625293457,
      "grad_norm": 6.618101596832275,
      "learning_rate": 2.2463561594144131e-07,
      "loss": 0.8128,
      "num_input_tokens_seen": 42144192,
      "step": 3360,
      "train_runtime": 40370.5163,
      "train_tokens_per_second": 1043.935
    },
    {
      "epoch": 0.6869449831581096,
      "grad_norm": 6.518598556518555,
      "learning_rate": 2.232988841084728e-07,
      "loss": 0.8479,
      "num_input_tokens_seen": 42207376,
      "step": 3365,
      "train_runtime": 40424.0151,
      "train_tokens_per_second": 1044.116
    },
    {
      "epoch": 0.6879657037868735,
      "grad_norm": 7.301942348480225,
      "learning_rate": 2.21964996968567e-07,
      "loss": 0.8591,
      "num_input_tokens_seen": 42270976,
      "step": 3370,
      "train_runtime": 40477.8687,
      "train_tokens_per_second": 1044.298
    },
    {
      "epoch": 0.6889864244156374,
      "grad_norm": 7.435168266296387,
      "learning_rate": 2.2063396823507462e-07,
      "loss": 0.998,
      "num_input_tokens_seen": 42333408,
      "step": 3375,
      "train_runtime": 40530.8292,
      "train_tokens_per_second": 1044.474
    },
    {
      "epoch": 0.6900071450444013,
      "grad_norm": 5.902925491333008,
      "learning_rate": 2.1930581159196033e-07,
      "loss": 0.743,
      "num_input_tokens_seen": 42395120,
      "step": 3380,
      "train_runtime": 40583.3415,
      "train_tokens_per_second": 1044.643
    },
    {
      "epoch": 0.6910278656731652,
      "grad_norm": 5.007406234741211,
      "learning_rate": 2.1798054069366106e-07,
      "loss": 1.0554,
      "num_input_tokens_seen": 42457248,
      "step": 3385,
      "train_runtime": 40636.0889,
      "train_tokens_per_second": 1044.816
    },
    {
      "epoch": 0.6920485863019291,
      "grad_norm": 6.3342366218566895,
      "learning_rate": 2.1665816916494644e-07,
      "loss": 1.0121,
      "num_input_tokens_seen": 42520400,
      "step": 3390,
      "train_runtime": 40689.6025,
      "train_tokens_per_second": 1044.994
    },
    {
      "epoch": 0.693069306930693,
      "grad_norm": 5.225078582763672,
      "learning_rate": 2.1533871060077818e-07,
      "loss": 1.0207,
      "num_input_tokens_seen": 42582656,
      "step": 3395,
      "train_runtime": 40742.5274,
      "train_tokens_per_second": 1045.165
    },
    {
      "epoch": 0.6940900275594569,
      "grad_norm": 8.542516708374023,
      "learning_rate": 2.140221785661706e-07,
      "loss": 1.1067,
      "num_input_tokens_seen": 42644624,
      "step": 3400,
      "train_runtime": 40795.1962,
      "train_tokens_per_second": 1045.334
    },
    {
      "epoch": 0.6940900275594569,
      "eval_loss": 1.0200387239456177,
      "eval_runtime": 139.263,
      "eval_samples_per_second": 5.687,
      "eval_steps_per_second": 2.844,
      "num_input_tokens_seen": 42644624,
      "step": 3400
    },
    {
      "epoch": 0.6951107481882209,
      "grad_norm": 5.605860233306885,
      "learning_rate": 2.1270858659605156e-07,
      "loss": 1.0544,
      "num_input_tokens_seen": 42708304,
      "step": 3405,
      "train_runtime": 40988.7172,
      "train_tokens_per_second": 1041.953
    },
    {
      "epoch": 0.6961314688169848,
      "grad_norm": 7.2262115478515625,
      "learning_rate": 2.1139794819512235e-07,
      "loss": 0.8815,
      "num_input_tokens_seen": 42769712,
      "step": 3410,
      "train_runtime": 41040.9321,
      "train_tokens_per_second": 1042.123
    },
    {
      "epoch": 0.6971521894457487,
      "grad_norm": 5.775408744812012,
      "learning_rate": 2.1009027683771947e-07,
      "loss": 0.794,
      "num_input_tokens_seen": 42832368,
      "step": 3415,
      "train_runtime": 41094.2703,
      "train_tokens_per_second": 1042.295
    },
    {
      "epoch": 0.6981729100745127,
      "grad_norm": 7.263384819030762,
      "learning_rate": 2.0878558596767599e-07,
      "loss": 0.9336,
      "num_input_tokens_seen": 42895536,
      "step": 3420,
      "train_runtime": 41147.7956,
      "train_tokens_per_second": 1042.475
    },
    {
      "epoch": 0.6991936307032766,
      "grad_norm": 5.848640441894531,
      "learning_rate": 2.0748388899818375e-07,
      "loss": 0.8693,
      "num_input_tokens_seen": 42958688,
      "step": 3425,
      "train_runtime": 41201.2498,
      "train_tokens_per_second": 1042.655
    },
    {
      "epoch": 0.7002143513320405,
      "grad_norm": 5.30861234664917,
      "learning_rate": 2.0618519931165468e-07,
      "loss": 0.9722,
      "num_input_tokens_seen": 43021616,
      "step": 3430,
      "train_runtime": 41254.7062,
      "train_tokens_per_second": 1042.829
    },
    {
      "epoch": 0.7012350719608044,
      "grad_norm": 5.6079912185668945,
      "learning_rate": 2.0488953025958362e-07,
      "loss": 0.8856,
      "num_input_tokens_seen": 43084176,
      "step": 3435,
      "train_runtime": 41307.7723,
      "train_tokens_per_second": 1043.004
    },
    {
      "epoch": 0.7022557925895683,
      "grad_norm": 6.588621139526367,
      "learning_rate": 2.035968951624109e-07,
      "loss": 0.7885,
      "num_input_tokens_seen": 43146768,
      "step": 3440,
      "train_runtime": 41360.9688,
      "train_tokens_per_second": 1043.176
    },
    {
      "epoch": 0.7032765132183322,
      "grad_norm": 5.802103042602539,
      "learning_rate": 2.023073073093862e-07,
      "loss": 0.9865,
      "num_input_tokens_seen": 43209312,
      "step": 3445,
      "train_runtime": 41414.0257,
      "train_tokens_per_second": 1043.35
    },
    {
      "epoch": 0.7042972338470961,
      "grad_norm": 6.747570991516113,
      "learning_rate": 2.0102077995843065e-07,
      "loss": 0.9353,
      "num_input_tokens_seen": 43272592,
      "step": 3450,
      "train_runtime": 41467.6183,
      "train_tokens_per_second": 1043.527
    },
    {
      "epoch": 0.70531795447586,
      "grad_norm": 3.784517526626587,
      "learning_rate": 1.9973732633600127e-07,
      "loss": 1.0617,
      "num_input_tokens_seen": 43335872,
      "step": 3455,
      "train_runtime": 41521.2302,
      "train_tokens_per_second": 1043.704
    },
    {
      "epoch": 0.7063386751046239,
      "grad_norm": 5.296961784362793,
      "learning_rate": 1.984569596369549e-07,
      "loss": 1.0026,
      "num_input_tokens_seen": 43399296,
      "step": 3460,
      "train_runtime": 41574.9818,
      "train_tokens_per_second": 1043.88
    },
    {
      "epoch": 0.7073593957333878,
      "grad_norm": 5.549280166625977,
      "learning_rate": 1.9717969302441238e-07,
      "loss": 0.9977,
      "num_input_tokens_seen": 43462176,
      "step": 3465,
      "train_runtime": 41628.3448,
      "train_tokens_per_second": 1044.052
    },
    {
      "epoch": 0.7083801163621517,
      "grad_norm": 4.181572914123535,
      "learning_rate": 1.9590553962962392e-07,
      "loss": 0.9798,
      "num_input_tokens_seen": 43524208,
      "step": 3470,
      "train_runtime": 41681.0011,
      "train_tokens_per_second": 1044.222
    },
    {
      "epoch": 0.7094008369909156,
      "grad_norm": 3.768317937850952,
      "learning_rate": 1.9463451255183283e-07,
      "loss": 0.8678,
      "num_input_tokens_seen": 43586928,
      "step": 3475,
      "train_runtime": 41734.2228,
      "train_tokens_per_second": 1044.393
    },
    {
      "epoch": 0.7104215576196795,
      "grad_norm": 6.069814682006836,
      "learning_rate": 1.9336662485814176e-07,
      "loss": 1.0956,
      "num_input_tokens_seen": 43648912,
      "step": 3480,
      "train_runtime": 41786.8294,
      "train_tokens_per_second": 1044.561
    },
    {
      "epoch": 0.7114422782484434,
      "grad_norm": 3.985752820968628,
      "learning_rate": 1.9210188958337865e-07,
      "loss": 1.0406,
      "num_input_tokens_seen": 43710688,
      "step": 3485,
      "train_runtime": 41839.3945,
      "train_tokens_per_second": 1044.726
    },
    {
      "epoch": 0.7124629988772073,
      "grad_norm": 5.931200981140137,
      "learning_rate": 1.908403197299615e-07,
      "loss": 0.9331,
      "num_input_tokens_seen": 43773872,
      "step": 3490,
      "train_runtime": 41892.8908,
      "train_tokens_per_second": 1044.9
    },
    {
      "epoch": 0.7134837195059712,
      "grad_norm": 8.441361427307129,
      "learning_rate": 1.8958192826776592e-07,
      "loss": 1.0938,
      "num_input_tokens_seen": 43835888,
      "step": 3495,
      "train_runtime": 41945.7075,
      "train_tokens_per_second": 1045.063
    },
    {
      "epoch": 0.7145044401347351,
      "grad_norm": 5.137105941772461,
      "learning_rate": 1.8832672813399104e-07,
      "loss": 1.0199,
      "num_input_tokens_seen": 43898864,
      "step": 3500,
      "train_runtime": 41999.105,
      "train_tokens_per_second": 1045.233
    },
    {
      "epoch": 0.7145044401347351,
      "eval_loss": 1.0178990364074707,
      "eval_runtime": 139.081,
      "eval_samples_per_second": 5.695,
      "eval_steps_per_second": 2.847,
      "num_input_tokens_seen": 43898864,
      "step": 3500
    },
    {
      "epoch": 0.715525160763499,
      "grad_norm": 8.307478904724121,
      "learning_rate": 1.870747322330269e-07,
      "loss": 0.9081,
      "num_input_tokens_seen": 43961616,
      "step": 3505,
      "train_runtime": 42191.8545,
      "train_tokens_per_second": 1041.946
    },
    {
      "epoch": 0.7165458813922629,
      "grad_norm": 7.6189703941345215,
      "learning_rate": 1.8582595343632142e-07,
      "loss": 0.7884,
      "num_input_tokens_seen": 44024032,
      "step": 3510,
      "train_runtime": 42244.7976,
      "train_tokens_per_second": 1042.117
    },
    {
      "epoch": 0.7175666020210268,
      "grad_norm": 9.483945846557617,
      "learning_rate": 1.8458040458224882e-07,
      "loss": 0.9553,
      "num_input_tokens_seen": 44086176,
      "step": 3515,
      "train_runtime": 42297.4588,
      "train_tokens_per_second": 1042.289
    },
    {
      "epoch": 0.7185873226497907,
      "grad_norm": 5.585888862609863,
      "learning_rate": 1.833380984759764e-07,
      "loss": 0.7944,
      "num_input_tokens_seen": 44148544,
      "step": 3520,
      "train_runtime": 42350.3801,
      "train_tokens_per_second": 1042.459
    },
    {
      "epoch": 0.7196080432785547,
      "grad_norm": 5.054805278778076,
      "learning_rate": 1.8209904788933444e-07,
      "loss": 0.9098,
      "num_input_tokens_seen": 44211600,
      "step": 3525,
      "train_runtime": 42403.7871,
      "train_tokens_per_second": 1042.633
    },
    {
      "epoch": 0.7206287639073186,
      "grad_norm": 4.440540313720703,
      "learning_rate": 1.8086326556068333e-07,
      "loss": 1.136,
      "num_input_tokens_seen": 44274560,
      "step": 3530,
      "train_runtime": 42457.2197,
      "train_tokens_per_second": 1042.804
    },
    {
      "epoch": 0.7216494845360825,
      "grad_norm": 5.651664733886719,
      "learning_rate": 1.7963076419478364e-07,
      "loss": 0.9544,
      "num_input_tokens_seen": 44337072,
      "step": 3535,
      "train_runtime": 42510.3312,
      "train_tokens_per_second": 1042.972
    },
    {
      "epoch": 0.7226702051648464,
      "grad_norm": 7.289750576019287,
      "learning_rate": 1.7840155646266508e-07,
      "loss": 0.7628,
      "num_input_tokens_seen": 44398720,
      "step": 3540,
      "train_runtime": 42562.7821,
      "train_tokens_per_second": 1043.135
    },
    {
      "epoch": 0.7236909257936103,
      "grad_norm": 10.64663028717041,
      "learning_rate": 1.7717565500149662e-07,
      "loss": 0.8591,
      "num_input_tokens_seen": 44462656,
      "step": 3545,
      "train_runtime": 42616.8014,
      "train_tokens_per_second": 1043.313
    },
    {
      "epoch": 0.7247116464223742,
      "grad_norm": 7.837863922119141,
      "learning_rate": 1.7595307241445596e-07,
      "loss": 0.8949,
      "num_input_tokens_seen": 44525008,
      "step": 3550,
      "train_runtime": 42669.6752,
      "train_tokens_per_second": 1043.481
    },
    {
      "epoch": 0.7257323670511381,
      "grad_norm": 9.940572738647461,
      "learning_rate": 1.7473382127060045e-07,
      "loss": 1.1549,
      "num_input_tokens_seen": 44588976,
      "step": 3555,
      "train_runtime": 42723.8965,
      "train_tokens_per_second": 1043.654
    },
    {
      "epoch": 0.726753087679902,
      "grad_norm": 6.205425262451172,
      "learning_rate": 1.735179141047378e-07,
      "loss": 1.1579,
      "num_input_tokens_seen": 44652208,
      "step": 3560,
      "train_runtime": 42777.5551,
      "train_tokens_per_second": 1043.823
    },
    {
      "epoch": 0.7277738083086659,
      "grad_norm": 7.112247467041016,
      "learning_rate": 1.723053634172974e-07,
      "loss": 0.8843,
      "num_input_tokens_seen": 44714160,
      "step": 3565,
      "train_runtime": 42830.2691,
      "train_tokens_per_second": 1043.985
    },
    {
      "epoch": 0.7287945289374298,
      "grad_norm": 5.607700824737549,
      "learning_rate": 1.7109618167420104e-07,
      "loss": 0.9684,
      "num_input_tokens_seen": 44778320,
      "step": 3570,
      "train_runtime": 42884.6114,
      "train_tokens_per_second": 1044.158
    },
    {
      "epoch": 0.7298152495661937,
      "grad_norm": 6.456324100494385,
      "learning_rate": 1.6989038130673538e-07,
      "loss": 0.9744,
      "num_input_tokens_seen": 44841696,
      "step": 3575,
      "train_runtime": 42938.2606,
      "train_tokens_per_second": 1044.33
    },
    {
      "epoch": 0.7308359701949576,
      "grad_norm": 6.745334148406982,
      "learning_rate": 1.6868797471142432e-07,
      "loss": 0.9279,
      "num_input_tokens_seen": 44904480,
      "step": 3580,
      "train_runtime": 42991.5165,
      "train_tokens_per_second": 1044.496
    },
    {
      "epoch": 0.7318566908237215,
      "grad_norm": 4.779250621795654,
      "learning_rate": 1.6748897424990088e-07,
      "loss": 1.0004,
      "num_input_tokens_seen": 44966944,
      "step": 3585,
      "train_runtime": 43044.4439,
      "train_tokens_per_second": 1044.663
    },
    {
      "epoch": 0.7328774114524854,
      "grad_norm": 12.792655944824219,
      "learning_rate": 1.6629339224878068e-07,
      "loss": 1.2279,
      "num_input_tokens_seen": 45029680,
      "step": 3590,
      "train_runtime": 43097.6396,
      "train_tokens_per_second": 1044.829
    },
    {
      "epoch": 0.7338981320812493,
      "grad_norm": 6.781522750854492,
      "learning_rate": 1.65101240999535e-07,
      "loss": 1.0673,
      "num_input_tokens_seen": 45092432,
      "step": 3595,
      "train_runtime": 43150.8829,
      "train_tokens_per_second": 1044.994
    },
    {
      "epoch": 0.7349188527100132,
      "grad_norm": 9.183736801147461,
      "learning_rate": 1.6391253275836463e-07,
      "loss": 1.0858,
      "num_input_tokens_seen": 45155648,
      "step": 3600,
      "train_runtime": 43204.4092,
      "train_tokens_per_second": 1045.163
    },
    {
      "epoch": 0.7349188527100132,
      "eval_loss": 1.0177255868911743,
      "eval_runtime": 138.9548,
      "eval_samples_per_second": 5.7,
      "eval_steps_per_second": 2.85,
      "num_input_tokens_seen": 45155648,
      "step": 3600
    },
    {
      "epoch": 0.7359395733387771,
      "grad_norm": 5.9311699867248535,
      "learning_rate": 1.6272727974607332e-07,
      "loss": 0.8694,
      "num_input_tokens_seen": 45217968,
      "step": 3605,
      "train_runtime": 43396.7442,
      "train_tokens_per_second": 1041.967
    },
    {
      "epoch": 0.736960293967541,
      "grad_norm": 4.915717601776123,
      "learning_rate": 1.6154549414794288e-07,
      "loss": 0.7642,
      "num_input_tokens_seen": 45280176,
      "step": 3610,
      "train_runtime": 43449.5995,
      "train_tokens_per_second": 1042.131
    },
    {
      "epoch": 0.7379810145963049,
      "grad_norm": 5.3420820236206055,
      "learning_rate": 1.6036718811360722e-07,
      "loss": 1.2173,
      "num_input_tokens_seen": 45342320,
      "step": 3615,
      "train_runtime": 43502.2801,
      "train_tokens_per_second": 1042.298
    },
    {
      "epoch": 0.7390017352250688,
      "grad_norm": 4.242177963256836,
      "learning_rate": 1.5919237375692795e-07,
      "loss": 0.9435,
      "num_input_tokens_seen": 45404912,
      "step": 3620,
      "train_runtime": 43555.443,
      "train_tokens_per_second": 1042.462
    },
    {
      "epoch": 0.7400224558538329,
      "grad_norm": 6.61102294921875,
      "learning_rate": 1.5802106315586944e-07,
      "loss": 0.9625,
      "num_input_tokens_seen": 45468352,
      "step": 3625,
      "train_runtime": 43609.1066,
      "train_tokens_per_second": 1042.634
    },
    {
      "epoch": 0.7410431764825968,
      "grad_norm": 5.01363468170166,
      "learning_rate": 1.5685326835237505e-07,
      "loss": 0.8656,
      "num_input_tokens_seen": 45531488,
      "step": 3630,
      "train_runtime": 43662.6417,
      "train_tokens_per_second": 1042.802
    },
    {
      "epoch": 0.7420638971113607,
      "grad_norm": 5.550558090209961,
      "learning_rate": 1.5568900135224277e-07,
      "loss": 0.7232,
      "num_input_tokens_seen": 45594400,
      "step": 3635,
      "train_runtime": 43715.9408,
      "train_tokens_per_second": 1042.97
    },
    {
      "epoch": 0.7430846177401246,
      "grad_norm": 8.905698776245117,
      "learning_rate": 1.5452827412500285e-07,
      "loss": 0.9445,
      "num_input_tokens_seen": 45656896,
      "step": 3640,
      "train_runtime": 43769.0566,
      "train_tokens_per_second": 1043.132
    },
    {
      "epoch": 0.7441053383688885,
      "grad_norm": 5.115056037902832,
      "learning_rate": 1.5337109860379345e-07,
      "loss": 1.2952,
      "num_input_tokens_seen": 45720560,
      "step": 3645,
      "train_runtime": 43822.9731,
      "train_tokens_per_second": 1043.301
    },
    {
      "epoch": 0.7451260589976524,
      "grad_norm": 4.751903533935547,
      "learning_rate": 1.522174866852385e-07,
      "loss": 0.7482,
      "num_input_tokens_seen": 45783264,
      "step": 3650,
      "train_runtime": 43876.0484,
      "train_tokens_per_second": 1043.468
    },
    {
      "epoch": 0.7461467796264163,
      "grad_norm": 7.72766637802124,
      "learning_rate": 1.51067450229326e-07,
      "loss": 1.0526,
      "num_input_tokens_seen": 45846352,
      "step": 3655,
      "train_runtime": 43929.5004,
      "train_tokens_per_second": 1043.635
    },
    {
      "epoch": 0.7471675002551802,
      "grad_norm": 8.493313789367676,
      "learning_rate": 1.4992100105928507e-07,
      "loss": 1.0554,
      "num_input_tokens_seen": 45908496,
      "step": 3660,
      "train_runtime": 43982.4036,
      "train_tokens_per_second": 1043.792
    },
    {
      "epoch": 0.7481882208839441,
      "grad_norm": 4.4461565017700195,
      "learning_rate": 1.48778150961465e-07,
      "loss": 1.1372,
      "num_input_tokens_seen": 45970896,
      "step": 3665,
      "train_runtime": 44035.2729,
      "train_tokens_per_second": 1043.956
    },
    {
      "epoch": 0.749208941512708,
      "grad_norm": 5.630451679229736,
      "learning_rate": 1.4763891168521393e-07,
      "loss": 0.8005,
      "num_input_tokens_seen": 46034144,
      "step": 3670,
      "train_runtime": 44089.1134,
      "train_tokens_per_second": 1044.116
    },
    {
      "epoch": 0.7502296621414719,
      "grad_norm": 6.2404937744140625,
      "learning_rate": 1.4650329494275787e-07,
      "loss": 0.8255,
      "num_input_tokens_seen": 46098400,
      "step": 3675,
      "train_runtime": 44143.4149,
      "train_tokens_per_second": 1044.287
    },
    {
      "epoch": 0.7512503827702358,
      "grad_norm": 4.672713756561279,
      "learning_rate": 1.4537131240908113e-07,
      "loss": 0.8829,
      "num_input_tokens_seen": 46162064,
      "step": 3680,
      "train_runtime": 44197.2044,
      "train_tokens_per_second": 1044.457
    },
    {
      "epoch": 0.7522711033989997,
      "grad_norm": 7.4568681716918945,
      "learning_rate": 1.4424297572180505e-07,
      "loss": 1.4291,
      "num_input_tokens_seen": 46225744,
      "step": 3685,
      "train_runtime": 44251.0888,
      "train_tokens_per_second": 1044.624
    },
    {
      "epoch": 0.7532918240277636,
      "grad_norm": 4.714420318603516,
      "learning_rate": 1.4311829648106904e-07,
      "loss": 1.0024,
      "num_input_tokens_seen": 46288544,
      "step": 3690,
      "train_runtime": 44304.3351,
      "train_tokens_per_second": 1044.786
    },
    {
      "epoch": 0.7543125446565275,
      "grad_norm": 4.1157755851745605,
      "learning_rate": 1.4199728624941132e-07,
      "loss": 0.9572,
      "num_input_tokens_seen": 46350928,
      "step": 3695,
      "train_runtime": 44357.3686,
      "train_tokens_per_second": 1044.943
    },
    {
      "epoch": 0.7553332652852914,
      "grad_norm": 8.61734676361084,
      "learning_rate": 1.408799565516502e-07,
      "loss": 1.1275,
      "num_input_tokens_seen": 46413824,
      "step": 3700,
      "train_runtime": 44410.6412,
      "train_tokens_per_second": 1045.106
    },
    {
      "epoch": 0.7553332652852914,
      "eval_loss": 1.017177700996399,
      "eval_runtime": 139.0266,
      "eval_samples_per_second": 5.697,
      "eval_steps_per_second": 2.848,
      "num_input_tokens_seen": 46413824,
      "step": 3700
    },
    {
      "epoch": 0.7563539859140553,
      "grad_norm": 5.583923816680908,
      "learning_rate": 1.3976631887476504e-07,
      "loss": 0.9707,
      "num_input_tokens_seen": 46476048,
      "step": 3705,
      "train_runtime": 44602.9547,
      "train_tokens_per_second": 1041.995
    },
    {
      "epoch": 0.7573747065428192,
      "grad_norm": 6.441941738128662,
      "learning_rate": 1.3865638466777863e-07,
      "loss": 1.0576,
      "num_input_tokens_seen": 46539472,
      "step": 3710,
      "train_runtime": 44656.9385,
      "train_tokens_per_second": 1042.155
    },
    {
      "epoch": 0.7583954271715831,
      "grad_norm": 6.795119762420654,
      "learning_rate": 1.375501653416391e-07,
      "loss": 1.2845,
      "num_input_tokens_seen": 46602304,
      "step": 3715,
      "train_runtime": 44710.3593,
      "train_tokens_per_second": 1042.316
    },
    {
      "epoch": 0.759416147800347,
      "grad_norm": 7.0702738761901855,
      "learning_rate": 1.3644767226910335e-07,
      "loss": 1.0975,
      "num_input_tokens_seen": 46665472,
      "step": 3720,
      "train_runtime": 44763.9482,
      "train_tokens_per_second": 1042.479
    },
    {
      "epoch": 0.760436868429111,
      "grad_norm": 5.551024436950684,
      "learning_rate": 1.353489167846193e-07,
      "loss": 0.9671,
      "num_input_tokens_seen": 46727936,
      "step": 3725,
      "train_runtime": 44816.9606,
      "train_tokens_per_second": 1042.64
    },
    {
      "epoch": 0.7614575890578749,
      "grad_norm": 6.616237163543701,
      "learning_rate": 1.3425391018420957e-07,
      "loss": 1.0845,
      "num_input_tokens_seen": 46790576,
      "step": 3730,
      "train_runtime": 44870.156,
      "train_tokens_per_second": 1042.799
    },
    {
      "epoch": 0.7624783096866388,
      "grad_norm": 6.859769821166992,
      "learning_rate": 1.3316266372535567e-07,
      "loss": 1.0492,
      "num_input_tokens_seen": 46852304,
      "step": 3735,
      "train_runtime": 44922.5512,
      "train_tokens_per_second": 1042.957
    },
    {
      "epoch": 0.7634990303154027,
      "grad_norm": 5.50518274307251,
      "learning_rate": 1.3207518862688182e-07,
      "loss": 1.0155,
      "num_input_tokens_seen": 46914944,
      "step": 3740,
      "train_runtime": 44975.8563,
      "train_tokens_per_second": 1043.114
    },
    {
      "epoch": 0.7645197509441666,
      "grad_norm": 9.936107635498047,
      "learning_rate": 1.3099149606884042e-07,
      "loss": 0.8986,
      "num_input_tokens_seen": 46976816,
      "step": 3745,
      "train_runtime": 45028.5772,
      "train_tokens_per_second": 1043.267
    },
    {
      "epoch": 0.7655404715729305,
      "grad_norm": 4.284589767456055,
      "learning_rate": 1.299115971923958e-07,
      "loss": 1.1063,
      "num_input_tokens_seen": 47040064,
      "step": 3750,
      "train_runtime": 45082.1195,
      "train_tokens_per_second": 1043.431
    },
    {
      "epoch": 0.7665611922016944,
      "grad_norm": 5.2984843254089355,
      "learning_rate": 1.2883550309971108e-07,
      "loss": 0.9038,
      "num_input_tokens_seen": 47102704,
      "step": 3755,
      "train_runtime": 45135.2647,
      "train_tokens_per_second": 1043.59
    },
    {
      "epoch": 0.7675819128304583,
      "grad_norm": 6.143093109130859,
      "learning_rate": 1.2776322485383295e-07,
      "loss": 0.9573,
      "num_input_tokens_seen": 47165392,
      "step": 3760,
      "train_runtime": 45188.3544,
      "train_tokens_per_second": 1043.751
    },
    {
      "epoch": 0.7686026334592222,
      "grad_norm": 8.628202438354492,
      "learning_rate": 1.266947734785785e-07,
      "loss": 0.9827,
      "num_input_tokens_seen": 47228176,
      "step": 3765,
      "train_runtime": 45241.5621,
      "train_tokens_per_second": 1043.911
    },
    {
      "epoch": 0.7696233540879861,
      "grad_norm": 4.090152740478516,
      "learning_rate": 1.2563015995842163e-07,
      "loss": 1.065,
      "num_input_tokens_seen": 47290144,
      "step": 3770,
      "train_runtime": 45294.1944,
      "train_tokens_per_second": 1044.066
    },
    {
      "epoch": 0.77064407471675,
      "grad_norm": 6.606918811798096,
      "learning_rate": 1.245693952383805e-07,
      "loss": 0.9401,
      "num_input_tokens_seen": 47353568,
      "step": 3775,
      "train_runtime": 45347.955,
      "train_tokens_per_second": 1044.227
    },
    {
      "epoch": 0.7716647953455139,
      "grad_norm": 7.634079456329346,
      "learning_rate": 1.2351249022390438e-07,
      "loss": 0.8401,
      "num_input_tokens_seen": 47415680,
      "step": 3780,
      "train_runtime": 45400.5887,
      "train_tokens_per_second": 1044.385
    },
    {
      "epoch": 0.7726855159742778,
      "grad_norm": 21.83013343811035,
      "learning_rate": 1.224594557807624e-07,
      "loss": 1.2098,
      "num_input_tokens_seen": 47477856,
      "step": 3785,
      "train_runtime": 45453.5333,
      "train_tokens_per_second": 1044.536
    },
    {
      "epoch": 0.7737062366030417,
      "grad_norm": 8.529077529907227,
      "learning_rate": 1.214103027349307e-07,
      "loss": 0.9134,
      "num_input_tokens_seen": 47540736,
      "step": 3790,
      "train_runtime": 45506.761,
      "train_tokens_per_second": 1044.696
    },
    {
      "epoch": 0.7747269572318056,
      "grad_norm": 5.670898914337158,
      "learning_rate": 1.2036504187248247e-07,
      "loss": 1.0075,
      "num_input_tokens_seen": 47602848,
      "step": 3795,
      "train_runtime": 45559.353,
      "train_tokens_per_second": 1044.853
    },
    {
      "epoch": 0.7757476778605695,
      "grad_norm": 4.498054027557373,
      "learning_rate": 1.193236839394759e-07,
      "loss": 0.9278,
      "num_input_tokens_seen": 47665648,
      "step": 3800,
      "train_runtime": 45612.5323,
      "train_tokens_per_second": 1045.012
    },
    {
      "epoch": 0.7757476778605695,
      "eval_loss": 1.0167315006256104,
      "eval_runtime": 139.2883,
      "eval_samples_per_second": 5.686,
      "eval_steps_per_second": 2.843,
      "num_input_tokens_seen": 47665648,
      "step": 3800
    },
    {
      "epoch": 0.7767683984893334,
      "grad_norm": 5.291418075561523,
      "learning_rate": 1.182862396418442e-07,
      "loss": 1.2151,
      "num_input_tokens_seen": 47727712,
      "step": 3805,
      "train_runtime": 45804.9454,
      "train_tokens_per_second": 1041.977
    },
    {
      "epoch": 0.7777891191180973,
      "grad_norm": 7.1012115478515625,
      "learning_rate": 1.1725271964528566e-07,
      "loss": 0.9798,
      "num_input_tokens_seen": 47790304,
      "step": 3810,
      "train_runtime": 45858.1462,
      "train_tokens_per_second": 1042.133
    },
    {
      "epoch": 0.7788098397468612,
      "grad_norm": 10.384283065795898,
      "learning_rate": 1.162231345751537e-07,
      "loss": 1.0001,
      "num_input_tokens_seen": 47854000,
      "step": 3815,
      "train_runtime": 45912.0457,
      "train_tokens_per_second": 1042.297
    },
    {
      "epoch": 0.7798305603756251,
      "grad_norm": 6.866764068603516,
      "learning_rate": 1.1519749501634785e-07,
      "loss": 1.1055,
      "num_input_tokens_seen": 47916656,
      "step": 3820,
      "train_runtime": 45965.1092,
      "train_tokens_per_second": 1042.457
    },
    {
      "epoch": 0.780851281004389,
      "grad_norm": 4.60667610168457,
      "learning_rate": 1.1417581151320471e-07,
      "loss": 0.9716,
      "num_input_tokens_seen": 47979040,
      "step": 3825,
      "train_runtime": 46018.1468,
      "train_tokens_per_second": 1042.611
    },
    {
      "epoch": 0.781872001633153,
      "grad_norm": 4.499264240264893,
      "learning_rate": 1.1315809456939008e-07,
      "loss": 1.2453,
      "num_input_tokens_seen": 48041008,
      "step": 3830,
      "train_runtime": 46070.9735,
      "train_tokens_per_second": 1042.761
    },
    {
      "epoch": 0.7828927222619169,
      "grad_norm": 5.539493083953857,
      "learning_rate": 1.1214435464779003e-07,
      "loss": 0.975,
      "num_input_tokens_seen": 48104496,
      "step": 3835,
      "train_runtime": 46124.7434,
      "train_tokens_per_second": 1042.922
    },
    {
      "epoch": 0.7839134428906808,
      "grad_norm": 6.44614315032959,
      "learning_rate": 1.1113460217040443e-07,
      "loss": 1.1599,
      "num_input_tokens_seen": 48167984,
      "step": 3840,
      "train_runtime": 46178.5247,
      "train_tokens_per_second": 1043.082
    },
    {
      "epoch": 0.7849341635194448,
      "grad_norm": 5.475955963134766,
      "learning_rate": 1.1012884751823892e-07,
      "loss": 0.8893,
      "num_input_tokens_seen": 48231392,
      "step": 3845,
      "train_runtime": 46232.2479,
      "train_tokens_per_second": 1043.241
    },
    {
      "epoch": 0.7859548841482087,
      "grad_norm": 211.255615234375,
      "learning_rate": 1.0912710103119854e-07,
      "loss": 0.9367,
      "num_input_tokens_seen": 48294720,
      "step": 3850,
      "train_runtime": 46285.7319,
      "train_tokens_per_second": 1043.404
    },
    {
      "epoch": 0.7869756047769726,
      "grad_norm": 5.825006484985352,
      "learning_rate": 1.0812937300798158e-07,
      "loss": 0.9368,
      "num_input_tokens_seen": 48357936,
      "step": 3855,
      "train_runtime": 46339.232,
      "train_tokens_per_second": 1043.564
    },
    {
      "epoch": 0.7879963254057365,
      "grad_norm": 6.810013771057129,
      "learning_rate": 1.0713567370597331e-07,
      "loss": 0.8637,
      "num_input_tokens_seen": 48421152,
      "step": 3860,
      "train_runtime": 46392.8425,
      "train_tokens_per_second": 1043.72
    },
    {
      "epoch": 0.7890170460345004,
      "grad_norm": 8.710183143615723,
      "learning_rate": 1.0614601334114098e-07,
      "loss": 0.9084,
      "num_input_tokens_seen": 48484176,
      "step": 3865,
      "train_runtime": 46446.3461,
      "train_tokens_per_second": 1043.875
    },
    {
      "epoch": 0.7900377666632643,
      "grad_norm": 5.780780792236328,
      "learning_rate": 1.0516040208792832e-07,
      "loss": 1.0654,
      "num_input_tokens_seen": 48548144,
      "step": 3870,
      "train_runtime": 46500.5177,
      "train_tokens_per_second": 1044.034
    },
    {
      "epoch": 0.7910584872920282,
      "grad_norm": 5.912722587585449,
      "learning_rate": 1.0417885007915156e-07,
      "loss": 1.1118,
      "num_input_tokens_seen": 48611248,
      "step": 3875,
      "train_runtime": 46554.184,
      "train_tokens_per_second": 1044.186
    },
    {
      "epoch": 0.7920792079207921,
      "grad_norm": 6.890778541564941,
      "learning_rate": 1.0320136740589436e-07,
      "loss": 1.0732,
      "num_input_tokens_seen": 48673648,
      "step": 3880,
      "train_runtime": 46607.0492,
      "train_tokens_per_second": 1044.341
    },
    {
      "epoch": 0.793099928549556,
      "grad_norm": 6.0933732986450195,
      "learning_rate": 1.0222796411740525e-07,
      "loss": 1.0984,
      "num_input_tokens_seen": 48735824,
      "step": 3885,
      "train_runtime": 46659.9775,
      "train_tokens_per_second": 1044.489
    },
    {
      "epoch": 0.7941206491783199,
      "grad_norm": 5.486189842224121,
      "learning_rate": 1.0125865022099306e-07,
      "loss": 1.1169,
      "num_input_tokens_seen": 48798176,
      "step": 3890,
      "train_runtime": 46712.9119,
      "train_tokens_per_second": 1044.64
    },
    {
      "epoch": 0.7951413698070838,
      "grad_norm": 5.166478633880615,
      "learning_rate": 1.0029343568192494e-07,
      "loss": 1.1358,
      "num_input_tokens_seen": 48860768,
      "step": 3895,
      "train_runtime": 46766.0525,
      "train_tokens_per_second": 1044.791
    },
    {
      "epoch": 0.7961620904358477,
      "grad_norm": 4.785730361938477,
      "learning_rate": 9.93323304233234e-08,
      "loss": 0.9836,
      "num_input_tokens_seen": 48924240,
      "step": 3900,
      "train_runtime": 46819.7453,
      "train_tokens_per_second": 1044.949
    },
    {
      "epoch": 0.7961620904358477,
      "eval_loss": 1.0163170099258423,
      "eval_runtime": 139.1474,
      "eval_samples_per_second": 5.692,
      "eval_steps_per_second": 2.846,
      "num_input_tokens_seen": 48924240,
      "step": 3900
    },
    {
      "epoch": 0.7971828110646116,
      "grad_norm": 6.4773712158203125,
      "learning_rate": 9.837534432606471e-08,
      "loss": 0.8898,
      "num_input_tokens_seen": 48987232,
      "step": 3905,
      "train_runtime": 47012.6871,
      "train_tokens_per_second": 1042.0
    },
    {
      "epoch": 0.7982035316933755,
      "grad_norm": 10.452618598937988,
      "learning_rate": 9.74224872286768e-08,
      "loss": 1.0158,
      "num_input_tokens_seen": 49049696,
      "step": 3910,
      "train_runtime": 47065.8082,
      "train_tokens_per_second": 1042.151
    },
    {
      "epoch": 0.7992242523221395,
      "grad_norm": 7.048309803009033,
      "learning_rate": 9.647376892723891e-08,
      "loss": 1.0525,
      "num_input_tokens_seen": 49111568,
      "step": 3915,
      "train_runtime": 47118.4626,
      "train_tokens_per_second": 1042.3
    },
    {
      "epoch": 0.8002449729509034,
      "grad_norm": 4.847088813781738,
      "learning_rate": 9.55291991752799e-08,
      "loss": 0.8565,
      "num_input_tokens_seen": 49173712,
      "step": 3920,
      "train_runtime": 47171.134,
      "train_tokens_per_second": 1042.453
    },
    {
      "epoch": 0.8012656935796673,
      "grad_norm": 7.113495349884033,
      "learning_rate": 9.458878768367862e-08,
      "loss": 1.234,
      "num_input_tokens_seen": 49236688,
      "step": 3925,
      "train_runtime": 47224.512,
      "train_tokens_per_second": 1042.609
    },
    {
      "epoch": 0.8022864142084312,
      "grad_norm": 6.137209892272949,
      "learning_rate": 9.365254412056406e-08,
      "loss": 0.9955,
      "num_input_tokens_seen": 49299728,
      "step": 3930,
      "train_runtime": 47277.8473,
      "train_tokens_per_second": 1042.766
    },
    {
      "epoch": 0.8033071348371951,
      "grad_norm": 6.46328067779541,
      "learning_rate": 9.272047811121569e-08,
      "loss": 1.0709,
      "num_input_tokens_seen": 49362320,
      "step": 3935,
      "train_runtime": 47331.0634,
      "train_tokens_per_second": 1042.916
    },
    {
      "epoch": 0.804327855465959,
      "grad_norm": 8.223533630371094,
      "learning_rate": 9.179259923796456e-08,
      "loss": 1.298,
      "num_input_tokens_seen": 49424768,
      "step": 3940,
      "train_runtime": 47384.1397,
      "train_tokens_per_second": 1043.066
    },
    {
      "epoch": 0.8053485760947229,
      "grad_norm": 5.201224327087402,
      "learning_rate": 9.086891704009475e-08,
      "loss": 1.1855,
      "num_input_tokens_seen": 49488080,
      "step": 3945,
      "train_runtime": 47437.7221,
      "train_tokens_per_second": 1043.222
    },
    {
      "epoch": 0.8063692967234868,
      "grad_norm": 12.144189834594727,
      "learning_rate": 8.994944101374585e-08,
      "loss": 0.9899,
      "num_input_tokens_seen": 49552208,
      "step": 3950,
      "train_runtime": 47492.0448,
      "train_tokens_per_second": 1043.379
    },
    {
      "epoch": 0.8073900173522507,
      "grad_norm": 5.257981300354004,
      "learning_rate": 8.903418061181445e-08,
      "loss": 1.2597,
      "num_input_tokens_seen": 49614176,
      "step": 3955,
      "train_runtime": 47544.6637,
      "train_tokens_per_second": 1043.528
    },
    {
      "epoch": 0.8084107379810146,
      "grad_norm": 5.421973705291748,
      "learning_rate": 8.812314524385755e-08,
      "loss": 1.0636,
      "num_input_tokens_seen": 49678528,
      "step": 3960,
      "train_runtime": 47599.1241,
      "train_tokens_per_second": 1043.686
    },
    {
      "epoch": 0.8094314586097785,
      "grad_norm": 5.689068794250488,
      "learning_rate": 8.72163442759955e-08,
      "loss": 1.1692,
      "num_input_tokens_seen": 49741648,
      "step": 3965,
      "train_runtime": 47652.5187,
      "train_tokens_per_second": 1043.841
    },
    {
      "epoch": 0.8104521792385424,
      "grad_norm": 4.852738380432129,
      "learning_rate": 8.631378703081626e-08,
      "loss": 1.0398,
      "num_input_tokens_seen": 49804544,
      "step": 3970,
      "train_runtime": 47705.8223,
      "train_tokens_per_second": 1043.993
    },
    {
      "epoch": 0.8114728998673063,
      "grad_norm": 7.353608131408691,
      "learning_rate": 8.541548278727879e-08,
      "loss": 0.9601,
      "num_input_tokens_seen": 49867248,
      "step": 3975,
      "train_runtime": 47759.0432,
      "train_tokens_per_second": 1044.143
    },
    {
      "epoch": 0.8124936204960702,
      "grad_norm": 7.772989273071289,
      "learning_rate": 8.452144078061818e-08,
      "loss": 1.1244,
      "num_input_tokens_seen": 49930720,
      "step": 3980,
      "train_runtime": 47812.7346,
      "train_tokens_per_second": 1044.298
    },
    {
      "epoch": 0.8135143411248341,
      "grad_norm": 6.141856670379639,
      "learning_rate": 8.363167020225037e-08,
      "loss": 1.0689,
      "num_input_tokens_seen": 49993808,
      "step": 3985,
      "train_runtime": 47866.1097,
      "train_tokens_per_second": 1044.451
    },
    {
      "epoch": 0.814535061753598,
      "grad_norm": 4.434154987335205,
      "learning_rate": 8.274618019967833e-08,
      "loss": 0.8659,
      "num_input_tokens_seen": 50056640,
      "step": 3990,
      "train_runtime": 47919.5847,
      "train_tokens_per_second": 1044.597
    },
    {
      "epoch": 0.8155557823823619,
      "grad_norm": 4.845359802246094,
      "learning_rate": 8.186497987639701e-08,
      "loss": 0.8787,
      "num_input_tokens_seen": 50119216,
      "step": 3995,
      "train_runtime": 47972.6663,
      "train_tokens_per_second": 1044.745
    },
    {
      "epoch": 0.8165765030111258,
      "grad_norm": 5.775053977966309,
      "learning_rate": 8.098807829180055e-08,
      "loss": 1.0544,
      "num_input_tokens_seen": 50181760,
      "step": 4000,
      "train_runtime": 48025.7828,
      "train_tokens_per_second": 1044.892
    },
    {
      "epoch": 0.8165765030111258,
      "eval_loss": 1.016510009765625,
      "eval_runtime": 139.0455,
      "eval_samples_per_second": 5.696,
      "eval_steps_per_second": 2.848,
      "num_input_tokens_seen": 50181760,
      "step": 4000
    },
    {
      "epoch": 0.8175972236398897,
      "grad_norm": 6.94951057434082,
      "learning_rate": 8.011548446108868e-08,
      "loss": 0.9503,
      "num_input_tokens_seen": 50244240,
      "step": 4005,
      "train_runtime": 48218.3402,
      "train_tokens_per_second": 1042.015
    },
    {
      "epoch": 0.8186179442686536,
      "grad_norm": 6.583745956420898,
      "learning_rate": 7.924720735517431e-08,
      "loss": 0.8308,
      "num_input_tokens_seen": 50307296,
      "step": 4010,
      "train_runtime": 48271.8607,
      "train_tokens_per_second": 1042.166
    },
    {
      "epoch": 0.8196386648974175,
      "grad_norm": 9.208976745605469,
      "learning_rate": 7.838325590059147e-08,
      "loss": 1.0342,
      "num_input_tokens_seen": 50370192,
      "step": 4015,
      "train_runtime": 48325.1313,
      "train_tokens_per_second": 1042.319
    },
    {
      "epoch": 0.8206593855261815,
      "grad_norm": 6.422208309173584,
      "learning_rate": 7.752363897940283e-08,
      "loss": 1.0781,
      "num_input_tokens_seen": 50432272,
      "step": 4020,
      "train_runtime": 48378.0743,
      "train_tokens_per_second": 1042.461
    },
    {
      "epoch": 0.8216801061549454,
      "grad_norm": 6.244872570037842,
      "learning_rate": 7.666836542910898e-08,
      "loss": 0.9184,
      "num_input_tokens_seen": 50495616,
      "step": 4025,
      "train_runtime": 48431.8266,
      "train_tokens_per_second": 1042.612
    },
    {
      "epoch": 0.8227008267837093,
      "grad_norm": 5.912007808685303,
      "learning_rate": 7.58174440425577e-08,
      "loss": 0.7909,
      "num_input_tokens_seen": 50559408,
      "step": 4030,
      "train_runtime": 48485.8515,
      "train_tokens_per_second": 1042.766
    },
    {
      "epoch": 0.8237215474124732,
      "grad_norm": 7.13372278213501,
      "learning_rate": 7.497088356785302e-08,
      "loss": 1.1659,
      "num_input_tokens_seen": 50621760,
      "step": 4035,
      "train_runtime": 48538.7721,
      "train_tokens_per_second": 1042.914
    },
    {
      "epoch": 0.8247422680412371,
      "grad_norm": 3.77030873298645,
      "learning_rate": 7.412869270826555e-08,
      "loss": 1.087,
      "num_input_tokens_seen": 50684944,
      "step": 4040,
      "train_runtime": 48592.5025,
      "train_tokens_per_second": 1043.061
    },
    {
      "epoch": 0.825762988670001,
      "grad_norm": 5.7828826904296875,
      "learning_rate": 7.329088012214312e-08,
      "loss": 0.883,
      "num_input_tokens_seen": 50748352,
      "step": 4045,
      "train_runtime": 48646.3117,
      "train_tokens_per_second": 1043.211
    },
    {
      "epoch": 0.8267837092987649,
      "grad_norm": 5.085469722747803,
      "learning_rate": 7.245745442282165e-08,
      "loss": 1.05,
      "num_input_tokens_seen": 50810672,
      "step": 4050,
      "train_runtime": 48699.1722,
      "train_tokens_per_second": 1043.358
    },
    {
      "epoch": 0.8278044299275288,
      "grad_norm": 3.8606340885162354,
      "learning_rate": 7.162842417853643e-08,
      "loss": 1.031,
      "num_input_tokens_seen": 50874304,
      "step": 4055,
      "train_runtime": 48752.994,
      "train_tokens_per_second": 1043.511
    },
    {
      "epoch": 0.8288251505562927,
      "grad_norm": 6.328223705291748,
      "learning_rate": 7.080379791233454e-08,
      "loss": 1.1313,
      "num_input_tokens_seen": 50936848,
      "step": 4060,
      "train_runtime": 48806.1269,
      "train_tokens_per_second": 1043.657
    },
    {
      "epoch": 0.8298458711850567,
      "grad_norm": 3.668347120285034,
      "learning_rate": 6.998358410198651e-08,
      "loss": 0.8392,
      "num_input_tokens_seen": 51000240,
      "step": 4065,
      "train_runtime": 48859.8495,
      "train_tokens_per_second": 1043.807
    },
    {
      "epoch": 0.8308665918138206,
      "grad_norm": 8.362772941589355,
      "learning_rate": 6.916779117989996e-08,
      "loss": 1.0838,
      "num_input_tokens_seen": 51062736,
      "step": 4070,
      "train_runtime": 48912.7098,
      "train_tokens_per_second": 1043.956
    },
    {
      "epoch": 0.8318873124425845,
      "grad_norm": 5.755902290344238,
      "learning_rate": 6.835642753303216e-08,
      "loss": 0.9149,
      "num_input_tokens_seen": 51125824,
      "step": 4075,
      "train_runtime": 48966.1169,
      "train_tokens_per_second": 1044.106
    },
    {
      "epoch": 0.8329080330713484,
      "grad_norm": 6.877366542816162,
      "learning_rate": 6.754950150280425e-08,
      "loss": 0.923,
      "num_input_tokens_seen": 51188480,
      "step": 4080,
      "train_runtime": 49019.3131,
      "train_tokens_per_second": 1044.251
    },
    {
      "epoch": 0.8339287537001123,
      "grad_norm": 5.2096848487854,
      "learning_rate": 6.674702138501536e-08,
      "loss": 0.9787,
      "num_input_tokens_seen": 51252464,
      "step": 4085,
      "train_runtime": 49073.2972,
      "train_tokens_per_second": 1044.406
    },
    {
      "epoch": 0.8349494743288762,
      "grad_norm": 3.438173532485962,
      "learning_rate": 6.594899542975735e-08,
      "loss": 1.0186,
      "num_input_tokens_seen": 51315152,
      "step": 4090,
      "train_runtime": 49126.4491,
      "train_tokens_per_second": 1044.552
    },
    {
      "epoch": 0.8359701949576401,
      "grad_norm": 7.202823638916016,
      "learning_rate": 6.515543184132999e-08,
      "loss": 0.8438,
      "num_input_tokens_seen": 51378576,
      "step": 4095,
      "train_runtime": 49180.2925,
      "train_tokens_per_second": 1044.698
    },
    {
      "epoch": 0.836990915586404,
      "grad_norm": 14.145129203796387,
      "learning_rate": 6.436633877815651e-08,
      "loss": 1.1331,
      "num_input_tokens_seen": 51440192,
      "step": 4100,
      "train_runtime": 49232.6953,
      "train_tokens_per_second": 1044.838
    },
    {
      "epoch": 0.836990915586404,
      "eval_loss": 1.0159064531326294,
      "eval_runtime": 138.9827,
      "eval_samples_per_second": 5.699,
      "eval_steps_per_second": 2.849,
      "num_input_tokens_seen": 51440192,
      "step": 4100
    },
    {
      "epoch": 0.8380116362151679,
      "grad_norm": 5.150675296783447,
      "learning_rate": 6.35817243527001e-08,
      "loss": 1.0141,
      "num_input_tokens_seen": 51502192,
      "step": 4105,
      "train_runtime": 49424.661,
      "train_tokens_per_second": 1042.034
    },
    {
      "epoch": 0.8390323568439318,
      "grad_norm": 6.212120532989502,
      "learning_rate": 6.280159663137973e-08,
      "loss": 0.9682,
      "num_input_tokens_seen": 51564592,
      "step": 4110,
      "train_runtime": 49477.6623,
      "train_tokens_per_second": 1042.179
    },
    {
      "epoch": 0.8400530774726958,
      "grad_norm": 6.472250938415527,
      "learning_rate": 6.202596363448836e-08,
      "loss": 0.9544,
      "num_input_tokens_seen": 51627968,
      "step": 4115,
      "train_runtime": 49531.2283,
      "train_tokens_per_second": 1042.332
    },
    {
      "epoch": 0.8410737981014597,
      "grad_norm": 7.522763252258301,
      "learning_rate": 6.125483333610937e-08,
      "loss": 1.0268,
      "num_input_tokens_seen": 51690016,
      "step": 4120,
      "train_runtime": 49584.0039,
      "train_tokens_per_second": 1042.474
    },
    {
      "epoch": 0.8420945187302236,
      "grad_norm": 5.488574028015137,
      "learning_rate": 6.048821366403512e-08,
      "loss": 0.8925,
      "num_input_tokens_seen": 51752816,
      "step": 4125,
      "train_runtime": 49637.3108,
      "train_tokens_per_second": 1042.619
    },
    {
      "epoch": 0.8431152393589875,
      "grad_norm": 4.533886909484863,
      "learning_rate": 5.972611249968546e-08,
      "loss": 1.0881,
      "num_input_tokens_seen": 51815936,
      "step": 4130,
      "train_runtime": 49690.8432,
      "train_tokens_per_second": 1042.766
    },
    {
      "epoch": 0.8441359599877514,
      "grad_norm": 4.34670877456665,
      "learning_rate": 5.896853767802668e-08,
      "loss": 0.8377,
      "num_input_tokens_seen": 51879264,
      "step": 4135,
      "train_runtime": 49744.516,
      "train_tokens_per_second": 1042.914
    },
    {
      "epoch": 0.8451566806165153,
      "grad_norm": 5.441451549530029,
      "learning_rate": 5.821549698749062e-08,
      "loss": 0.7857,
      "num_input_tokens_seen": 51941664,
      "step": 4140,
      "train_runtime": 49797.507,
      "train_tokens_per_second": 1043.058
    },
    {
      "epoch": 0.8461774012452792,
      "grad_norm": 5.213082313537598,
      "learning_rate": 5.746699816989537e-08,
      "loss": 1.1787,
      "num_input_tokens_seen": 52004480,
      "step": 4145,
      "train_runtime": 49850.7346,
      "train_tokens_per_second": 1043.204
    },
    {
      "epoch": 0.8471981218740431,
      "grad_norm": 5.7690558433532715,
      "learning_rate": 5.6723048920364725e-08,
      "loss": 0.8615,
      "num_input_tokens_seen": 52068144,
      "step": 4150,
      "train_runtime": 49904.5412,
      "train_tokens_per_second": 1043.355
    },
    {
      "epoch": 0.848218842502807,
      "grad_norm": 6.218332290649414,
      "learning_rate": 5.5983656887249876e-08,
      "loss": 0.939,
      "num_input_tokens_seen": 52130368,
      "step": 4155,
      "train_runtime": 49957.4611,
      "train_tokens_per_second": 1043.495
    },
    {
      "epoch": 0.8492395631315709,
      "grad_norm": 6.499001502990723,
      "learning_rate": 5.524882967205024e-08,
      "loss": 0.8891,
      "num_input_tokens_seen": 52192480,
      "step": 4160,
      "train_runtime": 50010.2024,
      "train_tokens_per_second": 1043.637
    },
    {
      "epoch": 0.8502602837603348,
      "grad_norm": 5.726968288421631,
      "learning_rate": 5.451857482933553e-08,
      "loss": 0.9841,
      "num_input_tokens_seen": 52255376,
      "step": 4165,
      "train_runtime": 50063.6548,
      "train_tokens_per_second": 1043.779
    },
    {
      "epoch": 0.8512810043890987,
      "grad_norm": 7.448807716369629,
      "learning_rate": 5.3792899866668185e-08,
      "loss": 1.0948,
      "num_input_tokens_seen": 52317664,
      "step": 4170,
      "train_runtime": 50116.5044,
      "train_tokens_per_second": 1043.921
    },
    {
      "epoch": 0.8523017250178626,
      "grad_norm": 5.609278202056885,
      "learning_rate": 5.307181224452595e-08,
      "loss": 0.9658,
      "num_input_tokens_seen": 52380992,
      "step": 4175,
      "train_runtime": 50170.237,
      "train_tokens_per_second": 1044.065
    },
    {
      "epoch": 0.8533224456466265,
      "grad_norm": 5.5560688972473145,
      "learning_rate": 5.235531937622523e-08,
      "loss": 1.2199,
      "num_input_tokens_seen": 52443344,
      "step": 4180,
      "train_runtime": 50223.1615,
      "train_tokens_per_second": 1044.206
    },
    {
      "epoch": 0.8543431662753904,
      "grad_norm": 6.523702144622803,
      "learning_rate": 5.164342862784532e-08,
      "loss": 0.8896,
      "num_input_tokens_seen": 52505680,
      "step": 4185,
      "train_runtime": 50276.2835,
      "train_tokens_per_second": 1044.343
    },
    {
      "epoch": 0.8553638869041543,
      "grad_norm": 5.274738311767578,
      "learning_rate": 5.093614731815199e-08,
      "loss": 0.9396,
      "num_input_tokens_seen": 52568272,
      "step": 4190,
      "train_runtime": 50329.4055,
      "train_tokens_per_second": 1044.484
    },
    {
      "epoch": 0.8563846075329182,
      "grad_norm": 6.905245304107666,
      "learning_rate": 5.023348271852246e-08,
      "loss": 1.076,
      "num_input_tokens_seen": 52630768,
      "step": 4195,
      "train_runtime": 50382.5082,
      "train_tokens_per_second": 1044.624
    },
    {
      "epoch": 0.8574053281616821,
      "grad_norm": 6.13261079788208,
      "learning_rate": 4.953544205287108e-08,
      "loss": 1.1639,
      "num_input_tokens_seen": 52693232,
      "step": 4200,
      "train_runtime": 50435.4283,
      "train_tokens_per_second": 1044.766
    },
    {
      "epoch": 0.8574053281616821,
      "eval_loss": 1.0151170492172241,
      "eval_runtime": 139.1656,
      "eval_samples_per_second": 5.691,
      "eval_steps_per_second": 2.846,
      "num_input_tokens_seen": 52693232,
      "step": 4200
    },
    {
      "epoch": 0.858426048790446,
      "grad_norm": 6.215978145599365,
      "learning_rate": 4.884203249757446e-08,
      "loss": 0.952,
      "num_input_tokens_seen": 52755312,
      "step": 4205,
      "train_runtime": 50627.6886,
      "train_tokens_per_second": 1042.025
    },
    {
      "epoch": 0.8594467694192099,
      "grad_norm": 6.047678470611572,
      "learning_rate": 4.815326118139812e-08,
      "loss": 0.9742,
      "num_input_tokens_seen": 52817456,
      "step": 4210,
      "train_runtime": 50680.4582,
      "train_tokens_per_second": 1042.166
    },
    {
      "epoch": 0.8604674900479738,
      "grad_norm": 12.024614334106445,
      "learning_rate": 4.746913518542284e-08,
      "loss": 1.3084,
      "num_input_tokens_seen": 52880304,
      "step": 4215,
      "train_runtime": 50733.7441,
      "train_tokens_per_second": 1042.31
    },
    {
      "epoch": 0.8614882106767378,
      "grad_norm": 4.4237446784973145,
      "learning_rate": 4.6789661542972146e-08,
      "loss": 1.0584,
      "num_input_tokens_seen": 52943056,
      "step": 4220,
      "train_runtime": 50786.9957,
      "train_tokens_per_second": 1042.453
    },
    {
      "epoch": 0.8625089313055017,
      "grad_norm": 4.652379512786865,
      "learning_rate": 4.6114847239540064e-08,
      "loss": 1.0153,
      "num_input_tokens_seen": 53005456,
      "step": 4225,
      "train_runtime": 50840.0418,
      "train_tokens_per_second": 1042.593
    },
    {
      "epoch": 0.8635296519342656,
      "grad_norm": 5.2552361488342285,
      "learning_rate": 4.5444699212718985e-08,
      "loss": 0.9515,
      "num_input_tokens_seen": 53067360,
      "step": 4230,
      "train_runtime": 50892.7029,
      "train_tokens_per_second": 1042.73
    },
    {
      "epoch": 0.8645503725630295,
      "grad_norm": 6.152981758117676,
      "learning_rate": 4.4779224352128555e-08,
      "loss": 0.9083,
      "num_input_tokens_seen": 53130192,
      "step": 4235,
      "train_runtime": 50945.9174,
      "train_tokens_per_second": 1042.874
    },
    {
      "epoch": 0.8655710931917934,
      "grad_norm": 5.939335346221924,
      "learning_rate": 4.411842949934469e-08,
      "loss": 1.0012,
      "num_input_tokens_seen": 53192384,
      "step": 4240,
      "train_runtime": 50998.6757,
      "train_tokens_per_second": 1043.015
    },
    {
      "epoch": 0.8665918138205573,
      "grad_norm": 4.398706436157227,
      "learning_rate": 4.346232144782963e-08,
      "loss": 1.0037,
      "num_input_tokens_seen": 53255936,
      "step": 4245,
      "train_runtime": 51052.5342,
      "train_tokens_per_second": 1043.159
    },
    {
      "epoch": 0.8676125344493212,
      "grad_norm": 5.22591495513916,
      "learning_rate": 4.2810906942861545e-08,
      "loss": 1.0205,
      "num_input_tokens_seen": 53317840,
      "step": 4250,
      "train_runtime": 51105.2313,
      "train_tokens_per_second": 1043.295
    },
    {
      "epoch": 0.8686332550780851,
      "grad_norm": 5.459828853607178,
      "learning_rate": 4.2164192681465536e-08,
      "loss": 0.9827,
      "num_input_tokens_seen": 53380496,
      "step": 4255,
      "train_runtime": 51158.3894,
      "train_tokens_per_second": 1043.436
    },
    {
      "epoch": 0.869653975706849,
      "grad_norm": 7.88948917388916,
      "learning_rate": 4.152218531234464e-08,
      "loss": 0.9798,
      "num_input_tokens_seen": 53442960,
      "step": 4260,
      "train_runtime": 51211.4149,
      "train_tokens_per_second": 1043.575
    },
    {
      "epoch": 0.8706746963356129,
      "grad_norm": 6.53001594543457,
      "learning_rate": 4.0884891435811865e-08,
      "loss": 0.9674,
      "num_input_tokens_seen": 53506736,
      "step": 4265,
      "train_runtime": 51265.3142,
      "train_tokens_per_second": 1043.722
    },
    {
      "epoch": 0.8716954169643768,
      "grad_norm": 6.560766220092773,
      "learning_rate": 4.025231760372161e-08,
      "loss": 0.8957,
      "num_input_tokens_seen": 53569280,
      "step": 4270,
      "train_runtime": 51318.4457,
      "train_tokens_per_second": 1043.86
    },
    {
      "epoch": 0.8727161375931407,
      "grad_norm": 4.351288318634033,
      "learning_rate": 3.962447031940291e-08,
      "loss": 0.9031,
      "num_input_tokens_seen": 53632992,
      "step": 4275,
      "train_runtime": 51372.325,
      "train_tokens_per_second": 1044.006
    },
    {
      "epoch": 0.8737368582219046,
      "grad_norm": 5.739984035491943,
      "learning_rate": 3.900135603759236e-08,
      "loss": 1.0621,
      "num_input_tokens_seen": 53695472,
      "step": 4280,
      "train_runtime": 51425.3399,
      "train_tokens_per_second": 1044.144
    },
    {
      "epoch": 0.8747575788506686,
      "grad_norm": 5.436971187591553,
      "learning_rate": 3.838298116436767e-08,
      "loss": 0.9313,
      "num_input_tokens_seen": 53758176,
      "step": 4285,
      "train_runtime": 51478.5426,
      "train_tokens_per_second": 1044.283
    },
    {
      "epoch": 0.8757782994794325,
      "grad_norm": 6.393293380737305,
      "learning_rate": 3.776935205708209e-08,
      "loss": 0.774,
      "num_input_tokens_seen": 53819760,
      "step": 4290,
      "train_runtime": 51530.884,
      "train_tokens_per_second": 1044.418
    },
    {
      "epoch": 0.8767990201081964,
      "grad_norm": 5.839706897735596,
      "learning_rate": 3.716047502429881e-08,
      "loss": 0.8576,
      "num_input_tokens_seen": 53883184,
      "step": 4295,
      "train_runtime": 51584.4604,
      "train_tokens_per_second": 1044.562
    },
    {
      "epoch": 0.8778197407369603,
      "grad_norm": 6.032539367675781,
      "learning_rate": 3.655635632572596e-08,
      "loss": 0.9071,
      "num_input_tokens_seen": 53946352,
      "step": 4300,
      "train_runtime": 51638.1093,
      "train_tokens_per_second": 1044.7
    },
    {
      "epoch": 0.8778197407369603,
      "eval_loss": 1.015584111213684,
      "eval_runtime": 139.0279,
      "eval_samples_per_second": 5.697,
      "eval_steps_per_second": 2.848,
      "num_input_tokens_seen": 53946352,
      "step": 4300
    },
    {
      "epoch": 0.8788404613657242,
      "grad_norm": 11.1469144821167,
      "learning_rate": 3.5957002172152806e-08,
      "loss": 1.1058,
      "num_input_tokens_seen": 54008848,
      "step": 4305,
      "train_runtime": 51830.5821,
      "train_tokens_per_second": 1042.027
    },
    {
      "epoch": 0.8798611819944882,
      "grad_norm": 6.346685886383057,
      "learning_rate": 3.5362418725385315e-08,
      "loss": 1.1434,
      "num_input_tokens_seen": 54071568,
      "step": 4310,
      "train_runtime": 51883.5662,
      "train_tokens_per_second": 1042.171
    },
    {
      "epoch": 0.880881902623252,
      "grad_norm": 6.294712066650391,
      "learning_rate": 3.477261209818311e-08,
      "loss": 1.1847,
      "num_input_tokens_seen": 54135312,
      "step": 4315,
      "train_runtime": 51937.5921,
      "train_tokens_per_second": 1042.315
    },
    {
      "epoch": 0.881902623252016,
      "grad_norm": 7.292888641357422,
      "learning_rate": 3.41875883541965e-08,
      "loss": 0.9636,
      "num_input_tokens_seen": 54198768,
      "step": 4320,
      "train_runtime": 51991.4093,
      "train_tokens_per_second": 1042.456
    },
    {
      "epoch": 0.8829233438807799,
      "grad_norm": 4.254830360412598,
      "learning_rate": 3.3607353507904276e-08,
      "loss": 1.108,
      "num_input_tokens_seen": 54263056,
      "step": 4325,
      "train_runtime": 52045.8657,
      "train_tokens_per_second": 1042.601
    },
    {
      "epoch": 0.8839440645095438,
      "grad_norm": 6.506487846374512,
      "learning_rate": 3.3031913524551705e-08,
      "loss": 1.2645,
      "num_input_tokens_seen": 54325472,
      "step": 4330,
      "train_runtime": 52098.8999,
      "train_tokens_per_second": 1042.737
    },
    {
      "epoch": 0.8849647851383077,
      "grad_norm": 4.774284839630127,
      "learning_rate": 3.246127432008944e-08,
      "loss": 1.0714,
      "num_input_tokens_seen": 54388336,
      "step": 4335,
      "train_runtime": 52152.3353,
      "train_tokens_per_second": 1042.874
    },
    {
      "epoch": 0.8859855057670716,
      "grad_norm": 5.73151159286499,
      "learning_rate": 3.1895441761112586e-08,
      "loss": 1.0703,
      "num_input_tokens_seen": 54450848,
      "step": 4340,
      "train_runtime": 52205.3863,
      "train_tokens_per_second": 1043.012
    },
    {
      "epoch": 0.8870062263958355,
      "grad_norm": 6.699420928955078,
      "learning_rate": 3.133442166480016e-08,
      "loss": 0.9577,
      "num_input_tokens_seen": 54513664,
      "step": 4345,
      "train_runtime": 52258.5473,
      "train_tokens_per_second": 1043.153
    },
    {
      "epoch": 0.8880269470245994,
      "grad_norm": 6.126852512359619,
      "learning_rate": 3.0778219798855645e-08,
      "loss": 0.9116,
      "num_input_tokens_seen": 54576416,
      "step": 4350,
      "train_runtime": 52311.8142,
      "train_tokens_per_second": 1043.29
    },
    {
      "epoch": 0.8890476676533633,
      "grad_norm": 6.375929355621338,
      "learning_rate": 3.0226841881447475e-08,
      "loss": 1.0322,
      "num_input_tokens_seen": 54638720,
      "step": 4355,
      "train_runtime": 52364.6907,
      "train_tokens_per_second": 1043.427
    },
    {
      "epoch": 0.8900683882821272,
      "grad_norm": 5.598355293273926,
      "learning_rate": 2.968029358115026e-08,
      "loss": 0.8579,
      "num_input_tokens_seen": 54701968,
      "step": 4360,
      "train_runtime": 52418.2759,
      "train_tokens_per_second": 1043.567
    },
    {
      "epoch": 0.8910891089108911,
      "grad_norm": 4.340453147888184,
      "learning_rate": 2.9138580516886668e-08,
      "loss": 1.046,
      "num_input_tokens_seen": 54764256,
      "step": 4365,
      "train_runtime": 52471.0842,
      "train_tokens_per_second": 1043.704
    },
    {
      "epoch": 0.892109829539655,
      "grad_norm": 4.994600772857666,
      "learning_rate": 2.860170825786945e-08,
      "loss": 0.8418,
      "num_input_tokens_seen": 54826992,
      "step": 4370,
      "train_runtime": 52524.33,
      "train_tokens_per_second": 1043.84
    },
    {
      "epoch": 0.8931305501684189,
      "grad_norm": 6.083825588226318,
      "learning_rate": 2.8069682323544342e-08,
      "loss": 0.9611,
      "num_input_tokens_seen": 54890208,
      "step": 4375,
      "train_runtime": 52578.0069,
      "train_tokens_per_second": 1043.977
    },
    {
      "epoch": 0.8941512707971828,
      "grad_norm": 7.798730850219727,
      "learning_rate": 2.7542508183533265e-08,
      "loss": 1.0987,
      "num_input_tokens_seen": 54952384,
      "step": 4380,
      "train_runtime": 52630.7845,
      "train_tokens_per_second": 1044.111
    },
    {
      "epoch": 0.8951719914259467,
      "grad_norm": 10.190933227539062,
      "learning_rate": 2.702019125757815e-08,
      "loss": 1.0925,
      "num_input_tokens_seen": 55015152,
      "step": 4385,
      "train_runtime": 52684.0418,
      "train_tokens_per_second": 1044.247
    },
    {
      "epoch": 0.8961927120547106,
      "grad_norm": 8.243084907531738,
      "learning_rate": 2.6502736915484993e-08,
      "loss": 1.0976,
      "num_input_tokens_seen": 55077920,
      "step": 4390,
      "train_runtime": 52737.1817,
      "train_tokens_per_second": 1044.385
    },
    {
      "epoch": 0.8972134326834745,
      "grad_norm": 9.158975601196289,
      "learning_rate": 2.5990150477068985e-08,
      "loss": 1.0794,
      "num_input_tokens_seen": 55141088,
      "step": 4395,
      "train_runtime": 52790.6013,
      "train_tokens_per_second": 1044.525
    },
    {
      "epoch": 0.8982341533122384,
      "grad_norm": 7.247620582580566,
      "learning_rate": 2.5482437212099472e-08,
      "loss": 1.0794,
      "num_input_tokens_seen": 55203296,
      "step": 4400,
      "train_runtime": 52843.2792,
      "train_tokens_per_second": 1044.661
    },
    {
      "epoch": 0.8982341533122384,
      "eval_loss": 1.0153083801269531,
      "eval_runtime": 139.035,
      "eval_samples_per_second": 5.696,
      "eval_steps_per_second": 2.848,
      "num_input_tokens_seen": 55203296,
      "step": 4400
    },
    {
      "epoch": 0.8992548739410023,
      "grad_norm": 6.016491413116455,
      "learning_rate": 2.4979602340246043e-08,
      "loss": 0.9571,
      "num_input_tokens_seen": 55265248,
      "step": 4405,
      "train_runtime": 53035.4209,
      "train_tokens_per_second": 1042.044
    },
    {
      "epoch": 0.9002755945697662,
      "grad_norm": 4.059850215911865,
      "learning_rate": 2.4481651031024896e-08,
      "loss": 1.122,
      "num_input_tokens_seen": 55326880,
      "step": 4410,
      "train_runtime": 53087.9546,
      "train_tokens_per_second": 1042.174
    },
    {
      "epoch": 0.9012963151985302,
      "grad_norm": 5.175541877746582,
      "learning_rate": 2.3988588403745347e-08,
      "loss": 0.9177,
      "num_input_tokens_seen": 55390720,
      "step": 4415,
      "train_runtime": 53142.0369,
      "train_tokens_per_second": 1042.315
    },
    {
      "epoch": 0.9023170358272941,
      "grad_norm": 6.392214298248291,
      "learning_rate": 2.3500419527457683e-08,
      "loss": 0.9969,
      "num_input_tokens_seen": 55453872,
      "step": 4420,
      "train_runtime": 53195.5969,
      "train_tokens_per_second": 1042.452
    },
    {
      "epoch": 0.903337756456058,
      "grad_norm": 5.665618896484375,
      "learning_rate": 2.3017149420900493e-08,
      "loss": 0.8875,
      "num_input_tokens_seen": 55516432,
      "step": 4425,
      "train_runtime": 53248.6482,
      "train_tokens_per_second": 1042.589
    },
    {
      "epoch": 0.9043584770848219,
      "grad_norm": 7.286947727203369,
      "learning_rate": 2.2538783052449773e-08,
      "loss": 0.9381,
      "num_input_tokens_seen": 55578496,
      "step": 4430,
      "train_runtime": 53301.2732,
      "train_tokens_per_second": 1042.724
    },
    {
      "epoch": 0.9053791977135858,
      "grad_norm": 7.877214431762695,
      "learning_rate": 2.206532534006711e-08,
      "loss": 1.1037,
      "num_input_tokens_seen": 55640960,
      "step": 4435,
      "train_runtime": 53354.2373,
      "train_tokens_per_second": 1042.859
    },
    {
      "epoch": 0.9063999183423497,
      "grad_norm": 6.66598653793335,
      "learning_rate": 2.1596781151249523e-08,
      "loss": 1.0689,
      "num_input_tokens_seen": 55703056,
      "step": 4440,
      "train_runtime": 53407.0064,
      "train_tokens_per_second": 1042.992
    },
    {
      "epoch": 0.9074206389711136,
      "grad_norm": 6.25523042678833,
      "learning_rate": 2.1133155302979488e-08,
      "loss": 0.9558,
      "num_input_tokens_seen": 55766928,
      "step": 4445,
      "train_runtime": 53460.9825,
      "train_tokens_per_second": 1043.133
    },
    {
      "epoch": 0.9084413595998775,
      "grad_norm": 5.195558071136475,
      "learning_rate": 2.0674452561675103e-08,
      "loss": 0.9375,
      "num_input_tokens_seen": 55829840,
      "step": 4450,
      "train_runtime": 53514.4548,
      "train_tokens_per_second": 1043.267
    },
    {
      "epoch": 0.9094620802286414,
      "grad_norm": 5.233370780944824,
      "learning_rate": 2.022067764314145e-08,
      "loss": 1.0008,
      "num_input_tokens_seen": 55892432,
      "step": 4455,
      "train_runtime": 53567.6661,
      "train_tokens_per_second": 1043.399
    },
    {
      "epoch": 0.9104828008574053,
      "grad_norm": 5.070940971374512,
      "learning_rate": 1.9771835212521847e-08,
      "loss": 1.1662,
      "num_input_tokens_seen": 55955536,
      "step": 4460,
      "train_runtime": 53621.1442,
      "train_tokens_per_second": 1043.535
    },
    {
      "epoch": 0.9115035214861692,
      "grad_norm": 6.36668586730957,
      "learning_rate": 1.932792988424997e-08,
      "loss": 0.887,
      "num_input_tokens_seen": 56017888,
      "step": 4465,
      "train_runtime": 53674.2458,
      "train_tokens_per_second": 1043.664
    },
    {
      "epoch": 0.9125242421149331,
      "grad_norm": 5.418996810913086,
      "learning_rate": 1.8888966222002313e-08,
      "loss": 1.0881,
      "num_input_tokens_seen": 56079952,
      "step": 4470,
      "train_runtime": 53727.0222,
      "train_tokens_per_second": 1043.794
    },
    {
      "epoch": 0.913544962743697,
      "grad_norm": 8.562504768371582,
      "learning_rate": 1.8454948738651666e-08,
      "loss": 1.0845,
      "num_input_tokens_seen": 56142704,
      "step": 4475,
      "train_runtime": 53780.2102,
      "train_tokens_per_second": 1043.929
    },
    {
      "epoch": 0.9145656833724609,
      "grad_norm": 6.513468265533447,
      "learning_rate": 1.8025881896220118e-08,
      "loss": 1.1006,
      "num_input_tokens_seen": 56204432,
      "step": 4480,
      "train_runtime": 53832.4976,
      "train_tokens_per_second": 1044.061
    },
    {
      "epoch": 0.9155864040012248,
      "grad_norm": 8.888486862182617,
      "learning_rate": 1.7601770105833634e-08,
      "loss": 0.9715,
      "num_input_tokens_seen": 56267488,
      "step": 4485,
      "train_runtime": 53886.033,
      "train_tokens_per_second": 1044.194
    },
    {
      "epoch": 0.9166071246299887,
      "grad_norm": 5.733795642852783,
      "learning_rate": 1.718261772767654e-08,
      "loss": 0.8519,
      "num_input_tokens_seen": 56329776,
      "step": 4490,
      "train_runtime": 53938.8696,
      "train_tokens_per_second": 1044.326
    },
    {
      "epoch": 0.9176278452587526,
      "grad_norm": 4.907618999481201,
      "learning_rate": 1.6768429070946665e-08,
      "loss": 0.9293,
      "num_input_tokens_seen": 56392688,
      "step": 4495,
      "train_runtime": 53992.1936,
      "train_tokens_per_second": 1044.46
    },
    {
      "epoch": 0.9186485658875165,
      "grad_norm": 7.04560661315918,
      "learning_rate": 1.6359208393811274e-08,
      "loss": 0.9604,
      "num_input_tokens_seen": 56455264,
      "step": 4500,
      "train_runtime": 54045.5747,
      "train_tokens_per_second": 1044.586
    },
    {
      "epoch": 0.9186485658875165,
      "eval_loss": 1.015303373336792,
      "eval_runtime": 139.1233,
      "eval_samples_per_second": 5.693,
      "eval_steps_per_second": 2.846,
      "num_input_tokens_seen": 56455264,
      "step": 4500
    },
    {
      "epoch": 0.9196692865162805,
      "grad_norm": 4.728431701660156,
      "learning_rate": 1.5954959903362875e-08,
      "loss": 1.2604,
      "num_input_tokens_seen": 56517328,
      "step": 4505,
      "train_runtime": 54237.8942,
      "train_tokens_per_second": 1042.027
    },
    {
      "epoch": 0.9206900071450445,
      "grad_norm": 4.510565280914307,
      "learning_rate": 1.5555687755576365e-08,
      "loss": 0.9162,
      "num_input_tokens_seen": 56579440,
      "step": 4510,
      "train_runtime": 54290.7735,
      "train_tokens_per_second": 1042.156
    },
    {
      "epoch": 0.9217107277738084,
      "grad_norm": 6.0603861808776855,
      "learning_rate": 1.5161396055266008e-08,
      "loss": 0.8271,
      "num_input_tokens_seen": 56641568,
      "step": 4515,
      "train_runtime": 54343.5703,
      "train_tokens_per_second": 1042.286
    },
    {
      "epoch": 0.9227314484025723,
      "grad_norm": 6.252667427062988,
      "learning_rate": 1.477208885604353e-08,
      "loss": 0.7901,
      "num_input_tokens_seen": 56704432,
      "step": 4520,
      "train_runtime": 54396.8923,
      "train_tokens_per_second": 1042.42
    },
    {
      "epoch": 0.9237521690313362,
      "grad_norm": 6.464204788208008,
      "learning_rate": 1.4387770160276147e-08,
      "loss": 1.1223,
      "num_input_tokens_seen": 56767280,
      "step": 4525,
      "train_runtime": 54450.3535,
      "train_tokens_per_second": 1042.551
    },
    {
      "epoch": 0.9247728896601001,
      "grad_norm": 11.2150297164917,
      "learning_rate": 1.4008443919045598e-08,
      "loss": 0.9948,
      "num_input_tokens_seen": 56829760,
      "step": 4530,
      "train_runtime": 54503.3463,
      "train_tokens_per_second": 1042.684
    },
    {
      "epoch": 0.925793610288864,
      "grad_norm": 5.7429327964782715,
      "learning_rate": 1.3634114032107513e-08,
      "loss": 0.8932,
      "num_input_tokens_seen": 56892304,
      "step": 4535,
      "train_runtime": 54556.4322,
      "train_tokens_per_second": 1042.816
    },
    {
      "epoch": 0.9268143309176279,
      "grad_norm": 13.736210823059082,
      "learning_rate": 1.3264784347851332e-08,
      "loss": 1.1287,
      "num_input_tokens_seen": 56954368,
      "step": 4540,
      "train_runtime": 54609.2335,
      "train_tokens_per_second": 1042.944
    },
    {
      "epoch": 0.9278350515463918,
      "grad_norm": 5.348590850830078,
      "learning_rate": 1.2900458663260505e-08,
      "loss": 0.976,
      "num_input_tokens_seen": 57017968,
      "step": 4545,
      "train_runtime": 54663.1423,
      "train_tokens_per_second": 1043.079
    },
    {
      "epoch": 0.9288557721751557,
      "grad_norm": 24.47606658935547,
      "learning_rate": 1.25411407238738e-08,
      "loss": 1.3845,
      "num_input_tokens_seen": 57080528,
      "step": 4550,
      "train_runtime": 54716.2736,
      "train_tokens_per_second": 1043.209
    },
    {
      "epoch": 0.9298764928039196,
      "grad_norm": 5.384186744689941,
      "learning_rate": 1.2186834223746612e-08,
      "loss": 1.336,
      "num_input_tokens_seen": 57143616,
      "step": 4555,
      "train_runtime": 54769.7743,
      "train_tokens_per_second": 1043.342
    },
    {
      "epoch": 0.9308972134326835,
      "grad_norm": 4.572782039642334,
      "learning_rate": 1.1837542805413048e-08,
      "loss": 1.1166,
      "num_input_tokens_seen": 57206112,
      "step": 4560,
      "train_runtime": 54822.8797,
      "train_tokens_per_second": 1043.471
    },
    {
      "epoch": 0.9319179340614474,
      "grad_norm": 6.644834518432617,
      "learning_rate": 1.1493270059848348e-08,
      "loss": 0.8422,
      "num_input_tokens_seen": 57269536,
      "step": 4565,
      "train_runtime": 54876.5717,
      "train_tokens_per_second": 1043.606
    },
    {
      "epoch": 0.9329386546902113,
      "grad_norm": 6.078365802764893,
      "learning_rate": 1.115401952643219e-08,
      "loss": 1.1253,
      "num_input_tokens_seen": 57331440,
      "step": 4570,
      "train_runtime": 54929.0325,
      "train_tokens_per_second": 1043.737
    },
    {
      "epoch": 0.9339593753189752,
      "grad_norm": 6.128454208374023,
      "learning_rate": 1.081979469291211e-08,
      "loss": 0.9982,
      "num_input_tokens_seen": 57395392,
      "step": 4575,
      "train_runtime": 54983.0807,
      "train_tokens_per_second": 1043.874
    },
    {
      "epoch": 0.9349800959477391,
      "grad_norm": 4.160944938659668,
      "learning_rate": 1.0490598995367806e-08,
      "loss": 1.0309,
      "num_input_tokens_seen": 57457200,
      "step": 4580,
      "train_runtime": 55035.5549,
      "train_tokens_per_second": 1044.001
    },
    {
      "epoch": 0.936000816576503,
      "grad_norm": 5.4791789054870605,
      "learning_rate": 1.01664358181755e-08,
      "loss": 1.0694,
      "num_input_tokens_seen": 57519600,
      "step": 4585,
      "train_runtime": 55088.5406,
      "train_tokens_per_second": 1044.13
    },
    {
      "epoch": 0.9370215372052669,
      "grad_norm": 8.675246238708496,
      "learning_rate": 9.847308493973583e-09,
      "loss": 1.0605,
      "num_input_tokens_seen": 57581952,
      "step": 4590,
      "train_runtime": 55141.4459,
      "train_tokens_per_second": 1044.259
    },
    {
      "epoch": 0.9380422578340308,
      "grad_norm": 5.295896053314209,
      "learning_rate": 9.533220303628076e-09,
      "loss": 1.0953,
      "num_input_tokens_seen": 57644880,
      "step": 4595,
      "train_runtime": 55194.8843,
      "train_tokens_per_second": 1044.388
    },
    {
      "epoch": 0.9390629784627947,
      "grad_norm": 13.305904388427734,
      "learning_rate": 9.224174476198887e-09,
      "loss": 1.0676,
      "num_input_tokens_seen": 57708192,
      "step": 4600,
      "train_runtime": 55248.4877,
      "train_tokens_per_second": 1044.521
    },
    {
      "epoch": 0.9390629784627947,
      "eval_loss": 1.015921950340271,
      "eval_runtime": 139.4815,
      "eval_samples_per_second": 5.678,
      "eval_steps_per_second": 2.839,
      "num_input_tokens_seen": 57708192,
      "step": 4600
    },
    {
      "epoch": 0.9400836990915586,
      "grad_norm": 5.53229284286499,
      "learning_rate": 8.920174188906782e-09,
      "loss": 1.0684,
      "num_input_tokens_seen": 57770144,
      "step": 4605,
      "train_runtime": 55441.0696,
      "train_tokens_per_second": 1042.01
    },
    {
      "epoch": 0.9411044197203225,
      "grad_norm": 5.753739356994629,
      "learning_rate": 8.621222567100572e-09,
      "loss": 1.0972,
      "num_input_tokens_seen": 57831888,
      "step": 4610,
      "train_runtime": 55493.5739,
      "train_tokens_per_second": 1042.137
    },
    {
      "epoch": 0.9421251403490865,
      "grad_norm": 5.54595422744751,
      "learning_rate": 8.327322684225147e-09,
      "loss": 1.0785,
      "num_input_tokens_seen": 57894272,
      "step": 4615,
      "train_runtime": 55546.5423,
      "train_tokens_per_second": 1042.266
    },
    {
      "epoch": 0.9431458609778504,
      "grad_norm": 13.622679710388184,
      "learning_rate": 8.038477561789547e-09,
      "loss": 1.0726,
      "num_input_tokens_seen": 57957616,
      "step": 4620,
      "train_runtime": 55600.2086,
      "train_tokens_per_second": 1042.399
    },
    {
      "epoch": 0.9441665816066143,
      "grad_norm": 6.591879367828369,
      "learning_rate": 7.754690169336331e-09,
      "loss": 1.1243,
      "num_input_tokens_seen": 58020192,
      "step": 4625,
      "train_runtime": 55653.4142,
      "train_tokens_per_second": 1042.527
    },
    {
      "epoch": 0.9451873022353782,
      "grad_norm": 13.896249771118164,
      "learning_rate": 7.475963424410702e-09,
      "loss": 1.0202,
      "num_input_tokens_seen": 58083568,
      "step": 4630,
      "train_runtime": 55707.1971,
      "train_tokens_per_second": 1042.658
    },
    {
      "epoch": 0.9462080228641421,
      "grad_norm": 6.650022983551025,
      "learning_rate": 7.202300192530708e-09,
      "loss": 1.0411,
      "num_input_tokens_seen": 58147472,
      "step": 4635,
      "train_runtime": 55761.1595,
      "train_tokens_per_second": 1042.795
    },
    {
      "epoch": 0.947228743492906,
      "grad_norm": 6.117101669311523,
      "learning_rate": 6.933703287157699e-09,
      "loss": 1.128,
      "num_input_tokens_seen": 58210928,
      "step": 4640,
      "train_runtime": 55815.3392,
      "train_tokens_per_second": 1042.92
    },
    {
      "epoch": 0.9482494641216699,
      "grad_norm": 6.723245143890381,
      "learning_rate": 6.670175469667416e-09,
      "loss": 0.7653,
      "num_input_tokens_seen": 58273472,
      "step": 4645,
      "train_runtime": 55868.3441,
      "train_tokens_per_second": 1043.05
    },
    {
      "epoch": 0.9492701847504338,
      "grad_norm": 5.707523822784424,
      "learning_rate": 6.411719449321673e-09,
      "loss": 0.9772,
      "num_input_tokens_seen": 58335152,
      "step": 4650,
      "train_runtime": 55920.9052,
      "train_tokens_per_second": 1043.173
    },
    {
      "epoch": 0.9502909053791977,
      "grad_norm": 8.197057723999023,
      "learning_rate": 6.158337883240328e-09,
      "loss": 0.9308,
      "num_input_tokens_seen": 58398128,
      "step": 4655,
      "train_runtime": 55974.1405,
      "train_tokens_per_second": 1043.305
    },
    {
      "epoch": 0.9513116260079616,
      "grad_norm": 5.707666397094727,
      "learning_rate": 5.910033376374246e-09,
      "loss": 1.1051,
      "num_input_tokens_seen": 58460112,
      "step": 4660,
      "train_runtime": 56026.7743,
      "train_tokens_per_second": 1043.432
    },
    {
      "epoch": 0.9523323466367255,
      "grad_norm": 6.0706658363342285,
      "learning_rate": 5.666808481478213e-09,
      "loss": 0.9507,
      "num_input_tokens_seen": 58523072,
      "step": 4665,
      "train_runtime": 56080.193,
      "train_tokens_per_second": 1043.56
    },
    {
      "epoch": 0.9533530672654894,
      "grad_norm": 6.436392307281494,
      "learning_rate": 5.428665699084789e-09,
      "loss": 1.0559,
      "num_input_tokens_seen": 58585008,
      "step": 4670,
      "train_runtime": 56132.8903,
      "train_tokens_per_second": 1043.684
    },
    {
      "epoch": 0.9543737878942533,
      "grad_norm": 6.596588134765625,
      "learning_rate": 5.1956074774788246e-09,
      "loss": 1.261,
      "num_input_tokens_seen": 58647424,
      "step": 4675,
      "train_runtime": 56185.7868,
      "train_tokens_per_second": 1043.812
    },
    {
      "epoch": 0.9553945085230172,
      "grad_norm": 7.195201396942139,
      "learning_rate": 4.967636212671933e-09,
      "loss": 1.0625,
      "num_input_tokens_seen": 58710048,
      "step": 4680,
      "train_runtime": 56238.8879,
      "train_tokens_per_second": 1043.94
    },
    {
      "epoch": 0.9564152291517811,
      "grad_norm": 9.628085136413574,
      "learning_rate": 4.744754248378113e-09,
      "loss": 1.0291,
      "num_input_tokens_seen": 58772848,
      "step": 4685,
      "train_runtime": 56292.0871,
      "train_tokens_per_second": 1044.069
    },
    {
      "epoch": 0.957435949780545,
      "grad_norm": 5.049630165100098,
      "learning_rate": 4.526963875989553e-09,
      "loss": 1.0323,
      "num_input_tokens_seen": 58835680,
      "step": 4690,
      "train_runtime": 56345.5364,
      "train_tokens_per_second": 1044.194
    },
    {
      "epoch": 0.9584566704093089,
      "grad_norm": 5.315362453460693,
      "learning_rate": 4.3142673345531455e-09,
      "loss": 1.1179,
      "num_input_tokens_seen": 58899184,
      "step": 4695,
      "train_runtime": 56399.4343,
      "train_tokens_per_second": 1044.322
    },
    {
      "epoch": 0.9594773910380728,
      "grad_norm": 4.9882330894470215,
      "learning_rate": 4.1066668107472835e-09,
      "loss": 0.9799,
      "num_input_tokens_seen": 58961456,
      "step": 4700,
      "train_runtime": 56452.2226,
      "train_tokens_per_second": 1044.449
    },
    {
      "epoch": 0.9594773910380728,
      "eval_loss": 1.0148866176605225,
      "eval_runtime": 139.0888,
      "eval_samples_per_second": 5.694,
      "eval_steps_per_second": 2.847,
      "num_input_tokens_seen": 58961456,
      "step": 4700
    },
    {
      "epoch": 0.9604981116668367,
      "grad_norm": 7.160247325897217,
      "learning_rate": 3.9041644388597136e-09,
      "loss": 1.0536,
      "num_input_tokens_seen": 59024832,
      "step": 4705,
      "train_runtime": 56645.5831,
      "train_tokens_per_second": 1042.002
    },
    {
      "epoch": 0.9615188322956006,
      "grad_norm": 4.658154487609863,
      "learning_rate": 3.706762300765276e-09,
      "loss": 0.7904,
      "num_input_tokens_seen": 59087856,
      "step": 4710,
      "train_runtime": 56699.0216,
      "train_tokens_per_second": 1042.132
    },
    {
      "epoch": 0.9625395529243645,
      "grad_norm": 4.177311897277832,
      "learning_rate": 3.5144624259045297e-09,
      "loss": 0.9551,
      "num_input_tokens_seen": 59150880,
      "step": 4715,
      "train_runtime": 56752.4749,
      "train_tokens_per_second": 1042.261
    },
    {
      "epoch": 0.9635602735531285,
      "grad_norm": 13.053224563598633,
      "learning_rate": 3.327266791263106e-09,
      "loss": 0.8132,
      "num_input_tokens_seen": 59213168,
      "step": 4720,
      "train_runtime": 56805.401,
      "train_tokens_per_second": 1042.386
    },
    {
      "epoch": 0.9645809941818925,
      "grad_norm": 4.993270397186279,
      "learning_rate": 3.1451773213511666e-09,
      "loss": 1.1277,
      "num_input_tokens_seen": 59275136,
      "step": 4725,
      "train_runtime": 56857.9685,
      "train_tokens_per_second": 1042.512
    },
    {
      "epoch": 0.9656017148106564,
      "grad_norm": 5.061180591583252,
      "learning_rate": 2.9681958881838665e-09,
      "loss": 0.9304,
      "num_input_tokens_seen": 59337760,
      "step": 4730,
      "train_runtime": 56911.0881,
      "train_tokens_per_second": 1042.64
    },
    {
      "epoch": 0.9666224354394203,
      "grad_norm": 6.284245014190674,
      "learning_rate": 2.7963243112618108e-09,
      "loss": 1.1203,
      "num_input_tokens_seen": 59400032,
      "step": 4735,
      "train_runtime": 56963.9804,
      "train_tokens_per_second": 1042.765
    },
    {
      "epoch": 0.9676431560681842,
      "grad_norm": 4.974850654602051,
      "learning_rate": 2.629564357552461e-09,
      "loss": 1.0736,
      "num_input_tokens_seen": 59463152,
      "step": 4740,
      "train_runtime": 57017.5952,
      "train_tokens_per_second": 1042.891
    },
    {
      "epoch": 0.9686638766969481,
      "grad_norm": 5.618214130401611,
      "learning_rate": 2.4679177414720363e-09,
      "loss": 0.843,
      "num_input_tokens_seen": 59526528,
      "step": 4745,
      "train_runtime": 57071.2592,
      "train_tokens_per_second": 1043.021
    },
    {
      "epoch": 0.969684597325712,
      "grad_norm": 4.603795051574707,
      "learning_rate": 2.3113861248679756e-09,
      "loss": 1.1394,
      "num_input_tokens_seen": 59590224,
      "step": 4750,
      "train_runtime": 57125.068,
      "train_tokens_per_second": 1043.154
    },
    {
      "epoch": 0.9707053179544759,
      "grad_norm": 6.713715553283691,
      "learning_rate": 2.1599711170014467e-09,
      "loss": 1.2121,
      "num_input_tokens_seen": 59652144,
      "step": 4755,
      "train_runtime": 57177.7211,
      "train_tokens_per_second": 1043.276
    },
    {
      "epoch": 0.9717260385832398,
      "grad_norm": 6.140477180480957,
      "learning_rate": 2.0136742745313073e-09,
      "loss": 1.1096,
      "num_input_tokens_seen": 59715392,
      "step": 4760,
      "train_runtime": 57231.4216,
      "train_tokens_per_second": 1043.402
    },
    {
      "epoch": 0.9727467592120037,
      "grad_norm": 4.505092144012451,
      "learning_rate": 1.872497101497783e-09,
      "loss": 0.9238,
      "num_input_tokens_seen": 59777952,
      "step": 4765,
      "train_runtime": 57284.5083,
      "train_tokens_per_second": 1043.527
    },
    {
      "epoch": 0.9737674798407676,
      "grad_norm": 7.310968399047852,
      "learning_rate": 1.7364410493071469e-09,
      "loss": 0.9188,
      "num_input_tokens_seen": 59840704,
      "step": 4770,
      "train_runtime": 57337.6588,
      "train_tokens_per_second": 1043.654
    },
    {
      "epoch": 0.9747882004695315,
      "grad_norm": 6.955079078674316,
      "learning_rate": 1.6055075167166754e-09,
      "loss": 0.74,
      "num_input_tokens_seen": 59904464,
      "step": 4775,
      "train_runtime": 57391.5824,
      "train_tokens_per_second": 1043.785
    },
    {
      "epoch": 0.9758089210982954,
      "grad_norm": 9.225369453430176,
      "learning_rate": 1.4796978498203272e-09,
      "loss": 0.9804,
      "num_input_tokens_seen": 59967904,
      "step": 4780,
      "train_runtime": 57445.262,
      "train_tokens_per_second": 1043.914
    },
    {
      "epoch": 0.9768296417270593,
      "grad_norm": 6.9040961265563965,
      "learning_rate": 1.3590133420350314e-09,
      "loss": 0.8645,
      "num_input_tokens_seen": 60030800,
      "step": 4785,
      "train_runtime": 57498.6205,
      "train_tokens_per_second": 1044.039
    },
    {
      "epoch": 0.9778503623558232,
      "grad_norm": 6.556224346160889,
      "learning_rate": 1.2434552340871428e-09,
      "loss": 0.8378,
      "num_input_tokens_seen": 60094032,
      "step": 4790,
      "train_runtime": 57552.1764,
      "train_tokens_per_second": 1044.166
    },
    {
      "epoch": 0.9788710829845871,
      "grad_norm": 7.522918701171875,
      "learning_rate": 1.1330247139998972e-09,
      "loss": 0.923,
      "num_input_tokens_seen": 60157376,
      "step": 4795,
      "train_runtime": 57605.8678,
      "train_tokens_per_second": 1044.293
    },
    {
      "epoch": 0.979891803613351,
      "grad_norm": 8.098791122436523,
      "learning_rate": 1.0277229170811419e-09,
      "loss": 1.0384,
      "num_input_tokens_seen": 60220304,
      "step": 4800,
      "train_runtime": 57659.5314,
      "train_tokens_per_second": 1044.412
    },
    {
      "epoch": 0.979891803613351,
      "eval_loss": 1.0150505304336548,
      "eval_runtime": 139.139,
      "eval_samples_per_second": 5.692,
      "eval_steps_per_second": 2.846,
      "num_input_tokens_seen": 60220304,
      "step": 4800
    },
    {
      "epoch": 0.980912524242115,
      "grad_norm": 8.479836463928223,
      "learning_rate": 9.275509259114578e-10,
      "loss": 1.0178,
      "num_input_tokens_seen": 60283632,
      "step": 4805,
      "train_runtime": 57852.7692,
      "train_tokens_per_second": 1042.018
    },
    {
      "epoch": 0.9819332448708789,
      "grad_norm": 6.086810111999512,
      "learning_rate": 8.325097703334449e-10,
      "loss": 0.8107,
      "num_input_tokens_seen": 60345184,
      "step": 4810,
      "train_runtime": 57905.0548,
      "train_tokens_per_second": 1042.14
    },
    {
      "epoch": 0.9829539654996428,
      "grad_norm": 7.197019577026367,
      "learning_rate": 7.426004274407871e-10,
      "loss": 0.996,
      "num_input_tokens_seen": 60408224,
      "step": 4815,
      "train_runtime": 57958.3456,
      "train_tokens_per_second": 1042.27
    },
    {
      "epoch": 0.9839746861284067,
      "grad_norm": 7.8293232917785645,
      "learning_rate": 6.578238215683152e-10,
      "loss": 0.8774,
      "num_input_tokens_seen": 60472400,
      "step": 4820,
      "train_runtime": 58012.6888,
      "train_tokens_per_second": 1042.4
    },
    {
      "epoch": 0.9849954067571706,
      "grad_norm": 5.51485538482666,
      "learning_rate": 5.781808242825703e-10,
      "loss": 1.0924,
      "num_input_tokens_seen": 60534464,
      "step": 4825,
      "train_runtime": 58065.5859,
      "train_tokens_per_second": 1042.519
    },
    {
      "epoch": 0.9860161273859345,
      "grad_norm": 6.819395542144775,
      "learning_rate": 5.036722543726446e-10,
      "loss": 0.8397,
      "num_input_tokens_seen": 60597360,
      "step": 4830,
      "train_runtime": 58119.0877,
      "train_tokens_per_second": 1042.641
    },
    {
      "epoch": 0.9870368480146984,
      "grad_norm": 6.528038501739502,
      "learning_rate": 4.342988778420764e-10,
      "loss": 1.202,
      "num_input_tokens_seen": 60660432,
      "step": 4835,
      "train_runtime": 58172.5689,
      "train_tokens_per_second": 1042.767
    },
    {
      "epoch": 0.9880575686434623,
      "grad_norm": 35.79803466796875,
      "learning_rate": 3.700614079006903e-10,
      "loss": 1.0737,
      "num_input_tokens_seen": 60723216,
      "step": 4840,
      "train_runtime": 58225.9455,
      "train_tokens_per_second": 1042.889
    },
    {
      "epoch": 0.9890782892722262,
      "grad_norm": 7.716414451599121,
      "learning_rate": 3.10960504957436e-10,
      "loss": 1.0668,
      "num_input_tokens_seen": 60786048,
      "step": 4845,
      "train_runtime": 58279.3355,
      "train_tokens_per_second": 1043.012
    },
    {
      "epoch": 0.9900990099009901,
      "grad_norm": 4.0221381187438965,
      "learning_rate": 2.5699677661344954e-10,
      "loss": 0.9703,
      "num_input_tokens_seen": 60848096,
      "step": 4850,
      "train_runtime": 58332.098,
      "train_tokens_per_second": 1043.132
    },
    {
      "epoch": 0.991119730529754,
      "grad_norm": 8.701247215270996,
      "learning_rate": 2.0817077765605816e-10,
      "loss": 1.0885,
      "num_input_tokens_seen": 60910768,
      "step": 4855,
      "train_runtime": 58385.2556,
      "train_tokens_per_second": 1043.256
    },
    {
      "epoch": 0.9921404511585179,
      "grad_norm": 7.229775905609131,
      "learning_rate": 1.6448301005267396e-10,
      "loss": 0.9087,
      "num_input_tokens_seen": 60972400,
      "step": 4860,
      "train_runtime": 58437.8535,
      "train_tokens_per_second": 1043.372
    },
    {
      "epoch": 0.9931611717872818,
      "grad_norm": 6.535231590270996,
      "learning_rate": 1.2593392294613092e-10,
      "loss": 1.1078,
      "num_input_tokens_seen": 61034320,
      "step": 4865,
      "train_runtime": 58490.5265,
      "train_tokens_per_second": 1043.491
    },
    {
      "epoch": 0.9941818924160457,
      "grad_norm": 6.460030555725098,
      "learning_rate": 9.252391264957804e-11,
      "loss": 1.0362,
      "num_input_tokens_seen": 61097008,
      "step": 4870,
      "train_runtime": 58543.7079,
      "train_tokens_per_second": 1043.614
    },
    {
      "epoch": 0.9952026130448096,
      "grad_norm": 5.745837688446045,
      "learning_rate": 6.425332264281547e-11,
      "loss": 1.0611,
      "num_input_tokens_seen": 61159504,
      "step": 4875,
      "train_runtime": 58596.6699,
      "train_tokens_per_second": 1043.737
    },
    {
      "epoch": 0.9962233336735735,
      "grad_norm": 3.566666603088379,
      "learning_rate": 4.112244356851979e-11,
      "loss": 1.093,
      "num_input_tokens_seen": 61221680,
      "step": 4880,
      "train_runtime": 58649.5292,
      "train_tokens_per_second": 1043.856
    },
    {
      "epoch": 0.9972440543023374,
      "grad_norm": 6.8197784423828125,
      "learning_rate": 2.313151322930196e-11,
      "loss": 0.9885,
      "num_input_tokens_seen": 61284000,
      "step": 4885,
      "train_runtime": 58702.5735,
      "train_tokens_per_second": 1043.975
    },
    {
      "epoch": 0.9982647749311013,
      "grad_norm": 6.027496814727783,
      "learning_rate": 1.0280716585264747e-11,
      "loss": 1.0136,
      "num_input_tokens_seen": 61347680,
      "step": 4890,
      "train_runtime": 58756.5769,
      "train_tokens_per_second": 1044.099
    },
    {
      "epoch": 0.9992854955598652,
      "grad_norm": 5.818605422973633,
      "learning_rate": 2.570185752170939e-12,
      "loss": 0.9577,
      "num_input_tokens_seen": 61410784,
      "step": 4895,
      "train_runtime": 58810.0257,
      "train_tokens_per_second": 1044.223
    },
    {
      "epoch": 1.0,
      "num_input_tokens_seen": 61455152,
      "step": 4899,
      "total_flos": 2.3763290171759e+17,
      "train_loss": 1.274114225416384,
      "train_runtime": 58848.3212,
      "train_samples_per_second": 1.332,
      "train_steps_per_second": 0.083
    }
  ],
  "logging_steps": 5,
  "max_steps": 4899,
  "num_input_tokens_seen": 61455152,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.3763290171759e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
