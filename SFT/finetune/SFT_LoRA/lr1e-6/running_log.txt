[INFO|2025-10-22 19:45:25] tokenization_utils_base.py:2093 >> loading file vocab.json
[INFO|2025-10-22 19:45:25] tokenization_utils_base.py:2093 >> loading file merges.txt
[INFO|2025-10-22 19:45:25] tokenization_utils_base.py:2093 >> loading file tokenizer.json
[INFO|2025-10-22 19:45:25] tokenization_utils_base.py:2093 >> loading file added_tokens.json
[INFO|2025-10-22 19:45:25] tokenization_utils_base.py:2093 >> loading file special_tokens_map.json
[INFO|2025-10-22 19:45:25] tokenization_utils_base.py:2093 >> loading file tokenizer_config.json
[INFO|2025-10-22 19:45:25] tokenization_utils_base.py:2093 >> loading file chat_template.jinja
[INFO|2025-10-22 19:45:26] tokenization_utils_base.py:2364 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-10-22 19:45:26] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 19:45:26] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 19:45:26] tokenization_utils_base.py:2093 >> loading file vocab.json
[INFO|2025-10-22 19:45:26] tokenization_utils_base.py:2093 >> loading file merges.txt
[INFO|2025-10-22 19:45:26] tokenization_utils_base.py:2093 >> loading file tokenizer.json
[INFO|2025-10-22 19:45:26] tokenization_utils_base.py:2093 >> loading file added_tokens.json
[INFO|2025-10-22 19:45:26] tokenization_utils_base.py:2093 >> loading file special_tokens_map.json
[INFO|2025-10-22 19:45:26] tokenization_utils_base.py:2093 >> loading file tokenizer_config.json
[INFO|2025-10-22 19:45:26] tokenization_utils_base.py:2093 >> loading file chat_template.jinja
[INFO|2025-10-22 19:45:26] tokenization_utils_base.py:2364 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-10-22 19:45:26] logging.py:143 >> Loading dataset /media/data/users/liqz/llama_factory/LLaMA-Factory/data/train_top5_alpaca.jsonl...
[INFO|2025-10-22 19:45:27] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 19:45:27] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 19:45:27] logging.py:143 >> Loading 4-bit AWQ-quantized model.
[INFO|2025-10-22 19:45:27] logging.py:143 >> KV cache is disabled during training.
[WARNING|2025-10-22 19:45:27] logging.py:328 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|2025-10-22 19:45:27] auto.py:242 >> 
[WARNING|2025-10-22 19:45:27] quantizer_awq.py:102 >> `torch.bfloat16` is not supported for AWQ CUDA/XPU kernels yet. Casting to `torch.float16`.
[INFO|2025-10-22 19:45:27] modeling_utils.py:1169 >> loading weights file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/model.safetensors.index.json
[INFO|2025-10-22 19:45:27] modeling_utils.py:2341 >> Instantiating Qwen3ForCausalLM model under default dtype torch.float16.
[INFO|2025-10-22 19:45:27] configuration_utils.py:986 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

[INFO|2025-10-22 19:45:29] configuration_utils.py:939 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/generation_config.json
[INFO|2025-10-22 19:45:29] configuration_utils.py:986 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

[INFO|2025-10-22 19:45:29] dynamic_module_utils.py:423 >> Could not locate the custom_generate/generate.py inside /media/data/users/liqz/Qwen/Qwen3-8B-AWQ.
[INFO|2025-10-22 19:45:30] logging.py:143 >> Gradient checkpointing enabled.
[INFO|2025-10-22 19:45:30] logging.py:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-10-22 19:45:30] logging.py:143 >> Upcasting trainable params to float32.
[INFO|2025-10-22 19:45:30] logging.py:143 >> Fine-tuning method: LoRA
[INFO|2025-10-22 19:45:30] logging.py:143 >> Found linear modules: gate_proj,down_proj,q_proj,up_proj,v_proj,k_proj,o_proj
[INFO|2025-10-22 19:45:30] logging.py:143 >> trainable params: 21,823,488 || all params: 1,266,791,424 || trainable%: 1.7227
[WARNING|2025-10-22 19:45:30] trainer.py:906 >> The model is already on multiple devices. Skipping the move to device specified in `args`.
[INFO|2025-10-22 19:45:30] trainer.py:749 >> Using auto half precision backend
[WARNING|2025-10-22 19:45:30] trainer.py:982 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|2025-10-22 19:45:30] trainer.py:2519 >> ***** Running training *****
[INFO|2025-10-22 19:45:30] trainer.py:2520 >>   Num examples = 78,376
[INFO|2025-10-22 19:45:30] trainer.py:2521 >>   Num Epochs = 1
[INFO|2025-10-22 19:45:30] trainer.py:2522 >>   Instantaneous batch size per device = 2
[INFO|2025-10-22 19:45:30] trainer.py:2525 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|2025-10-22 19:45:30] trainer.py:2526 >>   Gradient Accumulation steps = 8
[INFO|2025-10-22 19:45:30] trainer.py:2527 >>   Total optimization steps = 4,899
[INFO|2025-10-22 19:45:30] trainer.py:2528 >>   Number of trainable parameters = 21,823,488
[INFO|2025-10-22 19:46:25] logging.py:143 >> {'loss': 9.5986, 'learning_rate': 1.0000e-06, 'epoch': 0.00, 'throughput': 1160.19}
[INFO|2025-10-22 19:47:18] logging.py:143 >> {'loss': 10.1343, 'learning_rate': 9.9999e-07, 'epoch': 0.00, 'throughput': 1170.70}
[INFO|2025-10-22 19:48:12] logging.py:143 >> {'loss': 9.8379, 'learning_rate': 9.9998e-07, 'epoch': 0.00, 'throughput': 1171.74}
[INFO|2025-10-22 19:49:04] logging.py:143 >> {'loss': 10.1201, 'learning_rate': 9.9996e-07, 'epoch': 0.00, 'throughput': 1172.32}
[INFO|2025-10-22 19:49:58] logging.py:143 >> {'loss': 9.0671, 'learning_rate': 9.9994e-07, 'epoch': 0.01, 'throughput': 1173.86}
[INFO|2025-10-22 19:50:51] logging.py:143 >> {'loss': 10.0013, 'learning_rate': 9.9991e-07, 'epoch': 0.01, 'throughput': 1174.36}
[INFO|2025-10-22 19:51:44] logging.py:143 >> {'loss': 9.5303, 'learning_rate': 9.9988e-07, 'epoch': 0.01, 'throughput': 1174.96}
[INFO|2025-10-22 19:52:38] logging.py:143 >> {'loss': 9.6047, 'learning_rate': 9.9984e-07, 'epoch': 0.01, 'throughput': 1175.45}
[INFO|2025-10-22 19:53:31] logging.py:143 >> {'loss': 8.3994, 'learning_rate': 9.9980e-07, 'epoch': 0.01, 'throughput': 1175.62}
[INFO|2025-10-22 19:54:25] logging.py:143 >> {'loss': 8.8185, 'learning_rate': 9.9975e-07, 'epoch': 0.01, 'throughput': 1176.26}
[INFO|2025-10-22 19:55:19] logging.py:143 >> {'loss': 8.0236, 'learning_rate': 9.9970e-07, 'epoch': 0.01, 'throughput': 1176.52}
[INFO|2025-10-22 19:56:13] logging.py:143 >> {'loss': 7.5213, 'learning_rate': 9.9964e-07, 'epoch': 0.01, 'throughput': 1176.54}
[INFO|2025-10-22 19:57:07] logging.py:143 >> {'loss': 7.6900, 'learning_rate': 9.9958e-07, 'epoch': 0.01, 'throughput': 1176.79}
[INFO|2025-10-22 19:58:00] logging.py:143 >> {'loss': 8.1544, 'learning_rate': 9.9951e-07, 'epoch': 0.01, 'throughput': 1176.79}
[INFO|2025-10-22 19:58:54] logging.py:143 >> {'loss': 7.4283, 'learning_rate': 9.9944e-07, 'epoch': 0.02, 'throughput': 1177.13}
[INFO|2025-10-22 19:59:47] logging.py:143 >> {'loss': 7.2821, 'learning_rate': 9.9936e-07, 'epoch': 0.02, 'throughput': 1177.12}
[INFO|2025-10-22 20:00:40] logging.py:143 >> {'loss': 7.3816, 'learning_rate': 9.9927e-07, 'epoch': 0.02, 'throughput': 1177.17}
[INFO|2025-10-22 20:01:34] logging.py:143 >> {'loss': 6.1827, 'learning_rate': 9.9919e-07, 'epoch': 0.02, 'throughput': 1177.52}
[INFO|2025-10-22 20:02:27] logging.py:143 >> {'loss': 6.9480, 'learning_rate': 9.9909e-07, 'epoch': 0.02, 'throughput': 1177.49}
[INFO|2025-10-22 20:03:20] logging.py:143 >> {'loss': 6.0614, 'learning_rate': 9.9899e-07, 'epoch': 0.02, 'throughput': 1177.67}
[INFO|2025-10-22 20:03:20] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-22 20:03:20] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-22 20:03:20] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-22 20:05:40] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-100
[INFO|2025-10-22 20:05:40] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 20:05:40] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 20:05:40] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-100/chat_template.jinja
[INFO|2025-10-22 20:05:40] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-100/tokenizer_config.json
[INFO|2025-10-22 20:05:40] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-100/special_tokens_map.json
[INFO|2025-10-22 20:06:33] logging.py:143 >> {'loss': 5.9971, 'learning_rate': 9.9889e-07, 'epoch': 0.02, 'throughput': 1047.15}
[INFO|2025-10-22 20:07:27] logging.py:143 >> {'loss': 5.6988, 'learning_rate': 9.9878e-07, 'epoch': 0.02, 'throughput': 1052.78}
[INFO|2025-10-22 20:08:19] logging.py:143 >> {'loss': 6.0975, 'learning_rate': 9.9866e-07, 'epoch': 0.02, 'throughput': 1057.50}
[INFO|2025-10-22 20:09:12] logging.py:143 >> {'loss': 5.5240, 'learning_rate': 9.9854e-07, 'epoch': 0.02, 'throughput': 1061.97}
[INFO|2025-10-22 20:10:05] logging.py:143 >> {'loss': 5.0014, 'learning_rate': 9.9842e-07, 'epoch': 0.03, 'throughput': 1066.11}
[INFO|2025-10-22 20:10:59] logging.py:143 >> {'loss': 4.7580, 'learning_rate': 9.9829e-07, 'epoch': 0.03, 'throughput': 1070.18}
[INFO|2025-10-22 20:11:53] logging.py:143 >> {'loss': 4.5925, 'learning_rate': 9.9816e-07, 'epoch': 0.03, 'throughput': 1074.07}
[INFO|2025-10-22 20:12:46] logging.py:143 >> {'loss': 4.7356, 'learning_rate': 9.9801e-07, 'epoch': 0.03, 'throughput': 1077.40}
[INFO|2025-10-22 20:13:40] logging.py:143 >> {'loss': 4.1267, 'learning_rate': 9.9787e-07, 'epoch': 0.03, 'throughput': 1080.74}
[INFO|2025-10-22 20:14:34] logging.py:143 >> {'loss': 3.5905, 'learning_rate': 9.9772e-07, 'epoch': 0.03, 'throughput': 1083.85}
[INFO|2025-10-22 20:15:27] logging.py:143 >> {'loss': 4.1348, 'learning_rate': 9.9756e-07, 'epoch': 0.03, 'throughput': 1086.57}
[INFO|2025-10-22 20:16:19] logging.py:143 >> {'loss': 3.4915, 'learning_rate': 9.9740e-07, 'epoch': 0.03, 'throughput': 1089.10}
[INFO|2025-10-22 20:17:13] logging.py:143 >> {'loss': 3.5158, 'learning_rate': 9.9724e-07, 'epoch': 0.03, 'throughput': 1091.69}
[INFO|2025-10-22 20:18:06] logging.py:143 >> {'loss': 3.2095, 'learning_rate': 9.9707e-07, 'epoch': 0.03, 'throughput': 1094.07}
[INFO|2025-10-22 20:18:59] logging.py:143 >> {'loss': 2.9898, 'learning_rate': 9.9689e-07, 'epoch': 0.04, 'throughput': 1096.22}
[INFO|2025-10-22 20:19:52] logging.py:143 >> {'loss': 3.3946, 'learning_rate': 9.9671e-07, 'epoch': 0.04, 'throughput': 1098.32}
[INFO|2025-10-22 20:20:46] logging.py:143 >> {'loss': 2.6575, 'learning_rate': 9.9652e-07, 'epoch': 0.04, 'throughput': 1100.34}
[INFO|2025-10-22 20:21:38] logging.py:143 >> {'loss': 2.6020, 'learning_rate': 9.9633e-07, 'epoch': 0.04, 'throughput': 1102.21}
[INFO|2025-10-22 20:22:32] logging.py:143 >> {'loss': 2.2301, 'learning_rate': 9.9614e-07, 'epoch': 0.04, 'throughput': 1104.00}
[INFO|2025-10-22 20:23:24] logging.py:143 >> {'loss': 2.6531, 'learning_rate': 9.9593e-07, 'epoch': 0.04, 'throughput': 1105.75}
[INFO|2025-10-22 20:23:24] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-22 20:23:24] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-22 20:23:24] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-22 20:25:43] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-200
[INFO|2025-10-22 20:25:43] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 20:25:43] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 20:25:43] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-200/chat_template.jinja
[INFO|2025-10-22 20:25:43] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-200/tokenizer_config.json
[INFO|2025-10-22 20:25:43] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-200/special_tokens_map.json
[INFO|2025-10-22 20:26:37] logging.py:143 >> {'loss': 2.1212, 'learning_rate': 9.9573e-07, 'epoch': 0.04, 'throughput': 1044.72}
[INFO|2025-10-22 20:27:30] logging.py:143 >> {'loss': 1.8144, 'learning_rate': 9.9552e-07, 'epoch': 0.04, 'throughput': 1047.49}
[INFO|2025-10-22 20:28:23] logging.py:143 >> {'loss': 2.2061, 'learning_rate': 9.9530e-07, 'epoch': 0.04, 'throughput': 1050.16}
[INFO|2025-10-22 20:29:16] logging.py:143 >> {'loss': 1.6785, 'learning_rate': 9.9508e-07, 'epoch': 0.04, 'throughput': 1052.73}
[INFO|2025-10-22 20:30:09] logging.py:143 >> {'loss': 2.1270, 'learning_rate': 9.9485e-07, 'epoch': 0.05, 'throughput': 1055.21}
[INFO|2025-10-22 20:31:02] logging.py:143 >> {'loss': 1.9282, 'learning_rate': 9.9462e-07, 'epoch': 0.05, 'throughput': 1057.67}
[INFO|2025-10-22 20:31:56] logging.py:143 >> {'loss': 1.6109, 'learning_rate': 9.9438e-07, 'epoch': 0.05, 'throughput': 1059.97}
[INFO|2025-10-22 20:32:50] logging.py:143 >> {'loss': 1.4484, 'learning_rate': 9.9414e-07, 'epoch': 0.05, 'throughput': 1062.12}
[INFO|2025-10-22 20:33:43] logging.py:143 >> {'loss': 2.0576, 'learning_rate': 9.9389e-07, 'epoch': 0.05, 'throughput': 1064.23}
[INFO|2025-10-22 20:34:35] logging.py:143 >> {'loss': 1.6287, 'learning_rate': 9.9364e-07, 'epoch': 0.05, 'throughput': 1066.23}
[INFO|2025-10-22 20:35:28] logging.py:143 >> {'loss': 1.6431, 'learning_rate': 9.9338e-07, 'epoch': 0.05, 'throughput': 1068.25}
[INFO|2025-10-22 20:36:21] logging.py:143 >> {'loss': 1.4746, 'learning_rate': 9.9312e-07, 'epoch': 0.05, 'throughput': 1070.28}
[INFO|2025-10-22 20:37:14] logging.py:143 >> {'loss': 1.6952, 'learning_rate': 9.9285e-07, 'epoch': 0.05, 'throughput': 1072.16}
[INFO|2025-10-22 20:38:07] logging.py:143 >> {'loss': 1.5275, 'learning_rate': 9.9258e-07, 'epoch': 0.06, 'throughput': 1073.99}
[INFO|2025-10-22 20:39:00] logging.py:143 >> {'loss': 1.4348, 'learning_rate': 9.9230e-07, 'epoch': 0.06, 'throughput': 1075.73}
[INFO|2025-10-22 20:39:54] logging.py:143 >> {'loss': 1.6168, 'learning_rate': 9.9202e-07, 'epoch': 0.06, 'throughput': 1077.43}
[INFO|2025-10-22 20:40:46] logging.py:143 >> {'loss': 1.9046, 'learning_rate': 9.9173e-07, 'epoch': 0.06, 'throughput': 1079.00}
[INFO|2025-10-22 20:41:40] logging.py:143 >> {'loss': 1.4245, 'learning_rate': 9.9144e-07, 'epoch': 0.06, 'throughput': 1080.59}
[INFO|2025-10-22 20:42:33] logging.py:143 >> {'loss': 1.5326, 'learning_rate': 9.9114e-07, 'epoch': 0.06, 'throughput': 1082.06}
[INFO|2025-10-22 20:43:26] logging.py:143 >> {'loss': 1.0980, 'learning_rate': 9.9084e-07, 'epoch': 0.06, 'throughput': 1083.53}
[INFO|2025-10-22 20:43:26] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-22 20:43:26] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-22 20:43:26] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-22 20:45:45] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-300
[INFO|2025-10-22 20:45:45] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 20:45:45] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 20:45:45] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-300/chat_template.jinja
[INFO|2025-10-22 20:45:45] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-300/tokenizer_config.json
[INFO|2025-10-22 20:45:45] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-300/special_tokens_map.json
[INFO|2025-10-22 20:46:39] logging.py:143 >> {'loss': 1.4630, 'learning_rate': 9.9053e-07, 'epoch': 0.06, 'throughput': 1043.67}
[INFO|2025-10-22 20:47:32] logging.py:143 >> {'loss': 1.3755, 'learning_rate': 9.9022e-07, 'epoch': 0.06, 'throughput': 1045.61}
[INFO|2025-10-22 20:48:25] logging.py:143 >> {'loss': 1.5379, 'learning_rate': 9.8990e-07, 'epoch': 0.06, 'throughput': 1047.43}
[INFO|2025-10-22 20:49:18] logging.py:143 >> {'loss': 1.4138, 'learning_rate': 9.8957e-07, 'epoch': 0.07, 'throughput': 1049.28}
[INFO|2025-10-22 20:50:12] logging.py:143 >> {'loss': 1.5573, 'learning_rate': 9.8925e-07, 'epoch': 0.07, 'throughput': 1051.10}
[INFO|2025-10-22 20:51:06] logging.py:143 >> {'loss': 1.3235, 'learning_rate': 9.8891e-07, 'epoch': 0.07, 'throughput': 1052.84}
[INFO|2025-10-22 20:51:59] logging.py:143 >> {'loss': 1.2947, 'learning_rate': 9.8857e-07, 'epoch': 0.07, 'throughput': 1054.53}
[INFO|2025-10-22 20:52:52] logging.py:143 >> {'loss': 1.5548, 'learning_rate': 9.8823e-07, 'epoch': 0.07, 'throughput': 1056.19}
[INFO|2025-10-22 20:53:45] logging.py:143 >> {'loss': 0.9226, 'learning_rate': 9.8788e-07, 'epoch': 0.07, 'throughput': 1057.79}
[INFO|2025-10-22 20:54:39] logging.py:143 >> {'loss': 1.2797, 'learning_rate': 9.8753e-07, 'epoch': 0.07, 'throughput': 1059.38}
[INFO|2025-10-22 20:55:32] logging.py:143 >> {'loss': 1.4614, 'learning_rate': 9.8717e-07, 'epoch': 0.07, 'throughput': 1060.83}
[INFO|2025-10-22 20:56:25] logging.py:143 >> {'loss': 1.2482, 'learning_rate': 9.8681e-07, 'epoch': 0.07, 'throughput': 1062.35}
[INFO|2025-10-22 20:57:17] logging.py:143 >> {'loss': 1.4017, 'learning_rate': 9.8644e-07, 'epoch': 0.07, 'throughput': 1063.76}
[INFO|2025-10-22 20:58:11] logging.py:143 >> {'loss': 1.0371, 'learning_rate': 9.8607e-07, 'epoch': 0.08, 'throughput': 1065.17}
[INFO|2025-10-22 20:59:04] logging.py:143 >> {'loss': 1.4009, 'learning_rate': 9.8569e-07, 'epoch': 0.08, 'throughput': 1066.48}
[INFO|2025-10-22 20:59:57] logging.py:143 >> {'loss': 1.3900, 'learning_rate': 9.8531e-07, 'epoch': 0.08, 'throughput': 1067.81}
[INFO|2025-10-22 21:00:50] logging.py:143 >> {'loss': 1.4861, 'learning_rate': 9.8492e-07, 'epoch': 0.08, 'throughput': 1069.17}
[INFO|2025-10-22 21:01:43] logging.py:143 >> {'loss': 1.3469, 'learning_rate': 9.8452e-07, 'epoch': 0.08, 'throughput': 1070.40}
[INFO|2025-10-22 21:02:37] logging.py:143 >> {'loss': 1.0411, 'learning_rate': 9.8413e-07, 'epoch': 0.08, 'throughput': 1071.63}
[INFO|2025-10-22 21:03:30] logging.py:143 >> {'loss': 1.5435, 'learning_rate': 9.8372e-07, 'epoch': 0.08, 'throughput': 1072.85}
[INFO|2025-10-22 21:03:30] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-22 21:03:30] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-22 21:03:30] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-22 21:05:49] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-400
[INFO|2025-10-22 21:05:49] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 21:05:49] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 21:05:49] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-400/chat_template.jinja
[INFO|2025-10-22 21:05:49] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-400/tokenizer_config.json
[INFO|2025-10-22 21:05:49] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-400/special_tokens_map.json
[INFO|2025-10-22 21:06:44] logging.py:143 >> {'loss': 1.2759, 'learning_rate': 9.8331e-07, 'epoch': 0.08, 'throughput': 1043.37}
[INFO|2025-10-22 21:07:36] logging.py:143 >> {'loss': 1.3396, 'learning_rate': 9.8290e-07, 'epoch': 0.08, 'throughput': 1044.79}
[INFO|2025-10-22 21:08:29] logging.py:143 >> {'loss': 1.3118, 'learning_rate': 9.8248e-07, 'epoch': 0.08, 'throughput': 1046.16}
[INFO|2025-10-22 21:09:22] logging.py:143 >> {'loss': 1.4598, 'learning_rate': 9.8206e-07, 'epoch': 0.09, 'throughput': 1047.57}
[INFO|2025-10-22 21:10:15] logging.py:143 >> {'loss': 1.2585, 'learning_rate': 9.8163e-07, 'epoch': 0.09, 'throughput': 1048.95}
[INFO|2025-10-22 21:11:09] logging.py:143 >> {'loss': 1.0566, 'learning_rate': 9.8120e-07, 'epoch': 0.09, 'throughput': 1050.26}
[INFO|2025-10-22 21:12:02] logging.py:143 >> {'loss': 1.3680, 'learning_rate': 9.8076e-07, 'epoch': 0.09, 'throughput': 1051.56}
[INFO|2025-10-22 21:12:56] logging.py:143 >> {'loss': 1.2410, 'learning_rate': 9.8032e-07, 'epoch': 0.09, 'throughput': 1052.91}
[INFO|2025-10-22 21:13:49] logging.py:143 >> {'loss': 1.2609, 'learning_rate': 9.7987e-07, 'epoch': 0.09, 'throughput': 1054.18}
[INFO|2025-10-22 21:14:42] logging.py:143 >> {'loss': 1.5927, 'learning_rate': 9.7942e-07, 'epoch': 0.09, 'throughput': 1055.41}
[INFO|2025-10-22 21:15:35] logging.py:143 >> {'loss': 1.3925, 'learning_rate': 9.7896e-07, 'epoch': 0.09, 'throughput': 1056.61}
[INFO|2025-10-22 21:16:29] logging.py:143 >> {'loss': 1.1849, 'learning_rate': 9.7850e-07, 'epoch': 0.09, 'throughput': 1057.81}
[INFO|2025-10-22 21:17:21] logging.py:143 >> {'loss': 1.4223, 'learning_rate': 9.7803e-07, 'epoch': 0.09, 'throughput': 1058.95}
[INFO|2025-10-22 21:18:14] logging.py:143 >> {'loss': 1.3283, 'learning_rate': 9.7756e-07, 'epoch': 0.10, 'throughput': 1060.00}
[INFO|2025-10-22 21:19:08] logging.py:143 >> {'loss': 1.1056, 'learning_rate': 9.7708e-07, 'epoch': 0.10, 'throughput': 1061.16}
[INFO|2025-10-22 21:20:01] logging.py:143 >> {'loss': 1.2825, 'learning_rate': 9.7660e-07, 'epoch': 0.10, 'throughput': 1062.24}
[INFO|2025-10-22 21:20:54] logging.py:143 >> {'loss': 1.4229, 'learning_rate': 9.7611e-07, 'epoch': 0.10, 'throughput': 1063.34}
[INFO|2025-10-22 21:21:48] logging.py:143 >> {'loss': 1.7782, 'learning_rate': 9.7562e-07, 'epoch': 0.10, 'throughput': 1064.46}
[INFO|2025-10-22 21:22:41] logging.py:143 >> {'loss': 1.2517, 'learning_rate': 9.7512e-07, 'epoch': 0.10, 'throughput': 1065.48}
[INFO|2025-10-22 21:23:34] logging.py:143 >> {'loss': 1.1338, 'learning_rate': 9.7462e-07, 'epoch': 0.10, 'throughput': 1066.50}
[INFO|2025-10-22 21:23:34] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-22 21:23:34] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-22 21:23:34] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-22 21:25:54] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-500
[INFO|2025-10-22 21:25:54] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 21:25:54] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 21:25:54] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-500/chat_template.jinja
[INFO|2025-10-22 21:25:54] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-500/tokenizer_config.json
[INFO|2025-10-22 21:25:54] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-500/special_tokens_map.json
[INFO|2025-10-22 21:26:47] logging.py:143 >> {'loss': 1.0663, 'learning_rate': 9.7411e-07, 'epoch': 0.10, 'throughput': 1043.01}
[INFO|2025-10-22 21:27:41] logging.py:143 >> {'loss': 1.1005, 'learning_rate': 9.7360e-07, 'epoch': 0.10, 'throughput': 1044.22}
[INFO|2025-10-22 21:28:34] logging.py:143 >> {'loss': 1.3177, 'learning_rate': 9.7308e-07, 'epoch': 0.11, 'throughput': 1045.38}
[INFO|2025-10-22 21:29:27] logging.py:143 >> {'loss': 1.1643, 'learning_rate': 9.7256e-07, 'epoch': 0.11, 'throughput': 1046.50}
[INFO|2025-10-22 21:30:21] logging.py:143 >> {'loss': 1.6688, 'learning_rate': 9.7204e-07, 'epoch': 0.11, 'throughput': 1047.63}
[INFO|2025-10-22 21:31:14] logging.py:143 >> {'loss': 1.3887, 'learning_rate': 9.7151e-07, 'epoch': 0.11, 'throughput': 1048.70}
[INFO|2025-10-22 21:32:07] logging.py:143 >> {'loss': 1.5607, 'learning_rate': 9.7097e-07, 'epoch': 0.11, 'throughput': 1049.77}
[INFO|2025-10-22 21:33:00] logging.py:143 >> {'loss': 0.9418, 'learning_rate': 9.7043e-07, 'epoch': 0.11, 'throughput': 1050.82}
[INFO|2025-10-22 21:33:53] logging.py:143 >> {'loss': 1.2726, 'learning_rate': 9.6988e-07, 'epoch': 0.11, 'throughput': 1051.88}
[INFO|2025-10-22 21:34:46] logging.py:143 >> {'loss': 1.3147, 'learning_rate': 9.6933e-07, 'epoch': 0.11, 'throughput': 1052.92}
[INFO|2025-10-22 21:35:39] logging.py:143 >> {'loss': 1.3069, 'learning_rate': 9.6878e-07, 'epoch': 0.11, 'throughput': 1053.94}
[INFO|2025-10-22 21:36:34] logging.py:143 >> {'loss': 0.9346, 'learning_rate': 9.6822e-07, 'epoch': 0.11, 'throughput': 1055.02}
[INFO|2025-10-22 21:37:28] logging.py:143 >> {'loss': 1.1065, 'learning_rate': 9.6765e-07, 'epoch': 0.12, 'throughput': 1056.01}
[INFO|2025-10-22 21:38:21] logging.py:143 >> {'loss': 1.2214, 'learning_rate': 9.6708e-07, 'epoch': 0.12, 'throughput': 1057.00}
[INFO|2025-10-22 21:39:14] logging.py:143 >> {'loss': 1.3618, 'learning_rate': 9.6651e-07, 'epoch': 0.12, 'throughput': 1057.94}
[INFO|2025-10-22 21:40:07] logging.py:143 >> {'loss': 1.1541, 'learning_rate': 9.6593e-07, 'epoch': 0.12, 'throughput': 1058.86}
[INFO|2025-10-22 21:41:00] logging.py:143 >> {'loss': 1.3770, 'learning_rate': 9.6534e-07, 'epoch': 0.12, 'throughput': 1059.79}
[INFO|2025-10-22 21:41:54] logging.py:143 >> {'loss': 1.5244, 'learning_rate': 9.6476e-07, 'epoch': 0.12, 'throughput': 1060.77}
[INFO|2025-10-22 21:42:48] logging.py:143 >> {'loss': 1.1618, 'learning_rate': 9.6416e-07, 'epoch': 0.12, 'throughput': 1061.67}
[INFO|2025-10-22 21:43:40] logging.py:143 >> {'loss': 1.2851, 'learning_rate': 9.6356e-07, 'epoch': 0.12, 'throughput': 1062.53}
[INFO|2025-10-22 21:43:40] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-22 21:43:40] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-22 21:43:40] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-22 21:45:59] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-600
[INFO|2025-10-22 21:45:59] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 21:45:59] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 21:45:59] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-600/chat_template.jinja
[INFO|2025-10-22 21:45:59] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-600/tokenizer_config.json
[INFO|2025-10-22 21:45:59] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-600/special_tokens_map.json
[INFO|2025-10-22 21:46:53] logging.py:143 >> {'loss': 1.1964, 'learning_rate': 9.6296e-07, 'epoch': 0.12, 'throughput': 1043.00}
[INFO|2025-10-22 21:47:46] logging.py:143 >> {'loss': 1.2130, 'learning_rate': 9.6235e-07, 'epoch': 0.12, 'throughput': 1043.97}
[INFO|2025-10-22 21:48:39] logging.py:143 >> {'loss': 1.3921, 'learning_rate': 9.6174e-07, 'epoch': 0.13, 'throughput': 1044.90}
[INFO|2025-10-22 21:49:33] logging.py:143 >> {'loss': 1.5748, 'learning_rate': 9.6112e-07, 'epoch': 0.13, 'throughput': 1045.86}
[INFO|2025-10-22 21:50:25] logging.py:143 >> {'loss': 1.1757, 'learning_rate': 9.6050e-07, 'epoch': 0.13, 'throughput': 1046.76}
[INFO|2025-10-22 21:51:18] logging.py:143 >> {'loss': 1.1708, 'learning_rate': 9.5987e-07, 'epoch': 0.13, 'throughput': 1047.70}
[INFO|2025-10-22 21:52:12] logging.py:143 >> {'loss': 1.1875, 'learning_rate': 9.5924e-07, 'epoch': 0.13, 'throughput': 1048.64}
[INFO|2025-10-22 21:53:04] logging.py:143 >> {'loss': 1.2142, 'learning_rate': 9.5861e-07, 'epoch': 0.13, 'throughput': 1049.50}
[INFO|2025-10-22 21:53:58] logging.py:143 >> {'loss': 1.3467, 'learning_rate': 9.5796e-07, 'epoch': 0.13, 'throughput': 1050.43}
[INFO|2025-10-22 21:54:52] logging.py:143 >> {'loss': 1.2338, 'learning_rate': 9.5732e-07, 'epoch': 0.13, 'throughput': 1051.31}
[INFO|2025-10-22 21:55:45] logging.py:143 >> {'loss': 1.1653, 'learning_rate': 9.5667e-07, 'epoch': 0.13, 'throughput': 1052.17}
[INFO|2025-10-22 21:56:39] logging.py:143 >> {'loss': 0.9080, 'learning_rate': 9.5601e-07, 'epoch': 0.13, 'throughput': 1053.05}
[INFO|2025-10-22 21:57:31] logging.py:143 >> {'loss': 1.1425, 'learning_rate': 9.5535e-07, 'epoch': 0.14, 'throughput': 1053.88}
[INFO|2025-10-22 21:58:24] logging.py:143 >> {'loss': 1.0613, 'learning_rate': 9.5469e-07, 'epoch': 0.14, 'throughput': 1054.70}
[INFO|2025-10-22 21:59:17] logging.py:143 >> {'loss': 1.1924, 'learning_rate': 9.5402e-07, 'epoch': 0.14, 'throughput': 1055.53}
[INFO|2025-10-22 22:00:10] logging.py:143 >> {'loss': 1.2060, 'learning_rate': 9.5335e-07, 'epoch': 0.14, 'throughput': 1056.31}
[INFO|2025-10-22 22:01:03] logging.py:143 >> {'loss': 1.0452, 'learning_rate': 9.5267e-07, 'epoch': 0.14, 'throughput': 1057.08}
[INFO|2025-10-22 22:01:55] logging.py:143 >> {'loss': 1.2383, 'learning_rate': 9.5198e-07, 'epoch': 0.14, 'throughput': 1057.83}
[INFO|2025-10-22 22:02:49] logging.py:143 >> {'loss': 1.0131, 'learning_rate': 9.5130e-07, 'epoch': 0.14, 'throughput': 1058.62}
[INFO|2025-10-22 22:03:43] logging.py:143 >> {'loss': 1.1784, 'learning_rate': 9.5060e-07, 'epoch': 0.14, 'throughput': 1059.40}
[INFO|2025-10-22 22:03:43] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-22 22:03:43] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-22 22:03:43] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-22 22:06:02] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-700
[INFO|2025-10-22 22:06:02] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 22:06:02] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 22:06:02] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-700/chat_template.jinja
[INFO|2025-10-22 22:06:02] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-700/tokenizer_config.json
[INFO|2025-10-22 22:06:02] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-700/special_tokens_map.json
[INFO|2025-10-22 22:06:54] logging.py:143 >> {'loss': 1.2195, 'learning_rate': 9.4991e-07, 'epoch': 0.14, 'throughput': 1042.70}
[INFO|2025-10-22 22:07:47] logging.py:143 >> {'loss': 1.0803, 'learning_rate': 9.4920e-07, 'epoch': 0.14, 'throughput': 1043.55}
[INFO|2025-10-22 22:08:40] logging.py:143 >> {'loss': 1.1747, 'learning_rate': 9.4850e-07, 'epoch': 0.15, 'throughput': 1044.39}
[INFO|2025-10-22 22:09:33] logging.py:143 >> {'loss': 1.1775, 'learning_rate': 9.4779e-07, 'epoch': 0.15, 'throughput': 1045.22}
[INFO|2025-10-22 22:10:26] logging.py:143 >> {'loss': 1.3119, 'learning_rate': 9.4707e-07, 'epoch': 0.15, 'throughput': 1046.01}
[INFO|2025-10-22 22:11:20] logging.py:143 >> {'loss': 1.3282, 'learning_rate': 9.4635e-07, 'epoch': 0.15, 'throughput': 1046.82}
[INFO|2025-10-22 22:12:13] logging.py:143 >> {'loss': 1.2177, 'learning_rate': 9.4563e-07, 'epoch': 0.15, 'throughput': 1047.59}
[INFO|2025-10-22 22:13:08] logging.py:143 >> {'loss': 1.1414, 'learning_rate': 9.4490e-07, 'epoch': 0.15, 'throughput': 1048.40}
[INFO|2025-10-22 22:14:01] logging.py:143 >> {'loss': 1.3332, 'learning_rate': 9.4416e-07, 'epoch': 0.15, 'throughput': 1049.15}
[INFO|2025-10-22 22:14:53] logging.py:143 >> {'loss': 1.0126, 'learning_rate': 9.4343e-07, 'epoch': 0.15, 'throughput': 1049.89}
[INFO|2025-10-22 22:15:47] logging.py:143 >> {'loss': 1.2056, 'learning_rate': 9.4268e-07, 'epoch': 0.15, 'throughput': 1050.65}
[INFO|2025-10-22 22:16:41] logging.py:143 >> {'loss': 1.1842, 'learning_rate': 9.4193e-07, 'epoch': 0.16, 'throughput': 1051.41}
[INFO|2025-10-22 22:17:34] logging.py:143 >> {'loss': 1.1914, 'learning_rate': 9.4118e-07, 'epoch': 0.16, 'throughput': 1052.16}
[INFO|2025-10-22 22:18:27] logging.py:143 >> {'loss': 1.2519, 'learning_rate': 9.4043e-07, 'epoch': 0.16, 'throughput': 1052.89}
[INFO|2025-10-22 22:19:21] logging.py:143 >> {'loss': 1.0140, 'learning_rate': 9.3966e-07, 'epoch': 0.16, 'throughput': 1053.60}
[INFO|2025-10-22 22:20:14] logging.py:143 >> {'loss': 0.9698, 'learning_rate': 9.3890e-07, 'epoch': 0.16, 'throughput': 1054.29}
[INFO|2025-10-22 22:21:07] logging.py:143 >> {'loss': 1.0227, 'learning_rate': 9.3813e-07, 'epoch': 0.16, 'throughput': 1055.01}
[INFO|2025-10-22 22:22:00] logging.py:143 >> {'loss': 1.1936, 'learning_rate': 9.3735e-07, 'epoch': 0.16, 'throughput': 1055.67}
[INFO|2025-10-22 22:22:53] logging.py:143 >> {'loss': 1.1236, 'learning_rate': 9.3657e-07, 'epoch': 0.16, 'throughput': 1056.35}
[INFO|2025-10-22 22:23:46] logging.py:143 >> {'loss': 0.9174, 'learning_rate': 9.3579e-07, 'epoch': 0.16, 'throughput': 1057.03}
[INFO|2025-10-22 22:23:46] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-22 22:23:46] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-22 22:23:46] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-22 22:26:05] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-800
[INFO|2025-10-22 22:26:05] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 22:26:05] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 22:26:05] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-800/chat_template.jinja
[INFO|2025-10-22 22:26:05] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-800/tokenizer_config.json
[INFO|2025-10-22 22:26:05] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-800/special_tokens_map.json
[INFO|2025-10-22 22:26:59] logging.py:143 >> {'loss': 1.1303, 'learning_rate': 9.3500e-07, 'epoch': 0.16, 'throughput': 1042.46}
[INFO|2025-10-22 22:27:52] logging.py:143 >> {'loss': 1.0125, 'learning_rate': 9.3421e-07, 'epoch': 0.17, 'throughput': 1043.19}
[INFO|2025-10-22 22:28:45] logging.py:143 >> {'loss': 1.2042, 'learning_rate': 9.3341e-07, 'epoch': 0.17, 'throughput': 1043.90}
[INFO|2025-10-22 22:29:38] logging.py:143 >> {'loss': 1.0708, 'learning_rate': 9.3261e-07, 'epoch': 0.17, 'throughput': 1044.61}
[INFO|2025-10-22 22:30:31] logging.py:143 >> {'loss': 1.3239, 'learning_rate': 9.3181e-07, 'epoch': 0.17, 'throughput': 1045.35}
[INFO|2025-10-22 22:31:25] logging.py:143 >> {'loss': 1.0108, 'learning_rate': 9.3099e-07, 'epoch': 0.17, 'throughput': 1046.05}
[INFO|2025-10-22 22:32:18] logging.py:143 >> {'loss': 1.0299, 'learning_rate': 9.3018e-07, 'epoch': 0.17, 'throughput': 1046.74}
[INFO|2025-10-22 22:33:12] logging.py:143 >> {'loss': 1.4128, 'learning_rate': 9.2936e-07, 'epoch': 0.17, 'throughput': 1047.46}
[INFO|2025-10-22 22:34:05] logging.py:143 >> {'loss': 1.0328, 'learning_rate': 9.2854e-07, 'epoch': 0.17, 'throughput': 1048.13}
[INFO|2025-10-22 22:35:00] logging.py:143 >> {'loss': 1.2490, 'learning_rate': 9.2771e-07, 'epoch': 0.17, 'throughput': 1048.83}
[INFO|2025-10-22 22:35:52] logging.py:143 >> {'loss': 1.2023, 'learning_rate': 9.2688e-07, 'epoch': 0.17, 'throughput': 1049.51}
[INFO|2025-10-22 22:36:46] logging.py:143 >> {'loss': 1.2658, 'learning_rate': 9.2604e-07, 'epoch': 0.18, 'throughput': 1050.14}
[INFO|2025-10-22 22:37:39] logging.py:143 >> {'loss': 1.1157, 'learning_rate': 9.2520e-07, 'epoch': 0.18, 'throughput': 1050.80}
[INFO|2025-10-22 22:38:33] logging.py:143 >> {'loss': 1.0096, 'learning_rate': 9.2435e-07, 'epoch': 0.18, 'throughput': 1051.45}
[INFO|2025-10-22 22:39:26] logging.py:143 >> {'loss': 1.1479, 'learning_rate': 9.2350e-07, 'epoch': 0.18, 'throughput': 1052.09}
[INFO|2025-10-22 22:40:20] logging.py:143 >> {'loss': 0.8663, 'learning_rate': 9.2265e-07, 'epoch': 0.18, 'throughput': 1052.74}
[INFO|2025-10-22 22:41:12] logging.py:143 >> {'loss': 0.8719, 'learning_rate': 9.2179e-07, 'epoch': 0.18, 'throughput': 1053.37}
[INFO|2025-10-22 22:42:06] logging.py:143 >> {'loss': 1.2305, 'learning_rate': 9.2093e-07, 'epoch': 0.18, 'throughput': 1053.99}
[INFO|2025-10-22 22:42:58] logging.py:143 >> {'loss': 1.0534, 'learning_rate': 9.2006e-07, 'epoch': 0.18, 'throughput': 1054.59}
[INFO|2025-10-22 22:43:52] logging.py:143 >> {'loss': 1.0962, 'learning_rate': 9.1919e-07, 'epoch': 0.18, 'throughput': 1055.19}
[INFO|2025-10-22 22:43:52] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-22 22:43:52] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-22 22:43:52] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-22 22:46:11] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-900
[INFO|2025-10-22 22:46:11] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 22:46:11] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 22:46:11] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-900/chat_template.jinja
[INFO|2025-10-22 22:46:11] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-900/tokenizer_config.json
[INFO|2025-10-22 22:46:11] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-900/special_tokens_map.json
[INFO|2025-10-22 22:47:04] logging.py:143 >> {'loss': 1.0396, 'learning_rate': 9.1831e-07, 'epoch': 0.18, 'throughput': 1042.24}
[INFO|2025-10-22 22:47:58] logging.py:143 >> {'loss': 1.0020, 'learning_rate': 9.1743e-07, 'epoch': 0.19, 'throughput': 1042.87}
[INFO|2025-10-22 22:48:51] logging.py:143 >> {'loss': 0.9816, 'learning_rate': 9.1655e-07, 'epoch': 0.19, 'throughput': 1043.54}
[INFO|2025-10-22 22:49:45] logging.py:143 >> {'loss': 1.3295, 'learning_rate': 9.1566e-07, 'epoch': 0.19, 'throughput': 1044.20}
[INFO|2025-10-22 22:50:38] logging.py:143 >> {'loss': 1.3334, 'learning_rate': 9.1476e-07, 'epoch': 0.19, 'throughput': 1044.80}
[INFO|2025-10-22 22:51:30] logging.py:143 >> {'loss': 1.1544, 'learning_rate': 9.1387e-07, 'epoch': 0.19, 'throughput': 1045.40}
[INFO|2025-10-22 22:52:24] logging.py:143 >> {'loss': 1.0667, 'learning_rate': 9.1296e-07, 'epoch': 0.19, 'throughput': 1046.04}
[INFO|2025-10-22 22:53:17] logging.py:143 >> {'loss': 1.1873, 'learning_rate': 9.1206e-07, 'epoch': 0.19, 'throughput': 1046.65}
[INFO|2025-10-22 22:54:10] logging.py:143 >> {'loss': 1.0437, 'learning_rate': 9.1115e-07, 'epoch': 0.19, 'throughput': 1047.28}
[INFO|2025-10-22 22:55:03] logging.py:143 >> {'loss': 1.1750, 'learning_rate': 9.1023e-07, 'epoch': 0.19, 'throughput': 1047.88}
[INFO|2025-10-22 22:55:56] logging.py:143 >> {'loss': 0.9467, 'learning_rate': 9.0932e-07, 'epoch': 0.19, 'throughput': 1048.46}
[INFO|2025-10-22 22:56:51] logging.py:143 >> {'loss': 1.1159, 'learning_rate': 9.0839e-07, 'epoch': 0.20, 'throughput': 1049.10}
[INFO|2025-10-22 22:57:44] logging.py:143 >> {'loss': 0.9868, 'learning_rate': 9.0747e-07, 'epoch': 0.20, 'throughput': 1049.70}
[INFO|2025-10-22 22:58:38] logging.py:143 >> {'loss': 0.8944, 'learning_rate': 9.0653e-07, 'epoch': 0.20, 'throughput': 1050.28}
[INFO|2025-10-22 22:59:31] logging.py:143 >> {'loss': 0.8976, 'learning_rate': 9.0560e-07, 'epoch': 0.20, 'throughput': 1050.86}
[INFO|2025-10-22 23:00:25] logging.py:143 >> {'loss': 1.1117, 'learning_rate': 9.0466e-07, 'epoch': 0.20, 'throughput': 1051.47}
[INFO|2025-10-22 23:01:18] logging.py:143 >> {'loss': 1.1038, 'learning_rate': 9.0372e-07, 'epoch': 0.20, 'throughput': 1052.06}
[INFO|2025-10-22 23:02:12] logging.py:143 >> {'loss': 1.1534, 'learning_rate': 9.0277e-07, 'epoch': 0.20, 'throughput': 1052.65}
[INFO|2025-10-22 23:03:05] logging.py:143 >> {'loss': 1.0820, 'learning_rate': 9.0182e-07, 'epoch': 0.20, 'throughput': 1053.22}
[INFO|2025-10-22 23:03:59] logging.py:143 >> {'loss': 1.0201, 'learning_rate': 9.0086e-07, 'epoch': 0.20, 'throughput': 1053.78}
[INFO|2025-10-22 23:03:59] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-22 23:03:59] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-22 23:03:59] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-22 23:06:18] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1000
[INFO|2025-10-22 23:06:18] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 23:06:18] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 23:06:19] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1000/chat_template.jinja
[INFO|2025-10-22 23:06:19] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1000/tokenizer_config.json
[INFO|2025-10-22 23:06:19] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1000/special_tokens_map.json
[INFO|2025-10-22 23:07:12] logging.py:143 >> {'loss': 1.2772, 'learning_rate': 8.9990e-07, 'epoch': 0.21, 'throughput': 1042.18}
[INFO|2025-10-22 23:08:05] logging.py:143 >> {'loss': 1.1944, 'learning_rate': 8.9893e-07, 'epoch': 0.21, 'throughput': 1042.76}
[INFO|2025-10-22 23:08:58] logging.py:143 >> {'loss': 1.4113, 'learning_rate': 8.9797e-07, 'epoch': 0.21, 'throughput': 1043.33}
[INFO|2025-10-22 23:09:52] logging.py:143 >> {'loss': 1.1101, 'learning_rate': 8.9699e-07, 'epoch': 0.21, 'throughput': 1043.94}
[INFO|2025-10-22 23:10:45] logging.py:143 >> {'loss': 1.0409, 'learning_rate': 8.9602e-07, 'epoch': 0.21, 'throughput': 1044.51}
[INFO|2025-10-22 23:11:39] logging.py:143 >> {'loss': 1.2364, 'learning_rate': 8.9504e-07, 'epoch': 0.21, 'throughput': 1045.07}
[INFO|2025-10-22 23:12:32] logging.py:143 >> {'loss': 1.0499, 'learning_rate': 8.9405e-07, 'epoch': 0.21, 'throughput': 1045.65}
[INFO|2025-10-22 23:13:25] logging.py:143 >> {'loss': 1.1662, 'learning_rate': 8.9306e-07, 'epoch': 0.21, 'throughput': 1046.19}
[INFO|2025-10-22 23:14:18] logging.py:143 >> {'loss': 0.9263, 'learning_rate': 8.9207e-07, 'epoch': 0.21, 'throughput': 1046.75}
[INFO|2025-10-22 23:15:11] logging.py:143 >> {'loss': 0.9584, 'learning_rate': 8.9107e-07, 'epoch': 0.21, 'throughput': 1047.31}
[INFO|2025-10-22 23:16:05] logging.py:143 >> {'loss': 1.0123, 'learning_rate': 8.9007e-07, 'epoch': 0.22, 'throughput': 1047.87}
[INFO|2025-10-22 23:16:58] logging.py:143 >> {'loss': 1.0120, 'learning_rate': 8.8907e-07, 'epoch': 0.22, 'throughput': 1048.41}
[INFO|2025-10-22 23:17:51] logging.py:143 >> {'loss': 1.1102, 'learning_rate': 8.8806e-07, 'epoch': 0.22, 'throughput': 1048.94}
[INFO|2025-10-22 23:18:44] logging.py:143 >> {'loss': 1.0363, 'learning_rate': 8.8704e-07, 'epoch': 0.22, 'throughput': 1049.47}
[INFO|2025-10-22 23:19:38] logging.py:143 >> {'loss': 1.2636, 'learning_rate': 8.8603e-07, 'epoch': 0.22, 'throughput': 1049.99}
[INFO|2025-10-22 23:20:31] logging.py:143 >> {'loss': 0.8607, 'learning_rate': 8.8501e-07, 'epoch': 0.22, 'throughput': 1050.52}
[INFO|2025-10-22 23:21:24] logging.py:143 >> {'loss': 1.0705, 'learning_rate': 8.8398e-07, 'epoch': 0.22, 'throughput': 1051.05}
[INFO|2025-10-22 23:22:19] logging.py:143 >> {'loss': 1.2862, 'learning_rate': 8.8295e-07, 'epoch': 0.22, 'throughput': 1051.57}
[INFO|2025-10-22 23:23:13] logging.py:143 >> {'loss': 0.9468, 'learning_rate': 8.8192e-07, 'epoch': 0.22, 'throughput': 1052.12}
[INFO|2025-10-22 23:24:06] logging.py:143 >> {'loss': 1.2598, 'learning_rate': 8.8088e-07, 'epoch': 0.22, 'throughput': 1052.63}
[INFO|2025-10-22 23:24:06] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-22 23:24:06] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-22 23:24:06] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-22 23:26:25] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1100
[INFO|2025-10-22 23:26:25] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 23:26:25] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 23:26:25] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1100/chat_template.jinja
[INFO|2025-10-22 23:26:25] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1100/tokenizer_config.json
[INFO|2025-10-22 23:26:25] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1100/special_tokens_map.json
[INFO|2025-10-22 23:27:18] logging.py:143 >> {'loss': 1.0957, 'learning_rate': 8.7984e-07, 'epoch': 0.23, 'throughput': 1042.09}
[INFO|2025-10-22 23:28:12] logging.py:143 >> {'loss': 1.0672, 'learning_rate': 8.7880e-07, 'epoch': 0.23, 'throughput': 1042.65}
[INFO|2025-10-22 23:29:05] logging.py:143 >> {'loss': 1.1190, 'learning_rate': 8.7775e-07, 'epoch': 0.23, 'throughput': 1043.15}
[INFO|2025-10-22 23:29:57] logging.py:143 >> {'loss': 0.8959, 'learning_rate': 8.7670e-07, 'epoch': 0.23, 'throughput': 1043.67}
[INFO|2025-10-22 23:30:50] logging.py:143 >> {'loss': 1.0281, 'learning_rate': 8.7564e-07, 'epoch': 0.23, 'throughput': 1044.20}
[INFO|2025-10-22 23:31:43] logging.py:143 >> {'loss': 0.8625, 'learning_rate': 8.7458e-07, 'epoch': 0.23, 'throughput': 1044.71}
[INFO|2025-10-22 23:32:35] logging.py:143 >> {'loss': 1.0561, 'learning_rate': 8.7352e-07, 'epoch': 0.23, 'throughput': 1045.22}
[INFO|2025-10-22 23:33:29] logging.py:143 >> {'loss': 1.2368, 'learning_rate': 8.7245e-07, 'epoch': 0.23, 'throughput': 1045.75}
[INFO|2025-10-22 23:34:23] logging.py:143 >> {'loss': 1.1507, 'learning_rate': 8.7138e-07, 'epoch': 0.23, 'throughput': 1046.26}
[INFO|2025-10-22 23:35:16] logging.py:143 >> {'loss': 1.1966, 'learning_rate': 8.7030e-07, 'epoch': 0.23, 'throughput': 1046.76}
[INFO|2025-10-22 23:36:09] logging.py:143 >> {'loss': 1.0299, 'learning_rate': 8.6922e-07, 'epoch': 0.24, 'throughput': 1047.26}
[INFO|2025-10-22 23:37:03] logging.py:143 >> {'loss': 1.0653, 'learning_rate': 8.6814e-07, 'epoch': 0.24, 'throughput': 1047.79}
[INFO|2025-10-22 23:37:55] logging.py:143 >> {'loss': 1.1387, 'learning_rate': 8.6706e-07, 'epoch': 0.24, 'throughput': 1048.26}
[INFO|2025-10-22 23:38:48] logging.py:143 >> {'loss': 1.1085, 'learning_rate': 8.6596e-07, 'epoch': 0.24, 'throughput': 1048.77}
[INFO|2025-10-22 23:39:41] logging.py:143 >> {'loss': 1.0877, 'learning_rate': 8.6487e-07, 'epoch': 0.24, 'throughput': 1049.27}
[INFO|2025-10-22 23:40:35] logging.py:143 >> {'loss': 1.1449, 'learning_rate': 8.6377e-07, 'epoch': 0.24, 'throughput': 1049.74}
[INFO|2025-10-22 23:41:27] logging.py:143 >> {'loss': 1.1019, 'learning_rate': 8.6267e-07, 'epoch': 0.24, 'throughput': 1050.20}
[INFO|2025-10-22 23:42:21] logging.py:143 >> {'loss': 1.0045, 'learning_rate': 8.6157e-07, 'epoch': 0.24, 'throughput': 1050.68}
[INFO|2025-10-22 23:43:15] logging.py:143 >> {'loss': 0.9486, 'learning_rate': 8.6046e-07, 'epoch': 0.24, 'throughput': 1051.16}
[INFO|2025-10-22 23:44:07] logging.py:143 >> {'loss': 0.9701, 'learning_rate': 8.5934e-07, 'epoch': 0.24, 'throughput': 1051.64}
[INFO|2025-10-22 23:44:07] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-22 23:44:07] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-22 23:44:07] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-22 23:46:27] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1200
[INFO|2025-10-22 23:46:27] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-22 23:46:27] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-22 23:46:27] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1200/chat_template.jinja
[INFO|2025-10-22 23:46:27] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1200/tokenizer_config.json
[INFO|2025-10-22 23:46:27] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1200/special_tokens_map.json
[INFO|2025-10-22 23:47:21] logging.py:143 >> {'loss': 1.1477, 'learning_rate': 8.5823e-07, 'epoch': 0.25, 'throughput': 1041.99}
[INFO|2025-10-22 23:48:14] logging.py:143 >> {'loss': 1.1890, 'learning_rate': 8.5711e-07, 'epoch': 0.25, 'throughput': 1042.47}
[INFO|2025-10-22 23:49:07] logging.py:143 >> {'loss': 0.9723, 'learning_rate': 8.5598e-07, 'epoch': 0.25, 'throughput': 1042.94}
[INFO|2025-10-22 23:49:59] logging.py:143 >> {'loss': 1.2245, 'learning_rate': 8.5485e-07, 'epoch': 0.25, 'throughput': 1043.43}
[INFO|2025-10-22 23:50:54] logging.py:143 >> {'loss': 1.0377, 'learning_rate': 8.5372e-07, 'epoch': 0.25, 'throughput': 1043.94}
[INFO|2025-10-22 23:51:47] logging.py:143 >> {'loss': 0.7135, 'learning_rate': 8.5259e-07, 'epoch': 0.25, 'throughput': 1044.41}
[INFO|2025-10-22 23:52:41] logging.py:143 >> {'loss': 0.9305, 'learning_rate': 8.5145e-07, 'epoch': 0.25, 'throughput': 1044.91}
[INFO|2025-10-22 23:53:35] logging.py:143 >> {'loss': 1.0860, 'learning_rate': 8.5031e-07, 'epoch': 0.25, 'throughput': 1045.40}
[INFO|2025-10-22 23:54:29] logging.py:143 >> {'loss': 1.1317, 'learning_rate': 8.4916e-07, 'epoch': 0.25, 'throughput': 1045.89}
[INFO|2025-10-22 23:55:23] logging.py:143 >> {'loss': 1.0576, 'learning_rate': 8.4801e-07, 'epoch': 0.26, 'throughput': 1046.37}
[INFO|2025-10-22 23:56:16] logging.py:143 >> {'loss': 1.1721, 'learning_rate': 8.4686e-07, 'epoch': 0.26, 'throughput': 1046.85}
[INFO|2025-10-22 23:57:09] logging.py:143 >> {'loss': 0.9528, 'learning_rate': 8.4570e-07, 'epoch': 0.26, 'throughput': 1047.31}
[INFO|2025-10-22 23:58:02] logging.py:143 >> {'loss': 1.2471, 'learning_rate': 8.4454e-07, 'epoch': 0.26, 'throughput': 1047.76}
[INFO|2025-10-22 23:58:56] logging.py:143 >> {'loss': 1.3446, 'learning_rate': 8.4338e-07, 'epoch': 0.26, 'throughput': 1048.23}
[INFO|2025-10-22 23:59:50] logging.py:143 >> {'loss': 1.0005, 'learning_rate': 8.4221e-07, 'epoch': 0.26, 'throughput': 1048.70}
[INFO|2025-10-23 00:00:43] logging.py:143 >> {'loss': 1.0221, 'learning_rate': 8.4104e-07, 'epoch': 0.26, 'throughput': 1049.15}
[INFO|2025-10-23 00:01:35] logging.py:143 >> {'loss': 1.0263, 'learning_rate': 8.3987e-07, 'epoch': 0.26, 'throughput': 1049.60}
[INFO|2025-10-23 00:02:28] logging.py:143 >> {'loss': 0.8811, 'learning_rate': 8.3869e-07, 'epoch': 0.26, 'throughput': 1050.03}
[INFO|2025-10-23 00:03:21] logging.py:143 >> {'loss': 0.8169, 'learning_rate': 8.3751e-07, 'epoch': 0.26, 'throughput': 1050.48}
[INFO|2025-10-23 00:04:15] logging.py:143 >> {'loss': 0.9417, 'learning_rate': 8.3632e-07, 'epoch': 0.27, 'throughput': 1050.94}
[INFO|2025-10-23 00:04:15] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 00:04:15] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 00:04:15] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 00:06:35] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1300
[INFO|2025-10-23 00:06:35] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 00:06:35] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 00:06:35] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1300/chat_template.jinja
[INFO|2025-10-23 00:06:35] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1300/tokenizer_config.json
[INFO|2025-10-23 00:06:35] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1300/special_tokens_map.json
[INFO|2025-10-23 00:07:28] logging.py:143 >> {'loss': 0.9465, 'learning_rate': 8.3514e-07, 'epoch': 0.27, 'throughput': 1042.02}
[INFO|2025-10-23 00:08:22] logging.py:143 >> {'loss': 1.0023, 'learning_rate': 8.3395e-07, 'epoch': 0.27, 'throughput': 1042.47}
[INFO|2025-10-23 00:09:14] logging.py:143 >> {'loss': 0.9669, 'learning_rate': 8.3275e-07, 'epoch': 0.27, 'throughput': 1042.90}
[INFO|2025-10-23 00:10:07] logging.py:143 >> {'loss': 1.0117, 'learning_rate': 8.3155e-07, 'epoch': 0.27, 'throughput': 1043.36}
[INFO|2025-10-23 00:10:59] logging.py:143 >> {'loss': 0.9753, 'learning_rate': 8.3035e-07, 'epoch': 0.27, 'throughput': 1043.80}
[INFO|2025-10-23 00:11:53] logging.py:143 >> {'loss': 1.2425, 'learning_rate': 8.2915e-07, 'epoch': 0.27, 'throughput': 1044.23}
[INFO|2025-10-23 00:12:46] logging.py:143 >> {'loss': 0.8846, 'learning_rate': 8.2794e-07, 'epoch': 0.27, 'throughput': 1044.69}
[INFO|2025-10-23 00:13:39] logging.py:143 >> {'loss': 1.0828, 'learning_rate': 8.2672e-07, 'epoch': 0.27, 'throughput': 1045.13}
[INFO|2025-10-23 00:14:34] logging.py:143 >> {'loss': 1.0386, 'learning_rate': 8.2551e-07, 'epoch': 0.27, 'throughput': 1045.59}
[INFO|2025-10-23 00:15:26] logging.py:143 >> {'loss': 0.9580, 'learning_rate': 8.2429e-07, 'epoch': 0.28, 'throughput': 1046.02}
[INFO|2025-10-23 00:16:19] logging.py:143 >> {'loss': 1.0801, 'learning_rate': 8.2307e-07, 'epoch': 0.28, 'throughput': 1046.46}
[INFO|2025-10-23 00:17:12] logging.py:143 >> {'loss': 0.9979, 'learning_rate': 8.2184e-07, 'epoch': 0.28, 'throughput': 1046.89}
[INFO|2025-10-23 00:18:06] logging.py:143 >> {'loss': 1.0546, 'learning_rate': 8.2062e-07, 'epoch': 0.28, 'throughput': 1047.32}
[INFO|2025-10-23 00:18:59] logging.py:143 >> {'loss': 1.3725, 'learning_rate': 8.1938e-07, 'epoch': 0.28, 'throughput': 1047.73}
[INFO|2025-10-23 00:19:53] logging.py:143 >> {'loss': 1.0338, 'learning_rate': 8.1815e-07, 'epoch': 0.28, 'throughput': 1048.17}
[INFO|2025-10-23 00:20:46] logging.py:143 >> {'loss': 1.2381, 'learning_rate': 8.1691e-07, 'epoch': 0.28, 'throughput': 1048.58}
[INFO|2025-10-23 00:21:39] logging.py:143 >> {'loss': 1.1654, 'learning_rate': 8.1567e-07, 'epoch': 0.28, 'throughput': 1048.98}
[INFO|2025-10-23 00:22:33] logging.py:143 >> {'loss': 1.0128, 'learning_rate': 8.1442e-07, 'epoch': 0.28, 'throughput': 1049.40}
[INFO|2025-10-23 00:23:26] logging.py:143 >> {'loss': 0.9013, 'learning_rate': 8.1318e-07, 'epoch': 0.28, 'throughput': 1049.80}
[INFO|2025-10-23 00:24:19] logging.py:143 >> {'loss': 0.9842, 'learning_rate': 8.1192e-07, 'epoch': 0.29, 'throughput': 1050.19}
[INFO|2025-10-23 00:24:19] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 00:24:19] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 00:24:19] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 00:26:38] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1400
[INFO|2025-10-23 00:26:38] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 00:26:38] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 00:26:38] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1400/chat_template.jinja
[INFO|2025-10-23 00:26:38] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1400/tokenizer_config.json
[INFO|2025-10-23 00:26:38] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1400/special_tokens_map.json
[INFO|2025-10-23 00:27:31] logging.py:143 >> {'loss': 0.9814, 'learning_rate': 8.1067e-07, 'epoch': 0.29, 'throughput': 1041.94}
[INFO|2025-10-23 00:28:24] logging.py:143 >> {'loss': 1.0149, 'learning_rate': 8.0941e-07, 'epoch': 0.29, 'throughput': 1042.36}
[INFO|2025-10-23 00:29:17] logging.py:143 >> {'loss': 0.9884, 'learning_rate': 8.0815e-07, 'epoch': 0.29, 'throughput': 1042.78}
[INFO|2025-10-23 00:30:11] logging.py:143 >> {'loss': 1.0598, 'learning_rate': 8.0689e-07, 'epoch': 0.29, 'throughput': 1043.22}
[INFO|2025-10-23 00:31:04] logging.py:143 >> {'loss': 1.0783, 'learning_rate': 8.0562e-07, 'epoch': 0.29, 'throughput': 1043.64}
[INFO|2025-10-23 00:31:57] logging.py:143 >> {'loss': 0.8756, 'learning_rate': 8.0435e-07, 'epoch': 0.29, 'throughput': 1044.05}
[INFO|2025-10-23 00:32:50] logging.py:143 >> {'loss': 1.1578, 'learning_rate': 8.0308e-07, 'epoch': 0.29, 'throughput': 1044.44}
[INFO|2025-10-23 00:33:43] logging.py:143 >> {'loss': 1.5409, 'learning_rate': 8.0180e-07, 'epoch': 0.29, 'throughput': 1044.86}
[INFO|2025-10-23 00:34:36] logging.py:143 >> {'loss': 0.9140, 'learning_rate': 8.0052e-07, 'epoch': 0.29, 'throughput': 1045.25}
[INFO|2025-10-23 00:35:28] logging.py:143 >> {'loss': 1.1646, 'learning_rate': 7.9924e-07, 'epoch': 0.30, 'throughput': 1045.64}
[INFO|2025-10-23 00:36:21] logging.py:143 >> {'loss': 1.0132, 'learning_rate': 7.9795e-07, 'epoch': 0.30, 'throughput': 1046.05}
[INFO|2025-10-23 00:37:14] logging.py:143 >> {'loss': 1.0263, 'learning_rate': 7.9666e-07, 'epoch': 0.30, 'throughput': 1046.45}
[INFO|2025-10-23 00:38:08] logging.py:143 >> {'loss': 0.7597, 'learning_rate': 7.9537e-07, 'epoch': 0.30, 'throughput': 1046.86}
[INFO|2025-10-23 00:39:01] logging.py:143 >> {'loss': 0.7266, 'learning_rate': 7.9407e-07, 'epoch': 0.30, 'throughput': 1047.25}
[INFO|2025-10-23 00:39:55] logging.py:143 >> {'loss': 0.9457, 'learning_rate': 7.9278e-07, 'epoch': 0.30, 'throughput': 1047.66}
[INFO|2025-10-23 00:40:48] logging.py:143 >> {'loss': 0.9719, 'learning_rate': 7.9147e-07, 'epoch': 0.30, 'throughput': 1048.05}
[INFO|2025-10-23 00:41:42] logging.py:143 >> {'loss': 1.0129, 'learning_rate': 7.9017e-07, 'epoch': 0.30, 'throughput': 1048.46}
[INFO|2025-10-23 00:42:35] logging.py:143 >> {'loss': 1.2546, 'learning_rate': 7.8886e-07, 'epoch': 0.30, 'throughput': 1048.85}
[INFO|2025-10-23 00:43:29] logging.py:143 >> {'loss': 0.8799, 'learning_rate': 7.8755e-07, 'epoch': 0.31, 'throughput': 1049.26}
[INFO|2025-10-23 00:44:22] logging.py:143 >> {'loss': 1.0169, 'learning_rate': 7.8624e-07, 'epoch': 0.31, 'throughput': 1049.64}
[INFO|2025-10-23 00:44:22] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 00:44:22] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 00:44:22] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 00:46:41] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1500
[INFO|2025-10-23 00:46:41] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 00:46:41] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 00:46:41] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1500/chat_template.jinja
[INFO|2025-10-23 00:46:41] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1500/tokenizer_config.json
[INFO|2025-10-23 00:46:41] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1500/special_tokens_map.json
[INFO|2025-10-23 00:47:35] logging.py:143 >> {'loss': 1.1004, 'learning_rate': 7.8492e-07, 'epoch': 0.31, 'throughput': 1041.94}
[INFO|2025-10-23 00:48:28] logging.py:143 >> {'loss': 1.0180, 'learning_rate': 7.8361e-07, 'epoch': 0.31, 'throughput': 1042.35}
[INFO|2025-10-23 00:49:21] logging.py:143 >> {'loss': 0.7695, 'learning_rate': 7.8228e-07, 'epoch': 0.31, 'throughput': 1042.74}
[INFO|2025-10-23 00:50:15] logging.py:143 >> {'loss': 1.0941, 'learning_rate': 7.8096e-07, 'epoch': 0.31, 'throughput': 1043.14}
[INFO|2025-10-23 00:51:08] logging.py:143 >> {'loss': 1.2319, 'learning_rate': 7.7963e-07, 'epoch': 0.31, 'throughput': 1043.53}
[INFO|2025-10-23 00:52:01] logging.py:143 >> {'loss': 0.9940, 'learning_rate': 7.7830e-07, 'epoch': 0.31, 'throughput': 1043.92}
[INFO|2025-10-23 00:52:54] logging.py:143 >> {'loss': 0.7388, 'learning_rate': 7.7697e-07, 'epoch': 0.31, 'throughput': 1044.31}
[INFO|2025-10-23 00:53:48] logging.py:143 >> {'loss': 0.9228, 'learning_rate': 7.7563e-07, 'epoch': 0.31, 'throughput': 1044.70}
[INFO|2025-10-23 00:54:42] logging.py:143 >> {'loss': 0.9755, 'learning_rate': 7.7429e-07, 'epoch': 0.32, 'throughput': 1045.08}
[INFO|2025-10-23 00:55:35] logging.py:143 >> {'loss': 1.1448, 'learning_rate': 7.7295e-07, 'epoch': 0.32, 'throughput': 1045.45}
[INFO|2025-10-23 00:56:28] logging.py:143 >> {'loss': 1.0397, 'learning_rate': 7.7161e-07, 'epoch': 0.32, 'throughput': 1045.83}
[INFO|2025-10-23 00:57:20] logging.py:143 >> {'loss': 1.0535, 'learning_rate': 7.7026e-07, 'epoch': 0.32, 'throughput': 1046.20}
[INFO|2025-10-23 00:58:13] logging.py:143 >> {'loss': 1.1016, 'learning_rate': 7.6891e-07, 'epoch': 0.32, 'throughput': 1046.57}
[INFO|2025-10-23 00:59:06] logging.py:143 >> {'loss': 0.8962, 'learning_rate': 7.6756e-07, 'epoch': 0.32, 'throughput': 1046.95}
[INFO|2025-10-23 00:59:59] logging.py:143 >> {'loss': 0.9167, 'learning_rate': 7.6620e-07, 'epoch': 0.32, 'throughput': 1047.31}
[INFO|2025-10-23 01:00:52] logging.py:143 >> {'loss': 1.0814, 'learning_rate': 7.6484e-07, 'epoch': 0.32, 'throughput': 1047.67}
[INFO|2025-10-23 01:01:45] logging.py:143 >> {'loss': 1.2117, 'learning_rate': 7.6348e-07, 'epoch': 0.32, 'throughput': 1048.04}
[INFO|2025-10-23 01:02:39] logging.py:143 >> {'loss': 1.0034, 'learning_rate': 7.6212e-07, 'epoch': 0.32, 'throughput': 1048.40}
[INFO|2025-10-23 01:03:32] logging.py:143 >> {'loss': 1.1208, 'learning_rate': 7.6075e-07, 'epoch': 0.33, 'throughput': 1048.77}
[INFO|2025-10-23 01:04:25] logging.py:143 >> {'loss': 1.0352, 'learning_rate': 7.5938e-07, 'epoch': 0.33, 'throughput': 1049.13}
[INFO|2025-10-23 01:04:25] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 01:04:25] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 01:04:25] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 01:06:44] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1600
[INFO|2025-10-23 01:06:44] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 01:06:44] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 01:06:44] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1600/chat_template.jinja
[INFO|2025-10-23 01:06:44] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1600/tokenizer_config.json
[INFO|2025-10-23 01:06:44] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1600/special_tokens_map.json
[INFO|2025-10-23 01:07:38] logging.py:143 >> {'loss': 0.9776, 'learning_rate': 7.5801e-07, 'epoch': 0.33, 'throughput': 1041.92}
[INFO|2025-10-23 01:08:31] logging.py:143 >> {'loss': 1.0998, 'learning_rate': 7.5663e-07, 'epoch': 0.33, 'throughput': 1042.29}
[INFO|2025-10-23 01:09:24] logging.py:143 >> {'loss': 0.9538, 'learning_rate': 7.5526e-07, 'epoch': 0.33, 'throughput': 1042.65}
[INFO|2025-10-23 01:10:18] logging.py:143 >> {'loss': 1.0532, 'learning_rate': 7.5388e-07, 'epoch': 0.33, 'throughput': 1043.03}
[INFO|2025-10-23 01:11:11] logging.py:143 >> {'loss': 1.1181, 'learning_rate': 7.5249e-07, 'epoch': 0.33, 'throughput': 1043.40}
[INFO|2025-10-23 01:12:05] logging.py:143 >> {'loss': 1.1795, 'learning_rate': 7.5111e-07, 'epoch': 0.33, 'throughput': 1043.77}
[INFO|2025-10-23 01:12:57] logging.py:143 >> {'loss': 1.0880, 'learning_rate': 7.4972e-07, 'epoch': 0.33, 'throughput': 1044.13}
[INFO|2025-10-23 01:13:51] logging.py:143 >> {'loss': 1.0704, 'learning_rate': 7.4833e-07, 'epoch': 0.33, 'throughput': 1044.50}
[INFO|2025-10-23 01:14:44] logging.py:143 >> {'loss': 0.9455, 'learning_rate': 7.4694e-07, 'epoch': 0.34, 'throughput': 1044.87}
[INFO|2025-10-23 01:15:37] logging.py:143 >> {'loss': 0.9585, 'learning_rate': 7.4554e-07, 'epoch': 0.34, 'throughput': 1045.24}
[INFO|2025-10-23 01:16:31] logging.py:143 >> {'loss': 0.8266, 'learning_rate': 7.4415e-07, 'epoch': 0.34, 'throughput': 1045.59}
[INFO|2025-10-23 01:17:24] logging.py:143 >> {'loss': 0.9388, 'learning_rate': 7.4275e-07, 'epoch': 0.34, 'throughput': 1045.94}
[INFO|2025-10-23 01:18:17] logging.py:143 >> {'loss': 1.4145, 'learning_rate': 7.4134e-07, 'epoch': 0.34, 'throughput': 1046.29}
[INFO|2025-10-23 01:19:10] logging.py:143 >> {'loss': 0.9753, 'learning_rate': 7.3994e-07, 'epoch': 0.34, 'throughput': 1046.63}
[INFO|2025-10-23 01:20:04] logging.py:143 >> {'loss': 0.9230, 'learning_rate': 7.3853e-07, 'epoch': 0.34, 'throughput': 1046.97}
[INFO|2025-10-23 01:20:57] logging.py:143 >> {'loss': 0.9876, 'learning_rate': 7.3712e-07, 'epoch': 0.34, 'throughput': 1047.33}
[INFO|2025-10-23 01:21:51] logging.py:143 >> {'loss': 0.9719, 'learning_rate': 7.3571e-07, 'epoch': 0.34, 'throughput': 1047.68}
[INFO|2025-10-23 01:22:44] logging.py:143 >> {'loss': 0.9784, 'learning_rate': 7.3429e-07, 'epoch': 0.35, 'throughput': 1048.03}
[INFO|2025-10-23 01:23:37] logging.py:143 >> {'loss': 1.1203, 'learning_rate': 7.3287e-07, 'epoch': 0.35, 'throughput': 1048.36}
[INFO|2025-10-23 01:24:31] logging.py:143 >> {'loss': 0.9325, 'learning_rate': 7.3145e-07, 'epoch': 0.35, 'throughput': 1048.72}
[INFO|2025-10-23 01:24:31] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 01:24:31] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 01:24:31] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 01:26:50] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1700
[INFO|2025-10-23 01:26:50] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 01:26:50] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 01:26:50] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1700/chat_template.jinja
[INFO|2025-10-23 01:26:50] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1700/tokenizer_config.json
[INFO|2025-10-23 01:26:50] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1700/special_tokens_map.json
[INFO|2025-10-23 01:27:44] logging.py:143 >> {'loss': 1.1657, 'learning_rate': 7.3003e-07, 'epoch': 0.35, 'throughput': 1041.91}
[INFO|2025-10-23 01:28:38] logging.py:143 >> {'loss': 0.9070, 'learning_rate': 7.2861e-07, 'epoch': 0.35, 'throughput': 1042.28}
[INFO|2025-10-23 01:29:31] logging.py:143 >> {'loss': 0.9725, 'learning_rate': 7.2718e-07, 'epoch': 0.35, 'throughput': 1042.63}
[INFO|2025-10-23 01:30:25] logging.py:143 >> {'loss': 1.0485, 'learning_rate': 7.2575e-07, 'epoch': 0.35, 'throughput': 1042.99}
[INFO|2025-10-23 01:31:19] logging.py:143 >> {'loss': 0.8721, 'learning_rate': 7.2432e-07, 'epoch': 0.35, 'throughput': 1043.35}
[INFO|2025-10-23 01:32:12] logging.py:143 >> {'loss': 0.8712, 'learning_rate': 7.2289e-07, 'epoch': 0.35, 'throughput': 1043.70}
[INFO|2025-10-23 01:33:05] logging.py:143 >> {'loss': 0.9553, 'learning_rate': 7.2145e-07, 'epoch': 0.35, 'throughput': 1044.05}
[INFO|2025-10-23 01:33:58] logging.py:143 >> {'loss': 0.9564, 'learning_rate': 7.2001e-07, 'epoch': 0.36, 'throughput': 1044.39}
[INFO|2025-10-23 01:34:51] logging.py:143 >> {'loss': 1.0089, 'learning_rate': 7.1857e-07, 'epoch': 0.36, 'throughput': 1044.74}
[INFO|2025-10-23 01:35:45] logging.py:143 >> {'loss': 1.0446, 'learning_rate': 7.1713e-07, 'epoch': 0.36, 'throughput': 1045.10}
[INFO|2025-10-23 01:36:38] logging.py:143 >> {'loss': 1.1582, 'learning_rate': 7.1568e-07, 'epoch': 0.36, 'throughput': 1045.44}
[INFO|2025-10-23 01:37:31] logging.py:143 >> {'loss': 0.8331, 'learning_rate': 7.1423e-07, 'epoch': 0.36, 'throughput': 1045.78}
[INFO|2025-10-23 01:38:23] logging.py:143 >> {'loss': 1.0491, 'learning_rate': 7.1279e-07, 'epoch': 0.36, 'throughput': 1046.10}
[INFO|2025-10-23 01:39:17] logging.py:143 >> {'loss': 1.1038, 'learning_rate': 7.1133e-07, 'epoch': 0.36, 'throughput': 1046.44}
[INFO|2025-10-23 01:40:10] logging.py:143 >> {'loss': 0.8801, 'learning_rate': 7.0988e-07, 'epoch': 0.36, 'throughput': 1046.77}
[INFO|2025-10-23 01:41:03] logging.py:143 >> {'loss': 1.1868, 'learning_rate': 7.0842e-07, 'epoch': 0.36, 'throughput': 1047.10}
[INFO|2025-10-23 01:41:56] logging.py:143 >> {'loss': 1.1606, 'learning_rate': 7.0696e-07, 'epoch': 0.36, 'throughput': 1047.44}
[INFO|2025-10-23 01:42:49] logging.py:143 >> {'loss': 1.1343, 'learning_rate': 7.0550e-07, 'epoch': 0.37, 'throughput': 1047.77}
[INFO|2025-10-23 01:43:42] logging.py:143 >> {'loss': 0.9881, 'learning_rate': 7.0404e-07, 'epoch': 0.37, 'throughput': 1048.09}
[INFO|2025-10-23 01:44:35] logging.py:143 >> {'loss': 1.1896, 'learning_rate': 7.0258e-07, 'epoch': 0.37, 'throughput': 1048.42}
[INFO|2025-10-23 01:44:35] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 01:44:35] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 01:44:35] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 01:46:54] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1800
[INFO|2025-10-23 01:46:54] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 01:46:54] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 01:46:54] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1800/chat_template.jinja
[INFO|2025-10-23 01:46:54] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1800/tokenizer_config.json
[INFO|2025-10-23 01:46:54] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1800/special_tokens_map.json
[INFO|2025-10-23 01:47:48] logging.py:143 >> {'loss': 1.0538, 'learning_rate': 7.0111e-07, 'epoch': 0.37, 'throughput': 1042.02}
[INFO|2025-10-23 01:48:42] logging.py:143 >> {'loss': 1.2133, 'learning_rate': 6.9964e-07, 'epoch': 0.37, 'throughput': 1042.36}
[INFO|2025-10-23 01:49:35] logging.py:143 >> {'loss': 1.1914, 'learning_rate': 6.9817e-07, 'epoch': 0.37, 'throughput': 1042.69}
[INFO|2025-10-23 01:50:29] logging.py:143 >> {'loss': 0.8921, 'learning_rate': 6.9670e-07, 'epoch': 0.37, 'throughput': 1043.03}
[INFO|2025-10-23 01:51:22] logging.py:143 >> {'loss': 1.2736, 'learning_rate': 6.9522e-07, 'epoch': 0.37, 'throughput': 1043.36}
[INFO|2025-10-23 01:52:16] logging.py:143 >> {'loss': 1.2774, 'learning_rate': 6.9375e-07, 'epoch': 0.37, 'throughput': 1043.68}
[INFO|2025-10-23 01:53:09] logging.py:143 >> {'loss': 0.9000, 'learning_rate': 6.9227e-07, 'epoch': 0.37, 'throughput': 1044.02}
[INFO|2025-10-23 01:54:03] logging.py:143 >> {'loss': 1.0790, 'learning_rate': 6.9079e-07, 'epoch': 0.38, 'throughput': 1044.35}
[INFO|2025-10-23 01:54:55] logging.py:143 >> {'loss': 0.8975, 'learning_rate': 6.8930e-07, 'epoch': 0.38, 'throughput': 1044.66}
[INFO|2025-10-23 01:55:50] logging.py:143 >> {'loss': 1.2429, 'learning_rate': 6.8782e-07, 'epoch': 0.38, 'throughput': 1045.00}
[INFO|2025-10-23 01:56:43] logging.py:143 >> {'loss': 0.8476, 'learning_rate': 6.8633e-07, 'epoch': 0.38, 'throughput': 1045.30}
[INFO|2025-10-23 01:57:36] logging.py:143 >> {'loss': 1.0341, 'learning_rate': 6.8484e-07, 'epoch': 0.38, 'throughput': 1045.62}
[INFO|2025-10-23 01:58:29] logging.py:143 >> {'loss': 0.8869, 'learning_rate': 6.8335e-07, 'epoch': 0.38, 'throughput': 1045.94}
[INFO|2025-10-23 01:59:23] logging.py:143 >> {'loss': 1.0505, 'learning_rate': 6.8186e-07, 'epoch': 0.38, 'throughput': 1046.26}
[INFO|2025-10-23 02:00:17] logging.py:143 >> {'loss': 0.9419, 'learning_rate': 6.8037e-07, 'epoch': 0.38, 'throughput': 1046.57}
[INFO|2025-10-23 02:01:10] logging.py:143 >> {'loss': 1.0540, 'learning_rate': 6.7887e-07, 'epoch': 0.38, 'throughput': 1046.88}
[INFO|2025-10-23 02:02:02] logging.py:143 >> {'loss': 0.9194, 'learning_rate': 6.7737e-07, 'epoch': 0.38, 'throughput': 1047.18}
[INFO|2025-10-23 02:02:55] logging.py:143 >> {'loss': 0.8349, 'learning_rate': 6.7587e-07, 'epoch': 0.39, 'throughput': 1047.49}
[INFO|2025-10-23 02:03:48] logging.py:143 >> {'loss': 0.9589, 'learning_rate': 6.7437e-07, 'epoch': 0.39, 'throughput': 1047.79}
[INFO|2025-10-23 02:04:42] logging.py:143 >> {'loss': 0.8863, 'learning_rate': 6.7287e-07, 'epoch': 0.39, 'throughput': 1048.11}
[INFO|2025-10-23 02:04:42] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 02:04:42] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 02:04:42] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 02:07:01] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1900
[INFO|2025-10-23 02:07:01] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 02:07:01] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 02:07:01] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1900/chat_template.jinja
[INFO|2025-10-23 02:07:01] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1900/tokenizer_config.json
[INFO|2025-10-23 02:07:01] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-1900/special_tokens_map.json
[INFO|2025-10-23 02:07:55] logging.py:143 >> {'loss': 1.1109, 'learning_rate': 6.7136e-07, 'epoch': 0.39, 'throughput': 1042.05}
[INFO|2025-10-23 02:08:47] logging.py:143 >> {'loss': 0.9661, 'learning_rate': 6.6985e-07, 'epoch': 0.39, 'throughput': 1042.35}
[INFO|2025-10-23 02:09:40] logging.py:143 >> {'loss': 1.0663, 'learning_rate': 6.6835e-07, 'epoch': 0.39, 'throughput': 1042.67}
[INFO|2025-10-23 02:10:33] logging.py:143 >> {'loss': 1.1440, 'learning_rate': 6.6684e-07, 'epoch': 0.39, 'throughput': 1042.97}
[INFO|2025-10-23 02:11:26] logging.py:143 >> {'loss': 1.0181, 'learning_rate': 6.6532e-07, 'epoch': 0.39, 'throughput': 1043.27}
[INFO|2025-10-23 02:12:19] logging.py:143 >> {'loss': 0.8789, 'learning_rate': 6.6381e-07, 'epoch': 0.39, 'throughput': 1043.58}
[INFO|2025-10-23 02:13:12] logging.py:143 >> {'loss': 1.3359, 'learning_rate': 6.6229e-07, 'epoch': 0.40, 'throughput': 1043.89}
[INFO|2025-10-23 02:14:05] logging.py:143 >> {'loss': 1.1086, 'learning_rate': 6.6078e-07, 'epoch': 0.40, 'throughput': 1044.19}
[INFO|2025-10-23 02:14:59] logging.py:143 >> {'loss': 1.0226, 'learning_rate': 6.5926e-07, 'epoch': 0.40, 'throughput': 1044.50}
[INFO|2025-10-23 02:15:51] logging.py:143 >> {'loss': 1.0139, 'learning_rate': 6.5774e-07, 'epoch': 0.40, 'throughput': 1044.81}
[INFO|2025-10-23 02:16:44] logging.py:143 >> {'loss': 1.0039, 'learning_rate': 6.5622e-07, 'epoch': 0.40, 'throughput': 1045.11}
[INFO|2025-10-23 02:17:37] logging.py:143 >> {'loss': 1.1525, 'learning_rate': 6.5469e-07, 'epoch': 0.40, 'throughput': 1045.40}
[INFO|2025-10-23 02:18:31] logging.py:143 >> {'loss': 0.7880, 'learning_rate': 6.5317e-07, 'epoch': 0.40, 'throughput': 1045.72}
[INFO|2025-10-23 02:19:24] logging.py:143 >> {'loss': 1.1022, 'learning_rate': 6.5164e-07, 'epoch': 0.40, 'throughput': 1046.02}
[INFO|2025-10-23 02:20:16] logging.py:143 >> {'loss': 1.0142, 'learning_rate': 6.5011e-07, 'epoch': 0.40, 'throughput': 1046.32}
[INFO|2025-10-23 02:21:10] logging.py:143 >> {'loss': 1.1866, 'learning_rate': 6.4858e-07, 'epoch': 0.40, 'throughput': 1046.62}
[INFO|2025-10-23 02:22:03] logging.py:143 >> {'loss': 0.9087, 'learning_rate': 6.4705e-07, 'epoch': 0.41, 'throughput': 1046.93}
[INFO|2025-10-23 02:22:56] logging.py:143 >> {'loss': 1.0124, 'learning_rate': 6.4552e-07, 'epoch': 0.41, 'throughput': 1047.22}
[INFO|2025-10-23 02:23:50] logging.py:143 >> {'loss': 0.8369, 'learning_rate': 6.4398e-07, 'epoch': 0.41, 'throughput': 1047.52}
[INFO|2025-10-23 02:24:43] logging.py:143 >> {'loss': 1.1709, 'learning_rate': 6.4245e-07, 'epoch': 0.41, 'throughput': 1047.81}
[INFO|2025-10-23 02:24:43] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 02:24:43] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 02:24:43] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 02:27:02] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2000
[INFO|2025-10-23 02:27:02] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 02:27:02] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 02:27:02] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2000/chat_template.jinja
[INFO|2025-10-23 02:27:02] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2000/tokenizer_config.json
[INFO|2025-10-23 02:27:02] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2000/special_tokens_map.json
[INFO|2025-10-23 02:27:55] logging.py:143 >> {'loss': 0.9597, 'learning_rate': 6.4091e-07, 'epoch': 0.41, 'throughput': 1042.02}
[INFO|2025-10-23 02:28:49] logging.py:143 >> {'loss': 1.1645, 'learning_rate': 6.3937e-07, 'epoch': 0.41, 'throughput': 1042.33}
[INFO|2025-10-23 02:29:42] logging.py:143 >> {'loss': 1.1233, 'learning_rate': 6.3783e-07, 'epoch': 0.41, 'throughput': 1042.61}
[INFO|2025-10-23 02:30:35] logging.py:143 >> {'loss': 0.8682, 'learning_rate': 6.3629e-07, 'epoch': 0.41, 'throughput': 1042.89}
[INFO|2025-10-23 02:31:28] logging.py:143 >> {'loss': 1.1038, 'learning_rate': 6.3474e-07, 'epoch': 0.41, 'throughput': 1043.18}
[INFO|2025-10-23 02:32:21] logging.py:143 >> {'loss': 1.3148, 'learning_rate': 6.3320e-07, 'epoch': 0.41, 'throughput': 1043.47}
[INFO|2025-10-23 02:33:14] logging.py:143 >> {'loss': 1.0323, 'learning_rate': 6.3165e-07, 'epoch': 0.42, 'throughput': 1043.76}
[INFO|2025-10-23 02:34:08] logging.py:143 >> {'loss': 1.0603, 'learning_rate': 6.3011e-07, 'epoch': 0.42, 'throughput': 1044.06}
[INFO|2025-10-23 02:35:01] logging.py:143 >> {'loss': 1.0977, 'learning_rate': 6.2856e-07, 'epoch': 0.42, 'throughput': 1044.36}
[INFO|2025-10-23 02:35:55] logging.py:143 >> {'loss': 0.9059, 'learning_rate': 6.2701e-07, 'epoch': 0.42, 'throughput': 1044.65}
[INFO|2025-10-23 02:36:48] logging.py:143 >> {'loss': 0.8020, 'learning_rate': 6.2546e-07, 'epoch': 0.42, 'throughput': 1044.94}
[INFO|2025-10-23 02:37:42] logging.py:143 >> {'loss': 0.7879, 'learning_rate': 6.2390e-07, 'epoch': 0.42, 'throughput': 1045.22}
[INFO|2025-10-23 02:38:35] logging.py:143 >> {'loss': 1.0665, 'learning_rate': 6.2235e-07, 'epoch': 0.42, 'throughput': 1045.51}
[INFO|2025-10-23 02:39:28] logging.py:143 >> {'loss': 0.9241, 'learning_rate': 6.2080e-07, 'epoch': 0.42, 'throughput': 1045.80}
[INFO|2025-10-23 02:40:22] logging.py:143 >> {'loss': 1.0607, 'learning_rate': 6.1924e-07, 'epoch': 0.42, 'throughput': 1046.10}
[INFO|2025-10-23 02:41:16] logging.py:143 >> {'loss': 0.8482, 'learning_rate': 6.1768e-07, 'epoch': 0.42, 'throughput': 1046.37}
[INFO|2025-10-23 02:42:09] logging.py:143 >> {'loss': 0.8877, 'learning_rate': 6.1612e-07, 'epoch': 0.43, 'throughput': 1046.66}
[INFO|2025-10-23 02:43:02] logging.py:143 >> {'loss': 1.1654, 'learning_rate': 6.1456e-07, 'epoch': 0.43, 'throughput': 1046.94}
[INFO|2025-10-23 02:43:55] logging.py:143 >> {'loss': 0.9624, 'learning_rate': 6.1300e-07, 'epoch': 0.43, 'throughput': 1047.22}
[INFO|2025-10-23 02:44:48] logging.py:143 >> {'loss': 1.2518, 'learning_rate': 6.1144e-07, 'epoch': 0.43, 'throughput': 1047.49}
[INFO|2025-10-23 02:44:48] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 02:44:48] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 02:44:48] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 02:47:07] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2100
[INFO|2025-10-23 02:47:07] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 02:47:07] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 02:47:07] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2100/chat_template.jinja
[INFO|2025-10-23 02:47:07] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2100/tokenizer_config.json
[INFO|2025-10-23 02:47:07] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2100/special_tokens_map.json
[INFO|2025-10-23 02:48:00] logging.py:143 >> {'loss': 0.9762, 'learning_rate': 6.0988e-07, 'epoch': 0.43, 'throughput': 1042.01}
[INFO|2025-10-23 02:48:53] logging.py:143 >> {'loss': 0.8160, 'learning_rate': 6.0831e-07, 'epoch': 0.43, 'throughput': 1042.29}
[INFO|2025-10-23 02:49:46] logging.py:143 >> {'loss': 1.1025, 'learning_rate': 6.0675e-07, 'epoch': 0.43, 'throughput': 1042.57}
[INFO|2025-10-23 02:50:39] logging.py:143 >> {'loss': 1.0581, 'learning_rate': 6.0518e-07, 'epoch': 0.43, 'throughput': 1042.85}
[INFO|2025-10-23 02:51:32] logging.py:143 >> {'loss': 1.1524, 'learning_rate': 6.0361e-07, 'epoch': 0.43, 'throughput': 1043.13}
[INFO|2025-10-23 02:52:25] logging.py:143 >> {'loss': 1.2915, 'learning_rate': 6.0204e-07, 'epoch': 0.43, 'throughput': 1043.41}
[INFO|2025-10-23 02:53:19] logging.py:143 >> {'loss': 1.0366, 'learning_rate': 6.0047e-07, 'epoch': 0.44, 'throughput': 1043.69}
[INFO|2025-10-23 02:54:12] logging.py:143 >> {'loss': 0.8525, 'learning_rate': 5.9890e-07, 'epoch': 0.44, 'throughput': 1043.97}
[INFO|2025-10-23 02:55:06] logging.py:143 >> {'loss': 0.8329, 'learning_rate': 5.9733e-07, 'epoch': 0.44, 'throughput': 1044.24}
[INFO|2025-10-23 02:55:59] logging.py:143 >> {'loss': 1.2550, 'learning_rate': 5.9576e-07, 'epoch': 0.44, 'throughput': 1044.53}
[INFO|2025-10-23 02:56:52] logging.py:143 >> {'loss': 1.0816, 'learning_rate': 5.9418e-07, 'epoch': 0.44, 'throughput': 1044.81}
[INFO|2025-10-23 02:57:46] logging.py:143 >> {'loss': 1.1206, 'learning_rate': 5.9261e-07, 'epoch': 0.44, 'throughput': 1045.07}
[INFO|2025-10-23 02:58:40] logging.py:143 >> {'loss': 1.0948, 'learning_rate': 5.9103e-07, 'epoch': 0.44, 'throughput': 1045.36}
[INFO|2025-10-23 02:59:32] logging.py:143 >> {'loss': 1.0576, 'learning_rate': 5.8945e-07, 'epoch': 0.44, 'throughput': 1045.62}
[INFO|2025-10-23 03:00:25] logging.py:143 >> {'loss': 1.1917, 'learning_rate': 5.8788e-07, 'epoch': 0.44, 'throughput': 1045.89}
[INFO|2025-10-23 03:01:18] logging.py:143 >> {'loss': 0.8717, 'learning_rate': 5.8630e-07, 'epoch': 0.45, 'throughput': 1046.16}
[INFO|2025-10-23 03:02:11] logging.py:143 >> {'loss': 0.6681, 'learning_rate': 5.8472e-07, 'epoch': 0.45, 'throughput': 1046.41}
[INFO|2025-10-23 03:03:04] logging.py:143 >> {'loss': 0.8024, 'learning_rate': 5.8314e-07, 'epoch': 0.45, 'throughput': 1046.69}
[INFO|2025-10-23 03:03:58] logging.py:143 >> {'loss': 0.9990, 'learning_rate': 5.8156e-07, 'epoch': 0.45, 'throughput': 1046.97}
[INFO|2025-10-23 03:04:51] logging.py:143 >> {'loss': 1.0195, 'learning_rate': 5.7997e-07, 'epoch': 0.45, 'throughput': 1047.23}
[INFO|2025-10-23 03:04:51] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 03:04:51] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 03:04:51] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 03:07:10] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2200
[INFO|2025-10-23 03:07:10] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 03:07:10] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 03:07:10] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2200/chat_template.jinja
[INFO|2025-10-23 03:07:10] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2200/tokenizer_config.json
[INFO|2025-10-23 03:07:10] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2200/special_tokens_map.json
[INFO|2025-10-23 03:08:04] logging.py:143 >> {'loss': 1.0821, 'learning_rate': 5.7839e-07, 'epoch': 0.45, 'throughput': 1041.98}
[INFO|2025-10-23 03:08:57] logging.py:143 >> {'loss': 0.9579, 'learning_rate': 5.7681e-07, 'epoch': 0.45, 'throughput': 1042.25}
[INFO|2025-10-23 03:09:51] logging.py:143 >> {'loss': 1.1360, 'learning_rate': 5.7522e-07, 'epoch': 0.45, 'throughput': 1042.54}
[INFO|2025-10-23 03:10:44] logging.py:143 >> {'loss': 1.2460, 'learning_rate': 5.7364e-07, 'epoch': 0.45, 'throughput': 1042.80}
[INFO|2025-10-23 03:11:37] logging.py:143 >> {'loss': 1.1585, 'learning_rate': 5.7205e-07, 'epoch': 0.45, 'throughput': 1043.07}
[INFO|2025-10-23 03:12:30] logging.py:143 >> {'loss': 1.1864, 'learning_rate': 5.7046e-07, 'epoch': 0.46, 'throughput': 1043.34}
[INFO|2025-10-23 03:13:23] logging.py:143 >> {'loss': 0.9287, 'learning_rate': 5.6888e-07, 'epoch': 0.46, 'throughput': 1043.61}
[INFO|2025-10-23 03:14:16] logging.py:143 >> {'loss': 1.0120, 'learning_rate': 5.6729e-07, 'epoch': 0.46, 'throughput': 1043.87}
[INFO|2025-10-23 03:15:10] logging.py:143 >> {'loss': 0.9679, 'learning_rate': 5.6570e-07, 'epoch': 0.46, 'throughput': 1044.15}
[INFO|2025-10-23 03:16:03] logging.py:143 >> {'loss': 1.1017, 'learning_rate': 5.6411e-07, 'epoch': 0.46, 'throughput': 1044.41}
[INFO|2025-10-23 03:16:56] logging.py:143 >> {'loss': 1.0213, 'learning_rate': 5.6252e-07, 'epoch': 0.46, 'throughput': 1044.67}
[INFO|2025-10-23 03:17:49] logging.py:143 >> {'loss': 0.9343, 'learning_rate': 5.6093e-07, 'epoch': 0.46, 'throughput': 1044.93}
[INFO|2025-10-23 03:18:42] logging.py:143 >> {'loss': 1.0181, 'learning_rate': 5.5934e-07, 'epoch': 0.46, 'throughput': 1045.20}
[INFO|2025-10-23 03:19:35] logging.py:143 >> {'loss': 1.0523, 'learning_rate': 5.5775e-07, 'epoch': 0.46, 'throughput': 1045.46}
[INFO|2025-10-23 03:20:28] logging.py:143 >> {'loss': 0.9987, 'learning_rate': 5.5615e-07, 'epoch': 0.46, 'throughput': 1045.71}
[INFO|2025-10-23 03:21:21] logging.py:143 >> {'loss': 1.0277, 'learning_rate': 5.5456e-07, 'epoch': 0.47, 'throughput': 1045.97}
[INFO|2025-10-23 03:22:14] logging.py:143 >> {'loss': 0.9190, 'learning_rate': 5.5297e-07, 'epoch': 0.47, 'throughput': 1046.22}
[INFO|2025-10-23 03:23:07] logging.py:143 >> {'loss': 1.1417, 'learning_rate': 5.5137e-07, 'epoch': 0.47, 'throughput': 1046.48}
[INFO|2025-10-23 03:24:00] logging.py:143 >> {'loss': 1.1677, 'learning_rate': 5.4978e-07, 'epoch': 0.47, 'throughput': 1046.73}
[INFO|2025-10-23 03:24:53] logging.py:143 >> {'loss': 1.1781, 'learning_rate': 5.4818e-07, 'epoch': 0.47, 'throughput': 1046.98}
[INFO|2025-10-23 03:24:53] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 03:24:53] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 03:24:53] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 03:27:12] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2300
[INFO|2025-10-23 03:27:12] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 03:27:12] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 03:27:12] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2300/chat_template.jinja
[INFO|2025-10-23 03:27:12] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2300/tokenizer_config.json
[INFO|2025-10-23 03:27:12] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2300/special_tokens_map.json
[INFO|2025-10-23 03:28:05] logging.py:143 >> {'loss': 1.0602, 'learning_rate': 5.4658e-07, 'epoch': 0.47, 'throughput': 1041.96}
[INFO|2025-10-23 03:28:59] logging.py:143 >> {'loss': 0.8754, 'learning_rate': 5.4499e-07, 'epoch': 0.47, 'throughput': 1042.22}
[INFO|2025-10-23 03:29:51] logging.py:143 >> {'loss': 0.9987, 'learning_rate': 5.4339e-07, 'epoch': 0.47, 'throughput': 1042.47}
[INFO|2025-10-23 03:30:44] logging.py:143 >> {'loss': 0.8658, 'learning_rate': 5.4179e-07, 'epoch': 0.47, 'throughput': 1042.73}
[INFO|2025-10-23 03:31:37] logging.py:143 >> {'loss': 1.1730, 'learning_rate': 5.4020e-07, 'epoch': 0.47, 'throughput': 1042.98}
[INFO|2025-10-23 03:32:30] logging.py:143 >> {'loss': 1.0846, 'learning_rate': 5.3860e-07, 'epoch': 0.48, 'throughput': 1043.24}
[INFO|2025-10-23 03:33:23] logging.py:143 >> {'loss': 0.8456, 'learning_rate': 5.3700e-07, 'epoch': 0.48, 'throughput': 1043.50}
[INFO|2025-10-23 03:34:16] logging.py:143 >> {'loss': 1.0864, 'learning_rate': 5.3540e-07, 'epoch': 0.48, 'throughput': 1043.76}
[INFO|2025-10-23 03:35:09] logging.py:143 >> {'loss': 0.9268, 'learning_rate': 5.3380e-07, 'epoch': 0.48, 'throughput': 1044.00}
[INFO|2025-10-23 03:36:03] logging.py:143 >> {'loss': 0.9986, 'learning_rate': 5.3220e-07, 'epoch': 0.48, 'throughput': 1044.26}
[INFO|2025-10-23 03:36:56] logging.py:143 >> {'loss': 1.0729, 'learning_rate': 5.3060e-07, 'epoch': 0.48, 'throughput': 1044.52}
[INFO|2025-10-23 03:37:49] logging.py:143 >> {'loss': 1.0069, 'learning_rate': 5.2900e-07, 'epoch': 0.48, 'throughput': 1044.77}
[INFO|2025-10-23 03:38:43] logging.py:143 >> {'loss': 0.9939, 'learning_rate': 5.2740e-07, 'epoch': 0.48, 'throughput': 1045.03}
[INFO|2025-10-23 03:39:35] logging.py:143 >> {'loss': 1.0955, 'learning_rate': 5.2580e-07, 'epoch': 0.48, 'throughput': 1045.27}
[INFO|2025-10-23 03:40:28] logging.py:143 >> {'loss': 1.0197, 'learning_rate': 5.2420e-07, 'epoch': 0.48, 'throughput': 1045.51}
[INFO|2025-10-23 03:41:21] logging.py:143 >> {'loss': 0.9831, 'learning_rate': 5.2260e-07, 'epoch': 0.49, 'throughput': 1045.75}
[INFO|2025-10-23 03:42:14] logging.py:143 >> {'loss': 0.9834, 'learning_rate': 5.2100e-07, 'epoch': 0.49, 'throughput': 1046.00}
[INFO|2025-10-23 03:43:07] logging.py:143 >> {'loss': 0.8965, 'learning_rate': 5.1939e-07, 'epoch': 0.49, 'throughput': 1046.24}
[INFO|2025-10-23 03:44:00] logging.py:143 >> {'loss': 0.8017, 'learning_rate': 5.1779e-07, 'epoch': 0.49, 'throughput': 1046.48}
[INFO|2025-10-23 03:44:53] logging.py:143 >> {'loss': 0.6951, 'learning_rate': 5.1619e-07, 'epoch': 0.49, 'throughput': 1046.73}
[INFO|2025-10-23 03:44:53] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 03:44:53] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 03:44:53] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 03:47:12] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2400
[INFO|2025-10-23 03:47:12] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 03:47:12] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 03:47:12] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2400/chat_template.jinja
[INFO|2025-10-23 03:47:12] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2400/tokenizer_config.json
[INFO|2025-10-23 03:47:12] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2400/special_tokens_map.json
[INFO|2025-10-23 03:48:06] logging.py:143 >> {'loss': 1.0059, 'learning_rate': 5.1459e-07, 'epoch': 0.49, 'throughput': 1041.93}
[INFO|2025-10-23 03:49:00] logging.py:143 >> {'loss': 1.0160, 'learning_rate': 5.1298e-07, 'epoch': 0.49, 'throughput': 1042.20}
[INFO|2025-10-23 03:49:53] logging.py:143 >> {'loss': 0.9791, 'learning_rate': 5.1138e-07, 'epoch': 0.49, 'throughput': 1042.46}
[INFO|2025-10-23 03:50:46] logging.py:143 >> {'loss': 1.0246, 'learning_rate': 5.0978e-07, 'epoch': 0.49, 'throughput': 1042.70}
[INFO|2025-10-23 03:51:39] logging.py:143 >> {'loss': 0.9300, 'learning_rate': 5.0818e-07, 'epoch': 0.50, 'throughput': 1042.96}
[INFO|2025-10-23 03:52:33] logging.py:143 >> {'loss': 1.0642, 'learning_rate': 5.0657e-07, 'epoch': 0.50, 'throughput': 1043.21}
[INFO|2025-10-23 03:53:27] logging.py:143 >> {'loss': 1.2029, 'learning_rate': 5.0497e-07, 'epoch': 0.50, 'throughput': 1043.46}
[INFO|2025-10-23 03:54:20] logging.py:143 >> {'loss': 1.1218, 'learning_rate': 5.0337e-07, 'epoch': 0.50, 'throughput': 1043.71}
[INFO|2025-10-23 03:55:14] logging.py:143 >> {'loss': 1.0655, 'learning_rate': 5.0176e-07, 'epoch': 0.50, 'throughput': 1043.96}
[INFO|2025-10-23 03:56:06] logging.py:143 >> {'loss': 1.0043, 'learning_rate': 5.0016e-07, 'epoch': 0.50, 'throughput': 1044.19}
[INFO|2025-10-23 03:56:59] logging.py:143 >> {'loss': 1.0714, 'learning_rate': 4.9856e-07, 'epoch': 0.50, 'throughput': 1044.43}
[INFO|2025-10-23 03:57:52] logging.py:143 >> {'loss': 1.2512, 'learning_rate': 4.9695e-07, 'epoch': 0.50, 'throughput': 1044.67}
[INFO|2025-10-23 03:58:46] logging.py:143 >> {'loss': 1.5282, 'learning_rate': 4.9535e-07, 'epoch': 0.50, 'throughput': 1044.90}
[INFO|2025-10-23 03:59:39] logging.py:143 >> {'loss': 1.0207, 'learning_rate': 4.9375e-07, 'epoch': 0.50, 'throughput': 1045.14}
[INFO|2025-10-23 04:00:33] logging.py:143 >> {'loss': 1.0322, 'learning_rate': 4.9214e-07, 'epoch': 0.51, 'throughput': 1045.39}
[INFO|2025-10-23 04:01:26] logging.py:143 >> {'loss': 0.9872, 'learning_rate': 4.9054e-07, 'epoch': 0.51, 'throughput': 1045.62}
[INFO|2025-10-23 04:02:19] logging.py:143 >> {'loss': 0.9632, 'learning_rate': 4.8894e-07, 'epoch': 0.51, 'throughput': 1045.86}
[INFO|2025-10-23 04:03:13] logging.py:143 >> {'loss': 1.0670, 'learning_rate': 4.8734e-07, 'epoch': 0.51, 'throughput': 1046.09}
[INFO|2025-10-23 04:04:06] logging.py:143 >> {'loss': 1.0345, 'learning_rate': 4.8573e-07, 'epoch': 0.51, 'throughput': 1046.33}
[INFO|2025-10-23 04:04:59] logging.py:143 >> {'loss': 1.0146, 'learning_rate': 4.8413e-07, 'epoch': 0.51, 'throughput': 1046.56}
[INFO|2025-10-23 04:04:59] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 04:04:59] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 04:04:59] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 04:07:18] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2500
[INFO|2025-10-23 04:07:18] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 04:07:18] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 04:07:18] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2500/chat_template.jinja
[INFO|2025-10-23 04:07:18] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2500/tokenizer_config.json
[INFO|2025-10-23 04:07:18] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2500/special_tokens_map.json
[INFO|2025-10-23 04:08:12] logging.py:143 >> {'loss': 0.9900, 'learning_rate': 4.8253e-07, 'epoch': 0.51, 'throughput': 1041.96}
[INFO|2025-10-23 04:09:04] logging.py:143 >> {'loss': 1.1200, 'learning_rate': 4.8093e-07, 'epoch': 0.51, 'throughput': 1042.19}
[INFO|2025-10-23 04:09:56] logging.py:143 >> {'loss': 1.1548, 'learning_rate': 4.7932e-07, 'epoch': 0.51, 'throughput': 1042.42}
[INFO|2025-10-23 04:10:49] logging.py:143 >> {'loss': 1.0202, 'learning_rate': 4.7772e-07, 'epoch': 0.51, 'throughput': 1042.66}
[INFO|2025-10-23 04:11:42] logging.py:143 >> {'loss': 0.8816, 'learning_rate': 4.7612e-07, 'epoch': 0.52, 'throughput': 1042.89}
[INFO|2025-10-23 04:12:34] logging.py:143 >> {'loss': 1.1419, 'learning_rate': 4.7452e-07, 'epoch': 0.52, 'throughput': 1043.13}
[INFO|2025-10-23 04:13:28] logging.py:143 >> {'loss': 0.9766, 'learning_rate': 4.7292e-07, 'epoch': 0.52, 'throughput': 1043.37}
[INFO|2025-10-23 04:14:21] logging.py:143 >> {'loss': 0.8112, 'learning_rate': 4.7132e-07, 'epoch': 0.52, 'throughput': 1043.60}
[INFO|2025-10-23 04:15:15] logging.py:143 >> {'loss': 0.8253, 'learning_rate': 4.6972e-07, 'epoch': 0.52, 'throughput': 1043.85}
[INFO|2025-10-23 04:16:08] logging.py:143 >> {'loss': 1.0604, 'learning_rate': 4.6812e-07, 'epoch': 0.52, 'throughput': 1044.08}
[INFO|2025-10-23 04:17:01] logging.py:143 >> {'loss': 0.9312, 'learning_rate': 4.6652e-07, 'epoch': 0.52, 'throughput': 1044.31}
[INFO|2025-10-23 04:17:55] logging.py:143 >> {'loss': 0.8817, 'learning_rate': 4.6492e-07, 'epoch': 0.52, 'throughput': 1044.56}
[INFO|2025-10-23 04:18:49] logging.py:143 >> {'loss': 1.2083, 'learning_rate': 4.6332e-07, 'epoch': 0.52, 'throughput': 1044.80}
[INFO|2025-10-23 04:19:42] logging.py:143 >> {'loss': 0.9428, 'learning_rate': 4.6172e-07, 'epoch': 0.52, 'throughput': 1045.03}
[INFO|2025-10-23 04:20:34] logging.py:143 >> {'loss': 0.8883, 'learning_rate': 4.6012e-07, 'epoch': 0.53, 'throughput': 1045.24}
[INFO|2025-10-23 04:21:28] logging.py:143 >> {'loss': 1.0510, 'learning_rate': 4.5853e-07, 'epoch': 0.53, 'throughput': 1045.48}
[INFO|2025-10-23 04:22:20] logging.py:143 >> {'loss': 1.1854, 'learning_rate': 4.5693e-07, 'epoch': 0.53, 'throughput': 1045.70}
[INFO|2025-10-23 04:23:14] logging.py:143 >> {'loss': 1.0989, 'learning_rate': 4.5533e-07, 'epoch': 0.53, 'throughput': 1045.92}
[INFO|2025-10-23 04:24:06] logging.py:143 >> {'loss': 1.1496, 'learning_rate': 4.5373e-07, 'epoch': 0.53, 'throughput': 1046.14}
[INFO|2025-10-23 04:24:58] logging.py:143 >> {'loss': 0.9981, 'learning_rate': 4.5214e-07, 'epoch': 0.53, 'throughput': 1046.36}
[INFO|2025-10-23 04:24:58] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 04:24:58] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 04:24:58] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 04:27:17] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2600
[INFO|2025-10-23 04:27:17] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 04:27:17] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 04:27:18] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2600/chat_template.jinja
[INFO|2025-10-23 04:27:18] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2600/tokenizer_config.json
[INFO|2025-10-23 04:27:18] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2600/special_tokens_map.json
[INFO|2025-10-23 04:28:10] logging.py:143 >> {'loss': 1.0200, 'learning_rate': 4.5054e-07, 'epoch': 0.53, 'throughput': 1041.92}
[INFO|2025-10-23 04:29:05] logging.py:143 >> {'loss': 0.9095, 'learning_rate': 4.4895e-07, 'epoch': 0.53, 'throughput': 1042.15}
[INFO|2025-10-23 04:29:57] logging.py:143 >> {'loss': 0.9451, 'learning_rate': 4.4735e-07, 'epoch': 0.53, 'throughput': 1042.37}
[INFO|2025-10-23 04:30:50] logging.py:143 >> {'loss': 0.9615, 'learning_rate': 4.4576e-07, 'epoch': 0.53, 'throughput': 1042.60}
[INFO|2025-10-23 04:31:43] logging.py:143 >> {'loss': 1.0836, 'learning_rate': 4.4417e-07, 'epoch': 0.54, 'throughput': 1042.83}
[INFO|2025-10-23 04:32:36] logging.py:143 >> {'loss': 1.2479, 'learning_rate': 4.4257e-07, 'epoch': 0.54, 'throughput': 1043.06}
[INFO|2025-10-23 04:33:29] logging.py:143 >> {'loss': 0.9420, 'learning_rate': 4.4098e-07, 'epoch': 0.54, 'throughput': 1043.29}
[INFO|2025-10-23 04:34:23] logging.py:143 >> {'loss': 1.0554, 'learning_rate': 4.3939e-07, 'epoch': 0.54, 'throughput': 1043.51}
[INFO|2025-10-23 04:35:16] logging.py:143 >> {'loss': 1.1399, 'learning_rate': 4.3780e-07, 'epoch': 0.54, 'throughput': 1043.74}
[INFO|2025-10-23 04:36:10] logging.py:143 >> {'loss': 1.2524, 'learning_rate': 4.3621e-07, 'epoch': 0.54, 'throughput': 1043.97}
[INFO|2025-10-23 04:37:03] logging.py:143 >> {'loss': 0.7667, 'learning_rate': 4.3462e-07, 'epoch': 0.54, 'throughput': 1044.19}
[INFO|2025-10-23 04:37:56] logging.py:143 >> {'loss': 0.8542, 'learning_rate': 4.3303e-07, 'epoch': 0.54, 'throughput': 1044.42}
[INFO|2025-10-23 04:38:49] logging.py:143 >> {'loss': 1.0071, 'learning_rate': 4.3144e-07, 'epoch': 0.54, 'throughput': 1044.64}
[INFO|2025-10-23 04:39:42] logging.py:143 >> {'loss': 1.0819, 'learning_rate': 4.2985e-07, 'epoch': 0.55, 'throughput': 1044.86}
[INFO|2025-10-23 04:40:35] logging.py:143 >> {'loss': 1.3632, 'learning_rate': 4.2827e-07, 'epoch': 0.55, 'throughput': 1045.08}
[INFO|2025-10-23 04:41:28] logging.py:143 >> {'loss': 1.0230, 'learning_rate': 4.2668e-07, 'epoch': 0.55, 'throughput': 1045.30}
[INFO|2025-10-23 04:42:20] logging.py:143 >> {'loss': 1.0693, 'learning_rate': 4.2509e-07, 'epoch': 0.55, 'throughput': 1045.52}
[INFO|2025-10-23 04:43:14] logging.py:143 >> {'loss': 1.0562, 'learning_rate': 4.2351e-07, 'epoch': 0.55, 'throughput': 1045.73}
[INFO|2025-10-23 04:44:07] logging.py:143 >> {'loss': 0.9766, 'learning_rate': 4.2193e-07, 'epoch': 0.55, 'throughput': 1045.95}
[INFO|2025-10-23 04:45:00] logging.py:143 >> {'loss': 1.1426, 'learning_rate': 4.2034e-07, 'epoch': 0.55, 'throughput': 1046.16}
[INFO|2025-10-23 04:45:00] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 04:45:00] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 04:45:00] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 04:47:19] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2700
[INFO|2025-10-23 04:47:19] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 04:47:19] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 04:47:19] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2700/chat_template.jinja
[INFO|2025-10-23 04:47:19] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2700/tokenizer_config.json
[INFO|2025-10-23 04:47:19] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2700/special_tokens_map.json
[INFO|2025-10-23 04:48:12] logging.py:143 >> {'loss': 1.0808, 'learning_rate': 4.1876e-07, 'epoch': 0.55, 'throughput': 1041.89}
[INFO|2025-10-23 04:49:04] logging.py:143 >> {'loss': 0.8191, 'learning_rate': 4.1718e-07, 'epoch': 0.55, 'throughput': 1042.10}
[INFO|2025-10-23 04:49:58] logging.py:143 >> {'loss': 0.8109, 'learning_rate': 4.1560e-07, 'epoch': 0.55, 'throughput': 1042.34}
[INFO|2025-10-23 04:50:51] logging.py:143 >> {'loss': 0.9989, 'learning_rate': 4.1402e-07, 'epoch': 0.56, 'throughput': 1042.55}
[INFO|2025-10-23 04:51:44] logging.py:143 >> {'loss': 1.0287, 'learning_rate': 4.1244e-07, 'epoch': 0.56, 'throughput': 1042.77}
[INFO|2025-10-23 04:52:37] logging.py:143 >> {'loss': 1.1871, 'learning_rate': 4.1086e-07, 'epoch': 0.56, 'throughput': 1042.99}
[INFO|2025-10-23 04:53:31] logging.py:143 >> {'loss': 0.9574, 'learning_rate': 4.0928e-07, 'epoch': 0.56, 'throughput': 1043.22}
[INFO|2025-10-23 04:54:25] logging.py:143 >> {'loss': 0.9469, 'learning_rate': 4.0771e-07, 'epoch': 0.56, 'throughput': 1043.45}
[INFO|2025-10-23 04:55:17] logging.py:143 >> {'loss': 0.8507, 'learning_rate': 4.0613e-07, 'epoch': 0.56, 'throughput': 1043.67}
[INFO|2025-10-23 04:56:10] logging.py:143 >> {'loss': 1.1882, 'learning_rate': 4.0456e-07, 'epoch': 0.56, 'throughput': 1043.88}
[INFO|2025-10-23 04:57:04] logging.py:143 >> {'loss': 0.9223, 'learning_rate': 4.0299e-07, 'epoch': 0.56, 'throughput': 1044.10}
[INFO|2025-10-23 04:57:57] logging.py:143 >> {'loss': 1.2756, 'learning_rate': 4.0141e-07, 'epoch': 0.56, 'throughput': 1044.31}
[INFO|2025-10-23 04:58:50] logging.py:143 >> {'loss': 0.9607, 'learning_rate': 3.9984e-07, 'epoch': 0.56, 'throughput': 1044.53}
[INFO|2025-10-23 04:59:43] logging.py:143 >> {'loss': 1.0963, 'learning_rate': 3.9827e-07, 'epoch': 0.57, 'throughput': 1044.74}
[INFO|2025-10-23 05:00:36] logging.py:143 >> {'loss': 1.3391, 'learning_rate': 3.9670e-07, 'epoch': 0.57, 'throughput': 1044.95}
[INFO|2025-10-23 05:01:29] logging.py:143 >> {'loss': 1.0144, 'learning_rate': 3.9513e-07, 'epoch': 0.57, 'throughput': 1045.16}
[INFO|2025-10-23 05:02:22] logging.py:143 >> {'loss': 0.9616, 'learning_rate': 3.9357e-07, 'epoch': 0.57, 'throughput': 1045.37}
[INFO|2025-10-23 05:03:15] logging.py:143 >> {'loss': 1.0045, 'learning_rate': 3.9200e-07, 'epoch': 0.57, 'throughput': 1045.58}
[INFO|2025-10-23 05:04:08] logging.py:143 >> {'loss': 0.9721, 'learning_rate': 3.9044e-07, 'epoch': 0.57, 'throughput': 1045.79}
[INFO|2025-10-23 05:05:02] logging.py:143 >> {'loss': 0.9509, 'learning_rate': 3.8887e-07, 'epoch': 0.57, 'throughput': 1046.00}
[INFO|2025-10-23 05:05:02] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 05:05:02] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 05:05:02] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 05:07:21] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2800
[INFO|2025-10-23 05:07:21] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 05:07:21] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 05:07:21] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2800/chat_template.jinja
[INFO|2025-10-23 05:07:21] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2800/tokenizer_config.json
[INFO|2025-10-23 05:07:21] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2800/special_tokens_map.json
[INFO|2025-10-23 05:08:14] logging.py:143 >> {'loss': 0.9644, 'learning_rate': 3.8731e-07, 'epoch': 0.57, 'throughput': 1041.89}
[INFO|2025-10-23 05:09:08] logging.py:143 >> {'loss': 0.8759, 'learning_rate': 3.8575e-07, 'epoch': 0.57, 'throughput': 1042.11}
[INFO|2025-10-23 05:10:01] logging.py:143 >> {'loss': 0.8811, 'learning_rate': 3.8419e-07, 'epoch': 0.57, 'throughput': 1042.33}
[INFO|2025-10-23 05:10:54] logging.py:143 >> {'loss': 0.8170, 'learning_rate': 3.8263e-07, 'epoch': 0.58, 'throughput': 1042.54}
[INFO|2025-10-23 05:11:47] logging.py:143 >> {'loss': 0.8543, 'learning_rate': 3.8107e-07, 'epoch': 0.58, 'throughput': 1042.76}
[INFO|2025-10-23 05:12:41] logging.py:143 >> {'loss': 0.8290, 'learning_rate': 3.7952e-07, 'epoch': 0.58, 'throughput': 1042.98}
[INFO|2025-10-23 05:13:34] logging.py:143 >> {'loss': 0.7701, 'learning_rate': 3.7796e-07, 'epoch': 0.58, 'throughput': 1043.19}
[INFO|2025-10-23 05:14:27] logging.py:143 >> {'loss': 1.1420, 'learning_rate': 3.7641e-07, 'epoch': 0.58, 'throughput': 1043.40}
[INFO|2025-10-23 05:15:20] logging.py:143 >> {'loss': 0.9015, 'learning_rate': 3.7485e-07, 'epoch': 0.58, 'throughput': 1043.60}
[INFO|2025-10-23 05:16:13] logging.py:143 >> {'loss': 1.1505, 'learning_rate': 3.7330e-07, 'epoch': 0.58, 'throughput': 1043.81}
[INFO|2025-10-23 05:17:06] logging.py:143 >> {'loss': 1.1457, 'learning_rate': 3.7175e-07, 'epoch': 0.58, 'throughput': 1044.01}
[INFO|2025-10-23 05:17:59] logging.py:143 >> {'loss': 1.1000, 'learning_rate': 3.7020e-07, 'epoch': 0.58, 'throughput': 1044.23}
[INFO|2025-10-23 05:18:52] logging.py:143 >> {'loss': 0.9936, 'learning_rate': 3.6866e-07, 'epoch': 0.58, 'throughput': 1044.44}
[INFO|2025-10-23 05:19:45] logging.py:143 >> {'loss': 0.9274, 'learning_rate': 3.6711e-07, 'epoch': 0.59, 'throughput': 1044.64}
[INFO|2025-10-23 05:20:38] logging.py:143 >> {'loss': 0.8224, 'learning_rate': 3.6556e-07, 'epoch': 0.59, 'throughput': 1044.84}
[INFO|2025-10-23 05:21:30] logging.py:143 >> {'loss': 0.9255, 'learning_rate': 3.6402e-07, 'epoch': 0.59, 'throughput': 1045.04}
[INFO|2025-10-23 05:22:23] logging.py:143 >> {'loss': 1.0466, 'learning_rate': 3.6248e-07, 'epoch': 0.59, 'throughput': 1045.24}
[INFO|2025-10-23 05:23:16] logging.py:143 >> {'loss': 0.9332, 'learning_rate': 3.6094e-07, 'epoch': 0.59, 'throughput': 1045.45}
[INFO|2025-10-23 05:24:10] logging.py:143 >> {'loss': 1.3036, 'learning_rate': 3.5940e-07, 'epoch': 0.59, 'throughput': 1045.65}
[INFO|2025-10-23 05:25:03] logging.py:143 >> {'loss': 0.8919, 'learning_rate': 3.5786e-07, 'epoch': 0.59, 'throughput': 1045.87}
[INFO|2025-10-23 05:25:03] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 05:25:03] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 05:25:03] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 05:27:22] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2900
[INFO|2025-10-23 05:27:22] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 05:27:22] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 05:27:22] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2900/chat_template.jinja
[INFO|2025-10-23 05:27:22] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2900/tokenizer_config.json
[INFO|2025-10-23 05:27:22] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-2900/special_tokens_map.json
[INFO|2025-10-23 05:28:16] logging.py:143 >> {'loss': 1.0323, 'learning_rate': 3.5633e-07, 'epoch': 0.59, 'throughput': 1041.90}
[INFO|2025-10-23 05:29:09] logging.py:143 >> {'loss': 1.0334, 'learning_rate': 3.5479e-07, 'epoch': 0.59, 'throughput': 1042.11}
[INFO|2025-10-23 05:30:02] logging.py:143 >> {'loss': 1.2214, 'learning_rate': 3.5326e-07, 'epoch': 0.60, 'throughput': 1042.31}
[INFO|2025-10-23 05:30:56] logging.py:143 >> {'loss': 0.9490, 'learning_rate': 3.5173e-07, 'epoch': 0.60, 'throughput': 1042.53}
[INFO|2025-10-23 05:31:49] logging.py:143 >> {'loss': 1.0155, 'learning_rate': 3.5020e-07, 'epoch': 0.60, 'throughput': 1042.74}
[INFO|2025-10-23 05:32:43] logging.py:143 >> {'loss': 0.8660, 'learning_rate': 3.4867e-07, 'epoch': 0.60, 'throughput': 1042.94}
[INFO|2025-10-23 05:33:36] logging.py:143 >> {'loss': 0.9342, 'learning_rate': 3.4714e-07, 'epoch': 0.60, 'throughput': 1043.14}
[INFO|2025-10-23 05:34:30] logging.py:143 >> {'loss': 1.1150, 'learning_rate': 3.4561e-07, 'epoch': 0.60, 'throughput': 1043.36}
[INFO|2025-10-23 05:35:23] logging.py:143 >> {'loss': 1.1219, 'learning_rate': 3.4409e-07, 'epoch': 0.60, 'throughput': 1043.56}
[INFO|2025-10-23 05:36:17] logging.py:143 >> {'loss': 1.0141, 'learning_rate': 3.4257e-07, 'epoch': 0.60, 'throughput': 1043.76}
[INFO|2025-10-23 05:37:11] logging.py:143 >> {'loss': 0.9419, 'learning_rate': 3.4105e-07, 'epoch': 0.60, 'throughput': 1043.97}
[INFO|2025-10-23 05:38:04] logging.py:143 >> {'loss': 0.9297, 'learning_rate': 3.3953e-07, 'epoch': 0.60, 'throughput': 1044.16}
[INFO|2025-10-23 05:38:57] logging.py:143 >> {'loss': 1.1310, 'learning_rate': 3.3801e-07, 'epoch': 0.61, 'throughput': 1044.36}
[INFO|2025-10-23 05:39:50] logging.py:143 >> {'loss': 1.0161, 'learning_rate': 3.3649e-07, 'epoch': 0.61, 'throughput': 1044.57}
[INFO|2025-10-23 05:40:43] logging.py:143 >> {'loss': 1.1799, 'learning_rate': 3.3498e-07, 'epoch': 0.61, 'throughput': 1044.76}
[INFO|2025-10-23 05:41:36] logging.py:143 >> {'loss': 1.0429, 'learning_rate': 3.3347e-07, 'epoch': 0.61, 'throughput': 1044.96}
[INFO|2025-10-23 05:42:29] logging.py:143 >> {'loss': 0.9938, 'learning_rate': 3.3196e-07, 'epoch': 0.61, 'throughput': 1045.16}
[INFO|2025-10-23 05:43:23] logging.py:143 >> {'loss': 0.6821, 'learning_rate': 3.3045e-07, 'epoch': 0.61, 'throughput': 1045.37}
[INFO|2025-10-23 05:44:16] logging.py:143 >> {'loss': 1.1425, 'learning_rate': 3.2894e-07, 'epoch': 0.61, 'throughput': 1045.56}
[INFO|2025-10-23 05:45:08] logging.py:143 >> {'loss': 0.8978, 'learning_rate': 3.2743e-07, 'epoch': 0.61, 'throughput': 1045.75}
[INFO|2025-10-23 05:45:09] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 05:45:09] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 05:45:09] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 05:47:28] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3000
[INFO|2025-10-23 05:47:28] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 05:47:28] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 05:47:28] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3000/chat_template.jinja
[INFO|2025-10-23 05:47:28] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3000/tokenizer_config.json
[INFO|2025-10-23 05:47:28] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3000/special_tokens_map.json
[INFO|2025-10-23 05:48:21] logging.py:143 >> {'loss': 1.0552, 'learning_rate': 3.2593e-07, 'epoch': 0.61, 'throughput': 1041.91}
[INFO|2025-10-23 05:49:15] logging.py:143 >> {'loss': 1.0603, 'learning_rate': 3.2443e-07, 'epoch': 0.61, 'throughput': 1042.11}
[INFO|2025-10-23 05:50:08] logging.py:143 >> {'loss': 1.3695, 'learning_rate': 3.2293e-07, 'epoch': 0.62, 'throughput': 1042.32}
[INFO|2025-10-23 05:51:02] logging.py:143 >> {'loss': 1.1944, 'learning_rate': 3.2143e-07, 'epoch': 0.62, 'throughput': 1042.52}
[INFO|2025-10-23 05:51:56] logging.py:143 >> {'loss': 1.2924, 'learning_rate': 3.1993e-07, 'epoch': 0.62, 'throughput': 1042.73}
[INFO|2025-10-23 05:52:49] logging.py:143 >> {'loss': 0.9714, 'learning_rate': 3.1844e-07, 'epoch': 0.62, 'throughput': 1042.92}
[INFO|2025-10-23 05:53:43] logging.py:143 >> {'loss': 1.0811, 'learning_rate': 3.1695e-07, 'epoch': 0.62, 'throughput': 1043.12}
[INFO|2025-10-23 05:54:36] logging.py:143 >> {'loss': 1.0379, 'learning_rate': 3.1545e-07, 'epoch': 0.62, 'throughput': 1043.32}
[INFO|2025-10-23 05:55:30] logging.py:143 >> {'loss': 1.0558, 'learning_rate': 3.1397e-07, 'epoch': 0.62, 'throughput': 1043.53}
[INFO|2025-10-23 05:56:22] logging.py:143 >> {'loss': 0.9163, 'learning_rate': 3.1248e-07, 'epoch': 0.62, 'throughput': 1043.72}
[INFO|2025-10-23 05:57:15] logging.py:143 >> {'loss': 0.9913, 'learning_rate': 3.1099e-07, 'epoch': 0.62, 'throughput': 1043.91}
[INFO|2025-10-23 05:58:08] logging.py:143 >> {'loss': 0.8022, 'learning_rate': 3.0951e-07, 'epoch': 0.62, 'throughput': 1044.10}
[INFO|2025-10-23 05:59:01] logging.py:143 >> {'loss': 0.9699, 'learning_rate': 3.0803e-07, 'epoch': 0.63, 'throughput': 1044.30}
[INFO|2025-10-23 05:59:54] logging.py:143 >> {'loss': 0.9015, 'learning_rate': 3.0655e-07, 'epoch': 0.63, 'throughput': 1044.50}
[INFO|2025-10-23 06:00:48] logging.py:143 >> {'loss': 1.1532, 'learning_rate': 3.0507e-07, 'epoch': 0.63, 'throughput': 1044.69}
[INFO|2025-10-23 06:01:41] logging.py:143 >> {'loss': 1.2660, 'learning_rate': 3.0360e-07, 'epoch': 0.63, 'throughput': 1044.88}
[INFO|2025-10-23 06:02:35] logging.py:143 >> {'loss': 1.3037, 'learning_rate': 3.0212e-07, 'epoch': 0.63, 'throughput': 1045.08}
[INFO|2025-10-23 06:03:28] logging.py:143 >> {'loss': 1.3764, 'learning_rate': 3.0065e-07, 'epoch': 0.63, 'throughput': 1045.27}
[INFO|2025-10-23 06:04:21] logging.py:143 >> {'loss': 1.2018, 'learning_rate': 2.9918e-07, 'epoch': 0.63, 'throughput': 1045.45}
[INFO|2025-10-23 06:05:14] logging.py:143 >> {'loss': 1.2021, 'learning_rate': 2.9772e-07, 'epoch': 0.63, 'throughput': 1045.64}
[INFO|2025-10-23 06:05:14] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 06:05:14] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 06:05:14] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 06:07:33] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3100
[INFO|2025-10-23 06:07:33] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 06:07:33] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 06:07:33] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3100/chat_template.jinja
[INFO|2025-10-23 06:07:33] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3100/tokenizer_config.json
[INFO|2025-10-23 06:07:33] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3100/special_tokens_map.json
[INFO|2025-10-23 06:08:27] logging.py:143 >> {'loss': 0.8902, 'learning_rate': 2.9625e-07, 'epoch': 0.63, 'throughput': 1041.94}
[INFO|2025-10-23 06:09:20] logging.py:143 >> {'loss': 1.0952, 'learning_rate': 2.9479e-07, 'epoch': 0.63, 'throughput': 1042.13}
[INFO|2025-10-23 06:10:13] logging.py:143 >> {'loss': 1.0066, 'learning_rate': 2.9333e-07, 'epoch': 0.64, 'throughput': 1042.32}
[INFO|2025-10-23 06:11:06] logging.py:143 >> {'loss': 1.0240, 'learning_rate': 2.9187e-07, 'epoch': 0.64, 'throughput': 1042.51}
[INFO|2025-10-23 06:11:59] logging.py:143 >> {'loss': 1.0501, 'learning_rate': 2.9041e-07, 'epoch': 0.64, 'throughput': 1042.71}
[INFO|2025-10-23 06:12:52] logging.py:143 >> {'loss': 1.1136, 'learning_rate': 2.8896e-07, 'epoch': 0.64, 'throughput': 1042.90}
[INFO|2025-10-23 06:13:46] logging.py:143 >> {'loss': 1.0298, 'learning_rate': 2.8750e-07, 'epoch': 0.64, 'throughput': 1043.10}
[INFO|2025-10-23 06:14:38] logging.py:143 >> {'loss': 0.9073, 'learning_rate': 2.8605e-07, 'epoch': 0.64, 'throughput': 1043.28}
[INFO|2025-10-23 06:15:31] logging.py:143 >> {'loss': 0.9469, 'learning_rate': 2.8461e-07, 'epoch': 0.64, 'throughput': 1043.47}
[INFO|2025-10-23 06:16:25] logging.py:143 >> {'loss': 1.2873, 'learning_rate': 2.8316e-07, 'epoch': 0.64, 'throughput': 1043.67}
[INFO|2025-10-23 06:17:19] logging.py:143 >> {'loss': 0.9757, 'learning_rate': 2.8172e-07, 'epoch': 0.64, 'throughput': 1043.86}
[INFO|2025-10-23 06:18:12] logging.py:143 >> {'loss': 0.9534, 'learning_rate': 2.8028e-07, 'epoch': 0.65, 'throughput': 1044.04}
[INFO|2025-10-23 06:19:05] logging.py:143 >> {'loss': 1.1536, 'learning_rate': 2.7884e-07, 'epoch': 0.65, 'throughput': 1044.23}
[INFO|2025-10-23 06:19:58] logging.py:143 >> {'loss': 1.3029, 'learning_rate': 2.7740e-07, 'epoch': 0.65, 'throughput': 1044.41}
[INFO|2025-10-23 06:20:51] logging.py:143 >> {'loss': 1.0947, 'learning_rate': 2.7597e-07, 'epoch': 0.65, 'throughput': 1044.60}
[INFO|2025-10-23 06:21:44] logging.py:143 >> {'loss': 0.8653, 'learning_rate': 2.7453e-07, 'epoch': 0.65, 'throughput': 1044.79}
[INFO|2025-10-23 06:22:38] logging.py:143 >> {'loss': 1.0945, 'learning_rate': 2.7310e-07, 'epoch': 0.65, 'throughput': 1044.98}
[INFO|2025-10-23 06:23:31] logging.py:143 >> {'loss': 1.3297, 'learning_rate': 2.7168e-07, 'epoch': 0.65, 'throughput': 1045.17}
[INFO|2025-10-23 06:24:24] logging.py:143 >> {'loss': 0.8808, 'learning_rate': 2.7025e-07, 'epoch': 0.65, 'throughput': 1045.35}
[INFO|2025-10-23 06:25:17] logging.py:143 >> {'loss': 1.0809, 'learning_rate': 2.6883e-07, 'epoch': 0.65, 'throughput': 1045.54}
[INFO|2025-10-23 06:25:17] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 06:25:17] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 06:25:17] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 06:27:36] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3200
[INFO|2025-10-23 06:27:36] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 06:27:36] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 06:27:36] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3200/chat_template.jinja
[INFO|2025-10-23 06:27:36] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3200/tokenizer_config.json
[INFO|2025-10-23 06:27:36] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3200/special_tokens_map.json
[INFO|2025-10-23 06:28:29] logging.py:143 >> {'loss': 0.9070, 'learning_rate': 2.6741e-07, 'epoch': 0.65, 'throughput': 1041.94}
[INFO|2025-10-23 06:29:23] logging.py:143 >> {'loss': 0.9036, 'learning_rate': 2.6599e-07, 'epoch': 0.66, 'throughput': 1042.13}
[INFO|2025-10-23 06:30:17] logging.py:143 >> {'loss': 1.1227, 'learning_rate': 2.6458e-07, 'epoch': 0.66, 'throughput': 1042.32}
[INFO|2025-10-23 06:31:09] logging.py:143 >> {'loss': 0.9542, 'learning_rate': 2.6316e-07, 'epoch': 0.66, 'throughput': 1042.50}
[INFO|2025-10-23 06:32:02] logging.py:143 >> {'loss': 0.8012, 'learning_rate': 2.6175e-07, 'epoch': 0.66, 'throughput': 1042.68}
[INFO|2025-10-23 06:32:55] logging.py:143 >> {'loss': 0.7998, 'learning_rate': 2.6034e-07, 'epoch': 0.66, 'throughput': 1042.87}
[INFO|2025-10-23 06:33:48] logging.py:143 >> {'loss': 1.2500, 'learning_rate': 2.5894e-07, 'epoch': 0.66, 'throughput': 1043.05}
[INFO|2025-10-23 06:34:41] logging.py:143 >> {'loss': 1.0685, 'learning_rate': 2.5753e-07, 'epoch': 0.66, 'throughput': 1043.23}
[INFO|2025-10-23 06:35:35] logging.py:143 >> {'loss': 0.8447, 'learning_rate': 2.5613e-07, 'epoch': 0.66, 'throughput': 1043.42}
[INFO|2025-10-23 06:36:28] logging.py:143 >> {'loss': 0.8089, 'learning_rate': 2.5474e-07, 'epoch': 0.66, 'throughput': 1043.60}
[INFO|2025-10-23 06:37:21] logging.py:143 >> {'loss': 1.1190, 'learning_rate': 2.5334e-07, 'epoch': 0.66, 'throughput': 1043.78}
[INFO|2025-10-23 06:38:14] logging.py:143 >> {'loss': 0.9037, 'learning_rate': 2.5195e-07, 'epoch': 0.67, 'throughput': 1043.96}
[INFO|2025-10-23 06:39:08] logging.py:143 >> {'loss': 1.2472, 'learning_rate': 2.5056e-07, 'epoch': 0.67, 'throughput': 1044.16}
[INFO|2025-10-23 06:40:01] logging.py:143 >> {'loss': 0.7704, 'learning_rate': 2.4917e-07, 'epoch': 0.67, 'throughput': 1044.34}
[INFO|2025-10-23 06:40:54] logging.py:143 >> {'loss': 0.8498, 'learning_rate': 2.4778e-07, 'epoch': 0.67, 'throughput': 1044.51}
[INFO|2025-10-23 06:41:49] logging.py:143 >> {'loss': 1.1359, 'learning_rate': 2.4640e-07, 'epoch': 0.67, 'throughput': 1044.70}
[INFO|2025-10-23 06:42:42] logging.py:143 >> {'loss': 0.8906, 'learning_rate': 2.4502e-07, 'epoch': 0.67, 'throughput': 1044.88}
[INFO|2025-10-23 06:43:35] logging.py:143 >> {'loss': 1.0148, 'learning_rate': 2.4364e-07, 'epoch': 0.67, 'throughput': 1045.06}
[INFO|2025-10-23 06:44:28] logging.py:143 >> {'loss': 1.0379, 'learning_rate': 2.4227e-07, 'epoch': 0.67, 'throughput': 1045.24}
[INFO|2025-10-23 06:45:21] logging.py:143 >> {'loss': 0.9673, 'learning_rate': 2.4089e-07, 'epoch': 0.67, 'throughput': 1045.42}
[INFO|2025-10-23 06:45:21] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 06:45:21] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 06:45:21] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 06:47:40] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3300
[INFO|2025-10-23 06:47:41] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 06:47:41] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 06:47:41] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3300/chat_template.jinja
[INFO|2025-10-23 06:47:41] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3300/tokenizer_config.json
[INFO|2025-10-23 06:47:41] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3300/special_tokens_map.json
[INFO|2025-10-23 06:48:35] logging.py:143 >> {'loss': 0.9222, 'learning_rate': 2.3952e-07, 'epoch': 0.67, 'throughput': 1041.93}
[INFO|2025-10-23 06:49:28] logging.py:143 >> {'loss': 1.0621, 'learning_rate': 2.3816e-07, 'epoch': 0.68, 'throughput': 1042.12}
[INFO|2025-10-23 06:50:21] logging.py:143 >> {'loss': 1.3506, 'learning_rate': 2.3679e-07, 'epoch': 0.68, 'throughput': 1042.30}
[INFO|2025-10-23 06:51:15] logging.py:143 >> {'loss': 0.9210, 'learning_rate': 2.3543e-07, 'epoch': 0.68, 'throughput': 1042.48}
[INFO|2025-10-23 06:52:08] logging.py:143 >> {'loss': 1.0729, 'learning_rate': 2.3407e-07, 'epoch': 0.68, 'throughput': 1042.67}
[INFO|2025-10-23 06:53:01] logging.py:143 >> {'loss': 0.7786, 'learning_rate': 2.3271e-07, 'epoch': 0.68, 'throughput': 1042.85}
[INFO|2025-10-23 06:53:55] logging.py:143 >> {'loss': 1.0294, 'learning_rate': 2.3136e-07, 'epoch': 0.68, 'throughput': 1043.03}
[INFO|2025-10-23 06:54:47] logging.py:143 >> {'loss': 0.7863, 'learning_rate': 2.3001e-07, 'epoch': 0.68, 'throughput': 1043.21}
[INFO|2025-10-23 06:55:41] logging.py:143 >> {'loss': 0.9283, 'learning_rate': 2.2866e-07, 'epoch': 0.68, 'throughput': 1043.40}
[INFO|2025-10-23 06:56:34] logging.py:143 >> {'loss': 0.8548, 'learning_rate': 2.2732e-07, 'epoch': 0.68, 'throughput': 1043.57}
[INFO|2025-10-23 06:57:27] logging.py:143 >> {'loss': 1.0978, 'learning_rate': 2.2598e-07, 'epoch': 0.68, 'throughput': 1043.75}
[INFO|2025-10-23 06:58:21] logging.py:143 >> {'loss': 0.8128, 'learning_rate': 2.2464e-07, 'epoch': 0.69, 'throughput': 1043.94}
[INFO|2025-10-23 06:59:15] logging.py:143 >> {'loss': 0.8479, 'learning_rate': 2.2330e-07, 'epoch': 0.69, 'throughput': 1044.12}
[INFO|2025-10-23 07:00:08] logging.py:143 >> {'loss': 0.8591, 'learning_rate': 2.2196e-07, 'epoch': 0.69, 'throughput': 1044.30}
[INFO|2025-10-23 07:01:01] logging.py:143 >> {'loss': 0.9980, 'learning_rate': 2.2063e-07, 'epoch': 0.69, 'throughput': 1044.47}
[INFO|2025-10-23 07:01:54] logging.py:143 >> {'loss': 0.7430, 'learning_rate': 2.1931e-07, 'epoch': 0.69, 'throughput': 1044.64}
[INFO|2025-10-23 07:02:47] logging.py:143 >> {'loss': 1.0554, 'learning_rate': 2.1798e-07, 'epoch': 0.69, 'throughput': 1044.82}
[INFO|2025-10-23 07:03:40] logging.py:143 >> {'loss': 1.0121, 'learning_rate': 2.1666e-07, 'epoch': 0.69, 'throughput': 1044.99}
[INFO|2025-10-23 07:04:33] logging.py:143 >> {'loss': 1.0207, 'learning_rate': 2.1534e-07, 'epoch': 0.69, 'throughput': 1045.16}
[INFO|2025-10-23 07:05:26] logging.py:143 >> {'loss': 1.1067, 'learning_rate': 2.1402e-07, 'epoch': 0.69, 'throughput': 1045.33}
[INFO|2025-10-23 07:05:26] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 07:05:26] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 07:05:26] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 07:07:45] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3400
[INFO|2025-10-23 07:07:45] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 07:07:45] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 07:07:45] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3400/chat_template.jinja
[INFO|2025-10-23 07:07:45] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3400/tokenizer_config.json
[INFO|2025-10-23 07:07:45] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3400/special_tokens_map.json
[INFO|2025-10-23 07:08:39] logging.py:143 >> {'loss': 1.0544, 'learning_rate': 2.1271e-07, 'epoch': 0.70, 'throughput': 1041.95}
[INFO|2025-10-23 07:09:31] logging.py:143 >> {'loss': 0.8815, 'learning_rate': 2.1140e-07, 'epoch': 0.70, 'throughput': 1042.12}
[INFO|2025-10-23 07:10:25] logging.py:143 >> {'loss': 0.7940, 'learning_rate': 2.1009e-07, 'epoch': 0.70, 'throughput': 1042.30}
[INFO|2025-10-23 07:11:18] logging.py:143 >> {'loss': 0.9336, 'learning_rate': 2.0879e-07, 'epoch': 0.70, 'throughput': 1042.47}
[INFO|2025-10-23 07:12:12] logging.py:143 >> {'loss': 0.8693, 'learning_rate': 2.0748e-07, 'epoch': 0.70, 'throughput': 1042.66}
[INFO|2025-10-23 07:13:05] logging.py:143 >> {'loss': 0.9722, 'learning_rate': 2.0619e-07, 'epoch': 0.70, 'throughput': 1042.83}
[INFO|2025-10-23 07:13:58] logging.py:143 >> {'loss': 0.8856, 'learning_rate': 2.0489e-07, 'epoch': 0.70, 'throughput': 1043.00}
[INFO|2025-10-23 07:14:51] logging.py:143 >> {'loss': 0.7885, 'learning_rate': 2.0360e-07, 'epoch': 0.70, 'throughput': 1043.18}
[INFO|2025-10-23 07:15:45] logging.py:143 >> {'loss': 0.9865, 'learning_rate': 2.0231e-07, 'epoch': 0.70, 'throughput': 1043.35}
[INFO|2025-10-23 07:16:38] logging.py:143 >> {'loss': 0.9353, 'learning_rate': 2.0102e-07, 'epoch': 0.70, 'throughput': 1043.53}
[INFO|2025-10-23 07:17:32] logging.py:143 >> {'loss': 1.0617, 'learning_rate': 1.9974e-07, 'epoch': 0.71, 'throughput': 1043.70}
[INFO|2025-10-23 07:18:25] logging.py:143 >> {'loss': 1.0026, 'learning_rate': 1.9846e-07, 'epoch': 0.71, 'throughput': 1043.88}
[INFO|2025-10-23 07:19:19] logging.py:143 >> {'loss': 0.9977, 'learning_rate': 1.9718e-07, 'epoch': 0.71, 'throughput': 1044.05}
[INFO|2025-10-23 07:20:11] logging.py:143 >> {'loss': 0.9798, 'learning_rate': 1.9591e-07, 'epoch': 0.71, 'throughput': 1044.22}
[INFO|2025-10-23 07:21:05] logging.py:143 >> {'loss': 0.8678, 'learning_rate': 1.9463e-07, 'epoch': 0.71, 'throughput': 1044.39}
[INFO|2025-10-23 07:21:57] logging.py:143 >> {'loss': 1.0956, 'learning_rate': 1.9337e-07, 'epoch': 0.71, 'throughput': 1044.56}
[INFO|2025-10-23 07:22:50] logging.py:143 >> {'loss': 1.0406, 'learning_rate': 1.9210e-07, 'epoch': 0.71, 'throughput': 1044.73}
[INFO|2025-10-23 07:23:43] logging.py:143 >> {'loss': 0.9331, 'learning_rate': 1.9084e-07, 'epoch': 0.71, 'throughput': 1044.90}
[INFO|2025-10-23 07:24:36] logging.py:143 >> {'loss': 1.0938, 'learning_rate': 1.8958e-07, 'epoch': 0.71, 'throughput': 1045.06}
[INFO|2025-10-23 07:25:30] logging.py:143 >> {'loss': 1.0199, 'learning_rate': 1.8833e-07, 'epoch': 0.71, 'throughput': 1045.23}
[INFO|2025-10-23 07:25:30] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 07:25:30] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 07:25:30] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 07:27:49] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3500
[INFO|2025-10-23 07:27:49] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 07:27:49] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 07:27:49] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3500/chat_template.jinja
[INFO|2025-10-23 07:27:49] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3500/tokenizer_config.json
[INFO|2025-10-23 07:27:49] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3500/special_tokens_map.json
[INFO|2025-10-23 07:28:42] logging.py:143 >> {'loss': 0.9081, 'learning_rate': 1.8707e-07, 'epoch': 0.72, 'throughput': 1041.95}
[INFO|2025-10-23 07:29:35] logging.py:143 >> {'loss': 0.7884, 'learning_rate': 1.8583e-07, 'epoch': 0.72, 'throughput': 1042.12}
[INFO|2025-10-23 07:30:28] logging.py:143 >> {'loss': 0.9553, 'learning_rate': 1.8458e-07, 'epoch': 0.72, 'throughput': 1042.29}
[INFO|2025-10-23 07:31:21] logging.py:143 >> {'loss': 0.7944, 'learning_rate': 1.8334e-07, 'epoch': 0.72, 'throughput': 1042.46}
[INFO|2025-10-23 07:32:14] logging.py:143 >> {'loss': 0.9098, 'learning_rate': 1.8210e-07, 'epoch': 0.72, 'throughput': 1042.63}
[INFO|2025-10-23 07:33:08] logging.py:143 >> {'loss': 1.1360, 'learning_rate': 1.8086e-07, 'epoch': 0.72, 'throughput': 1042.80}
[INFO|2025-10-23 07:34:01] logging.py:143 >> {'loss': 0.9544, 'learning_rate': 1.7963e-07, 'epoch': 0.72, 'throughput': 1042.97}
[INFO|2025-10-23 07:34:53] logging.py:143 >> {'loss': 0.7628, 'learning_rate': 1.7840e-07, 'epoch': 0.72, 'throughput': 1043.13}
[INFO|2025-10-23 07:35:47] logging.py:143 >> {'loss': 0.8591, 'learning_rate': 1.7718e-07, 'epoch': 0.72, 'throughput': 1043.31}
[INFO|2025-10-23 07:36:40] logging.py:143 >> {'loss': 0.8949, 'learning_rate': 1.7595e-07, 'epoch': 0.72, 'throughput': 1043.48}
[INFO|2025-10-23 07:37:34] logging.py:143 >> {'loss': 1.1549, 'learning_rate': 1.7473e-07, 'epoch': 0.73, 'throughput': 1043.65}
[INFO|2025-10-23 07:38:28] logging.py:143 >> {'loss': 1.1579, 'learning_rate': 1.7352e-07, 'epoch': 0.73, 'throughput': 1043.82}
[INFO|2025-10-23 07:39:21] logging.py:143 >> {'loss': 0.8843, 'learning_rate': 1.7231e-07, 'epoch': 0.73, 'throughput': 1043.99}
[INFO|2025-10-23 07:40:15] logging.py:143 >> {'loss': 0.9684, 'learning_rate': 1.7110e-07, 'epoch': 0.73, 'throughput': 1044.16}
[INFO|2025-10-23 07:41:09] logging.py:143 >> {'loss': 0.9744, 'learning_rate': 1.6989e-07, 'epoch': 0.73, 'throughput': 1044.33}
[INFO|2025-10-23 07:42:02] logging.py:143 >> {'loss': 0.9279, 'learning_rate': 1.6869e-07, 'epoch': 0.73, 'throughput': 1044.50}
[INFO|2025-10-23 07:42:55] logging.py:143 >> {'loss': 1.0004, 'learning_rate': 1.6749e-07, 'epoch': 0.73, 'throughput': 1044.66}
[INFO|2025-10-23 07:43:48] logging.py:143 >> {'loss': 1.2279, 'learning_rate': 1.6629e-07, 'epoch': 0.73, 'throughput': 1044.83}
[INFO|2025-10-23 07:44:41] logging.py:143 >> {'loss': 1.0673, 'learning_rate': 1.6510e-07, 'epoch': 0.73, 'throughput': 1044.99}
[INFO|2025-10-23 07:45:35] logging.py:143 >> {'loss': 1.0858, 'learning_rate': 1.6391e-07, 'epoch': 0.73, 'throughput': 1045.16}
[INFO|2025-10-23 07:45:35] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 07:45:35] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 07:45:35] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 07:47:54] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3600
[INFO|2025-10-23 07:47:54] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 07:47:54] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 07:47:54] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3600/chat_template.jinja
[INFO|2025-10-23 07:47:54] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3600/tokenizer_config.json
[INFO|2025-10-23 07:47:54] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3600/special_tokens_map.json
[INFO|2025-10-23 07:48:47] logging.py:143 >> {'loss': 0.8694, 'learning_rate': 1.6273e-07, 'epoch': 0.74, 'throughput': 1041.97}
[INFO|2025-10-23 07:49:40] logging.py:143 >> {'loss': 0.7642, 'learning_rate': 1.6155e-07, 'epoch': 0.74, 'throughput': 1042.13}
[INFO|2025-10-23 07:50:33] logging.py:143 >> {'loss': 1.2173, 'learning_rate': 1.6037e-07, 'epoch': 0.74, 'throughput': 1042.30}
[INFO|2025-10-23 07:51:26] logging.py:143 >> {'loss': 0.9435, 'learning_rate': 1.5919e-07, 'epoch': 0.74, 'throughput': 1042.46}
[INFO|2025-10-23 07:52:20] logging.py:143 >> {'loss': 0.9625, 'learning_rate': 1.5802e-07, 'epoch': 0.74, 'throughput': 1042.63}
[INFO|2025-10-23 07:53:13] logging.py:143 >> {'loss': 0.8656, 'learning_rate': 1.5685e-07, 'epoch': 0.74, 'throughput': 1042.80}
[INFO|2025-10-23 07:54:06] logging.py:143 >> {'loss': 0.7232, 'learning_rate': 1.5569e-07, 'epoch': 0.74, 'throughput': 1042.97}
[INFO|2025-10-23 07:55:00] logging.py:143 >> {'loss': 0.9445, 'learning_rate': 1.5453e-07, 'epoch': 0.74, 'throughput': 1043.13}
[INFO|2025-10-23 07:55:53] logging.py:143 >> {'loss': 1.2952, 'learning_rate': 1.5337e-07, 'epoch': 0.74, 'throughput': 1043.30}
[INFO|2025-10-23 07:56:47] logging.py:143 >> {'loss': 0.7482, 'learning_rate': 1.5222e-07, 'epoch': 0.75, 'throughput': 1043.47}
[INFO|2025-10-23 07:57:40] logging.py:143 >> {'loss': 1.0526, 'learning_rate': 1.5107e-07, 'epoch': 0.75, 'throughput': 1043.63}
[INFO|2025-10-23 07:58:33] logging.py:143 >> {'loss': 1.0554, 'learning_rate': 1.4992e-07, 'epoch': 0.75, 'throughput': 1043.79}
[INFO|2025-10-23 07:59:26] logging.py:143 >> {'loss': 1.1372, 'learning_rate': 1.4878e-07, 'epoch': 0.75, 'throughput': 1043.96}
[INFO|2025-10-23 08:00:20] logging.py:143 >> {'loss': 0.8005, 'learning_rate': 1.4764e-07, 'epoch': 0.75, 'throughput': 1044.12}
[INFO|2025-10-23 08:01:14] logging.py:143 >> {'loss': 0.8255, 'learning_rate': 1.4650e-07, 'epoch': 0.75, 'throughput': 1044.29}
[INFO|2025-10-23 08:02:08] logging.py:143 >> {'loss': 0.8829, 'learning_rate': 1.4537e-07, 'epoch': 0.75, 'throughput': 1044.46}
[INFO|2025-10-23 08:03:02] logging.py:143 >> {'loss': 1.4291, 'learning_rate': 1.4424e-07, 'epoch': 0.75, 'throughput': 1044.62}
[INFO|2025-10-23 08:03:55] logging.py:143 >> {'loss': 1.0024, 'learning_rate': 1.4312e-07, 'epoch': 0.75, 'throughput': 1044.79}
[INFO|2025-10-23 08:04:48] logging.py:143 >> {'loss': 0.9572, 'learning_rate': 1.4200e-07, 'epoch': 0.75, 'throughput': 1044.94}
[INFO|2025-10-23 08:05:41] logging.py:143 >> {'loss': 1.1275, 'learning_rate': 1.4088e-07, 'epoch': 0.76, 'throughput': 1045.11}
[INFO|2025-10-23 08:05:41] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 08:05:41] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 08:05:41] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 08:08:00] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3700
[INFO|2025-10-23 08:08:00] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 08:08:00] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 08:08:00] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3700/chat_template.jinja
[INFO|2025-10-23 08:08:00] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3700/tokenizer_config.json
[INFO|2025-10-23 08:08:00] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3700/special_tokens_map.json
[INFO|2025-10-23 08:08:53] logging.py:143 >> {'loss': 0.9707, 'learning_rate': 1.3977e-07, 'epoch': 0.76, 'throughput': 1041.99}
[INFO|2025-10-23 08:09:47] logging.py:143 >> {'loss': 1.0576, 'learning_rate': 1.3866e-07, 'epoch': 0.76, 'throughput': 1042.16}
[INFO|2025-10-23 08:10:41] logging.py:143 >> {'loss': 1.2845, 'learning_rate': 1.3755e-07, 'epoch': 0.76, 'throughput': 1042.32}
[INFO|2025-10-23 08:11:34] logging.py:143 >> {'loss': 1.0975, 'learning_rate': 1.3645e-07, 'epoch': 0.76, 'throughput': 1042.48}
[INFO|2025-10-23 08:12:27] logging.py:143 >> {'loss': 0.9671, 'learning_rate': 1.3535e-07, 'epoch': 0.76, 'throughput': 1042.64}
[INFO|2025-10-23 08:13:21] logging.py:143 >> {'loss': 1.0845, 'learning_rate': 1.3425e-07, 'epoch': 0.76, 'throughput': 1042.80}
[INFO|2025-10-23 08:14:13] logging.py:143 >> {'loss': 1.0492, 'learning_rate': 1.3316e-07, 'epoch': 0.76, 'throughput': 1042.96}
[INFO|2025-10-23 08:15:06] logging.py:143 >> {'loss': 1.0155, 'learning_rate': 1.3208e-07, 'epoch': 0.76, 'throughput': 1043.11}
[INFO|2025-10-23 08:15:59] logging.py:143 >> {'loss': 0.8986, 'learning_rate': 1.3099e-07, 'epoch': 0.76, 'throughput': 1043.27}
[INFO|2025-10-23 08:16:53] logging.py:143 >> {'loss': 1.1063, 'learning_rate': 1.2991e-07, 'epoch': 0.77, 'throughput': 1043.43}
[INFO|2025-10-23 08:17:46] logging.py:143 >> {'loss': 0.9038, 'learning_rate': 1.2884e-07, 'epoch': 0.77, 'throughput': 1043.59}
[INFO|2025-10-23 08:18:39] logging.py:143 >> {'loss': 0.9573, 'learning_rate': 1.2776e-07, 'epoch': 0.77, 'throughput': 1043.75}
[INFO|2025-10-23 08:19:32] logging.py:143 >> {'loss': 0.9827, 'learning_rate': 1.2669e-07, 'epoch': 0.77, 'throughput': 1043.91}
[INFO|2025-10-23 08:20:25] logging.py:143 >> {'loss': 1.0650, 'learning_rate': 1.2563e-07, 'epoch': 0.77, 'throughput': 1044.07}
[INFO|2025-10-23 08:21:18] logging.py:143 >> {'loss': 0.9401, 'learning_rate': 1.2457e-07, 'epoch': 0.77, 'throughput': 1044.23}
[INFO|2025-10-23 08:22:11] logging.py:143 >> {'loss': 0.8401, 'learning_rate': 1.2351e-07, 'epoch': 0.77, 'throughput': 1044.38}
[INFO|2025-10-23 08:23:04] logging.py:143 >> {'loss': 1.2098, 'learning_rate': 1.2246e-07, 'epoch': 0.77, 'throughput': 1044.54}
[INFO|2025-10-23 08:23:57] logging.py:143 >> {'loss': 0.9134, 'learning_rate': 1.2141e-07, 'epoch': 0.77, 'throughput': 1044.70}
[INFO|2025-10-23 08:24:50] logging.py:143 >> {'loss': 1.0075, 'learning_rate': 1.2037e-07, 'epoch': 0.77, 'throughput': 1044.85}
[INFO|2025-10-23 08:25:43] logging.py:143 >> {'loss': 0.9278, 'learning_rate': 1.1932e-07, 'epoch': 0.78, 'throughput': 1045.01}
[INFO|2025-10-23 08:25:43] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 08:25:43] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 08:25:43] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 08:28:02] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3800
[INFO|2025-10-23 08:28:02] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 08:28:02] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 08:28:02] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3800/chat_template.jinja
[INFO|2025-10-23 08:28:02] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3800/tokenizer_config.json
[INFO|2025-10-23 08:28:02] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3800/special_tokens_map.json
[INFO|2025-10-23 08:28:55] logging.py:143 >> {'loss': 1.2151, 'learning_rate': 1.1829e-07, 'epoch': 0.78, 'throughput': 1041.98}
[INFO|2025-10-23 08:29:49] logging.py:143 >> {'loss': 0.9798, 'learning_rate': 1.1725e-07, 'epoch': 0.78, 'throughput': 1042.13}
[INFO|2025-10-23 08:30:43] logging.py:143 >> {'loss': 1.0001, 'learning_rate': 1.1622e-07, 'epoch': 0.78, 'throughput': 1042.30}
[INFO|2025-10-23 08:31:36] logging.py:143 >> {'loss': 1.1055, 'learning_rate': 1.1520e-07, 'epoch': 0.78, 'throughput': 1042.46}
[INFO|2025-10-23 08:32:29] logging.py:143 >> {'loss': 0.9716, 'learning_rate': 1.1418e-07, 'epoch': 0.78, 'throughput': 1042.61}
[INFO|2025-10-23 08:33:21] logging.py:143 >> {'loss': 1.2453, 'learning_rate': 1.1316e-07, 'epoch': 0.78, 'throughput': 1042.76}
[INFO|2025-10-23 08:34:15] logging.py:143 >> {'loss': 0.9750, 'learning_rate': 1.1214e-07, 'epoch': 0.78, 'throughput': 1042.92}
[INFO|2025-10-23 08:35:09] logging.py:143 >> {'loss': 1.1599, 'learning_rate': 1.1113e-07, 'epoch': 0.78, 'throughput': 1043.08}
[INFO|2025-10-23 08:36:03] logging.py:143 >> {'loss': 0.8893, 'learning_rate': 1.1013e-07, 'epoch': 0.78, 'throughput': 1043.24}
[INFO|2025-10-23 08:36:56] logging.py:143 >> {'loss': 0.9367, 'learning_rate': 1.0913e-07, 'epoch': 0.79, 'throughput': 1043.40}
[INFO|2025-10-23 08:37:50] logging.py:143 >> {'loss': 0.9368, 'learning_rate': 1.0813e-07, 'epoch': 0.79, 'throughput': 1043.56}
[INFO|2025-10-23 08:38:43] logging.py:143 >> {'loss': 0.8637, 'learning_rate': 1.0714e-07, 'epoch': 0.79, 'throughput': 1043.72}
[INFO|2025-10-23 08:39:37] logging.py:143 >> {'loss': 0.9084, 'learning_rate': 1.0615e-07, 'epoch': 0.79, 'throughput': 1043.88}
[INFO|2025-10-23 08:40:31] logging.py:143 >> {'loss': 1.0654, 'learning_rate': 1.0516e-07, 'epoch': 0.79, 'throughput': 1044.03}
[INFO|2025-10-23 08:41:25] logging.py:143 >> {'loss': 1.1118, 'learning_rate': 1.0418e-07, 'epoch': 0.79, 'throughput': 1044.19}
[INFO|2025-10-23 08:42:18] logging.py:143 >> {'loss': 1.0732, 'learning_rate': 1.0320e-07, 'epoch': 0.79, 'throughput': 1044.34}
[INFO|2025-10-23 08:43:10] logging.py:143 >> {'loss': 1.0984, 'learning_rate': 1.0223e-07, 'epoch': 0.79, 'throughput': 1044.49}
[INFO|2025-10-23 08:44:03] logging.py:143 >> {'loss': 1.1169, 'learning_rate': 1.0126e-07, 'epoch': 0.79, 'throughput': 1044.64}
[INFO|2025-10-23 08:44:57] logging.py:143 >> {'loss': 1.1358, 'learning_rate': 1.0029e-07, 'epoch': 0.80, 'throughput': 1044.79}
[INFO|2025-10-23 08:45:50] logging.py:143 >> {'loss': 0.9836, 'learning_rate': 9.9332e-08, 'epoch': 0.80, 'throughput': 1044.95}
[INFO|2025-10-23 08:45:50] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 08:45:50] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 08:45:50] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 08:48:09] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3900
[INFO|2025-10-23 08:48:09] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 08:48:09] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 08:48:10] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3900/chat_template.jinja
[INFO|2025-10-23 08:48:10] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3900/tokenizer_config.json
[INFO|2025-10-23 08:48:10] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-3900/special_tokens_map.json
[INFO|2025-10-23 08:49:03] logging.py:143 >> {'loss': 0.8898, 'learning_rate': 9.8375e-08, 'epoch': 0.80, 'throughput': 1042.00}
[INFO|2025-10-23 08:49:56] logging.py:143 >> {'loss': 1.0158, 'learning_rate': 9.7422e-08, 'epoch': 0.80, 'throughput': 1042.15}
[INFO|2025-10-23 08:50:49] logging.py:143 >> {'loss': 1.0525, 'learning_rate': 9.6474e-08, 'epoch': 0.80, 'throughput': 1042.30}
[INFO|2025-10-23 08:51:42] logging.py:143 >> {'loss': 0.8565, 'learning_rate': 9.5529e-08, 'epoch': 0.80, 'throughput': 1042.45}
[INFO|2025-10-23 08:52:35] logging.py:143 >> {'loss': 1.2340, 'learning_rate': 9.4589e-08, 'epoch': 0.80, 'throughput': 1042.61}
[INFO|2025-10-23 08:53:28] logging.py:143 >> {'loss': 0.9955, 'learning_rate': 9.3653e-08, 'epoch': 0.80, 'throughput': 1042.77}
[INFO|2025-10-23 08:54:22] logging.py:143 >> {'loss': 1.0709, 'learning_rate': 9.2720e-08, 'epoch': 0.80, 'throughput': 1042.92}
[INFO|2025-10-23 08:55:15] logging.py:143 >> {'loss': 1.2980, 'learning_rate': 9.1793e-08, 'epoch': 0.80, 'throughput': 1043.07}
[INFO|2025-10-23 08:56:08] logging.py:143 >> {'loss': 1.1855, 'learning_rate': 9.0869e-08, 'epoch': 0.81, 'throughput': 1043.22}
[INFO|2025-10-23 08:57:03] logging.py:143 >> {'loss': 0.9899, 'learning_rate': 8.9949e-08, 'epoch': 0.81, 'throughput': 1043.38}
[INFO|2025-10-23 08:57:55] logging.py:143 >> {'loss': 1.2597, 'learning_rate': 8.9034e-08, 'epoch': 0.81, 'throughput': 1043.53}
[INFO|2025-10-23 08:58:50] logging.py:143 >> {'loss': 1.0636, 'learning_rate': 8.8123e-08, 'epoch': 0.81, 'throughput': 1043.69}
[INFO|2025-10-23 08:59:43] logging.py:143 >> {'loss': 1.1692, 'learning_rate': 8.7216e-08, 'epoch': 0.81, 'throughput': 1043.84}
[INFO|2025-10-23 09:00:36] logging.py:143 >> {'loss': 1.0398, 'learning_rate': 8.6314e-08, 'epoch': 0.81, 'throughput': 1043.99}
[INFO|2025-10-23 09:01:30] logging.py:143 >> {'loss': 0.9601, 'learning_rate': 8.5415e-08, 'epoch': 0.81, 'throughput': 1044.14}
[INFO|2025-10-23 09:02:23] logging.py:143 >> {'loss': 1.1244, 'learning_rate': 8.4521e-08, 'epoch': 0.81, 'throughput': 1044.30}
[INFO|2025-10-23 09:03:17] logging.py:143 >> {'loss': 1.0689, 'learning_rate': 8.3632e-08, 'epoch': 0.81, 'throughput': 1044.45}
[INFO|2025-10-23 09:04:10] logging.py:143 >> {'loss': 0.8659, 'learning_rate': 8.2746e-08, 'epoch': 0.81, 'throughput': 1044.60}
[INFO|2025-10-23 09:05:03] logging.py:143 >> {'loss': 0.8787, 'learning_rate': 8.1865e-08, 'epoch': 0.82, 'throughput': 1044.75}
[INFO|2025-10-23 09:05:56] logging.py:143 >> {'loss': 1.0544, 'learning_rate': 8.0988e-08, 'epoch': 0.82, 'throughput': 1044.89}
[INFO|2025-10-23 09:05:56] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 09:05:56] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 09:05:56] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 09:08:15] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4000
[INFO|2025-10-23 09:08:15] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 09:08:15] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 09:08:15] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4000/chat_template.jinja
[INFO|2025-10-23 09:08:15] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4000/tokenizer_config.json
[INFO|2025-10-23 09:08:15] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4000/special_tokens_map.json
[INFO|2025-10-23 09:09:09] logging.py:143 >> {'loss': 0.9503, 'learning_rate': 8.0115e-08, 'epoch': 0.82, 'throughput': 1042.02}
[INFO|2025-10-23 09:10:02] logging.py:143 >> {'loss': 0.8308, 'learning_rate': 7.9247e-08, 'epoch': 0.82, 'throughput': 1042.17}
[INFO|2025-10-23 09:10:56] logging.py:143 >> {'loss': 1.0342, 'learning_rate': 7.8383e-08, 'epoch': 0.82, 'throughput': 1042.32}
[INFO|2025-10-23 09:11:49] logging.py:143 >> {'loss': 1.0781, 'learning_rate': 7.7524e-08, 'epoch': 0.82, 'throughput': 1042.46}
[INFO|2025-10-23 09:12:42] logging.py:143 >> {'loss': 0.9184, 'learning_rate': 7.6668e-08, 'epoch': 0.82, 'throughput': 1042.61}
[INFO|2025-10-23 09:13:36] logging.py:143 >> {'loss': 0.7909, 'learning_rate': 7.5817e-08, 'epoch': 0.82, 'throughput': 1042.77}
[INFO|2025-10-23 09:14:29] logging.py:143 >> {'loss': 1.1659, 'learning_rate': 7.4971e-08, 'epoch': 0.82, 'throughput': 1042.91}
[INFO|2025-10-23 09:15:23] logging.py:143 >> {'loss': 1.0870, 'learning_rate': 7.4129e-08, 'epoch': 0.82, 'throughput': 1043.06}
[INFO|2025-10-23 09:16:17] logging.py:143 >> {'loss': 0.8830, 'learning_rate': 7.3291e-08, 'epoch': 0.83, 'throughput': 1043.21}
[INFO|2025-10-23 09:17:10] logging.py:143 >> {'loss': 1.0500, 'learning_rate': 7.2457e-08, 'epoch': 0.83, 'throughput': 1043.36}
[INFO|2025-10-23 09:18:03] logging.py:143 >> {'loss': 1.0310, 'learning_rate': 7.1628e-08, 'epoch': 0.83, 'throughput': 1043.51}
[INFO|2025-10-23 09:18:57] logging.py:143 >> {'loss': 1.1313, 'learning_rate': 7.0804e-08, 'epoch': 0.83, 'throughput': 1043.66}
[INFO|2025-10-23 09:19:50] logging.py:143 >> {'loss': 0.8392, 'learning_rate': 6.9984e-08, 'epoch': 0.83, 'throughput': 1043.81}
[INFO|2025-10-23 09:20:43] logging.py:143 >> {'loss': 1.0838, 'learning_rate': 6.9168e-08, 'epoch': 0.83, 'throughput': 1043.96}
[INFO|2025-10-23 09:21:37] logging.py:143 >> {'loss': 0.9149, 'learning_rate': 6.8356e-08, 'epoch': 0.83, 'throughput': 1044.11}
[INFO|2025-10-23 09:22:30] logging.py:143 >> {'loss': 0.9230, 'learning_rate': 6.7550e-08, 'epoch': 0.83, 'throughput': 1044.25}
[INFO|2025-10-23 09:23:24] logging.py:143 >> {'loss': 0.9787, 'learning_rate': 6.6747e-08, 'epoch': 0.83, 'throughput': 1044.41}
[INFO|2025-10-23 09:24:17] logging.py:143 >> {'loss': 1.0186, 'learning_rate': 6.5949e-08, 'epoch': 0.83, 'throughput': 1044.55}
[INFO|2025-10-23 09:25:11] logging.py:143 >> {'loss': 0.8438, 'learning_rate': 6.5155e-08, 'epoch': 0.84, 'throughput': 1044.70}
[INFO|2025-10-23 09:26:03] logging.py:143 >> {'loss': 1.1331, 'learning_rate': 6.4366e-08, 'epoch': 0.84, 'throughput': 1044.84}
[INFO|2025-10-23 09:26:03] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 09:26:03] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 09:26:03] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 09:28:22] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4100
[INFO|2025-10-23 09:28:22] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 09:28:22] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 09:28:22] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4100/chat_template.jinja
[INFO|2025-10-23 09:28:22] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4100/tokenizer_config.json
[INFO|2025-10-23 09:28:22] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4100/special_tokens_map.json
[INFO|2025-10-23 09:29:15] logging.py:143 >> {'loss': 1.0141, 'learning_rate': 6.3582e-08, 'epoch': 0.84, 'throughput': 1042.03}
[INFO|2025-10-23 09:30:08] logging.py:143 >> {'loss': 0.9682, 'learning_rate': 6.2802e-08, 'epoch': 0.84, 'throughput': 1042.18}
[INFO|2025-10-23 09:31:02] logging.py:143 >> {'loss': 0.9544, 'learning_rate': 6.2026e-08, 'epoch': 0.84, 'throughput': 1042.33}
[INFO|2025-10-23 09:31:54] logging.py:143 >> {'loss': 1.0268, 'learning_rate': 6.1255e-08, 'epoch': 0.84, 'throughput': 1042.47}
[INFO|2025-10-23 09:32:48] logging.py:143 >> {'loss': 0.8925, 'learning_rate': 6.0488e-08, 'epoch': 0.84, 'throughput': 1042.62}
[INFO|2025-10-23 09:33:41] logging.py:143 >> {'loss': 1.0881, 'learning_rate': 5.9726e-08, 'epoch': 0.84, 'throughput': 1042.77}
[INFO|2025-10-23 09:34:35] logging.py:143 >> {'loss': 0.8377, 'learning_rate': 5.8969e-08, 'epoch': 0.84, 'throughput': 1042.91}
[INFO|2025-10-23 09:35:28] logging.py:143 >> {'loss': 0.7857, 'learning_rate': 5.8215e-08, 'epoch': 0.85, 'throughput': 1043.06}
[INFO|2025-10-23 09:36:21] logging.py:143 >> {'loss': 1.1787, 'learning_rate': 5.7467e-08, 'epoch': 0.85, 'throughput': 1043.20}
[INFO|2025-10-23 09:37:15] logging.py:143 >> {'loss': 0.8615, 'learning_rate': 5.6723e-08, 'epoch': 0.85, 'throughput': 1043.35}
[INFO|2025-10-23 09:38:08] logging.py:143 >> {'loss': 0.9390, 'learning_rate': 5.5984e-08, 'epoch': 0.85, 'throughput': 1043.50}
[INFO|2025-10-23 09:39:01] logging.py:143 >> {'loss': 0.8891, 'learning_rate': 5.5249e-08, 'epoch': 0.85, 'throughput': 1043.64}
[INFO|2025-10-23 09:39:54] logging.py:143 >> {'loss': 0.9841, 'learning_rate': 5.4519e-08, 'epoch': 0.85, 'throughput': 1043.78}
[INFO|2025-10-23 09:40:47] logging.py:143 >> {'loss': 1.0948, 'learning_rate': 5.3793e-08, 'epoch': 0.85, 'throughput': 1043.92}
[INFO|2025-10-23 09:41:41] logging.py:143 >> {'loss': 0.9658, 'learning_rate': 5.3072e-08, 'epoch': 0.85, 'throughput': 1044.07}
[INFO|2025-10-23 09:42:34] logging.py:143 >> {'loss': 1.2199, 'learning_rate': 5.2355e-08, 'epoch': 0.85, 'throughput': 1044.21}
[INFO|2025-10-23 09:43:27] logging.py:143 >> {'loss': 0.8896, 'learning_rate': 5.1643e-08, 'epoch': 0.85, 'throughput': 1044.34}
[INFO|2025-10-23 09:44:20] logging.py:143 >> {'loss': 0.9396, 'learning_rate': 5.0936e-08, 'epoch': 0.86, 'throughput': 1044.48}
[INFO|2025-10-23 09:45:13] logging.py:143 >> {'loss': 1.0760, 'learning_rate': 5.0233e-08, 'epoch': 0.86, 'throughput': 1044.62}
[INFO|2025-10-23 09:46:06] logging.py:143 >> {'loss': 1.1639, 'learning_rate': 4.9535e-08, 'epoch': 0.86, 'throughput': 1044.77}
[INFO|2025-10-23 09:46:06] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 09:46:06] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 09:46:06] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 09:48:25] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4200
[INFO|2025-10-23 09:48:25] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 09:48:25] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 09:48:25] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4200/chat_template.jinja
[INFO|2025-10-23 09:48:25] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4200/tokenizer_config.json
[INFO|2025-10-23 09:48:25] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4200/special_tokens_map.json
[INFO|2025-10-23 09:49:18] logging.py:143 >> {'loss': 0.9520, 'learning_rate': 4.8842e-08, 'epoch': 0.86, 'throughput': 1042.02}
[INFO|2025-10-23 09:50:11] logging.py:143 >> {'loss': 0.9742, 'learning_rate': 4.8153e-08, 'epoch': 0.86, 'throughput': 1042.17}
[INFO|2025-10-23 09:51:04] logging.py:143 >> {'loss': 1.3084, 'learning_rate': 4.7469e-08, 'epoch': 0.86, 'throughput': 1042.31}
[INFO|2025-10-23 09:51:57] logging.py:143 >> {'loss': 1.0584, 'learning_rate': 4.6790e-08, 'epoch': 0.86, 'throughput': 1042.45}
[INFO|2025-10-23 09:52:51] logging.py:143 >> {'loss': 1.0153, 'learning_rate': 4.6115e-08, 'epoch': 0.86, 'throughput': 1042.59}
[INFO|2025-10-23 09:53:43] logging.py:143 >> {'loss': 0.9515, 'learning_rate': 4.5445e-08, 'epoch': 0.86, 'throughput': 1042.73}
[INFO|2025-10-23 09:54:36] logging.py:143 >> {'loss': 0.9083, 'learning_rate': 4.4779e-08, 'epoch': 0.86, 'throughput': 1042.87}
[INFO|2025-10-23 09:55:29] logging.py:143 >> {'loss': 1.0012, 'learning_rate': 4.4118e-08, 'epoch': 0.87, 'throughput': 1043.02}
[INFO|2025-10-23 09:56:23] logging.py:143 >> {'loss': 1.0037, 'learning_rate': 4.3462e-08, 'epoch': 0.87, 'throughput': 1043.16}
[INFO|2025-10-23 09:57:16] logging.py:143 >> {'loss': 1.0205, 'learning_rate': 4.2811e-08, 'epoch': 0.87, 'throughput': 1043.30}
[INFO|2025-10-23 09:58:09] logging.py:143 >> {'loss': 0.9827, 'learning_rate': 4.2164e-08, 'epoch': 0.87, 'throughput': 1043.44}
[INFO|2025-10-23 09:59:02] logging.py:143 >> {'loss': 0.9798, 'learning_rate': 4.1522e-08, 'epoch': 0.87, 'throughput': 1043.58}
[INFO|2025-10-23 09:59:56] logging.py:143 >> {'loss': 0.9674, 'learning_rate': 4.0885e-08, 'epoch': 0.87, 'throughput': 1043.72}
[INFO|2025-10-23 10:00:49] logging.py:143 >> {'loss': 0.8957, 'learning_rate': 4.0252e-08, 'epoch': 0.87, 'throughput': 1043.86}
[INFO|2025-10-23 10:01:43] logging.py:143 >> {'loss': 0.9031, 'learning_rate': 3.9624e-08, 'epoch': 0.87, 'throughput': 1044.01}
[INFO|2025-10-23 10:02:36] logging.py:143 >> {'loss': 1.0621, 'learning_rate': 3.9001e-08, 'epoch': 0.87, 'throughput': 1044.14}
[INFO|2025-10-23 10:03:29] logging.py:143 >> {'loss': 0.9313, 'learning_rate': 3.8383e-08, 'epoch': 0.87, 'throughput': 1044.28}
[INFO|2025-10-23 10:04:21] logging.py:143 >> {'loss': 0.7740, 'learning_rate': 3.7769e-08, 'epoch': 0.88, 'throughput': 1044.42}
[INFO|2025-10-23 10:05:15] logging.py:143 >> {'loss': 0.8576, 'learning_rate': 3.7160e-08, 'epoch': 0.88, 'throughput': 1044.56}
[INFO|2025-10-23 10:06:09] logging.py:143 >> {'loss': 0.9071, 'learning_rate': 3.6556e-08, 'epoch': 0.88, 'throughput': 1044.70}
[INFO|2025-10-23 10:06:09] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 10:06:09] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 10:06:09] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 10:08:28] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4300
[INFO|2025-10-23 10:08:28] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 10:08:28] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 10:08:28] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4300/chat_template.jinja
[INFO|2025-10-23 10:08:28] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4300/tokenizer_config.json
[INFO|2025-10-23 10:08:28] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4300/special_tokens_map.json
[INFO|2025-10-23 10:09:21] logging.py:143 >> {'loss': 1.1058, 'learning_rate': 3.5957e-08, 'epoch': 0.88, 'throughput': 1042.03}
[INFO|2025-10-23 10:10:14] logging.py:143 >> {'loss': 1.1434, 'learning_rate': 3.5362e-08, 'epoch': 0.88, 'throughput': 1042.17}
[INFO|2025-10-23 10:11:08] logging.py:143 >> {'loss': 1.1847, 'learning_rate': 3.4773e-08, 'epoch': 0.88, 'throughput': 1042.31}
[INFO|2025-10-23 10:12:02] logging.py:143 >> {'loss': 0.9636, 'learning_rate': 3.4188e-08, 'epoch': 0.88, 'throughput': 1042.46}
[INFO|2025-10-23 10:12:56] logging.py:143 >> {'loss': 1.1080, 'learning_rate': 3.3607e-08, 'epoch': 0.88, 'throughput': 1042.60}
[INFO|2025-10-23 10:13:49] logging.py:143 >> {'loss': 1.2645, 'learning_rate': 3.3032e-08, 'epoch': 0.88, 'throughput': 1042.74}
[INFO|2025-10-23 10:14:43] logging.py:143 >> {'loss': 1.0714, 'learning_rate': 3.2461e-08, 'epoch': 0.88, 'throughput': 1042.87}
[INFO|2025-10-23 10:15:36] logging.py:143 >> {'loss': 1.0703, 'learning_rate': 3.1895e-08, 'epoch': 0.89, 'throughput': 1043.01}
[INFO|2025-10-23 10:16:29] logging.py:143 >> {'loss': 0.9577, 'learning_rate': 3.1334e-08, 'epoch': 0.89, 'throughput': 1043.15}
[INFO|2025-10-23 10:17:22] logging.py:143 >> {'loss': 0.9116, 'learning_rate': 3.0778e-08, 'epoch': 0.89, 'throughput': 1043.29}
[INFO|2025-10-23 10:18:15] logging.py:143 >> {'loss': 1.0322, 'learning_rate': 3.0227e-08, 'epoch': 0.89, 'throughput': 1043.43}
[INFO|2025-10-23 10:19:09] logging.py:143 >> {'loss': 0.8579, 'learning_rate': 2.9680e-08, 'epoch': 0.89, 'throughput': 1043.57}
[INFO|2025-10-23 10:20:02] logging.py:143 >> {'loss': 1.0460, 'learning_rate': 2.9139e-08, 'epoch': 0.89, 'throughput': 1043.70}
[INFO|2025-10-23 10:20:55] logging.py:143 >> {'loss': 0.8418, 'learning_rate': 2.8602e-08, 'epoch': 0.89, 'throughput': 1043.84}
[INFO|2025-10-23 10:21:49] logging.py:143 >> {'loss': 0.9611, 'learning_rate': 2.8070e-08, 'epoch': 0.89, 'throughput': 1043.98}
[INFO|2025-10-23 10:22:41] logging.py:143 >> {'loss': 1.0987, 'learning_rate': 2.7543e-08, 'epoch': 0.89, 'throughput': 1044.11}
[INFO|2025-10-23 10:23:35] logging.py:143 >> {'loss': 1.0925, 'learning_rate': 2.7020e-08, 'epoch': 0.90, 'throughput': 1044.25}
[INFO|2025-10-23 10:24:28] logging.py:143 >> {'loss': 1.0976, 'learning_rate': 2.6503e-08, 'epoch': 0.90, 'throughput': 1044.39}
[INFO|2025-10-23 10:25:21] logging.py:143 >> {'loss': 1.0794, 'learning_rate': 2.5990e-08, 'epoch': 0.90, 'throughput': 1044.52}
[INFO|2025-10-23 10:26:14] logging.py:143 >> {'loss': 1.0794, 'learning_rate': 2.5482e-08, 'epoch': 0.90, 'throughput': 1044.66}
[INFO|2025-10-23 10:26:14] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 10:26:14] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 10:26:14] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 10:28:33] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4400
[INFO|2025-10-23 10:28:33] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 10:28:33] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 10:28:33] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4400/chat_template.jinja
[INFO|2025-10-23 10:28:33] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4400/tokenizer_config.json
[INFO|2025-10-23 10:28:33] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4400/special_tokens_map.json
[INFO|2025-10-23 10:29:26] logging.py:143 >> {'loss': 0.9571, 'learning_rate': 2.4980e-08, 'epoch': 0.90, 'throughput': 1042.04}
[INFO|2025-10-23 10:30:18] logging.py:143 >> {'loss': 1.1220, 'learning_rate': 2.4482e-08, 'epoch': 0.90, 'throughput': 1042.17}
[INFO|2025-10-23 10:31:13] logging.py:143 >> {'loss': 0.9177, 'learning_rate': 2.3989e-08, 'epoch': 0.90, 'throughput': 1042.31}
[INFO|2025-10-23 10:32:06] logging.py:143 >> {'loss': 0.9969, 'learning_rate': 2.3500e-08, 'epoch': 0.90, 'throughput': 1042.45}
[INFO|2025-10-23 10:32:59] logging.py:143 >> {'loss': 0.8875, 'learning_rate': 2.3017e-08, 'epoch': 0.90, 'throughput': 1042.59}
[INFO|2025-10-23 10:33:52] logging.py:143 >> {'loss': 0.9381, 'learning_rate': 2.2539e-08, 'epoch': 0.90, 'throughput': 1042.72}
[INFO|2025-10-23 10:34:45] logging.py:143 >> {'loss': 1.1037, 'learning_rate': 2.2065e-08, 'epoch': 0.91, 'throughput': 1042.86}
[INFO|2025-10-23 10:35:38] logging.py:143 >> {'loss': 1.0689, 'learning_rate': 2.1597e-08, 'epoch': 0.91, 'throughput': 1042.99}
[INFO|2025-10-23 10:36:31] logging.py:143 >> {'loss': 0.9558, 'learning_rate': 2.1133e-08, 'epoch': 0.91, 'throughput': 1043.13}
[INFO|2025-10-23 10:37:25] logging.py:143 >> {'loss': 0.9375, 'learning_rate': 2.0674e-08, 'epoch': 0.91, 'throughput': 1043.27}
[INFO|2025-10-23 10:38:18] logging.py:143 >> {'loss': 1.0008, 'learning_rate': 2.0221e-08, 'epoch': 0.91, 'throughput': 1043.40}
[INFO|2025-10-23 10:39:12] logging.py:143 >> {'loss': 1.1662, 'learning_rate': 1.9772e-08, 'epoch': 0.91, 'throughput': 1043.54}
[INFO|2025-10-23 10:40:05] logging.py:143 >> {'loss': 0.8870, 'learning_rate': 1.9328e-08, 'epoch': 0.91, 'throughput': 1043.66}
[INFO|2025-10-23 10:40:58] logging.py:143 >> {'loss': 1.0881, 'learning_rate': 1.8889e-08, 'epoch': 0.91, 'throughput': 1043.79}
[INFO|2025-10-23 10:41:51] logging.py:143 >> {'loss': 1.0845, 'learning_rate': 1.8455e-08, 'epoch': 0.91, 'throughput': 1043.93}
[INFO|2025-10-23 10:42:43] logging.py:143 >> {'loss': 1.1006, 'learning_rate': 1.8026e-08, 'epoch': 0.91, 'throughput': 1044.06}
[INFO|2025-10-23 10:43:37] logging.py:143 >> {'loss': 0.9715, 'learning_rate': 1.7602e-08, 'epoch': 0.92, 'throughput': 1044.19}
[INFO|2025-10-23 10:44:29] logging.py:143 >> {'loss': 0.8519, 'learning_rate': 1.7183e-08, 'epoch': 0.92, 'throughput': 1044.33}
[INFO|2025-10-23 10:45:23] logging.py:143 >> {'loss': 0.9293, 'learning_rate': 1.6768e-08, 'epoch': 0.92, 'throughput': 1044.46}
[INFO|2025-10-23 10:46:16] logging.py:143 >> {'loss': 0.9604, 'learning_rate': 1.6359e-08, 'epoch': 0.92, 'throughput': 1044.59}
[INFO|2025-10-23 10:46:16] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 10:46:16] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 10:46:16] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 10:48:35] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4500
[INFO|2025-10-23 10:48:35] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 10:48:35] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 10:48:35] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4500/chat_template.jinja
[INFO|2025-10-23 10:48:35] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4500/tokenizer_config.json
[INFO|2025-10-23 10:48:35] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4500/special_tokens_map.json
[INFO|2025-10-23 10:49:28] logging.py:143 >> {'loss': 1.2604, 'learning_rate': 1.5955e-08, 'epoch': 0.92, 'throughput': 1042.03}
[INFO|2025-10-23 10:50:21] logging.py:143 >> {'loss': 0.9162, 'learning_rate': 1.5556e-08, 'epoch': 0.92, 'throughput': 1042.16}
[INFO|2025-10-23 10:51:14] logging.py:143 >> {'loss': 0.8271, 'learning_rate': 1.5161e-08, 'epoch': 0.92, 'throughput': 1042.29}
[INFO|2025-10-23 10:52:07] logging.py:143 >> {'loss': 0.7901, 'learning_rate': 1.4772e-08, 'epoch': 0.92, 'throughput': 1042.42}
[INFO|2025-10-23 10:53:01] logging.py:143 >> {'loss': 1.1223, 'learning_rate': 1.4388e-08, 'epoch': 0.92, 'throughput': 1042.55}
[INFO|2025-10-23 10:53:54] logging.py:143 >> {'loss': 0.9948, 'learning_rate': 1.4008e-08, 'epoch': 0.92, 'throughput': 1042.68}
[INFO|2025-10-23 10:54:47] logging.py:143 >> {'loss': 0.8932, 'learning_rate': 1.3634e-08, 'epoch': 0.93, 'throughput': 1042.82}
[INFO|2025-10-23 10:55:40] logging.py:143 >> {'loss': 1.1287, 'learning_rate': 1.3265e-08, 'epoch': 0.93, 'throughput': 1042.94}
[INFO|2025-10-23 10:56:34] logging.py:143 >> {'loss': 0.9760, 'learning_rate': 1.2900e-08, 'epoch': 0.93, 'throughput': 1043.08}
[INFO|2025-10-23 10:57:27] logging.py:143 >> {'loss': 1.3845, 'learning_rate': 1.2541e-08, 'epoch': 0.93, 'throughput': 1043.21}
[INFO|2025-10-23 10:58:20] logging.py:143 >> {'loss': 1.3360, 'learning_rate': 1.2187e-08, 'epoch': 0.93, 'throughput': 1043.34}
[INFO|2025-10-23 10:59:13] logging.py:143 >> {'loss': 1.1166, 'learning_rate': 1.1838e-08, 'epoch': 0.93, 'throughput': 1043.47}
[INFO|2025-10-23 11:00:07] logging.py:143 >> {'loss': 0.8422, 'learning_rate': 1.1493e-08, 'epoch': 0.93, 'throughput': 1043.61}
[INFO|2025-10-23 11:01:00] logging.py:143 >> {'loss': 1.1253, 'learning_rate': 1.1154e-08, 'epoch': 0.93, 'throughput': 1043.74}
[INFO|2025-10-23 11:01:54] logging.py:143 >> {'loss': 0.9982, 'learning_rate': 1.0820e-08, 'epoch': 0.93, 'throughput': 1043.87}
[INFO|2025-10-23 11:02:46] logging.py:143 >> {'loss': 1.0309, 'learning_rate': 1.0491e-08, 'epoch': 0.93, 'throughput': 1044.00}
[INFO|2025-10-23 11:03:39] logging.py:143 >> {'loss': 1.0694, 'learning_rate': 1.0166e-08, 'epoch': 0.94, 'throughput': 1044.13}
[INFO|2025-10-23 11:04:32] logging.py:143 >> {'loss': 1.0605, 'learning_rate': 9.8473e-09, 'epoch': 0.94, 'throughput': 1044.26}
[INFO|2025-10-23 11:05:25] logging.py:143 >> {'loss': 1.0953, 'learning_rate': 9.5332e-09, 'epoch': 0.94, 'throughput': 1044.39}
[INFO|2025-10-23 11:06:19] logging.py:143 >> {'loss': 1.0676, 'learning_rate': 9.2242e-09, 'epoch': 0.94, 'throughput': 1044.52}
[INFO|2025-10-23 11:06:19] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 11:06:19] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 11:06:19] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 11:08:38] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4600
[INFO|2025-10-23 11:08:38] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 11:08:38] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 11:08:39] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4600/chat_template.jinja
[INFO|2025-10-23 11:08:39] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4600/tokenizer_config.json
[INFO|2025-10-23 11:08:39] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4600/special_tokens_map.json
[INFO|2025-10-23 11:09:32] logging.py:143 >> {'loss': 1.0684, 'learning_rate': 8.9202e-09, 'epoch': 0.94, 'throughput': 1042.01}
[INFO|2025-10-23 11:10:24] logging.py:143 >> {'loss': 1.0972, 'learning_rate': 8.6212e-09, 'epoch': 0.94, 'throughput': 1042.14}
[INFO|2025-10-23 11:11:17] logging.py:143 >> {'loss': 1.0785, 'learning_rate': 8.3273e-09, 'epoch': 0.94, 'throughput': 1042.27}
[INFO|2025-10-23 11:12:11] logging.py:143 >> {'loss': 1.0726, 'learning_rate': 8.0385e-09, 'epoch': 0.94, 'throughput': 1042.40}
[INFO|2025-10-23 11:13:04] logging.py:143 >> {'loss': 1.1243, 'learning_rate': 7.7547e-09, 'epoch': 0.94, 'throughput': 1042.53}
[INFO|2025-10-23 11:13:58] logging.py:143 >> {'loss': 1.0202, 'learning_rate': 7.4760e-09, 'epoch': 0.95, 'throughput': 1042.66}
[INFO|2025-10-23 11:14:52] logging.py:143 >> {'loss': 1.0411, 'learning_rate': 7.2023e-09, 'epoch': 0.95, 'throughput': 1042.80}
[INFO|2025-10-23 11:15:46] logging.py:143 >> {'loss': 1.1280, 'learning_rate': 6.9337e-09, 'epoch': 0.95, 'throughput': 1042.92}
[INFO|2025-10-23 11:16:39] logging.py:143 >> {'loss': 0.7653, 'learning_rate': 6.6702e-09, 'epoch': 0.95, 'throughput': 1043.05}
[INFO|2025-10-23 11:17:31] logging.py:143 >> {'loss': 0.9772, 'learning_rate': 6.4117e-09, 'epoch': 0.95, 'throughput': 1043.17}
[INFO|2025-10-23 11:18:25] logging.py:143 >> {'loss': 0.9308, 'learning_rate': 6.1583e-09, 'epoch': 0.95, 'throughput': 1043.31}
[INFO|2025-10-23 11:19:17] logging.py:143 >> {'loss': 1.1051, 'learning_rate': 5.9100e-09, 'epoch': 0.95, 'throughput': 1043.43}
[INFO|2025-10-23 11:20:11] logging.py:143 >> {'loss': 0.9507, 'learning_rate': 5.6668e-09, 'epoch': 0.95, 'throughput': 1043.56}
[INFO|2025-10-23 11:21:03] logging.py:143 >> {'loss': 1.0559, 'learning_rate': 5.4287e-09, 'epoch': 0.95, 'throughput': 1043.68}
[INFO|2025-10-23 11:21:56] logging.py:143 >> {'loss': 1.2610, 'learning_rate': 5.1956e-09, 'epoch': 0.95, 'throughput': 1043.81}
[INFO|2025-10-23 11:22:49] logging.py:143 >> {'loss': 1.0625, 'learning_rate': 4.9676e-09, 'epoch': 0.96, 'throughput': 1043.94}
[INFO|2025-10-23 11:23:43] logging.py:143 >> {'loss': 1.0291, 'learning_rate': 4.7448e-09, 'epoch': 0.96, 'throughput': 1044.07}
[INFO|2025-10-23 11:24:36] logging.py:143 >> {'loss': 1.0323, 'learning_rate': 4.5270e-09, 'epoch': 0.96, 'throughput': 1044.19}
[INFO|2025-10-23 11:25:30] logging.py:143 >> {'loss': 1.1179, 'learning_rate': 4.3143e-09, 'epoch': 0.96, 'throughput': 1044.32}
[INFO|2025-10-23 11:26:23] logging.py:143 >> {'loss': 0.9799, 'learning_rate': 4.1067e-09, 'epoch': 0.96, 'throughput': 1044.45}
[INFO|2025-10-23 11:26:23] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 11:26:23] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 11:26:23] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 11:28:42] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4700
[INFO|2025-10-23 11:28:42] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 11:28:42] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 11:28:42] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4700/chat_template.jinja
[INFO|2025-10-23 11:28:42] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4700/tokenizer_config.json
[INFO|2025-10-23 11:28:42] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4700/special_tokens_map.json
[INFO|2025-10-23 11:29:36] logging.py:143 >> {'loss': 1.0536, 'learning_rate': 3.9042e-09, 'epoch': 0.96, 'throughput': 1042.00}
[INFO|2025-10-23 11:30:30] logging.py:143 >> {'loss': 0.7904, 'learning_rate': 3.7068e-09, 'epoch': 0.96, 'throughput': 1042.13}
[INFO|2025-10-23 11:31:23] logging.py:143 >> {'loss': 0.9551, 'learning_rate': 3.5145e-09, 'epoch': 0.96, 'throughput': 1042.26}
[INFO|2025-10-23 11:32:16] logging.py:143 >> {'loss': 0.8132, 'learning_rate': 3.3273e-09, 'epoch': 0.96, 'throughput': 1042.39}
[INFO|2025-10-23 11:33:08] logging.py:143 >> {'loss': 1.1277, 'learning_rate': 3.1452e-09, 'epoch': 0.96, 'throughput': 1042.51}
[INFO|2025-10-23 11:34:02] logging.py:143 >> {'loss': 0.9304, 'learning_rate': 2.9682e-09, 'epoch': 0.97, 'throughput': 1042.64}
[INFO|2025-10-23 11:34:54] logging.py:143 >> {'loss': 1.1203, 'learning_rate': 2.7963e-09, 'epoch': 0.97, 'throughput': 1042.76}
[INFO|2025-10-23 11:35:48] logging.py:143 >> {'loss': 1.0736, 'learning_rate': 2.6296e-09, 'epoch': 0.97, 'throughput': 1042.89}
[INFO|2025-10-23 11:36:42] logging.py:143 >> {'loss': 0.8430, 'learning_rate': 2.4679e-09, 'epoch': 0.97, 'throughput': 1043.02}
[INFO|2025-10-23 11:37:36] logging.py:143 >> {'loss': 1.1394, 'learning_rate': 2.3114e-09, 'epoch': 0.97, 'throughput': 1043.15}
[INFO|2025-10-23 11:38:28] logging.py:143 >> {'loss': 1.2121, 'learning_rate': 2.1600e-09, 'epoch': 0.97, 'throughput': 1043.28}
[INFO|2025-10-23 11:39:22] logging.py:143 >> {'loss': 1.1096, 'learning_rate': 2.0137e-09, 'epoch': 0.97, 'throughput': 1043.40}
[INFO|2025-10-23 11:40:15] logging.py:143 >> {'loss': 0.9238, 'learning_rate': 1.8725e-09, 'epoch': 0.97, 'throughput': 1043.53}
[INFO|2025-10-23 11:41:08] logging.py:143 >> {'loss': 0.9188, 'learning_rate': 1.7364e-09, 'epoch': 0.97, 'throughput': 1043.65}
[INFO|2025-10-23 11:42:02] logging.py:143 >> {'loss': 0.7400, 'learning_rate': 1.6055e-09, 'epoch': 0.97, 'throughput': 1043.78}
[INFO|2025-10-23 11:42:56] logging.py:143 >> {'loss': 0.9804, 'learning_rate': 1.4797e-09, 'epoch': 0.98, 'throughput': 1043.91}
[INFO|2025-10-23 11:43:49] logging.py:143 >> {'loss': 0.8645, 'learning_rate': 1.3590e-09, 'epoch': 0.98, 'throughput': 1044.04}
[INFO|2025-10-23 11:44:43] logging.py:143 >> {'loss': 0.8378, 'learning_rate': 1.2435e-09, 'epoch': 0.98, 'throughput': 1044.17}
[INFO|2025-10-23 11:45:36] logging.py:143 >> {'loss': 0.9230, 'learning_rate': 1.1330e-09, 'epoch': 0.98, 'throughput': 1044.29}
[INFO|2025-10-23 11:46:30] logging.py:143 >> {'loss': 1.0384, 'learning_rate': 1.0277e-09, 'epoch': 0.98, 'throughput': 1044.41}
[INFO|2025-10-23 11:46:30] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 11:46:30] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 11:46:30] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 11:48:49] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4800
[INFO|2025-10-23 11:48:49] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 11:48:49] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 11:48:49] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4800/chat_template.jinja
[INFO|2025-10-23 11:48:49] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4800/tokenizer_config.json
[INFO|2025-10-23 11:48:49] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4800/special_tokens_map.json
[INFO|2025-10-23 11:49:43] logging.py:143 >> {'loss': 1.0178, 'learning_rate': 9.2755e-10, 'epoch': 0.98, 'throughput': 1042.02}
[INFO|2025-10-23 11:50:36] logging.py:143 >> {'loss': 0.8107, 'learning_rate': 8.3251e-10, 'epoch': 0.98, 'throughput': 1042.14}
[INFO|2025-10-23 11:51:29] logging.py:143 >> {'loss': 0.9960, 'learning_rate': 7.4260e-10, 'epoch': 0.98, 'throughput': 1042.27}
[INFO|2025-10-23 11:52:23] logging.py:143 >> {'loss': 0.8774, 'learning_rate': 6.5782e-10, 'epoch': 0.98, 'throughput': 1042.40}
[INFO|2025-10-23 11:53:16] logging.py:143 >> {'loss': 1.0924, 'learning_rate': 5.7818e-10, 'epoch': 0.98, 'throughput': 1042.52}
[INFO|2025-10-23 11:54:10] logging.py:143 >> {'loss': 0.8397, 'learning_rate': 5.0367e-10, 'epoch': 0.99, 'throughput': 1042.64}
[INFO|2025-10-23 11:55:03] logging.py:143 >> {'loss': 1.2020, 'learning_rate': 4.3430e-10, 'epoch': 0.99, 'throughput': 1042.77}
[INFO|2025-10-23 11:55:56] logging.py:143 >> {'loss': 1.0737, 'learning_rate': 3.7006e-10, 'epoch': 0.99, 'throughput': 1042.89}
[INFO|2025-10-23 11:56:50] logging.py:143 >> {'loss': 1.0668, 'learning_rate': 3.1096e-10, 'epoch': 0.99, 'throughput': 1043.01}
[INFO|2025-10-23 11:57:43] logging.py:143 >> {'loss': 0.9703, 'learning_rate': 2.5700e-10, 'epoch': 0.99, 'throughput': 1043.13}
[INFO|2025-10-23 11:58:36] logging.py:143 >> {'loss': 1.0885, 'learning_rate': 2.0817e-10, 'epoch': 0.99, 'throughput': 1043.26}
[INFO|2025-10-23 11:59:28] logging.py:143 >> {'loss': 0.9087, 'learning_rate': 1.6448e-10, 'epoch': 0.99, 'throughput': 1043.37}
[INFO|2025-10-23 12:00:21] logging.py:143 >> {'loss': 1.1078, 'learning_rate': 1.2593e-10, 'epoch': 0.99, 'throughput': 1043.49}
[INFO|2025-10-23 12:01:14] logging.py:143 >> {'loss': 1.0362, 'learning_rate': 9.2524e-11, 'epoch': 0.99, 'throughput': 1043.61}
[INFO|2025-10-23 12:02:07] logging.py:143 >> {'loss': 1.0611, 'learning_rate': 6.4253e-11, 'epoch': 1.00, 'throughput': 1043.74}
[INFO|2025-10-23 12:03:00] logging.py:143 >> {'loss': 1.0930, 'learning_rate': 4.1122e-11, 'epoch': 1.00, 'throughput': 1043.86}
[INFO|2025-10-23 12:03:53] logging.py:143 >> {'loss': 0.9885, 'learning_rate': 2.3132e-11, 'epoch': 1.00, 'throughput': 1043.97}
[INFO|2025-10-23 12:04:47] logging.py:143 >> {'loss': 1.0136, 'learning_rate': 1.0281e-11, 'epoch': 1.00, 'throughput': 1044.10}
[INFO|2025-10-23 12:05:41] logging.py:143 >> {'loss': 0.9577, 'learning_rate': 2.5702e-12, 'epoch': 1.00, 'throughput': 1044.22}
[INFO|2025-10-23 12:06:18] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4899
[INFO|2025-10-23 12:06:18] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 12:06:18] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 12:06:18] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4899/chat_template.jinja
[INFO|2025-10-23 12:06:18] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4899/tokenizer_config.json
[INFO|2025-10-23 12:06:18] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/checkpoint-4899/special_tokens_map.json
[INFO|2025-10-23 12:06:19] trainer.py:2810 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|2025-10-23 12:06:19] trainer.py:4309 >> Saving model checkpoint to saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6
[INFO|2025-10-23 12:06:19] configuration_utils.py:763 >> loading configuration file /media/data/users/liqz/Qwen/Qwen3-8B-AWQ/config.json
[INFO|2025-10-23 12:06:19] configuration_utils.py:839 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "float16",
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "quantization_config": {
    "backend": "autoawq",
    "bits": 4,
    "do_fuse": false,
    "exllama_config": null,
    "fuse_max_seq_len": null,
    "group_size": 128,
    "modules_to_fuse": null,
    "modules_to_not_convert": null,
    "quant_method": "awq",
    "version": "gemm",
    "zero_point": true
  },
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-10-23 12:06:19] tokenization_utils_base.py:2421 >> chat template saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/chat_template.jinja
[INFO|2025-10-23 12:06:19] tokenization_utils_base.py:2590 >> tokenizer config file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/tokenizer_config.json
[INFO|2025-10-23 12:06:19] tokenization_utils_base.py:2599 >> Special tokens file saved in saves/Qwen3-8B-Thinking-AWQ/lora/train_lr_1e-6/special_tokens_map.json
[WARNING|2025-10-23 12:06:19] logging.py:148 >> No metric eval_accuracy to plot.
[INFO|2025-10-23 12:06:19] trainer.py:4643 >> 
***** Running Evaluation *****
[INFO|2025-10-23 12:06:19] trainer.py:4645 >>   Num examples = 792
[INFO|2025-10-23 12:06:19] trainer.py:4648 >>   Batch size = 2
[INFO|2025-10-23 12:08:38] modelcard.py:456 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
